Using statistical security parameter 40
No modulus found in /Player-Data//3-Rp-128/Params-Data, generating 128-bit prime
Current working directory: "/"
Current working directory: "/"
Using SGD
done with epoch 0done with epoch 1done with epoch 2done with epoch 3done with epoch 4done with epoch 5done with epoch 6done with epoch 7done with epoch 8done with epoch 9done with epoch 10done with epoch 11done with epoch 12done with epoch 13done with epoch 14done with epoch 15done with epoch 16done with epoch 17done with epoch 18done with epoch 19
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Using SGD
loss in batch 0: 0.776459/0.776459loss in batch 1: 0.662827/0.719635loss in batch 2: 0.598419/0.679245loss in batch 3: 0.706177/0.685974loss in batch 4: 0.636108/0.67601loss in batch 5: 0.594299/0.662384loss in batch 6: 0.55072/0.646423loss in batch 7: 0.585098/0.638763loss in batch 8: 0.560364/0.630051loss in batch 9: 0.553909/0.622437loss in batch 10: 0.54895/0.615768loss in batch 11: 0.674713/0.620667loss in batch 12: 0.702881/0.626999loss in batch 13: 0.519455/0.619308loss in batch 14: 0.729019/0.626633loss in batch 15: 0.665421/0.629044loss in batch 16: 0.396698/0.615387loss in batch 17: 0.394119/0.603104loss in batch 18: 0.80751/0.613846loss in batch 19: 0.3815/0.602234loss in batch 20: 0.33931/0.589706loss in batch 21: 0.693161/0.594421loss in batch 22: 0.321106/0.582535loss in batch 23: 0.319122/0.571548loss in batch 24: 0.288544/0.560226loss in batch 25: 0.826569/0.57048loss in batch 26: 0.762833/0.577606loss in batch 27: 0.847336/0.587234loss in batch 28: 0.945435/0.599594loss in batch 29: 1.4576/0.628189loss in batch 30: 0.90477/0.6371loss in batch 31: 1.23222/0.655701loss in batch 32: 0.799866/0.660065loss in batch 33: 0.714478/0.661682loss in batch 34: 0.449524/0.655609loss in batch 35: 0.728561/0.657654loss in batch 36: 0.688141/0.658463loss in batch 37: 0.400131/0.651672loss in batch 38: 0.667664/0.652069loss in batch 39: 0.542206/0.649323loss in batch 40: 0.68277/0.650146loss in batch 41: 0.528198/0.647232loss in batch 42: 0.601501/0.646179loss in batch 43: 0.695129/0.647293loss in batch 44: 0.619431/0.646667loss in batch 45: 0.604889/0.645767loss in batch 46: 0.642578/0.645706loss in batch 47: 0.668564/0.646164loss in batch 48: 0.506302/0.643326loss in batch 49: 0.633987/0.643127loss in batch 50: 0.739502/0.64502loss in batch 51: 0.580231/0.643768loss in batch 52: 0.557404/0.642151loss in batch 53: 0.411041/0.637863loss in batch 54: 0.474503/0.634903loss in batch 55: 0.633011/0.634857loss in batch 56: 0.533661/0.633087loss in batch 57: 0.625748/0.63295loss in batch 58: 0.67775/0.633713loss in batch 59: 0.608612/0.633301loss in batch 60: 0.494232/0.631027loss in batch 61: 0.639923/0.631165loss in batch 62: 0.533905/0.629623loss in batch 63: 0.614807/0.629379loss in batch 64: 0.44046/0.62648loss in batch 65: 0.609482/0.626221loss in batch 66: 0.49881/0.624313loss in batch 67: 0.615524/0.624191loss in batch 68: 0.435913/0.62146loss in batch 69: 0.69635/0.622528loss in batch 70: 0.471375/0.620407loss in batch 71: 0.601151/0.620132loss in batch 72: 0.485077/0.618286loss in batch 73: 0.523666/0.617004loss in batch 74: 0.390701/0.613983loss in batch 75: 0.589096/0.613663loss in batch 76: 0.667542/0.614365loss in batch 77: 0.466263/0.612473loss in batch 78: 0.551117/0.611694loss in batch 79: 0.565247/0.611115loss in batch 80: 0.673904/0.611877loss in batch 81: 0.496246/0.610474loss in batch 82: 0.580383/0.610107loss in batch 83: 0.502045/0.608826loss in batch 84: 0.468323/0.607178loss in batch 85: 0.591003/0.606979loss in batch 86: 0.469818/0.605408loss in batch 87: 0.524994/0.604492loss in batch 88: 0.39679/0.602158loss in batch 89: 0.490021/0.600922loss in batch 90: 0.411575/0.598846loss in batch 91: 0.550797/0.598312loss in batch 92: 0.539536/0.597672loss in batch 93: 0.487427/0.596512loss in batch 94: 0.543457/0.595947loss in batch 95: 0.359055/0.593475loss in batch 96: 0.393707/0.591415loss in batch 97: 0.398972/0.589447loss in batch 98: 0.492966/0.588486loss in batch 99: 0.47554/0.587357loss in batch 100: 0.680145/0.588272loss in batch 101: 0.852203/0.590866loss in batch 102: 0.539566/0.590363loss in batch 103: 0.523209/0.589706loss in batch 104: 0.650986/0.590286loss in batch 105: 0.568268/0.590103loss in batch 106: 0.620544/0.590378loss in batch 107: 0.575729/0.59024loss in batch 108: 0.583252/0.590179loss in batch 109: 0.396759/0.588425loss in batch 110: 0.561401/0.588181loss in batch 111: 0.353378/0.586075loss in batch 112: 0.502701/0.585342loss in batch 113: 0.576813/0.585251loss in batch 114: 0.467239/0.584244loss in batch 115: 0.354187/0.582245loss in batch 116: 0.344284/0.580215loss in batch 117: 0.501038/0.579559loss in batch 118: 0.398422/0.578033loss in batch 119: 0.637833/0.578522loss in batch 120: 0.570541/0.578461loss in batch 121: 0.882614/0.580963loss in batch 122: 0.611252/0.581192loss in batch 123: 0.320816/0.579102loss in batch 124: 0.889053/0.581573loss in batch 125: 0.722717/0.582687loss in batch 126: 0.284103/0.580338loss in batch 127: 0.356628/0.578598loss in batch 128: 0.463409/0.577698loss in batch 129: 0.410538/0.576416loss in batch 130: 0.390884/0.575012loss in batch 131: 0.382767/0.573547loss in batch 132: 0.497772/0.572968loss in batch 133: 0.501755/0.572449loss in batch 134: 0.632645/0.572891loss in batch 135: 0.49942/0.572357loss in batch 136: 0.395493/0.571075loss in batch 137: 0.396362/0.569809loss in batch 138: 0.447311/0.568909loss in batch 139: 0.29451/0.566956loss in batch 140: 0.359863/0.565475loss in batch 141: 0.326996/0.563812loss in batch 142: 0.443344/0.562973loss in batch 143: 0.450653/0.56218loss in batch 144: 0.301849/0.560394loss in batch 145: 0.356277/0.55899loss in batch 146: 0.302032/0.557236loss in batch 147: 1.19208/0.561523loss in batch 148: 0.285416/0.559677loss in batch 149: 0.45694/0.55899loss in batch 150: 0.367599/0.557724loss in batch 151: 0.237106/0.555618loss in batch 152: 0.568573/0.55571loss in batch 153: 0.461761/0.555099loss in batch 154: 0.520157/0.554871loss in batch 155: 0.593338/0.555115loss in batch 156: 0.250702/0.553177loss in batch 157: 0.450562/0.552521loss in batch 158: 0.226746/0.550476loss in batch 159: 0.314133/0.549011loss in batch 160: 0.289001/0.547379loss in batch 161: 0.707916/0.54837loss in batch 162: 0.264694/0.546631loss in batch 163: 0.315002/0.545212loss in batch 164: 0.209457/0.543198loss in batch 165: 0.258911/0.541473loss in batch 166: 0.724503/0.542572loss in batch 167: 0.534348/0.542526loss in batch 168: 0.703491/0.543472loss in batch 169: 0.648941/0.544098loss in batch 170: 0.429688/0.543427loss in batch 171: 0.588089/0.543686loss in batch 172: 0.74382/0.54483loss in batch 173: 0.304565/0.543457loss in batch 174: 0.383453/0.542557loss in batch 175: 0.406723/0.541779loss in batch 176: 0.437897/0.541183loss in batch 177: 0.481949/0.540848loss in batch 178: 0.518173/0.540726loss in batch 179: 0.288361/0.539337loss in batch 180: 0.176926/0.537338loss in batch 181: 0.439468/0.536789loss in batch 182: 0.348602/0.535767loss in batch 183: 0.329132/0.534637loss in batch 184: 0.379044/0.533798loss in batch 185: 0.445175/0.53331loss in batch 186: 0.193497/0.531509loss in batch 187: 0.50705/0.531357loss in batch 188: 0.412216/0.530731loss in batch 189: 0.721252/0.531738loss in batch 190: 0.386887/0.530975loss in batch 191: 0.480789/0.530731loss in batch 192: 0.486374/0.530502loss in batch 193: 0.412323/0.529892loss in batch 194: 0.79631/0.53125loss in batch 195: 0.517059/0.531174loss in batch 196: 0.332245/0.530167loss in batch 197: 0.190247/0.528458loss in batch 198: 0.231339/0.526962loss in batch 199: 0.232056/0.525482loss in batch 200: 0.331329/0.524521loss in batch 201: 0.601425/0.524902loss in batch 202: 0.706451/0.525787loss in batch 203: 0.449127/0.525406loss in batch 204: 0.257782/0.524109loss in batch 205: 0.214935/0.522614loss in batch 206: 0.658005/0.52327loss in batch 207: 0.235687/0.521881loss in batch 208: 0.590668/0.522217loss in batch 209: 0.222839/0.520782loss in batch 210: 0.944061/0.522797loss in batch 211: 0.177231/0.521164loss in batch 212: 0.520187/0.521149
done with epoch 0
train_acc: 0.805164 (343/426)
test loss: 0.520187
acc: 0.846154 (121/143)
loss in batch 0: 0.215286/0.215286loss in batch 1: 0.320923/0.268112loss in batch 2: 0.231186/0.255798loss in batch 3: 0.148804/0.22905loss in batch 4: 0.647614/0.312759loss in batch 5: 0.300034/0.310638loss in batch 6: 0.364197/0.318298loss in batch 7: 0.684326/0.364044loss in batch 8: 0.213989/0.347366loss in batch 9: 0.524643/0.365097loss in batch 10: 0.207016/0.350723loss in batch 11: 0.346237/0.350342loss in batch 12: 0.430862/0.356552loss in batch 13: 0.808121/0.388809loss in batch 14: 0.559265/0.400162loss in batch 15: 0.305923/0.394272loss in batch 16: 0.337433/0.39093loss in batch 17: 0.489929/0.396423loss in batch 18: 0.174484/0.38475loss in batch 19: 0.57196/0.394104loss in batch 20: 0.324203/0.390778loss in batch 21: 0.54686/0.397873loss in batch 22: 0.371765/0.396744loss in batch 23: 0.399948/0.396866loss in batch 24: 0.447632/0.398895loss in batch 25: 0.438477/0.400421loss in batch 26: 0.466965/0.402893loss in batch 27: 0.952225/0.422516loss in batch 28: 0.192215/0.414566loss in batch 29: 0.276718/0.409988loss in batch 30: 0.290314/0.406113loss in batch 31: 0.587646/0.411789loss in batch 32: 0.363205/0.410324loss in batch 33: 0.308762/0.407318loss in batch 34: 0.483582/0.409515loss in batch 35: 0.548431/0.413376loss in batch 36: 0.290314/0.410049loss in batch 37: 0.24675/0.405746loss in batch 38: 0.347336/0.404236loss in batch 39: 0.270477/0.400894loss in batch 40: 0.172592/0.395325loss in batch 41: 0.320892/0.393555loss in batch 42: 0.366714/0.392944loss in batch 43: 0.565399/0.396851loss in batch 44: 0.169022/0.391785loss in batch 45: 0.549271/0.395218loss in batch 46: 0.278366/0.392731loss in batch 47: 0.287048/0.390518loss in batch 48: 0.41449/0.391022loss in batch 49: 0.454926/0.392288loss in batch 50: 0.441467/0.39325loss in batch 51: 0.574448/0.396744loss in batch 52: 0.409103/0.396973loss in batch 53: 0.352463/0.396149loss in batch 54: 0.643341/0.400635loss in batch 55: 0.455017/0.401611loss in batch 56: 0.475784/0.402908loss in batch 57: 0.414978/0.403137loss in batch 58: 0.196686/0.399628loss in batch 59: 0.197372/0.396255loss in batch 60: 0.260727/0.394043loss in batch 61: 0.438171/0.394745loss in batch 62: 0.482788/0.396133loss in batch 63: 0.225189/0.393478loss in batch 64: 0.617538/0.396912loss in batch 65: 0.181717/0.393661loss in batch 66: 0.370514/0.393311loss in batch 67: 0.298538/0.391922loss in batch 68: 0.307449/0.390686loss in batch 69: 0.102081/0.386566loss in batch 70: 0.344894/0.385986loss in batch 71: 0.333374/0.385254loss in batch 72: 0.306854/0.384186loss in batch 73: 0.323654/0.383377loss in batch 74: 0.310974/0.382401loss in batch 75: 0.427536/0.382996loss in batch 76: 0.240585/0.381134loss in batch 77: 0.113052/0.377701loss in batch 78: 0.250076/0.376099loss in batch 79: 0.201981/0.373917loss in batch 80: 0.802032/0.379211loss in batch 81: 0.349594/0.378845loss in batch 82: 0.243439/0.377197loss in batch 83: 0.352829/0.376923loss in batch 84: 0.286179/0.375839loss in batch 85: 0.20517/0.373856loss in batch 86: 0.216675/0.372055loss in batch 87: 0.111984/0.36911loss in batch 88: 0.678085/0.372574loss in batch 89: 0.351685/0.372345loss in batch 90: 0.197281/0.370422loss in batch 91: 0.887573/0.376038loss in batch 92: 0.662445/0.37912loss in batch 93: 0.669876/0.382217loss in batch 94: 0.310318/0.381454loss in batch 95: 0.423279/0.381882loss in batch 96: 0.532593/0.383438loss in batch 97: 0.420151/0.38382loss in batch 98: 0.256363/0.382538loss in batch 99: 0.250229/0.38121loss in batch 100: 0.412933/0.381531loss in batch 101: 0.264236/0.380371loss in batch 102: 0.170807/0.378342loss in batch 103: 0.283875/0.377426loss in batch 104: 0.665619/0.380173loss in batch 105: 0.594193/0.382202loss in batch 106: 0.156616/0.380096loss in batch 107: 0.390091/0.380173loss in batch 108: 0.222275/0.378723loss in batch 109: 0.473053/0.379593loss in batch 110: 0.630768/0.381851loss in batch 111: 0.280716/0.380951loss in batch 112: 0.378204/0.38092loss in batch 113: 0.541214/0.382324loss in batch 114: 0.270447/0.381348loss in batch 115: 0.416794/0.381668loss in batch 116: 0.501343/0.382675loss in batch 117: 0.237427/0.381454loss in batch 118: 0.535873/0.382751loss in batch 119: 0.583359/0.384415loss in batch 120: 0.573853/0.385986loss in batch 121: 0.102158/0.383652loss in batch 122: 0.203323/0.382187loss in batch 123: 0.295654/0.381485loss in batch 124: 0.298218/0.380829loss in batch 125: 0.363968/0.380692loss in batch 126: 0.410507/0.380936loss in batch 127: 0.469009/0.381622loss in batch 128: 0.271866/0.380783loss in batch 129: 0.382553/0.380783loss in batch 130: 0.290207/0.380096loss in batch 131: 0.340485/0.379791loss in batch 132: 0.48674/0.3806loss in batch 133: 0.5737/0.382034loss in batch 134: 0.424835/0.382355loss in batch 135: 0.392868/0.382431loss in batch 136: 0.392426/0.382507loss in batch 137: 0.227234/0.381378loss in batch 138: 0.289932/0.380707loss in batch 139: 0.211792/0.379517loss in batch 140: 0.216034/0.378357loss in batch 141: 0.294785/0.377762loss in batch 142: 0.181503/0.376404loss in batch 143: 0.262299/0.375595loss in batch 144: 0.614822/0.377243loss in batch 145: 0.108612/0.375412loss in batch 146: 0.28801/0.374817loss in batch 147: 0.583252/0.376221loss in batch 148: 0.374008/0.376205loss in batch 149: 0.279251/0.375565loss in batch 150: 0.990753/0.379639loss in batch 151: 0.423492/0.379929loss in batch 152: 0.312469/0.379486loss in batch 153: 0.234512/0.378555loss in batch 154: 0.183472/0.377274loss in batch 155: 0.204803/0.376175loss in batch 156: 0.350128/0.376007loss in batch 157: 0.253052/0.375229loss in batch 158: 0.194626/0.3741loss in batch 159: 0.177017/0.372879loss in batch 160: 0.855957/0.37587loss in batch 161: 0.210571/0.374847loss in batch 162: 0.414932/0.375092loss in batch 163: 0.0830231/0.373306loss in batch 164: 0.316818/0.372971loss in batch 165: 0.373123/0.372971loss in batch 166: 0.650986/0.374634loss in batch 167: 0.131836/0.373199loss in batch 168: 0.177734/0.372025loss in batch 169: 0.296692/0.371597loss in batch 170: 0.147751/0.370285loss in batch 171: 0.172668/0.369125loss in batch 172: 0.117065/0.367676loss in batch 173: 0.35965/0.36763loss in batch 174: 0.764221/0.369904loss in batch 175: 0.144608/0.368622loss in batch 176: 0.111313/0.367172loss in batch 177: 0.171356/0.366074loss in batch 178: 0.21846/0.365234loss in batch 179: 0.289688/0.364822loss in batch 180: 0.521484/0.365677loss in batch 181: 0.344818/0.36557loss in batch 182: 0.0600739/0.363907loss in batch 183: 0.185638/0.36293loss in batch 184: 0.196487/0.36203loss in batch 185: 0.425781/0.362366loss in batch 186: 0.207413/0.361542loss in batch 187: 0.358841/0.361526loss in batch 188: 0.320343/0.361313loss in batch 189: 0.181992/0.360367loss in batch 190: 0.668793/0.361984loss in batch 191: 0.189697/0.361084loss in batch 192: 0.311783/0.360825loss in batch 193: 0.139389/0.359695loss in batch 194: 0.376938/0.359787loss in batch 195: 0.319244/0.359573loss in batch 196: 0.406387/0.359802loss in batch 197: 0.225494/0.359131loss in batch 198: 0.178802/0.358215loss in batch 199: 0.604568/0.359451loss in batch 200: 0.365219/0.359497loss in batch 201: 0.124313/0.358307loss in batch 202: 0.254639/0.357819loss in batch 203: 0.357574/0.357819loss in batch 204: 0.293884/0.357498loss in batch 205: 0.301224/0.357224loss in batch 206: 0.6772/0.358765loss in batch 207: 0.173416/0.35788loss in batch 208: 0.61412/0.359116loss in batch 209: 0.461014/0.359589loss in batch 210: 0.176926/0.358719loss in batch 211: 0.179977/0.35788loss in batch 212: 0.695816/0.359467
done with epoch 1
train_acc: 0.894366 (381/426)
test loss: 0.695816
acc: 0.909091 (130/143)
loss in batch 0: 0.1483/0.1483loss in batch 1: 0.288452/0.218369loss in batch 2: 0.364563/0.267105loss in batch 3: 0.609573/0.352722loss in batch 4: 0.610855/0.404343loss in batch 5: 0.456863/0.413101loss in batch 6: 0.743164/0.460251loss in batch 7: 0.213562/0.429413loss in batch 8: 0.528809/0.44046loss in batch 9: 0.11673/0.408081loss in batch 10: 0.241928/0.39299loss in batch 11: 0.147125/0.372482loss in batch 12: 0.313309/0.367935loss in batch 13: 0.19603/0.355667loss in batch 14: 0.20665/0.345718loss in batch 15: 0.408539/0.349655loss in batch 16: 0.146317/0.337692loss in batch 17: 0.181335/0.328995loss in batch 18: 0.682495/0.34761loss in batch 19: 0.278702/0.344177loss in batch 20: 0.313095/0.342682loss in batch 21: 0.495148/0.349609loss in batch 22: 0.345306/0.349426loss in batch 23: 0.262039/0.345795loss in batch 24: 0.185028/0.339355loss in batch 25: 0.121002/0.330948loss in batch 26: 0.497833/0.337128loss in batch 27: 0.371155/0.338348loss in batch 28: 0.195358/0.33342loss in batch 29: 0.198105/0.328903loss in batch 30: 1.03645/0.351746loss in batch 31: 0.251617/0.348602loss in batch 32: 0.792923/0.362076loss in batch 33: 0.165405/0.356277loss in batch 34: 0.350906/0.35614loss in batch 35: 0.244537/0.353043loss in batch 36: 0.526627/0.357727loss in batch 37: 0.208862/0.353806loss in batch 38: 0.522873/0.358154loss in batch 39: 0.377396/0.358627loss in batch 40: 0.198303/0.354706loss in batch 41: 0.25383/0.35231loss in batch 42: 0.382446/0.353012loss in batch 43: 0.327164/0.352417loss in batch 44: 0.12381/0.347351loss in batch 45: 0.23175/0.344833loss in batch 46: 0.186813/0.341461loss in batch 47: 0.283813/0.340256loss in batch 48: 0.515137/0.343842loss in batch 49: 0.0850525/0.338654loss in batch 50: 0.484161/0.341507loss in batch 51: 0.248932/0.339722loss in batch 52: 0.237091/0.337784loss in batch 53: 0.166809/0.334625loss in batch 54: 0.663162/0.340607loss in batch 55: 0.225754/0.338547loss in batch 56: 0.31662/0.338181loss in batch 57: 0.165863/0.33519loss in batch 58: 0.143005/0.33194loss in batch 59: 0.451263/0.333923loss in batch 60: 0.513962/0.336868loss in batch 61: 0.345413/0.337021loss in batch 62: 0.583023/0.340912loss in batch 63: 0.169037/0.338242loss in batch 64: 0.0859528/0.334351loss in batch 65: 0.164825/0.331787loss in batch 66: 0.25412/0.330627loss in batch 67: 0.037323/0.326309loss in batch 68: 0.140854/0.323624loss in batch 69: 0.0823059/0.320175loss in batch 70: 0.189957/0.318344loss in batch 71: 0.292801/0.317993loss in batch 72: 0.207626/0.316467loss in batch 73: 0.52327/0.319275loss in batch 74: 0.130035/0.316742loss in batch 75: 0.152618/0.31459loss in batch 76: 0.171661/0.312744loss in batch 77: 0.0910034/0.309891loss in batch 78: 0.167831/0.30809loss in batch 79: 0.139236/0.305984loss in batch 80: 0.339111/0.306396loss in batch 81: 0.259491/0.305832loss in batch 82: 0.178787/0.304291loss in batch 83: 0.177734/0.30278loss in batch 84: 0.208145/0.301666loss in batch 85: 0.156052/0.299973loss in batch 86: 0.0472107/0.297058loss in batch 87: 0.647293/0.301041loss in batch 88: 0.240601/0.300385loss in batch 89: 0.363861/0.301086loss in batch 90: 0.123596/0.299133loss in batch 91: 0.208099/0.298141loss in batch 92: 0.148087/0.296524loss in batch 93: 0.229813/0.295807loss in batch 94: 0.130417/0.294083loss in batch 95: 0.256866/0.293686loss in batch 96: 0.104752/0.291748loss in batch 97: 0.115311/0.289932loss in batch 98: 0.136749/0.288391loss in batch 99: 0.241318/0.287918loss in batch 100: 0.771667/0.292709loss in batch 101: 0.133301/0.291138loss in batch 102: 0.606308/0.294205loss in batch 103: 0.156296/0.292877loss in batch 104: 0.169037/0.291702loss in batch 105: 0.288727/0.291672loss in batch 106: 0.164673/0.290482loss in batch 107: 0.191788/0.289566loss in batch 108: 0.535156/0.291824loss in batch 109: 0.175583/0.290771loss in batch 110: 0.273636/0.290604loss in batch 111: 0.0952454/0.288879loss in batch 112: 0.315613/0.289093loss in batch 113: 0.149338/0.287888loss in batch 114: 0.0838776/0.286102loss in batch 115: 0.118607/0.284668loss in batch 116: 0.322632/0.284988loss in batch 117: 0.153137/0.283875loss in batch 118: 0.365158/0.284546loss in batch 119: 0.671722/0.287781loss in batch 120: 0.251099/0.287476loss in batch 121: 0.176865/0.28656loss in batch 122: 0.0976563/0.285019loss in batch 123: 0.0499573/0.283142loss in batch 124: 0.162445/0.282166loss in batch 125: 0.234283/0.281799loss in batch 126: 0.490387/0.283432loss in batch 127: 0.414093/0.284454loss in batch 128: 0.42215/0.285522loss in batch 129: 0.180099/0.284714loss in batch 130: 0.320023/0.284973loss in batch 131: 0.531265/0.286835loss in batch 132: 0.243088/0.28653loss in batch 133: 0.161591/0.285583loss in batch 134: 0.0982056/0.284195loss in batch 135: 0.243774/0.283905loss in batch 136: 0.641342/0.286499loss in batch 137: 0.144409/0.285477loss in batch 138: 0.238022/0.285141loss in batch 139: 0.159851/0.284256loss in batch 140: 0.143387/0.283249loss in batch 141: 0.11731/0.282074loss in batch 142: 0.396149/0.282867loss in batch 143: 0.131454/0.28183loss in batch 144: 0.190735/0.281204loss in batch 145: 0.34642/0.281647loss in batch 146: 0.150085/0.280746loss in batch 147: 0.505264/0.282257loss in batch 148: 0.145096/0.281342loss in batch 149: 0.309296/0.281525loss in batch 150: 0.291321/0.281601loss in batch 151: 0.586227/0.2836loss in batch 152: 0.734985/0.286545loss in batch 153: 0.309296/0.286697loss in batch 154: 0.123718/0.285645loss in batch 155: 0.138138/0.284698loss in batch 156: 0.548035/0.286377loss in batch 157: 0.0891876/0.285126loss in batch 158: 0.760086/0.288116loss in batch 159: 0.298141/0.288177loss in batch 160: 0.64534/0.290405loss in batch 161: 0.132019/0.289413loss in batch 162: 0.470856/0.290527loss in batch 163: 0.143692/0.289642loss in batch 164: 0.259323/0.289459loss in batch 165: 0.113693/0.288391loss in batch 166: 0.425308/0.289215loss in batch 167: 0.153458/0.288406loss in batch 168: 0.316727/0.288574loss in batch 169: 0.610153/0.290466loss in batch 170: 0.127975/0.289505loss in batch 171: 0.660004/0.291672loss in batch 172: 0.351288/0.292007loss in batch 173: 0.785126/0.294846loss in batch 174: 0.231934/0.294479loss in batch 175: 0.118195/0.293488loss in batch 176: 0.316025/0.29361loss in batch 177: 0.229614/0.293243loss in batch 178: 0.123627/0.292297loss in batch 179: 1.00098/0.296249loss in batch 180: 0.124588/0.295288loss in batch 181: 0.163162/0.294571loss in batch 182: 0.21051/0.294098loss in batch 183: 0.167786/0.293411loss in batch 184: 0.110016/0.292419loss in batch 185: 0.242371/0.29216loss in batch 186: 0.270035/0.292038loss in batch 187: 0.192062/0.291519loss in batch 188: 0.263138/0.291351loss in batch 189: 0.230942/0.291046loss in batch 190: 0.829544/0.293869loss in batch 191: 0.119385/0.292953loss in batch 192: 0.370804/0.293365loss in batch 193: 0.376907/0.293777loss in batch 194: 0.269501/0.293671loss in batch 195: 0.0813141/0.292572loss in batch 196: 0.609665/0.294189loss in batch 197: 0.281189/0.294113loss in batch 198: 0.225449/0.293777loss in batch 199: 0.279755/0.293716loss in batch 200: 0.410583/0.294296loss in batch 201: 0.290298/0.294266loss in batch 202: 0.380722/0.294693loss in batch 203: 0.139923/0.29393loss in batch 204: 0.326813/0.294098loss in batch 205: 0.126572/0.293289loss in batch 206: 0.271469/0.293167loss in batch 207: 0.195663/0.292709loss in batch 208: 0.605804/0.294205loss in batch 209: 0.128281/0.293427loss in batch 210: 0.275757/0.293335loss in batch 211: 0.178955/0.292786loss in batch 212: 0.569855/0.294098
done with epoch 2
train_acc: 0.915493 (390/426)
test loss: 0.569855
acc: 0.93007 (133/143)
loss in batch 0: 0.766937/0.766937loss in batch 1: 0.368835/0.567886loss in batch 2: 0.251999/0.462585loss in batch 3: 0.0521851/0.359985loss in batch 4: 0.110001/0.309982loss in batch 5: 0.185989/0.289322loss in batch 6: 0.171448/0.272491loss in batch 7: 0.288757/0.274521loss in batch 8: 0.654373/0.316727loss in batch 9: 0.138168/0.298874loss in batch 10: 0.168625/0.287033loss in batch 11: 0.171341/0.27739loss in batch 12: 0.212708/0.272415loss in batch 13: 0.688416/0.302139loss in batch 14: 0.202316/0.295471loss in batch 15: 0.483093/0.307205loss in batch 16: 0.383514/0.311691loss in batch 17: 0.205826/0.305801loss in batch 18: 0.396179/0.310577loss in batch 19: 0.295731/0.30983loss in batch 20: 0.242188/0.30661loss in batch 21: 0.072998/0.29599loss in batch 22: 0.143661/0.289368loss in batch 23: 0.169022/0.284348loss in batch 24: 0.265686/0.2836loss in batch 25: 0.172348/0.279312loss in batch 26: 0.110062/0.273056loss in batch 27: 0.141006/0.268326loss in batch 28: 0.0607452/0.261185loss in batch 29: 0.0996704/0.255798loss in batch 30: 0.021225/0.248215loss in batch 31: 1.06374/0.273712loss in batch 32: 0.143738/0.269775loss in batch 33: 0.238922/0.268875loss in batch 34: 0.155899/0.26564loss in batch 35: 0.19194/0.263596loss in batch 36: 0.312302/0.264908loss in batch 37: 0.146881/0.261795loss in batch 38: 0.0993652/0.257629loss in batch 39: 0.208084/0.256393loss in batch 40: 0.360596/0.258942loss in batch 41: 0.235565/0.258377loss in batch 42: 0.321655/0.259857loss in batch 43: 0.0472717/0.25502loss in batch 44: 0.347351/0.25708loss in batch 45: 0.0691528/0.252991loss in batch 46: 0.567337/0.259689loss in batch 47: 0.0570679/0.255463loss in batch 48: 0.120483/0.252701loss in batch 49: 0.0998993/0.249649loss in batch 50: 0.372238/0.252045loss in batch 51: 0.231262/0.251648loss in batch 52: 0.142334/0.249588loss in batch 53: 0.0693512/0.246246loss in batch 54: 0.140381/0.244324loss in batch 55: 0.210938/0.243729loss in batch 56: 0.022934/0.239853loss in batch 57: 0.167267/0.238602loss in batch 58: 0.140213/0.236938loss in batch 59: 0.069397/0.234146loss in batch 60: 0.664612/0.241196loss in batch 61: 0.150436/0.239731loss in batch 62: 0.145691/0.238251loss in batch 63: 0.108246/0.236206loss in batch 64: 0.0872498/0.233917loss in batch 65: 0.493118/0.237839loss in batch 66: 0.119171/0.236069loss in batch 67: 0.126236/0.234451loss in batch 68: 0.171707/0.233551loss in batch 69: 0.612869/0.238968loss in batch 70: 0.348175/0.240509loss in batch 71: 0.104141/0.238617loss in batch 72: 0.15921/0.237534loss in batch 73: 0.449585/0.240387loss in batch 74: 0.56842/0.244766loss in batch 75: 0.151962/0.243546loss in batch 76: 0.333176/0.244705loss in batch 77: 0.154037/0.243546loss in batch 78: 0.133835/0.242157loss in batch 79: 0.618942/0.246872loss in batch 80: 0.145691/0.245621loss in batch 81: 0.100418/0.243835loss in batch 82: 0.390182/0.245605loss in batch 83: 0.063446/0.243439loss in batch 84: 0.310028/0.244217loss in batch 85: 0.116882/0.242752loss in batch 86: 0.11377/0.241257loss in batch 87: 0.329697/0.242264loss in batch 88: 0.0607452/0.240234loss in batch 89: 0.205276/0.239838loss in batch 90: 0.193039/0.239334loss in batch 91: 0.297653/0.23996loss in batch 92: 0.0428009/0.237839loss in batch 93: 0.0665436/0.236008loss in batch 94: 0.196075/0.235596loss in batch 95: 0.168396/0.234894loss in batch 96: 0.366653/0.236252loss in batch 97: 0.341934/0.237335loss in batch 98: 0.495361/0.239929loss in batch 99: 0.204651/0.239594loss in batch 100: 0.102432/0.238235loss in batch 101: 0.110016/0.236969loss in batch 102: 0.0404663/0.235077loss in batch 103: 0.105606/0.233826loss in batch 104: 0.448257/0.23587loss in batch 105: 0.0503693/0.234116loss in batch 106: 0.29335/0.234665loss in batch 107: 0.262375/0.234924loss in batch 108: 1.19916/0.243774loss in batch 109: 0.238373/0.243713loss in batch 110: 0.13475/0.242737loss in batch 111: 0.148071/0.241898loss in batch 112: 0.3013/0.242416loss in batch 113: 0.10434/0.241211loss in batch 114: 0.210114/0.240936loss in batch 115: 0.121902/0.239914loss in batch 116: 0.175629/0.239365loss in batch 117: 0.21225/0.239136loss in batch 118: 0.47081/0.241089loss in batch 119: 0.145798/0.24028loss in batch 120: 0.143341/0.239487loss in batch 121: 0.127762/0.238556loss in batch 122: 0.182999/0.238113loss in batch 123: 0.294327/0.238571loss in batch 124: 0.267731/0.238815loss in batch 125: 0.457321/0.24054loss in batch 126: 0.358841/0.24147loss in batch 127: 0.240997/0.24147loss in batch 128: 0.289261/0.241837loss in batch 129: 0.199768/0.241516loss in batch 130: 0.248169/0.241562loss in batch 131: 0.117935/0.240616loss in batch 132: 0.164597/0.240051loss in batch 133: 0.358398/0.240936loss in batch 134: 0.288849/0.241302loss in batch 135: 0.278015/0.241562loss in batch 136: 0.362061/0.242432loss in batch 137: 0.0844421/0.241302loss in batch 138: 0.099411/0.24028loss in batch 139: 0.374084/0.241226loss in batch 140: 0.156174/0.240631loss in batch 141: 0.289749/0.240982loss in batch 142: 0.545151/0.243103loss in batch 143: 0.0531006/0.241791loss in batch 144: 0.526581/0.243744loss in batch 145: 0.115631/0.242874loss in batch 146: 0.0623779/0.241638loss in batch 147: 0.122192/0.240829loss in batch 148: 0.0860596/0.239792loss in batch 149: 0.5242/0.241699loss in batch 150: 0.124405/0.240906loss in batch 151: 0.166367/0.240433loss in batch 152: 0.0718384/0.239319loss in batch 153: 0.548019/0.241333loss in batch 154: 0.159576/0.240799loss in batch 155: 0.164322/0.240311loss in batch 156: 0.59729/0.242584loss in batch 157: 0.804611/0.246124loss in batch 158: 0.0878601/0.245148loss in batch 159: 0.521271/0.246872loss in batch 160: 0.221237/0.246704loss in batch 161: 0.120209/0.245941loss in batch 162: 0.17244/0.245468loss in batch 163: 0.220749/0.245316loss in batch 164: 0.173111/0.244904loss in batch 165: 0.578568/0.246902loss in batch 166: 0.335709/0.247437loss in batch 167: 0.197968/0.247131loss in batch 168: 0.420837/0.248169loss in batch 169: 0.0500336/0.246994loss in batch 170: 0.902893/0.250839loss in batch 171: 0.156342/0.25029loss in batch 172: 0.116364/0.249512loss in batch 173: 0.223907/0.249359loss in batch 174: 0.09375/0.248474loss in batch 175: 0.271576/0.248611loss in batch 176: 0.193893/0.248291loss in batch 177: 0.696899/0.250824loss in batch 178: 0.08078/0.249863loss in batch 179: 0.299332/0.250137loss in batch 180: 0.220047/0.249969loss in batch 181: 0.451355/0.251083loss in batch 182: 0.357101/0.251663loss in batch 183: 0.0533447/0.25058loss in batch 184: 0.193863/0.25029loss in batch 185: 0.914734/0.253845loss in batch 186: 0.322739/0.254227loss in batch 187: 0.478088/0.255417loss in batch 188: 0.822464/0.258408loss in batch 189: 0.149445/0.257828loss in batch 190: 0.413116/0.258652loss in batch 191: 0.395309/0.259369loss in batch 192: 0.209274/0.259094loss in batch 193: 0.395233/0.259796loss in batch 194: 0.175812/0.259369loss in batch 195: 0.494339/0.260574loss in batch 196: 0.261475/0.260574loss in batch 197: 0.473236/0.261642loss in batch 198: 0.129456/0.260986loss in batch 199: 0.123764/0.2603loss in batch 200: 0.206604/0.26004loss in batch 201: 0.220016/0.259842loss in batch 202: 0.665634/0.261826loss in batch 203: 0.570633/0.263351loss in batch 204: 0.100983/0.262558loss in batch 205: 0.513977/0.263763loss in batch 206: 0.9702/0.267181loss in batch 207: 0.183884/0.266785loss in batch 208: 0.24942/0.266708loss in batch 209: 0.28653/0.2668loss in batch 210: 0.0749359/0.265884loss in batch 211: 0.166733/0.265411loss in batch 212: 0.229507/0.265244
done with epoch 3
train_acc: 0.920188 (392/426)
test loss: 0.229507
acc: 0.923077 (132/143)
loss in batch 0: 0.335419/0.335419loss in batch 1: 0.212753/0.274094loss in batch 2: 0.144485/0.230881loss in batch 3: 0.216568/0.22731loss in batch 4: 0.308578/0.243561loss in batch 5: 0.384628/0.267075loss in batch 6: 0.182983/0.255066loss in batch 7: 0.663666/0.306122loss in batch 8: 0.33551/0.309402loss in batch 9: 0.185333/0.296997loss in batch 10: 0.0899353/0.278183loss in batch 11: 0.107651/0.263962loss in batch 12: 0.128769/0.253571loss in batch 13: 0.87236/0.29776loss in batch 14: 0.35405/0.301514loss in batch 15: 0.379074/0.306366loss in batch 16: 0.0310822/0.290161loss in batch 17: 0.0944519/0.279282loss in batch 18: 0.0442657/0.266922loss in batch 19: 0.167191/0.261932loss in batch 20: 0.261719/0.261917loss in batch 21: 0.419464/0.269089loss in batch 22: 0.219009/0.266907loss in batch 23: 0.114441/0.260559loss in batch 24: 0.321579/0.263loss in batch 25: 0.175415/0.259628loss in batch 26: 0.15358/0.255707loss in batch 27: 1.02382/0.283142loss in batch 28: 0.100189/0.276825loss in batch 29: 0.253876/0.276062loss in batch 30: 0.0572205/0.269012loss in batch 31: 0.198212/0.2668loss in batch 32: 0.417953/0.271362loss in batch 33: 0.0875092/0.265961loss in batch 34: 0.22374/0.264755loss in batch 35: 0.119766/0.260727loss in batch 36: 0.126358/0.257095loss in batch 37: 0.252884/0.256989loss in batch 38: 0.569656/0.264999loss in batch 39: 0.283661/0.265472loss in batch 40: 0.40683/0.268921loss in batch 41: 0.0921173/0.264709loss in batch 42: 0.0719757/0.260223loss in batch 43: 0.161789/0.25798loss in batch 44: 0.232285/0.257416loss in batch 45: 0.409378/0.260712loss in batch 46: 0.0254822/0.255722loss in batch 47: 0.219238/0.254959loss in batch 48: 0.0594482/0.250977loss in batch 49: 0.262848/0.251205loss in batch 50: 0.210571/0.250397loss in batch 51: 0.502991/0.255264loss in batch 52: 0.146866/0.25322loss in batch 53: 0.167877/0.251648loss in batch 54: 0.0417786/0.247833loss in batch 55: 0.10379/0.245255loss in batch 56: 0.0905457/0.242523loss in batch 57: 0.410477/0.245422loss in batch 58: 0.197739/0.244614loss in batch 59: 0.319443/0.245865loss in batch 60: 0.539444/0.250687loss in batch 61: 0.318436/0.251785loss in batch 62: 0.140762/0.250015loss in batch 63: 1.34161/0.267075loss in batch 64: 0.202087/0.266068loss in batch 65: 0.360138/0.267502loss in batch 66: 0.17659/0.266144loss in batch 67: 0.176804/0.264832loss in batch 68: 0.248764/0.264603loss in batch 69: 0.128738/0.26265loss in batch 70: 0.182144/0.26152loss in batch 71: 0.415558/0.263657loss in batch 72: 0.123886/0.261734loss in batch 73: 0.308487/0.262375loss in batch 74: 0.199982/0.261551loss in batch 75: 0.0744171/0.259079loss in batch 76: 0.0267639/0.256058loss in batch 77: 0.141205/0.254593loss in batch 78: 0.114029/0.252808loss in batch 79: 0.221588/0.252426loss in batch 80: 0.337372/0.253479loss in batch 81: 0.115433/0.251801loss in batch 82: 0.397247/0.25354loss in batch 83: 0.485809/0.256302loss in batch 84: 0.0354156/0.253708loss in batch 85: 0.45285/0.256027loss in batch 86: 0.00544739/0.253143loss in batch 87: 0.364716/0.25441loss in batch 88: 0.25119/0.254364loss in batch 89: 0.131866/0.253006loss in batch 90: 0.152206/0.251907loss in batch 91: 0.201797/0.251358loss in batch 92: 0.512436/0.254166loss in batch 93: 0.154694/0.253113loss in batch 94: 0.136703/0.251877loss in batch 95: 0.0830841/0.250122loss in batch 96: 0.421967/0.251892loss in batch 97: 0.0875092/0.250214loss in batch 98: 0.763168/0.255402loss in batch 99: 0.0774841/0.253616loss in batch 100: 0.205093/0.253143loss in batch 101: 0.440247/0.254974loss in batch 102: 0.190186/0.254349loss in batch 103: 0.152634/0.253372loss in batch 104: 0.146027/0.25235loss in batch 105: 0.123962/0.251129loss in batch 106: 0.113739/0.249847loss in batch 107: 0.062912/0.248123loss in batch 108: 0.185425/0.247543loss in batch 109: 0.521469/0.250031loss in batch 110: 0.0953369/0.248627loss in batch 111: 0.5242/0.251099loss in batch 112: 0.394577/0.252365loss in batch 113: 0.670258/0.256042loss in batch 114: 0.17514/0.255341loss in batch 115: 0.0718231/0.253738loss in batch 116: 0.230118/0.253555loss in batch 117: 0.0631714/0.251938loss in batch 118: 0.601456/0.254868loss in batch 119: 0.112686/0.253693loss in batch 120: 0.158844/0.252899loss in batch 121: 0.198074/0.252457loss in batch 122: 0.874634/0.257523loss in batch 123: 0.477921/0.259293loss in batch 124: 0.103714/0.258041loss in batch 125: 0.0329742/0.256256loss in batch 126: 0.536942/0.258469loss in batch 127: 0.0447845/0.256805loss in batch 128: 0.278473/0.256973loss in batch 129: 0.391968/0.258011loss in batch 130: 0.071167/0.256577loss in batch 131: 0.15036/0.255768loss in batch 132: 0.0902863/0.254532loss in batch 133: 0.421829/0.255783loss in batch 134: 0.122772/0.254807loss in batch 135: 0.145538/0.253983loss in batch 136: 0.147156/0.25322loss in batch 137: 0.356018/0.253952loss in batch 138: 0.157669/0.253265loss in batch 139: 0.173477/0.252701loss in batch 140: 0.0555573/0.251297loss in batch 141: 0.952469/0.256241loss in batch 142: 0.0714417/0.254944loss in batch 143: 0.225845/0.254745loss in batch 144: 0.528336/0.256622loss in batch 145: 0.114944/0.255661loss in batch 146: 0.154373/0.254974loss in batch 147: 0.130402/0.25412loss in batch 148: 0.385834/0.255005loss in batch 149: 0.0780182/0.253815loss in batch 150: 0.142227/0.253098loss in batch 151: 0.233948/0.25296loss in batch 152: 0.112061/0.252045loss in batch 153: 1.25183/0.25853loss in batch 154: 0.0993347/0.257507loss in batch 155: 0.0692596/0.256302loss in batch 156: 0.0343018/0.254883loss in batch 157: 0.184952/0.25444loss in batch 158: 0.27951/0.254608loss in batch 159: 0.116882/0.253738loss in batch 160: 0.112457/0.252853loss in batch 161: 0.383255/0.253662loss in batch 162: 0.142426/0.252991loss in batch 163: 0.353348/0.253586loss in batch 164: 0.140488/0.252914loss in batch 165: 0.091095/0.251938loss in batch 166: 0.20668/0.251663loss in batch 167: 0.0846863/0.250671loss in batch 168: 0.0695801/0.249603loss in batch 169: 0.920395/0.253555loss in batch 170: 0.22348/0.253372loss in batch 171: 0.151962/0.252777loss in batch 172: 0.0617218/0.251678loss in batch 173: 0.134155/0.251007loss in batch 174: 0.104828/0.250153loss in batch 175: 0.103073/0.249329loss in batch 176: 0.296432/0.249588loss in batch 177: 0.132782/0.248947loss in batch 178: 0.0812683/0.248001loss in batch 179: 0.909683/0.251678loss in batch 180: 0.439377/0.252731loss in batch 181: 0.697174/0.255157loss in batch 182: 0.210556/0.254929loss in batch 183: 0.0965881/0.254059loss in batch 184: 0.260651/0.254089loss in batch 185: 0.250595/0.254074loss in batch 186: 0.438583/0.255066loss in batch 187: 0.0635071/0.254028loss in batch 188: 0.367432/0.254639loss in batch 189: 0.106613/0.25386loss in batch 190: 0.13913/0.253265loss in batch 191: 0.337265/0.253693loss in batch 192: 0.0188599/0.252472loss in batch 193: 0.103928/0.251724loss in batch 194: 0.225891/0.251587loss in batch 195: 0.243057/0.251541loss in batch 196: 0.301376/0.251785loss in batch 197: 0.10054/0.251022loss in batch 198: 0.316833/0.251358loss in batch 199: 0.0978546/0.250595loss in batch 200: 0.29715/0.250824loss in batch 201: 0.158539/0.250366loss in batch 202: 0.205673/0.250137loss in batch 203: 0.212387/0.249954loss in batch 204: 0.0709381/0.249084loss in batch 205: 0.436569/0.25loss in batch 206: 0.0696869/0.24913loss in batch 207: 0.229904/0.249039loss in batch 208: 0.12674/0.248459loss in batch 209: 0.0466003/0.247482loss in batch 210: 0.168655/0.247116loss in batch 211: 0.467758/0.248154loss in batch 212: 0.0921783/0.247421
done with epoch 4
train_acc: 0.922535 (393/426)
test loss: 0.0921783
acc: 0.923077 (132/143)
loss in batch 0: 1.0621/1.0621loss in batch 1: 0.154175/0.608139loss in batch 2: 0.56459/0.593628loss in batch 3: 0.02388/0.451187loss in batch 4: 0.340057/0.428955loss in batch 5: 0.0976715/0.373734loss in batch 6: 0.174408/0.345276loss in batch 7: 0.0491486/0.308243loss in batch 8: 0.0599213/0.280655loss in batch 9: 0.347351/0.287338loss in batch 10: 0.418213/0.29924loss in batch 11: 0.242874/0.294525loss in batch 12: 0.307266/0.295517loss in batch 13: 0.243942/0.29184loss in batch 14: 0.251038/0.289108loss in batch 15: 0.0881653/0.27655loss in batch 16: 0.444931/0.286453loss in batch 17: 0.282379/0.28624loss in batch 18: 0.192245/0.281281loss in batch 19: 0.556564/0.295044loss in batch 20: 0.152145/0.288254loss in batch 21: 0.259811/0.286957loss in batch 22: 0.171051/0.281921loss in batch 23: 0.0631256/0.272781loss in batch 24: 0.156387/0.268143loss in batch 25: 0.108292/0.261993loss in batch 26: 0.341614/0.264938loss in batch 27: 0.213562/0.263107loss in batch 28: 0.312103/0.264801loss in batch 29: 0.453766/0.271088loss in batch 30: 0.125046/0.266388loss in batch 31: 0.178909/0.263641loss in batch 32: 0.0669708/0.25769loss in batch 33: 0.0780945/0.252411loss in batch 34: 0.18512/0.250488loss in batch 35: 0.121597/0.246902loss in batch 36: 0.150436/0.244293loss in batch 37: 0.261398/0.244751loss in batch 38: 0.181015/0.243118loss in batch 39: 0.17244/0.241348loss in batch 40: 0.224869/0.240936loss in batch 41: 0.0261688/0.23584loss in batch 42: 0.0711517/0.231995loss in batch 43: 0.085083/0.228668loss in batch 44: 0.560242/0.236038loss in batch 45: 0.464508/0.240997loss in batch 46: 0.0835419/0.23764loss in batch 47: 0.125626/0.235321loss in batch 48: 0.0996704/0.232544loss in batch 49: 0.182983/0.231552loss in batch 50: 0.256714/0.23204loss in batch 51: 0.163712/0.230743loss in batch 52: 0.383698/0.233627loss in batch 53: 0.281876/0.234512loss in batch 54: 0.0573273/0.231293loss in batch 55: 0.552261/0.23703loss in batch 56: 0.0643158/0.233994loss in batch 57: 0.667877/0.24147loss in batch 58: 0.273621/0.24202loss in batch 59: 0.0743866/0.239227loss in batch 60: 0.0911713/0.236801loss in batch 61: 0.370605/0.238953loss in batch 62: 0.319138/0.240234loss in batch 63: 0.394119/0.242645loss in batch 64: 0.315063/0.243744loss in batch 65: 0.0770416/0.241211loss in batch 66: 0.396439/0.24353loss in batch 67: 0.450562/0.246582loss in batch 68: 0.101395/0.244476loss in batch 69: 0.0532227/0.241745loss in batch 70: 0.226959/0.241547loss in batch 71: 0.614716/0.246719loss in batch 72: 0.107773/0.244812loss in batch 73: 0.23085/0.244629loss in batch 74: 0.0762634/0.242386loss in batch 75: 0.0995789/0.240509loss in batch 76: 0.382797/0.242355loss in batch 77: 0.39978/0.24437loss in batch 78: 0.0845032/0.242355loss in batch 79: 0.395432/0.244263loss in batch 80: 0.0712433/0.242126loss in batch 81: 0.0499115/0.239777loss in batch 82: 0.14325/0.238617loss in batch 83: 0.193924/0.238083loss in batch 84: 0.171158/0.237289loss in batch 85: 0.295975/0.237976loss in batch 86: 0.0449524/0.235764loss in batch 87: 0.443985/0.238129loss in batch 88: 0.0837708/0.236404loss in batch 89: 0.0778656/0.234634loss in batch 90: 0.0592804/0.232712loss in batch 91: 0.132553/0.231613loss in batch 92: 0.0258789/0.229416loss in batch 93: 0.442368/0.231674loss in batch 94: 0.100052/0.230286loss in batch 95: 0.391205/0.231964loss in batch 96: 0.114853/0.230759loss in batch 97: 0.135849/0.229797loss in batch 98: 0.426666/0.231766loss in batch 99: 0.347824/0.232925loss in batch 100: 0.318161/0.23378loss in batch 101: 0.0933533/0.232407loss in batch 102: 0.0566864/0.230682loss in batch 103: 0.10524/0.229492loss in batch 104: 0.130814/0.228561loss in batch 105: 0.168579/0.227982loss in batch 106: 0.980865/0.235016loss in batch 107: 0.464493/0.237152loss in batch 108: 0.320786/0.2379loss in batch 109: 0.224838/0.237793loss in batch 110: 0.212723/0.237564loss in batch 111: 0.11882/0.236511loss in batch 112: 0.156967/0.235809loss in batch 113: 0.107681/0.234665loss in batch 114: 0.0677643/0.233231loss in batch 115: 0.290131/0.233719loss in batch 116: 0.154999/0.233047loss in batch 117: 0.0530701/0.231522loss in batch 118: 0.209579/0.231339loss in batch 119: 0.905151/0.236954loss in batch 120: 0.0432434/0.235352loss in batch 121: 0.496185/0.237488loss in batch 122: 0.162064/0.236877loss in batch 123: 0.0453949/0.235336loss in batch 124: 0.140945/0.234573loss in batch 125: 0.056488/0.23317loss in batch 126: 0.308502/0.233749loss in batch 127: 0.0742035/0.232513loss in batch 128: 0.146271/0.231842loss in batch 129: 0.139481/0.23114loss in batch 130: 0.157211/0.23056loss in batch 131: 0.0620117/0.229294loss in batch 132: 0.0518341/0.227951loss in batch 133: 0.0433197/0.226578loss in batch 134: 0.0893097/0.225571loss in batch 135: 0.0926819/0.224579loss in batch 136: 0.119186/0.223816loss in batch 137: 0.00437927/0.222229loss in batch 138: 0.633347/0.225174loss in batch 139: 0.0858154/0.224197loss in batch 140: 0.0332947/0.222839loss in batch 141: 0.348999/0.223724loss in batch 142: 0.0795746/0.222702loss in batch 143: 0.486649/0.224548loss in batch 144: 0.108871/0.22374loss in batch 145: 0.166183/0.223358loss in batch 146: 0.337006/0.224121loss in batch 147: 0.106552/0.223328loss in batch 148: 0.0494843/0.222168loss in batch 149: 0.108383/0.221405loss in batch 150: 0.0290375/0.220123loss in batch 151: 0.313599/0.220749loss in batch 152: 1.13152/0.2267loss in batch 153: 0.28656/0.227097loss in batch 154: 0.0805817/0.226135loss in batch 155: 0.183182/0.225876loss in batch 156: 0.153381/0.225403loss in batch 157: 0.172287/0.225067loss in batch 158: 0.110962/0.22435loss in batch 159: 0.0484924/0.223251loss in batch 160: 0.1138/0.22258loss in batch 161: 0.617462/0.225021loss in batch 162: 0.566116/0.227097loss in batch 163: 0.0485229/0.226028loss in batch 164: 0.0962067/0.225235loss in batch 165: 0.0978241/0.224472loss in batch 166: 0.217697/0.224411loss in batch 167: 0.330429/0.225052loss in batch 168: 0.117386/0.224411loss in batch 169: 0.146805/0.223953loss in batch 170: 0.243591/0.224075loss in batch 171: 0.152283/0.223663loss in batch 172: 0.17923/0.223404loss in batch 173: 0.54892/0.225266loss in batch 174: 0.158173/0.224884loss in batch 175: 0.125137/0.224319loss in batch 176: 0.278961/0.224625loss in batch 177: 0.100296/0.223923loss in batch 178: 0.131104/0.223419loss in batch 179: 0.0459442/0.222427loss in batch 180: 0.0934143/0.22171loss in batch 181: 0.319168/0.22226loss in batch 182: 0.140045/0.221802loss in batch 183: 0.734802/0.224579loss in batch 184: 0.69133/0.227097loss in batch 185: 0.0964813/0.226395loss in batch 186: 0.213852/0.226334loss in batch 187: 0.694962/0.228836loss in batch 188: 0.180756/0.228577loss in batch 189: 0.648697/0.230789loss in batch 190: 0.0401001/0.229782loss in batch 191: 0.801559/0.232773loss in batch 192: 0.132141/0.232239loss in batch 193: 0.190994/0.23204loss in batch 194: 0.154144/0.231644loss in batch 195: 0.0276337/0.230606loss in batch 196: 0.169739/0.230286loss in batch 197: 0.0665436/0.229462loss in batch 198: 0.0305634/0.228455loss in batch 199: 0.0361938/0.227493loss in batch 200: 0.0844116/0.226776loss in batch 201: 0.158432/0.226456loss in batch 202: 0.303452/0.226822loss in batch 203: 0.11264/0.226273loss in batch 204: 0.0609131/0.225464loss in batch 205: 0.0134888/0.224426loss in batch 206: 0.42926/0.225418loss in batch 207: 0.118011/0.224915loss in batch 208: 0.0487823/0.22406loss in batch 209: 0.0454712/0.223221loss in batch 210: 0.948303/0.226654loss in batch 211: 0.217834/0.226608loss in batch 212: 0.469788/0.227753
done with epoch 5
train_acc: 0.922535 (393/426)
test loss: 0.469788
acc: 0.93007 (133/143)
loss in batch 0: 0.0791168/0.0791168loss in batch 1: 0.0533295/0.0662231loss in batch 2: 0.558807/0.230408loss in batch 3: 0.0307922/0.180511loss in batch 4: 0.00401306/0.145203loss in batch 5: 0.0914307/0.136261loss in batch 6: 0.0582123/0.125092loss in batch 7: 0.107452/0.122894loss in batch 8: 0.307541/0.143417loss in batch 9: 0.225204/0.151596loss in batch 10: 0.480362/0.181488loss in batch 11: 0.705917/0.225189loss in batch 12: 0.117935/0.216934loss in batch 13: 0.14241/0.211609loss in batch 14: 0.210785/0.211548loss in batch 15: 0.696716/0.241882loss in batch 16: 0.381744/0.250107loss in batch 17: 0.10083/0.241806loss in batch 18: 0.0518646/0.231812loss in batch 19: 0.0926971/0.224869loss in batch 20: 0.132568/0.220474loss in batch 21: 0.392471/0.228287loss in batch 22: 0.114471/0.223328loss in batch 23: 0.171631/0.221191loss in batch 24: 0.302368/0.224426loss in batch 25: 0.419128/0.231918loss in batch 26: 0.496689/0.24173loss in batch 27: 0.18251/0.239609loss in batch 28: 0.098175/0.234726loss in batch 29: 0.415161/0.240753loss in batch 30: 0.404556/0.246017loss in batch 31: 0.205338/0.244766loss in batch 32: 0.111984/0.240723loss in batch 33: 0.0606537/0.235428loss in batch 34: 0.398224/0.240097loss in batch 35: 0.166412/0.238037loss in batch 36: 0.0450439/0.232819loss in batch 37: 0.294876/0.234467loss in batch 38: 0.0375977/0.229416loss in batch 39: 0.19339/0.228516loss in batch 40: 1.30698/0.254822loss in batch 41: 0.151169/0.25235loss in batch 42: 0.214935/0.25148loss in batch 43: 0.119507/0.248474loss in batch 44: 0.0784912/0.244705loss in batch 45: 0.0849457/0.241226loss in batch 46: 1.04558/0.258347loss in batch 47: 0.0784149/0.254593loss in batch 48: 0.0551758/0.250519loss in batch 49: 0.343521/0.25238loss in batch 50: 0.147415/0.25032loss in batch 51: 0.0969086/0.247375loss in batch 52: 0.0267944/0.24321loss in batch 53: 0.0354462/0.239365loss in batch 54: 0.183716/0.238342loss in batch 55: 0.108612/0.236038loss in batch 56: 0.334549/0.237762loss in batch 57: 0.0387268/0.234344loss in batch 58: 0.271011/0.234955loss in batch 59: 0.107712/0.232834loss in batch 60: 0.331589/0.234451loss in batch 61: 0.0899506/0.232117loss in batch 62: 0.304733/0.233276loss in batch 63: 0.537598/0.238022loss in batch 64: 0.123367/0.236267loss in batch 65: 0.238403/0.236298loss in batch 66: 0.175262/0.235382loss in batch 67: 0.11293/0.233582loss in batch 68: 0.233109/0.233582loss in batch 69: 0.0436249/0.230865loss in batch 70: 0.113419/0.229218loss in batch 71: 0.0633545/0.226913loss in batch 72: 0.0849762/0.22496loss in batch 73: 0.065567/0.222809loss in batch 74: 0.089798/0.221024loss in batch 75: 0.0830231/0.219223loss in batch 76: 0.0496979/0.217026loss in batch 77: 0.0914917/0.215408loss in batch 78: 0.0960999/0.213898loss in batch 79: 0.73381/0.220398loss in batch 80: 0.231888/0.220551loss in batch 81: 0.119843/0.219315loss in batch 82: 0.188736/0.218948loss in batch 83: 0.228836/0.21907loss in batch 84: 0.356171/0.220673loss in batch 85: 0.5354/0.224335loss in batch 86: 0.28389/0.225021loss in batch 87: 0.554779/0.228775loss in batch 88: 0.0911713/0.227219loss in batch 89: 0.117706/0.226013loss in batch 90: 0.392136/0.227829loss in batch 91: 0.169128/0.227188loss in batch 92: 0.172867/0.226593loss in batch 93: 0.437454/0.228851loss in batch 94: 0.187592/0.228409loss in batch 95: 0.540955/0.231674loss in batch 96: 0.0639343/0.229935loss in batch 97: 0.112549/0.228729loss in batch 98: 0.1427/0.227875loss in batch 99: 0.332626/0.228912loss in batch 100: 0.415588/0.230759loss in batch 101: 0.317139/0.231613loss in batch 102: 0.151764/0.230835loss in batch 103: 0.23822/0.230911loss in batch 104: 0.0166473/0.228882loss in batch 105: 0.033905/0.22702loss in batch 106: 0.0148163/0.225052loss in batch 107: 0.329041/0.226013loss in batch 108: 0.105515/0.224899loss in batch 109: 0.112457/0.223877loss in batch 110: 0.303772/0.224594loss in batch 111: 0.196579/0.224365loss in batch 112: 0.0999298/0.223251loss in batch 113: 0.131287/0.222443loss in batch 114: 0.0726471/0.221146loss in batch 115: 0.0336609/0.219528loss in batch 116: 0.377838/0.220886loss in batch 117: 0.0369263/0.219315loss in batch 118: 0.0906982/0.218246loss in batch 119: 0.134109/0.217529loss in batch 120: 0.0652161/0.216278loss in batch 121: 0.154938/0.215775loss in batch 122: 0.391235/0.217209loss in batch 123: 0.397369/0.218658loss in batch 124: 0.0690308/0.217468loss in batch 125: 0.713104/0.221405loss in batch 126: 0.307373/0.222076loss in batch 127: 0.0756073/0.220917loss in batch 128: 0.401276/0.222321loss in batch 129: 0.0399628/0.220932loss in batch 130: 0.0713501/0.219788loss in batch 131: 0.0471191/0.218475loss in batch 132: 0.0726318/0.217377loss in batch 133: 0.111908/0.216583loss in batch 134: 0.206833/0.216507loss in batch 135: 0.106628/0.215714loss in batch 136: 0.400009/0.217056loss in batch 137: 0.0696259/0.215988loss in batch 138: 0.170013/0.215652loss in batch 139: 0.114868/0.214935loss in batch 140: 0.156326/0.214523loss in batch 141: 0.864059/0.219086loss in batch 142: 0.0268555/0.217743loss in batch 143: 0.112366/0.21701loss in batch 144: 0.130753/0.216415loss in batch 145: 0.392792/0.217636loss in batch 146: 0.0513611/0.216492loss in batch 147: 0.0480347/0.215363loss in batch 148: 0.489182/0.217194loss in batch 149: 0.608582/0.219803loss in batch 150: 0.0652161/0.218781loss in batch 151: 0.00790405/0.217392loss in batch 152: 0.208649/0.217331loss in batch 153: 0.227676/0.217407loss in batch 154: 0.190964/0.217224loss in batch 155: 0.223465/0.21727loss in batch 156: 0.185608/0.217087loss in batch 157: 0.696503/0.220108loss in batch 158: 0.244827/0.220261loss in batch 159: 0.208267/0.220184loss in batch 160: 1.00726/0.225082loss in batch 161: 0.0448761/0.223969loss in batch 162: 0.225571/0.223969loss in batch 163: 0.0887756/0.223145loss in batch 164: 0.281097/0.223495loss in batch 165: 0.353241/0.224289loss in batch 166: 0.106812/0.223572loss in batch 167: 0.906738/0.227646loss in batch 168: 0.293411/0.228027loss in batch 169: 0.0599823/0.227051loss in batch 170: 0.0477142/0.225998loss in batch 171: 0.134094/0.225464loss in batch 172: 0.0270081/0.224319loss in batch 173: 0.135712/0.223801loss in batch 174: 0.102219/0.223114loss in batch 175: 0.0170441/0.221939loss in batch 176: 0.34285/0.222626loss in batch 177: 0.0994415/0.221924loss in batch 178: 0.105865/0.221283loss in batch 179: 0.4095/0.222336loss in batch 180: 0.286209/0.222687loss in batch 181: 0.325058/0.223251loss in batch 182: 0.0720367/0.222427loss in batch 183: 0.164658/0.222107loss in batch 184: 0.0607758/0.221237loss in batch 185: 0.364929/0.222loss in batch 186: 0.495361/0.223465loss in batch 187: 0.188965/0.223282loss in batch 188: 0.354538/0.223984loss in batch 189: 0.0402527/0.223007loss in batch 190: 0.120468/0.222473loss in batch 191: 0.147583/0.222076loss in batch 192: 0.072876/0.221313loss in batch 193: 0.0864258/0.220612loss in batch 194: 0.0906982/0.219955loss in batch 195: 0.0961914/0.219315loss in batch 196: 0.0647125/0.218536loss in batch 197: 0.0601044/0.217743loss in batch 198: 0.148911/0.217392loss in batch 199: 0.363785/0.218124loss in batch 200: 0.559814/0.219818loss in batch 201: 0.062973/0.21904loss in batch 202: 0.0389404/0.218155loss in batch 203: 0.417679/0.219131loss in batch 204: 0.340454/0.219727loss in batch 205: 0.142059/0.219345loss in batch 206: 0.131287/0.218918loss in batch 207: 0.0539398/0.218124loss in batch 208: 0.0914917/0.217529loss in batch 209: 0.0951385/0.216934loss in batch 210: 0.424377/0.217911loss in batch 211: 0.183395/0.217758loss in batch 212: 0.0611725/0.217026
done with epoch 6
train_acc: 0.93662 (399/426)
test loss: 0.0611725
acc: 0.937063 (134/143)
loss in batch 0: 0.0668945/0.0668945loss in batch 1: 0.155212/0.111053loss in batch 2: 0.482315/0.234802loss in batch 3: 0.0665741/0.192749loss in batch 4: 0.0552673/0.165253loss in batch 5: 0.104172/0.155075loss in batch 6: 0.0216217/0.136002loss in batch 7: 0.014801/0.120865loss in batch 8: 0.0996094/0.1185loss in batch 9: 0.53833/0.160477loss in batch 10: 0.136383/0.158279loss in batch 11: 0.113617/0.154572loss in batch 12: 0.076828/0.148575loss in batch 13: 0.0541229/0.14183loss in batch 14: 0.6642/0.176666loss in batch 15: 0.699524/0.209335loss in batch 16: 0.00999451/0.197617loss in batch 17: 0.0957947/0.191956loss in batch 18: 0.0521851/0.184601loss in batch 19: 0.254593/0.188095loss in batch 20: 0.0674286/0.182358loss in batch 21: 0.361374/0.190491loss in batch 22: 0.238602/0.192581loss in batch 23: 0.408966/0.201599loss in batch 24: 0.718185/0.22226loss in batch 25: 0.246429/0.223206loss in batch 26: 0.845169/0.246231loss in batch 27: 0.429153/0.252762loss in batch 28: 0.13179/0.248581loss in batch 29: 0.239014/0.248276loss in batch 30: 0.206818/0.246933loss in batch 31: 0.250717/0.247055loss in batch 32: 0.0452423/0.240936loss in batch 33: 0.120789/0.237396loss in batch 34: 0.250504/0.237778loss in batch 35: 0.413544/0.242661loss in batch 36: 0.109375/0.239059loss in batch 37: 0.0559082/0.234238loss in batch 38: 0.0921478/0.230606loss in batch 39: 0.356613/0.233749loss in batch 40: 0.144623/0.231567loss in batch 41: 0.0367584/0.226944loss in batch 42: 0.17215/0.225662loss in batch 43: 0.0983887/0.222763loss in batch 44: 0.103149/0.220123loss in batch 45: 0.193802/0.219543loss in batch 46: 0.124664/0.217514loss in batch 47: 0.357605/0.220444loss in batch 48: 0.136383/0.218719loss in batch 49: 0.0681/0.215714loss in batch 50: 0.0506287/0.212463loss in batch 51: 0.0734711/0.209793loss in batch 52: 0.0329437/0.206467loss in batch 53: 0.166687/0.205719loss in batch 54: 0.710815/0.21492loss in batch 55: 0.26828/0.215851loss in batch 56: 0.207245/0.215714loss in batch 57: 0.0552368/0.212936loss in batch 58: 0.203384/0.212784loss in batch 59: 0.050293/0.210068loss in batch 60: 0.0245361/0.207031loss in batch 61: 0.0735474/0.20488loss in batch 62: 0.348969/0.207169loss in batch 63: 0.250809/0.20784loss in batch 64: 0.250717/0.208496loss in batch 65: 0.196915/0.208328loss in batch 66: 0.0907593/0.206573loss in batch 67: 0.144043/0.205658loss in batch 68: 0.273575/0.206635loss in batch 69: 0.103073/0.205154loss in batch 70: 0.45343/0.208664loss in batch 71: 0.0936279/0.207062loss in batch 72: 0.0551147/0.204971loss in batch 73: 0.452118/0.208328loss in batch 74: 0.253036/0.208923loss in batch 75: 0.187241/0.208633loss in batch 76: 0.39267/0.211029loss in batch 77: 0.408264/0.213547loss in batch 78: 0.0803986/0.211868loss in batch 79: 0.134247/0.210892loss in batch 80: 0.0260925/0.208603loss in batch 81: 0.0825958/0.207077loss in batch 82: 0.101959/0.205811loss in batch 83: 0.0532532/0.203995loss in batch 84: 0.0532532/0.202209loss in batch 85: 0.0292664/0.200211loss in batch 86: 0.237427/0.200638loss in batch 87: 0.287354/0.20163loss in batch 88: 0.275772/0.202454loss in batch 89: 0.301819/0.203552loss in batch 90: 0.0410461/0.201767loss in batch 91: 0.0428314/0.200043loss in batch 92: 0.252991/0.200623loss in batch 93: 0.0975952/0.199524loss in batch 94: 0.0861969/0.198318loss in batch 95: 0.0908356/0.197205loss in batch 96: 1.06503/0.206161loss in batch 97: 0.144287/0.205521loss in batch 98: 0.372284/0.207199loss in batch 99: 0.382004/0.208954loss in batch 100: 0.0301819/0.207184loss in batch 101: 0.117722/0.206314loss in batch 102: 0.129593/0.205566loss in batch 103: 0.608322/0.209427loss in batch 104: 0.212906/0.209457loss in batch 105: 0.358887/0.210876loss in batch 106: 0.115814/0.209991loss in batch 107: 0.153488/0.209457loss in batch 108: 0.0527954/0.208038loss in batch 109: 0.156097/0.207565loss in batch 110: 0.056366/0.206192loss in batch 111: 0.0312042/0.20462loss in batch 112: 0.855606/0.210388loss in batch 113: 0.225998/0.210526loss in batch 114: 0.00622559/0.208755loss in batch 115: 0.087204/0.207703loss in batch 116: 0.119614/0.206955loss in batch 117: 0.215042/0.207031loss in batch 118: 0.149994/0.206543loss in batch 119: 0.048172/0.205215loss in batch 120: 0.0926208/0.204285loss in batch 121: 0.175064/0.204041loss in batch 122: 0.079834/0.203033loss in batch 123: 0.10524/0.202255loss in batch 124: 0.0188446/0.200775loss in batch 125: 0.4953/0.203125loss in batch 126: 0.101929/0.202332loss in batch 127: 0.0630646/0.201248loss in batch 128: 0.575195/0.204147loss in batch 129: 0.0317688/0.202805loss in batch 130: 0.0525208/0.20166loss in batch 131: 0.235977/0.20192loss in batch 132: 0.146805/0.201508loss in batch 133: 0.274246/0.202057loss in batch 134: 0.263184/0.202499loss in batch 135: 0.0690308/0.201523loss in batch 136: 0.0402069/0.200348loss in batch 137: 0.0758057/0.199448loss in batch 138: 0.65036/0.202698loss in batch 139: 0.379501/0.203949loss in batch 140: 0.0571442/0.202911loss in batch 141: 0.0221863/0.201645loss in batch 142: 0.0657043/0.200684loss in batch 143: 0.0562286/0.199677loss in batch 144: 0.331543/0.200592loss in batch 145: 0.197357/0.200577loss in batch 146: 0.209244/0.200623loss in batch 147: 0.0747986/0.199783loss in batch 148: 0.174545/0.199615loss in batch 149: 0.691269/0.202896loss in batch 150: 0.887848/0.207413loss in batch 151: 0.0959167/0.206696loss in batch 152: 0.0198059/0.205475loss in batch 153: 0.221786/0.205582loss in batch 154: 0.198624/0.205536loss in batch 155: 0.223877/0.205643loss in batch 156: 0.560379/0.207901loss in batch 157: 0.0416565/0.206863loss in batch 158: 0.0391235/0.205811loss in batch 159: 0.211136/0.205841loss in batch 160: 0.10144/0.205185loss in batch 161: 0.0756073/0.204391loss in batch 162: 0.0173035/0.203232loss in batch 163: 0.459167/0.204788loss in batch 164: 0.0768433/0.20401loss in batch 165: 1.09177/0.209381loss in batch 166: 0.0486603/0.208405loss in batch 167: 0.238373/0.208588loss in batch 168: 1.2637/0.214828loss in batch 169: 0.133118/0.21434loss in batch 170: 0.0885773/0.213608loss in batch 171: 0.0835724/0.21286loss in batch 172: 0.0865173/0.212128loss in batch 173: 0.0339203/0.211105loss in batch 174: 0.0615387/0.210251loss in batch 175: 0.0822296/0.209518loss in batch 176: 0.161514/0.209259loss in batch 177: 0.323288/0.2099loss in batch 178: 0.221451/0.209961loss in batch 179: 0.141876/0.209564loss in batch 180: 0.0341034/0.208603loss in batch 181: 0.1922/0.208511loss in batch 182: 0.10524/0.207962loss in batch 183: 0.75267/0.210922loss in batch 184: 0.0717316/0.210159loss in batch 185: 0.109055/0.20961loss in batch 186: 0.0403748/0.20871loss in batch 187: 0.368927/0.209564loss in batch 188: 0.217911/0.20961loss in batch 189: 0.194504/0.209534loss in batch 190: 0.256134/0.209778loss in batch 191: 0.105179/0.209229loss in batch 192: 0.0570679/0.20845loss in batch 193: 0.0874634/0.207825loss in batch 194: 0.886658/0.211288loss in batch 195: 0.0409851/0.210419loss in batch 196: 0.497055/0.211884loss in batch 197: 0.360153/0.212631loss in batch 198: 0.0499268/0.211823loss in batch 199: 0.0624695/0.211075loss in batch 200: 0.0574799/0.210297loss in batch 201: 0.379532/0.211151loss in batch 202: 0.0625916/0.210419loss in batch 203: 0.0845947/0.209793loss in batch 204: 0.0520477/0.209015loss in batch 205: 0.250702/0.209229loss in batch 206: 0.0791473/0.208603loss in batch 207: 0.518707/0.210083loss in batch 208: 0.119675/0.209656loss in batch 209: 0.148483/0.209366loss in batch 210: 0.0834198/0.208755loss in batch 211: 0.105789/0.208282loss in batch 212: 0.25087/0.208481
done with epoch 7
train_acc: 0.931925 (397/426)
test loss: 0.25087
acc: 0.937063 (134/143)
loss in batch 0: 0.0744476/0.0744476loss in batch 1: 0.02034/0.0473938loss in batch 2: 0.062851/0.0525513loss in batch 3: 0.0577698/0.0538483loss in batch 4: 0.297119/0.102509loss in batch 5: 0.395859/0.151398loss in batch 6: 0.132324/0.148666loss in batch 7: 0.106003/0.143341loss in batch 8: 0.0526123/0.133255loss in batch 9: 0.196106/0.139542loss in batch 10: 0.480026/0.170502loss in batch 11: 0.0587921/0.161194loss in batch 12: 0.10762/0.157059loss in batch 13: 0.106308/0.153442loss in batch 14: 0.0871429/0.149017loss in batch 15: 0.193405/0.151794loss in batch 16: 0.694168/0.183701loss in batch 17: 0.223328/0.185898loss in batch 18: 0.0926056/0.181loss in batch 19: 0.766434/0.210251loss in batch 20: 0.348419/0.216827loss in batch 21: 0.0620117/0.209808loss in batch 22: 0.136032/0.206604loss in batch 23: 0.0360718/0.199493loss in batch 24: 0.261185/0.20195loss in batch 25: 0.0423279/0.195816loss in batch 26: 0.219345/0.196686loss in batch 27: 0.0741882/0.192322loss in batch 28: 0.618546/0.207016loss in batch 29: 0.0657349/0.202301loss in batch 30: 0.0475769/0.197311loss in batch 31: 0.0714722/0.193375loss in batch 32: 0.175049/0.192825loss in batch 33: 0.0284119/0.187988loss in batch 34: 0.0725555/0.184692loss in batch 35: 0.942001/0.205719loss in batch 36: 0.412292/0.211319loss in batch 37: 0.220795/0.211563loss in batch 38: 0.195221/0.211136loss in batch 39: 0.0570374/0.207291loss in batch 40: 0.272186/0.208878loss in batch 41: 0.209778/0.208893loss in batch 42: 0.20314/0.208755loss in batch 43: 0.181641/0.208145loss in batch 44: 0.963501/0.22493loss in batch 45: 0.154953/0.223404loss in batch 46: 0.385895/0.226868loss in batch 47: 0.286697/0.228119loss in batch 48: 0.0766449/0.225021loss in batch 49: 0.132599/0.223175loss in batch 50: 0.0666351/0.220093loss in batch 51: 0.0488281/0.216797loss in batch 52: 0.179596/0.21611loss in batch 53: 0.0548401/0.21312loss in batch 54: 0.0574036/0.210297loss in batch 55: 0.131439/0.208878loss in batch 56: 0.0685577/0.206421loss in batch 57: 0.00350952/0.202927loss in batch 58: 0.014679/0.199722loss in batch 59: 0.129608/0.198563loss in batch 60: 0.182037/0.198288loss in batch 61: 0.103363/0.196747loss in batch 62: 0.45784/0.200897loss in batch 63: 0.373383/0.203598loss in batch 64: 0.131302/0.202484loss in batch 65: 0.179276/0.202133loss in batch 66: 0.032074/0.1996loss in batch 67: 0.678375/0.206635loss in batch 68: 0.593765/0.21225loss in batch 69: 0.399765/0.21492loss in batch 70: 0.0302277/0.212326loss in batch 71: 0.395538/0.214859loss in batch 72: 0.00505066/0.212006loss in batch 73: 0.108429/0.210602loss in batch 74: 0.064743/0.208649loss in batch 75: 0.231583/0.208954loss in batch 76: 0.266312/0.209702loss in batch 77: 0.399628/0.212128loss in batch 78: 0.105835/0.210785loss in batch 79: 0.306061/0.211975loss in batch 80: 0.119949/0.210831loss in batch 81: 0.156982/0.21019loss in batch 82: 0.0904236/0.20874loss in batch 83: 0.42128/0.211273loss in batch 84: 0.518433/0.21489loss in batch 85: 0.214996/0.21489loss in batch 86: 0.131561/0.213928loss in batch 87: 0.109879/0.212753loss in batch 88: 0.102463/0.211517loss in batch 89: 0.0378265/0.209579loss in batch 90: 0.0895691/0.208252loss in batch 91: 0.0726776/0.206787loss in batch 92: 0.0404205/0.205002loss in batch 93: 0.113922/0.204041loss in batch 94: 0.453629/0.206665loss in batch 95: 0.0575256/0.205093loss in batch 96: 0.335922/0.206451loss in batch 97: 0.0598145/0.204956loss in batch 98: 0.0139008/0.203033loss in batch 99: 0.0436554/0.201431loss in batch 100: 0.409546/0.203491loss in batch 101: 0.467682/0.206085loss in batch 102: 0.0783234/0.204849loss in batch 103: 0.0402069/0.203247loss in batch 104: 0.0230713/0.201538loss in batch 105: 0.0211945/0.199844loss in batch 106: 0.35228/0.201263loss in batch 107: 0.0901642/0.200241loss in batch 108: 0.0536957/0.198883loss in batch 109: 0.0176849/0.19725loss in batch 110: 0.157883/0.196884loss in batch 111: 0.486084/0.199463loss in batch 112: 0.0118713/0.197815loss in batch 113: 0.109863/0.197052loss in batch 114: 0.133667/0.196487loss in batch 115: 0.0563812/0.195282loss in batch 116: 0.175247/0.195114loss in batch 117: 0.108551/0.194382loss in batch 118: 0.232971/0.194702loss in batch 119: 0.0377045/0.19339loss in batch 120: 0.239716/0.193787loss in batch 121: 0.481735/0.196136loss in batch 122: 0.0430145/0.194885loss in batch 123: 0.118958/0.194275loss in batch 124: 0.0568848/0.193176loss in batch 125: 0.120361/0.192596loss in batch 126: 0.277939/0.193283loss in batch 127: 0.094223/0.192505loss in batch 128: 0.0862732/0.191681loss in batch 129: 0.106079/0.19101loss in batch 130: 0.359756/0.192307loss in batch 131: 0.556656/0.195053loss in batch 132: 0.149506/0.194717loss in batch 133: 0.0333405/0.193527loss in batch 134: 0.117996/0.192963loss in batch 135: 0.090332/0.1922loss in batch 136: 0.216248/0.192383loss in batch 137: 0.271042/0.192947loss in batch 138: 0.54512/0.19548loss in batch 139: 0.427032/0.197144loss in batch 140: 0.122101/0.196609loss in batch 141: 0.0307312/0.195435loss in batch 142: 0.165253/0.195221loss in batch 143: 0.0548096/0.194244loss in batch 144: 0.650665/0.197403loss in batch 145: 0.458176/0.199173loss in batch 146: 0.0453339/0.198135loss in batch 147: 0.00816345/0.196854loss in batch 148: 0.0902252/0.196136loss in batch 149: 0.204636/0.196198loss in batch 150: 0.270828/0.196686loss in batch 151: 0.126648/0.196228loss in batch 152: 0.124451/0.195755loss in batch 153: 0.116241/0.195251loss in batch 154: 0.0898895/0.19455loss in batch 155: 0.0439758/0.193604loss in batch 156: 0.115189/0.1931loss in batch 157: 0.177597/0.193008loss in batch 158: 0.0869598/0.192322loss in batch 159: 0.0485229/0.191437loss in batch 160: 0.208984/0.191544loss in batch 161: 0.0517273/0.190689loss in batch 162: 0.0522919/0.189835loss in batch 163: 0.0342865/0.188889loss in batch 164: 0.0623474/0.188126loss in batch 165: 0.154587/0.187912loss in batch 166: 0.145004/0.187653loss in batch 167: 0.0937347/0.187103loss in batch 168: 0.0653992/0.186386loss in batch 169: 0.725952/0.189545loss in batch 170: 0.0776367/0.188904loss in batch 171: 0.0923157/0.188339loss in batch 172: 0.428848/0.189728loss in batch 173: 1.46765/0.197067loss in batch 174: 0.0505371/0.196243loss in batch 175: 0.442505/0.197632loss in batch 176: 0.497696/0.199326loss in batch 177: 0.0306396/0.19838loss in batch 178: 0.183441/0.198288loss in batch 179: 0.0551758/0.197495loss in batch 180: 0.194031/0.197495loss in batch 181: 0.0283356/0.196548loss in batch 182: 0.130051/0.196182loss in batch 183: 0.038559/0.195328loss in batch 184: 0.364044/0.196243loss in batch 185: 0.231277/0.196442loss in batch 186: 0.0723572/0.19577loss in batch 187: 0.626266/0.198059loss in batch 188: 0.410553/0.199188loss in batch 189: 0.161758/0.19899loss in batch 190: 0.0471039/0.198196loss in batch 191: 0.0111389/0.19722loss in batch 192: 0.227798/0.197372loss in batch 193: 0.54451/0.199158loss in batch 194: 0.460846/0.2005loss in batch 195: 0.0139313/0.199554loss in batch 196: 0.0923309/0.199005loss in batch 197: 0.0453339/0.198242loss in batch 198: 0.105209/0.197769loss in batch 199: 0.289948/0.198227loss in batch 200: 0.0147858/0.197311loss in batch 201: 0.048233/0.196579loss in batch 202: 0.0679016/0.195938loss in batch 203: 0.167969/0.195801loss in batch 204: 0.0198059/0.194946loss in batch 205: 0.218964/0.195053loss in batch 206: 0.862488/0.198303loss in batch 207: 0.0814514/0.197723loss in batch 208: 0.316574/0.198303loss in batch 209: 0.0509186/0.197586loss in batch 210: 0.292191/0.198044loss in batch 211: 0.0982971/0.197586loss in batch 212: 0.156357/0.197388
done with epoch 8
train_acc: 0.931925 (397/426)
test loss: 0.156357
acc: 0.937063 (134/143)
loss in batch 0: 0.090332/0.090332loss in batch 1: 0.308533/0.199432loss in batch 2: 1.40462/0.601166loss in batch 3: 0.197128/0.500153loss in batch 4: 0.0981903/0.419754loss in batch 5: 0.0674896/0.361038loss in batch 6: 0.124191/0.327209loss in batch 7: 0.334702/0.328156loss in batch 8: 0.0124512/0.293076loss in batch 9: 0.256149/0.289383loss in batch 10: 0.0964661/0.271835loss in batch 11: 0.0511475/0.253448loss in batch 12: 0.0788116/0.240021loss in batch 13: 0.206268/0.237595loss in batch 14: 0.0219574/0.223221loss in batch 15: 0.142059/0.218155loss in batch 16: 0.214951/0.217957loss in batch 17: 0.0646057/0.209442loss in batch 18: 0.591583/0.229553loss in batch 19: 0.0501099/0.220596loss in batch 20: 0.131973/0.216354loss in batch 21: 0.0499878/0.208801loss in batch 22: 0.0688324/0.202728loss in batch 23: 0.842422/0.229385loss in batch 24: 0.119476/0.224976loss in batch 25: 0.0258026/0.217316loss in batch 26: 0.39299/0.223831loss in batch 27: 0.0870514/0.218948loss in batch 28: 0.0453339/0.212952loss in batch 29: 0.241287/0.213898loss in batch 30: 0.0532532/0.20871loss in batch 31: 0.00579834/0.202362loss in batch 32: 0.595367/0.214294loss in batch 33: 0.213364/0.214264loss in batch 34: 0.388367/0.219238loss in batch 35: 0.189011/0.218384loss in batch 36: 0.0386505/0.213531loss in batch 37: 0.0369568/0.208893loss in batch 38: 0.0775452/0.205521loss in batch 39: 0.393936/0.21022loss in batch 40: 0.0378876/0.206024loss in batch 41: 0.0778046/0.202972loss in batch 42: 0.0769196/0.200043loss in batch 43: 0.36528/0.203796loss in batch 44: 0.0600586/0.200592loss in batch 45: 0.0531158/0.197388loss in batch 46: 0.0437164/0.194122loss in batch 47: 0.0816803/0.191788loss in batch 48: 0.0940704/0.189789loss in batch 49: 0.0560913/0.187119loss in batch 50: 0.223984/0.187836loss in batch 51: 0.711395/0.197906loss in batch 52: 0.287186/0.1996loss in batch 53: 0.211487/0.199814loss in batch 54: 0.373688/0.202972loss in batch 55: 0.640793/0.210785loss in batch 56: 0.0834503/0.208557loss in batch 57: 0.00686646/0.205078loss in batch 58: 0.0971069/0.203247loss in batch 59: 0.24115/0.203873loss in batch 60: 0.355377/0.206375loss in batch 61: 0.0868683/0.204437loss in batch 62: 0.12912/0.203232loss in batch 63: 0.0745697/0.201233loss in batch 64: 0.0622711/0.199097loss in batch 65: 0.337952/0.201202loss in batch 66: 0.1306/0.20015loss in batch 67: 0.436737/0.203629loss in batch 68: 0.0361938/0.201202loss in batch 69: 0.284119/0.202377loss in batch 70: 0.116989/0.201187loss in batch 71: 0.10527/0.199844loss in batch 72: 0.287628/0.20105loss in batch 73: 0.0726318/0.19931loss in batch 74: 0.33432/0.201111loss in batch 75: 0.0808258/0.199539loss in batch 76: 0.0734711/0.197891loss in batch 77: 0.0265656/0.195694loss in batch 78: 0.119995/0.194748loss in batch 79: 0.361649/0.196823loss in batch 80: 0.0301208/0.194763loss in batch 81: 0.0673523/0.193207loss in batch 82: 0.034256/0.191299loss in batch 83: 0.168243/0.191025loss in batch 84: 0.0219879/0.189026loss in batch 85: 0.170761/0.188828loss in batch 86: 0.0539246/0.187271loss in batch 87: 0.133957/0.186661loss in batch 88: 0.209625/0.18692loss in batch 89: 0.68689/0.192474loss in batch 90: 0.239258/0.192993loss in batch 91: 0.574554/0.197144loss in batch 92: 0.0718384/0.195801loss in batch 93: 0.12468/0.195038loss in batch 94: 0.554291/0.198822loss in batch 95: 0.138077/0.198196loss in batch 96: 0.520905/0.201508loss in batch 97: 0.182556/0.201309loss in batch 98: 0.0280609/0.19957loss in batch 99: 0.0325165/0.197891loss in batch 100: 0.116394/0.197083loss in batch 101: 0.188171/0.197006loss in batch 102: 0.0591125/0.195663loss in batch 103: 0.60849/0.199631loss in batch 104: 0.0640259/0.198334loss in batch 105: 0.0306702/0.196762loss in batch 106: 0.148117/0.196304loss in batch 107: 0.525467/0.199356loss in batch 108: 0.2715/0.200012loss in batch 109: 0.174683/0.199783loss in batch 110: 0.383682/0.201447loss in batch 111: 0.170334/0.201157loss in batch 112: 0.370331/0.202667loss in batch 113: 0.313416/0.203629loss in batch 114: 0.0284882/0.202103loss in batch 115: 0.0466003/0.20076loss in batch 116: 0.0445557/0.199432loss in batch 117: 0.0128479/0.197845loss in batch 118: 0.0209503/0.196365loss in batch 119: 0.486374/0.198776loss in batch 120: 0.276276/0.199432loss in batch 121: 0.0548096/0.198227loss in batch 122: 0.0695496/0.197205loss in batch 123: 0.0306702/0.195862loss in batch 124: 0.0255432/0.194489loss in batch 125: 0.0796814/0.193573loss in batch 126: 0.0575256/0.192505loss in batch 127: 0.295593/0.193314loss in batch 128: 0.490143/0.195618loss in batch 129: 0.220383/0.195801loss in batch 130: 0.192642/0.19577loss in batch 131: 0.249985/0.196198loss in batch 132: 0.0595551/0.19516loss in batch 133: 0.113266/0.19455loss in batch 134: 0.0984955/0.193848loss in batch 135: 0.0401459/0.192703loss in batch 136: 0.0399323/0.191605loss in batch 137: 0.186234/0.191559loss in batch 138: 0.0721436/0.190704loss in batch 139: 0.126389/0.190231loss in batch 140: 1.0295/0.196182loss in batch 141: 0.383987/0.19751loss in batch 142: 0.018692/0.196259loss in batch 143: 0.0811462/0.195465loss in batch 144: 0.0948944/0.194763loss in batch 145: 0.0361633/0.19368loss in batch 146: 0.181686/0.193588loss in batch 147: 0.0435791/0.192581loss in batch 148: 0.0250244/0.191467loss in batch 149: 0.0868225/0.190765loss in batch 150: 0.48877/0.192734loss in batch 151: 0.384521/0.194loss in batch 152: 0.0931702/0.193344loss in batch 153: 0.0615692/0.19249loss in batch 154: 0.0874176/0.191803loss in batch 155: 0.0811005/0.191101loss in batch 156: 0.0482941/0.190186loss in batch 157: 0.356964/0.191254loss in batch 158: 0.0943146/0.190628loss in batch 159: 0.0805054/0.189941loss in batch 160: 0.153473/0.189728loss in batch 161: 0.20845/0.189835loss in batch 162: 0.044693/0.18895loss in batch 163: 0.327469/0.189789loss in batch 164: 0.0402832/0.188889loss in batch 165: 0.113373/0.188431loss in batch 166: 0.136627/0.188126loss in batch 167: 0.12471/0.187744loss in batch 168: 0.0462799/0.186905loss in batch 169: 0.541214/0.18898loss in batch 170: 0.204834/0.189072loss in batch 171: 0.0350037/0.188187loss in batch 172: 0.234406/0.188461loss in batch 173: 0.0338898/0.187561loss in batch 174: 0.0455017/0.186752loss in batch 175: 0.335297/0.187592loss in batch 176: 0.0586853/0.186874loss in batch 177: 0.0390778/0.186035loss in batch 178: 0.0840912/0.185471loss in batch 179: 0.468811/0.187042loss in batch 180: 0.17572/0.186981loss in batch 181: 0.373947/0.188004loss in batch 182: 0.870453/0.191727loss in batch 183: 0.0796204/0.191116loss in batch 184: 0.0344086/0.190277loss in batch 185: 0.135544/0.189987loss in batch 186: 0.0349731/0.189148loss in batch 187: 0.107376/0.188721loss in batch 188: 0.0719452/0.18811loss in batch 189: 0.149261/0.187897loss in batch 190: 0.0641632/0.187241loss in batch 191: 0.15802/0.187103loss in batch 192: 0.0146027/0.186203loss in batch 193: 0.810623/0.189423loss in batch 194: 0.0324249/0.188614loss in batch 195: 0.124466/0.188293loss in batch 196: 0.219803/0.188446loss in batch 197: 0.306427/0.189041loss in batch 198: 0.0299225/0.188248loss in batch 199: 0.0823059/0.187714loss in batch 200: 1.08324/0.192169loss in batch 201: 0.32547/0.192825loss in batch 202: 0.0636902/0.1922loss in batch 203: 0.00764465/0.191299loss in batch 204: 0.156433/0.191116loss in batch 205: 0.22319/0.191269loss in batch 206: 0.323303/0.191925loss in batch 207: 0.457565/0.193192loss in batch 208: 0.351852/0.193954loss in batch 209: 0.0977631/0.193497loss in batch 210: 0.170807/0.193375loss in batch 211: 0.0841217/0.192871loss in batch 212: 0.0220032/0.192078
done with epoch 9
train_acc: 0.931925 (397/426)
test loss: 0.0220032
acc: 0.944056 (135/143)
loss in batch 0: 0.37941/0.37941loss in batch 1: 0.0718994/0.225647loss in batch 2: 0.153412/0.201569loss in batch 3: 0.407516/0.253052loss in batch 4: 0.0634766/0.215134loss in batch 5: 0.0428925/0.186432loss in batch 6: 0.0158539/0.162064loss in batch 7: 1.56099/0.336929loss in batch 8: 0.350418/0.338425loss in batch 9: 0.185165/0.323105loss in batch 10: 0.16655/0.308868loss in batch 11: 0.573425/0.330917loss in batch 12: 0.311295/0.329407loss in batch 13: 0.0345612/0.30835loss in batch 14: 0.0783844/0.293015loss in batch 15: 0.150391/0.284103loss in batch 16: 0.154709/0.276489loss in batch 17: 0.0700226/0.265015loss in batch 18: 0.264969/0.265015loss in batch 19: 0.105881/0.257065loss in batch 20: 0.0565033/0.247513loss in batch 21: 0.139511/0.242599loss in batch 22: 0.192291/0.240417loss in batch 23: 0.0472107/0.232361loss in batch 24: 0.0137787/0.223618loss in batch 25: 0.0409698/0.216583loss in batch 26: 0.221237/0.216782loss in batch 27: 0.402222/0.223404loss in batch 28: 0.143555/0.220642loss in batch 29: 0.292831/0.223053loss in batch 30: 0.144073/0.22049loss in batch 31: 0.0447998/0.215012loss in batch 32: 0.503555/0.223755loss in batch 33: 0.0691376/0.219208loss in batch 34: 0.117432/0.216293loss in batch 35: 0.0422821/0.211456loss in batch 36: 0.10611/0.208618loss in batch 37: 0.0912781/0.205521loss in batch 38: 0.0557861/0.201691loss in batch 39: 0.421555/0.207184loss in batch 40: 0.287262/0.209137loss in batch 41: 0.0709991/0.205841loss in batch 42: 1.22206/0.229477loss in batch 43: 0.110886/0.226791loss in batch 44: 0.0890808/0.223724loss in batch 45: 0.354782/0.226578loss in batch 46: 0.0639191/0.223114loss in batch 47: 0.166641/0.221939loss in batch 48: 0.416382/0.225906loss in batch 49: 0.0641174/0.222672loss in batch 50: 0.105972/0.220383loss in batch 51: 0.252304/0.221008loss in batch 52: 0.0335388/0.217453loss in batch 53: 0.00915527/0.213593loss in batch 54: 0.43158/0.217575loss in batch 55: 0.0639801/0.214813loss in batch 56: 0.28772/0.216095loss in batch 57: 0.0965729/0.21405loss in batch 58: 0.179916/0.21347loss in batch 59: 0.0453339/0.210648loss in batch 60: 0.0372772/0.207825loss in batch 61: 0.139587/0.206711loss in batch 62: 0.532578/0.211884loss in batch 63: 0.376328/0.214462loss in batch 64: 0.253174/0.215042loss in batch 65: 0.313675/0.216553loss in batch 66: 0.598099/0.222244loss in batch 67: 0.0415955/0.219574loss in batch 68: 0.0790863/0.217545loss in batch 69: 0.15509/0.216644loss in batch 70: 0.335785/0.218338loss in batch 71: 0.0542908/0.216049loss in batch 72: 0.0976563/0.214432loss in batch 73: 0.0507355/0.212219loss in batch 74: 0.0495148/0.210052loss in batch 75: 0.0634766/0.20813loss in batch 76: 0.789612/0.215683loss in batch 77: 0.0314484/0.213303loss in batch 78: 0.0434265/0.211151loss in batch 79: 0.426758/0.213852loss in batch 80: 0.0703735/0.212082loss in batch 81: 0.0934601/0.210648loss in batch 82: 0.040863/0.208588loss in batch 83: 0.0395355/0.206573loss in batch 84: 0.24762/0.207062loss in batch 85: 0.0681915/0.20546loss in batch 86: 0.0223083/0.203339loss in batch 87: 0.0964203/0.202133loss in batch 88: 0.167633/0.201736loss in batch 89: 0.0342102/0.199875loss in batch 90: 0.384918/0.201904loss in batch 91: 0.0201111/0.199936loss in batch 92: 0.000106812/0.1978loss in batch 93: 0.00930786/0.195786loss in batch 94: 0.00537109/0.193787loss in batch 95: 0.157196/0.19339loss in batch 96: 0.126358/0.192719loss in batch 97: 0.413223/0.194962loss in batch 98: 0.150803/0.194519loss in batch 99: 0.149094/0.194061loss in batch 100: 0.0435028/0.192566loss in batch 101: 1.09499/0.201401loss in batch 102: 0.314438/0.202515loss in batch 103: 0.0755768/0.201294loss in batch 104: 0.0594177/0.199936loss in batch 105: 0.0828857/0.198837loss in batch 106: 0.0433807/0.197372loss in batch 107: 0.347412/0.198761loss in batch 108: 0.0687103/0.197571loss in batch 109: 0.0816193/0.196533loss in batch 110: 0.539825/0.199615loss in batch 111: 0.321243/0.200699loss in batch 112: 0.0106964/0.19902loss in batch 113: 0.0610046/0.197815loss in batch 114: 0.128662/0.197205loss in batch 115: 0.0172119/0.195663loss in batch 116: 0.11557/0.194977loss in batch 117: 0.0955963/0.194138loss in batch 118: 0.213684/0.194305loss in batch 119: 0.319977/0.195343loss in batch 120: 0.045929/0.194122loss in batch 121: 0.374359/0.195587loss in batch 122: 0.0424042/0.194336loss in batch 123: 0.329987/0.195435loss in batch 124: 0.0455627/0.194229loss in batch 125: 0.0908813/0.19342loss in batch 126: 0.00460815/0.191925loss in batch 127: 0.0699768/0.190964loss in batch 128: 0.0656738/0.190002loss in batch 129: 0.564331/0.192886loss in batch 130: 0.114975/0.192291loss in batch 131: 0.392365/0.193802loss in batch 132: 0.0574646/0.19278loss in batch 133: 0.0213928/0.191498loss in batch 134: 0.0750427/0.190643loss in batch 135: 0.161682/0.19043loss in batch 136: 0.0572662/0.189453loss in batch 137: 0.393356/0.190933loss in batch 138: 0.85849/0.195724loss in batch 139: 0.228775/0.195969loss in batch 140: 0.0697937/0.195084loss in batch 141: 0.168945/0.194885loss in batch 142: 0.376816/0.196167loss in batch 143: 0.017868/0.194916loss in batch 144: 0.0450745/0.193893loss in batch 145: 0.0428009/0.192856loss in batch 146: 0.0475006/0.191864loss in batch 147: 0.0660248/0.191025loss in batch 148: 0.0528412/0.190094loss in batch 149: 0.0870667/0.189407loss in batch 150: 0.0281525/0.188324loss in batch 151: 0.0883789/0.187683loss in batch 152: 0.0556335/0.186813loss in batch 153: 0.0468292/0.185913loss in batch 154: 0.049408/0.185013loss in batch 155: 0.684616/0.188232loss in batch 156: 0.021286/0.187164loss in batch 157: 0.0138855/0.186066loss in batch 158: 0.0849762/0.185425loss in batch 159: 0.119904/0.185028loss in batch 160: 0.00532532/0.183914loss in batch 161: 0.70961/0.187149loss in batch 162: 0.0214539/0.186142loss in batch 163: 0.0234375/0.185135loss in batch 164: 0.177887/0.185104loss in batch 165: 0.0684204/0.184387loss in batch 166: 0.17923/0.184357loss in batch 167: 0.0377655/0.183487loss in batch 168: 0.125107/0.183136loss in batch 169: 0.0337219/0.182266loss in batch 170: 0.125015/0.181931loss in batch 171: 0.368866/0.183029loss in batch 172: 0.464005/0.184647loss in batch 173: 0.26033/0.185089loss in batch 174: 0.171036/0.184998loss in batch 175: 0.0474854/0.184219loss in batch 176: 0.0352173/0.183365loss in batch 177: 0.25563/0.183777loss in batch 178: 0.781372/0.187119loss in batch 179: 0.0962219/0.186615loss in batch 180: 0.0769043/0.18602loss in batch 181: 0.0868683/0.185471loss in batch 182: 0.0559235/0.184753loss in batch 183: 0.0712128/0.184143loss in batch 184: 0.316437/0.184845loss in batch 185: 0.285065/0.185394loss in batch 186: 0.11171/0.184998loss in batch 187: 0.0152283/0.184097loss in batch 188: 0.459991/0.185547loss in batch 189: 0.0866699/0.185028loss in batch 190: 0.145691/0.18483loss in batch 191: 0.328857/0.185577loss in batch 192: 0.0361176/0.184799loss in batch 193: 0.0489044/0.184097loss in batch 194: 0.444321/0.18544loss in batch 195: 0.106537/0.185028loss in batch 196: 0.307022/0.185654loss in batch 197: 0.257584/0.186005loss in batch 198: 0.0150299/0.185165loss in batch 199: 0.427933/0.186371loss in batch 200: 0.312576/0.186996loss in batch 201: 0.101807/0.186569loss in batch 202: 0.0259399/0.185791loss in batch 203: 0.293335/0.186325loss in batch 204: 0.0487823/0.185638loss in batch 205: 0.0930481/0.185196loss in batch 206: 0.53714/0.18689loss in batch 207: 0.0747986/0.186356loss in batch 208: 0.206863/0.186447loss in batch 209: 0.0405884/0.18576loss in batch 210: 0.0331421/0.185028loss in batch 211: 0.558014/0.186798loss in batch 212: 0.0398712/0.186111
done with epoch 10
train_acc: 0.938967 (400/426)
test loss: 0.0398712
acc: 0.937063 (134/143)
loss in batch 0: 0.239319/0.239319loss in batch 1: 0.0699768/0.154648loss in batch 2: 0.0315704/0.113617loss in batch 3: 0.172607/0.128372loss in batch 4: 0.233582/0.149414loss in batch 5: 0.279144/0.171036loss in batch 6: 0.0754089/0.157364loss in batch 7: 0.119614/0.152649loss in batch 8: 0.430206/0.183487loss in batch 9: 0.261414/0.191284loss in batch 10: 0.300491/0.201218loss in batch 11: 0.366745/0.215012loss in batch 12: 0.18309/0.212555loss in batch 13: 0.096817/0.204285loss in batch 14: 0.0405273/0.193359loss in batch 15: 0.0424957/0.183945loss in batch 16: 0.0849304/0.178116loss in batch 17: 0.280991/0.183838loss in batch 18: 0.0032959/0.174332loss in batch 19: 0.215897/0.176407loss in batch 20: 0.0501251/0.170395loss in batch 21: 0.110992/0.167694loss in batch 22: 0.0518036/0.162643loss in batch 23: 0.3293/0.169601loss in batch 24: 0.105804/0.167053loss in batch 25: 0.0295105/0.161758loss in batch 26: 0.815231/0.185959loss in batch 27: 0.0873718/0.182434loss in batch 28: 0.305511/0.186691loss in batch 29: 0.291763/0.190186loss in batch 30: 0.198914/0.19046loss in batch 31: 0.0226593/0.185226loss in batch 32: 0.0666962/0.181641loss in batch 33: 0.011795/0.176636loss in batch 34: 0.125427/0.175171loss in batch 35: 0.181564/0.175354loss in batch 36: 0.0384979/0.171661loss in batch 37: 0.0334778/0.168015loss in batch 38: 0.975449/0.188721loss in batch 39: 0.0476074/0.185196loss in batch 40: 0.416794/0.190826loss in batch 41: 0.0280151/0.186966loss in batch 42: 0.0341492/0.183411loss in batch 43: 0.0354919/0.180054loss in batch 44: 0.0545502/0.177261loss in batch 45: 0.161682/0.176926loss in batch 46: 0.0827942/0.174911loss in batch 47: 0.202835/0.175507loss in batch 48: 0.344559/0.17894loss in batch 49: 0.216995/0.179703loss in batch 50: 0.115448/0.178452loss in batch 51: 0.100662/0.176956loss in batch 52: 0.129959/0.176071loss in batch 53: 0.108047/0.174805loss in batch 54: 0.116302/0.173737loss in batch 55: 0.450912/0.178696loss in batch 56: 0.0738068/0.176865loss in batch 57: 0.37674/0.180298loss in batch 58: 0.0490875/0.17807loss in batch 59: 0.0688782/0.176254loss in batch 60: 0.568069/0.182678loss in batch 61: 0.0643616/0.180771loss in batch 62: 0.10585/0.179581loss in batch 63: 0.0483704/0.177521loss in batch 64: 0.0206299/0.175125loss in batch 65: 0.0327301/0.172958loss in batch 66: 0.0491943/0.171112loss in batch 67: 0.0874634/0.169891loss in batch 68: 0.307587/0.171875loss in batch 69: 0.574509/0.177628loss in batch 70: 0.078476/0.176239loss in batch 71: 0.0452576/0.174423loss in batch 72: 0.269623/0.17572loss in batch 73: 0.267578/0.176956loss in batch 74: 0.0568542/0.175369loss in batch 75: 0.0945587/0.174286loss in batch 76: 0.0111542/0.17218loss in batch 77: 0.0935516/0.171173loss in batch 78: 0.388199/0.17392loss in batch 79: 0.0470886/0.172333loss in batch 80: 0.123672/0.171722loss in batch 81: 0.182327/0.17186loss in batch 82: 0.306656/0.173477loss in batch 83: 0.0572662/0.172104loss in batch 84: 0.0498352/0.170654loss in batch 85: 0.032196/0.169052loss in batch 86: 0.0246887/0.167389loss in batch 87: 0.00944519/0.165604loss in batch 88: 0.0313721/0.164078loss in batch 89: 0.00894165/0.162369loss in batch 90: 0.203323/0.162827loss in batch 91: 0.0401764/0.161484loss in batch 92: 0.163818/0.161499loss in batch 93: 0.303528/0.163025loss in batch 94: 0.277542/0.164215loss in batch 95: 0.411301/0.166809loss in batch 96: 0.0575256/0.165665loss in batch 97: 0.621902/0.170319loss in batch 98: 0.044754/0.169052loss in batch 99: 0.0189362/0.167557loss in batch 100: 0.0456848/0.166351loss in batch 101: 0.0566711/0.165283loss in batch 102: 0.0153961/0.163818loss in batch 103: 0.301712/0.165146loss in batch 104: 0.333328/0.166748loss in batch 105: 0.075592/0.165894loss in batch 106: 0.0448151/0.164764loss in batch 107: 0.0474243/0.163666loss in batch 108: 0.258804/0.164536loss in batch 109: 0.357697/0.16629loss in batch 110: 0.1651/0.16629loss in batch 111: 0.165375/0.16629loss in batch 112: 0.0333252/0.1651loss in batch 113: 0.321945/0.166489loss in batch 114: 0.0684814/0.165634loss in batch 115: 0.0294647/0.164444loss in batch 116: 0.0450592/0.163437loss in batch 117: 0.0348358/0.162338loss in batch 118: 0.0695648/0.16156loss in batch 119: 0.00416565/0.160263loss in batch 120: 0.0248871/0.159134loss in batch 121: 0.136703/0.158936loss in batch 122: 0.0569763/0.158127loss in batch 123: 0.0501862/0.157242loss in batch 124: 0.0541687/0.156433loss in batch 125: 0.10556/0.156006loss in batch 126: 0.0947266/0.155548loss in batch 127: 0.0648193/0.154831loss in batch 128: 0.0671234/0.15416loss in batch 129: 0.0576324/0.153397loss in batch 130: 0.0849762/0.152893loss in batch 131: 0.134201/0.15274loss in batch 132: 0.485931/0.155258loss in batch 133: 0.170944/0.155365loss in batch 134: 0.0821533/0.154816loss in batch 135: 0.292404/0.155838loss in batch 136: 0.062912/0.155167loss in batch 137: 0.0774231/0.154587loss in batch 138: 0.0969696/0.154175loss in batch 139: 0.0712738/0.15358loss in batch 140: 0.472046/0.155838loss in batch 141: 0.193741/0.156113loss in batch 142: 0.0990906/0.155716loss in batch 143: 0.104935/0.15535loss in batch 144: 1.14252/0.16217loss in batch 145: 0.121567/0.161896loss in batch 146: 0.168381/0.161942loss in batch 147: 0.110977/0.161591loss in batch 148: 0.124924/0.161346loss in batch 149: 0.602203/0.164276loss in batch 150: 0.0479126/0.163513loss in batch 151: 0.0618134/0.162842loss in batch 152: 0.248428/0.163391loss in batch 153: 0.245651/0.16394loss in batch 154: 0.00727844/0.162918loss in batch 155: 0.0561523/0.162247loss in batch 156: 0.187134/0.162399loss in batch 157: 0.117691/0.162109loss in batch 158: 0.640945/0.165131loss in batch 159: 0.216904/0.165451loss in batch 160: 0.109802/0.1651loss in batch 161: 0.0308533/0.164276loss in batch 162: 0.0162048/0.163376loss in batch 163: 0.0190887/0.162491loss in batch 164: 0.325302/0.163483loss in batch 165: 0.259689/0.164063loss in batch 166: 0.701599/0.167267loss in batch 167: 0.0594788/0.166641loss in batch 168: 0.00660706/0.16568loss in batch 169: 0.14296/0.165543loss in batch 170: 0.419006/0.167038loss in batch 171: 0.1707/0.167068loss in batch 172: 0.077301/0.16655loss in batch 173: 0.108566/0.166214loss in batch 174: 0.0361786/0.165451loss in batch 175: 0.021698/0.164658loss in batch 176: 0.111847/0.164352loss in batch 177: 0.699036/0.167343loss in batch 178: 0.13327/0.16716loss in batch 179: 0.217255/0.16745loss in batch 180: 0.185623/0.167526loss in batch 181: 0.236252/0.167923loss in batch 182: 0.53064/0.169907loss in batch 183: 0.0396881/0.169189loss in batch 184: 0.0527954/0.168564loss in batch 185: 0.0715637/0.168045loss in batch 186: 0.155014/0.167969loss in batch 187: 0.0892944/0.167557loss in batch 188: 0.0544434/0.166962loss in batch 189: 0.401398/0.168198loss in batch 190: 0.14212/0.168045loss in batch 191: 0.425049/0.169388loss in batch 192: 0.0572662/0.168808loss in batch 193: 0.0279846/0.168076loss in batch 194: 0.0930939/0.167709loss in batch 195: 0.360199/0.168671loss in batch 196: 0.0744781/0.168198loss in batch 197: 0.320267/0.168961loss in batch 198: 0.0805054/0.168518loss in batch 199: 0.00889587/0.167725loss in batch 200: 0.0576477/0.167191loss in batch 201: 0.030426/0.166504loss in batch 202: 1.50545/0.173096loss in batch 203: 0.00788879/0.172287loss in batch 204: 0.0258331/0.17157loss in batch 205: 0.63707/0.173828loss in batch 206: 0.13205/0.17363loss in batch 207: 0.455185/0.174988loss in batch 208: 0.0340881/0.174316loss in batch 209: 0.375687/0.175278loss in batch 210: 0.378326/0.176224loss in batch 211: 1.0587/0.180389loss in batch 212: 0.078186/0.179916
done with epoch 11
train_acc: 0.943662 (402/426)
test loss: 0.078186
acc: 0.937063 (134/143)
loss in batch 0: 0.02948/0.02948loss in batch 1: 0.0193329/0.0243988loss in batch 2: 0.0170288/0.0219421loss in batch 3: 0.424179/0.122498loss in batch 4: 0.030426/0.10408loss in batch 5: 0.101776/0.103699loss in batch 6: 0.00238037/0.0892334loss in batch 7: 1.09779/0.215302loss in batch 8: 0.105621/0.20311loss in batch 9: 0.036377/0.186432loss in batch 10: 0.225769/0.190018loss in batch 11: 0.0386963/0.177414loss in batch 12: 0.0232697/0.165543loss in batch 13: 0.108444/0.161469loss in batch 14: 0.0397339/0.153351loss in batch 15: 0.0380554/0.146149loss in batch 16: 0.0794373/0.142227loss in batch 17: 0.0986633/0.139801loss in batch 18: 0.434814/0.155334loss in batch 19: 0.0627747/0.150696loss in batch 20: 0.0457764/0.145706loss in batch 21: 0.365479/0.155701loss in batch 22: 0.123306/0.154282loss in batch 23: 0.167755/0.154846loss in batch 24: 0.229874/0.157852loss in batch 25: 0.218338/0.160187loss in batch 26: 0.304642/0.165527loss in batch 27: 0.10997/0.163544loss in batch 28: 0.123291/0.162155loss in batch 29: 0.0126343/0.157181loss in batch 30: 0.0643768/0.154175loss in batch 31: 0.101563/0.152542loss in batch 32: 0.0710449/0.15007loss in batch 33: 0.221481/0.152161loss in batch 34: 1.078/0.178619loss in batch 35: 0.0356293/0.174652loss in batch 36: 0.134201/0.173553loss in batch 37: 0.0507202/0.170319loss in batch 38: 0.601974/0.181396loss in batch 39: 0.539246/0.190338loss in batch 40: 0.199081/0.190536loss in batch 41: 0.0319824/0.186768loss in batch 42: 0.213989/0.187408loss in batch 43: 0.356216/0.191238loss in batch 44: 0.022522/0.187485loss in batch 45: 0.296524/0.189865loss in batch 46: 0.384644/0.194loss in batch 47: 0.246124/0.195084loss in batch 48: 0.000305176/0.191116loss in batch 49: 0.355103/0.194397loss in batch 50: 0.00784302/0.190735loss in batch 51: 0.248947/0.191864loss in batch 52: 0.0293579/0.188782loss in batch 53: 0.0466766/0.186157loss in batch 54: 0.0912323/0.184433loss in batch 55: 0.115265/0.183197loss in batch 56: 0.340363/0.185959loss in batch 57: 0.294495/0.18782loss in batch 58: 0.616898/0.195099loss in batch 59: 0.256516/0.196121loss in batch 60: 0.0249481/0.193314loss in batch 61: 0.229492/0.193893loss in batch 62: 0.115524/0.192657loss in batch 63: 0.206329/0.192871loss in batch 64: 0.0340729/0.19043loss in batch 65: 0.0570526/0.1884loss in batch 66: 0.339279/0.190659loss in batch 67: 0.0956116/0.18927loss in batch 68: 0.334183/0.191376loss in batch 69: 0.122803/0.190384loss in batch 70: 0.181503/0.190262loss in batch 71: 0.0939026/0.188919loss in batch 72: 0.103638/0.187744loss in batch 73: 0.147766/0.18721loss in batch 74: 0.0963745/0.185989loss in batch 75: 0.0711517/0.184479loss in batch 76: 1.40126/0.200287loss in batch 77: 0.0166779/0.197937loss in batch 78: 0.242401/0.198502loss in batch 79: 0.300156/0.199768loss in batch 80: 0.0747223/0.198227loss in batch 81: 0.107178/0.197128loss in batch 82: 0.400665/0.19957loss in batch 83: 0.09021/0.198257loss in batch 84: 0.274628/0.199158loss in batch 85: 0.0418854/0.197342loss in batch 86: 0.106491/0.196289loss in batch 87: 0.104614/0.195236loss in batch 88: 0.165909/0.194916loss in batch 89: 0.0997314/0.193863loss in batch 90: 0.138229/0.193253loss in batch 91: 0.274979/0.194138loss in batch 92: 0.088974/0.193008loss in batch 93: 0.0938873/0.19194loss in batch 94: 0.0662689/0.190628loss in batch 95: 0.419312/0.193008loss in batch 96: 0.0384674/0.191422loss in batch 97: 0.0746002/0.190231loss in batch 98: 0.35762/0.191925loss in batch 99: 0.0078125/0.190079loss in batch 100: 0.00370789/0.188232loss in batch 101: 0.239426/0.188736loss in batch 102: 0.061203/0.187485loss in batch 103: 0.0154572/0.185837loss in batch 104: 0.185959/0.185837loss in batch 105: 0.115479/0.185181loss in batch 106: 0.0593567/0.184006loss in batch 107: 0.146698/0.183655loss in batch 108: 0.076416/0.182678loss in batch 109: 0.290375/0.183655loss in batch 110: 0.0594177/0.182526loss in batch 111: 0.0197601/0.181076loss in batch 112: 0.0548859/0.179962loss in batch 113: 0.542145/0.183136loss in batch 114: 0.161972/0.182953loss in batch 115: 0.105087/0.182281loss in batch 116: 0.236664/0.182755loss in batch 117: 0.457916/0.185089loss in batch 118: 0.0363312/0.183838loss in batch 119: 0.0347137/0.182587loss in batch 120: 0.781815/0.187546loss in batch 121: 0.0454254/0.186371loss in batch 122: 0.0648804/0.185394loss in batch 123: 0.118378/0.184845loss in batch 124: 0.290436/0.185699loss in batch 125: 0.0209045/0.184387loss in batch 126: 0.310074/0.185379loss in batch 127: 0.0842133/0.184586loss in batch 128: 0.0161591/0.183289loss in batch 129: 0.0707855/0.182419loss in batch 130: 0.324951/0.183502loss in batch 131: 0.239334/0.183929loss in batch 132: 0.0270691/0.182739loss in batch 133: 0.00856018/0.181442loss in batch 134: 0.279175/0.182175loss in batch 135: 0.0521393/0.181213loss in batch 136: 0.129639/0.180832loss in batch 137: 0.0247498/0.179703loss in batch 138: 0.06987/0.178909loss in batch 139: 0.00834656/0.177704loss in batch 140: 0.0327454/0.176666loss in batch 141: 0.0183105/0.175552loss in batch 142: 0.0818329/0.174896loss in batch 143: 0.219559/0.175217loss in batch 144: 0.131638/0.174911loss in batch 145: 0.0275574/0.173889loss in batch 146: 0.0179596/0.172836loss in batch 147: 0.146591/0.172668loss in batch 148: 0.157776/0.172562loss in batch 149: 0.049942/0.171738loss in batch 150: 0.342163/0.172867loss in batch 151: 0.0845795/0.172287loss in batch 152: 0.0108185/0.171234loss in batch 153: 0.544937/0.17366loss in batch 154: 0.0273438/0.172729loss in batch 155: 0.0831757/0.17215loss in batch 156: 0.0644073/0.171448loss in batch 157: 0.11496/0.171097loss in batch 158: 0.478928/0.173035loss in batch 159: 0.0608978/0.172348loss in batch 160: 0.344177/0.173416loss in batch 161: 0.0543365/0.172668loss in batch 162: 0.862976/0.17691loss in batch 163: 0.0256958/0.175995loss in batch 164: 0.465149/0.177734loss in batch 165: 0.0665588/0.177063loss in batch 166: 0.18364/0.177109loss in batch 167: 0.0833893/0.176544loss in batch 168: 0.0106812/0.175552loss in batch 169: 0.0611267/0.174896loss in batch 170: 0.202286/0.175064loss in batch 171: 0.153961/0.174927loss in batch 172: 0.566666/0.177185loss in batch 173: 0.273697/0.17775loss in batch 174: 0.0164185/0.176819loss in batch 175: 0.279129/0.177399loss in batch 176: 0.233658/0.177719loss in batch 177: 0.390839/0.178925loss in batch 178: 0.560318/0.181046loss in batch 179: 0.0492096/0.180328loss in batch 180: 0.195572/0.180405loss in batch 181: 0.0355835/0.179611loss in batch 182: 0.0398712/0.178848loss in batch 183: 0.0332336/0.178055loss in batch 184: 0.0391846/0.177307loss in batch 185: 0.427994/0.17865loss in batch 186: 0.0493317/0.177948loss in batch 187: 0.0430908/0.177246loss in batch 188: 0.312927/0.177963loss in batch 189: 0.0520325/0.177292loss in batch 190: 0.0658722/0.176712loss in batch 191: 0.100845/0.176315loss in batch 192: 0.359955/0.177261loss in batch 193: 0.427612/0.178558loss in batch 194: 0.101639/0.178162loss in batch 195: 0.0186615/0.177353loss in batch 196: 0.366547/0.178299loss in batch 197: 0.191177/0.178375loss in batch 198: 0.0525818/0.17775loss in batch 199: 0.0386047/0.177048loss in batch 200: 0.13472/0.176849loss in batch 201: 0.0512848/0.176208loss in batch 202: 0.0482941/0.175583loss in batch 203: 0.116821/0.175293loss in batch 204: 0.144379/0.175156loss in batch 205: 0.264648/0.175583loss in batch 206: 0.63031/0.17778loss in batch 207: 0.261063/0.178177loss in batch 208: 0.000228882/0.177322loss in batch 209: 0.0793304/0.176865loss in batch 210: 0.121368/0.17659loss in batch 211: 0.0474548/0.175995loss in batch 212: 0.0884094/0.175568
done with epoch 12
train_acc: 0.946009 (403/426)
test loss: 0.0884094
acc: 0.944056 (135/143)
loss in batch 0: 0.0218658/0.0218658loss in batch 1: 0.028717/0.0252838loss in batch 2: 0.242065/0.0975494loss in batch 3: 0.054184/0.0867004loss in batch 4: 0.192062/0.107773loss in batch 5: 0.04776/0.0977631loss in batch 6: 0.178543/0.109314loss in batch 7: 0.0302582/0.0994415loss in batch 8: 0.468094/0.140396loss in batch 9: 0.135864/0.139938loss in batch 10: 0.156311/0.141418loss in batch 11: 0.0373535/0.132767loss in batch 12: 0.155151/0.134476loss in batch 13: 0.202728/0.139343loss in batch 14: 0.0376892/0.132568loss in batch 15: 0.0210876/0.12561loss in batch 16: 0.049881/0.121155loss in batch 17: 0.107758/0.120407loss in batch 18: 0.0825043/0.118423loss in batch 19: 0.0311737/0.114059loss in batch 20: 1.39821/0.175201loss in batch 21: 0.273804/0.179688loss in batch 22: 0.28949/0.184464loss in batch 23: 0.0217285/0.177673loss in batch 24: 0.0553741/0.172791loss in batch 25: 0.130981/0.171188loss in batch 26: 0.0159302/0.165421loss in batch 27: 0.167374/0.165497loss in batch 28: 0.114212/0.163727loss in batch 29: 0.0205536/0.158966loss in batch 30: 0.0423279/0.155197loss in batch 31: 0.0173492/0.150894loss in batch 32: 0.19339/0.152176loss in batch 33: 0.366074/0.158463loss in batch 34: 0.0754395/0.156082loss in batch 35: 0.264664/0.159119loss in batch 36: 0.19429/0.160049loss in batch 37: 0.0152893/0.15625loss in batch 38: 0.074234/0.154144loss in batch 39: 0.167786/0.15448loss in batch 40: 0.0342712/0.151566loss in batch 41: 0.0498047/0.149124loss in batch 42: 0.241348/0.151291loss in batch 43: 0.378128/0.156433loss in batch 44: 0.207504/0.157578loss in batch 45: 0.460236/0.164139loss in batch 46: 0.0428619/0.161575loss in batch 47: 0.0176544/0.158585loss in batch 48: 0.528748/0.166122loss in batch 49: 0.333084/0.169464loss in batch 50: 0.0987091/0.168076loss in batch 51: 0.0483398/0.165787loss in batch 52: 0.0628204/0.163834loss in batch 53: 0.358307/0.167435loss in batch 54: 0.0288391/0.164917loss in batch 55: 0.00860596/0.162125loss in batch 56: 0.00390625/0.159348loss in batch 57: 0.0396423/0.157272loss in batch 58: 0.0765381/0.155914loss in batch 59: 0.0790863/0.154633loss in batch 60: 0.0823669/0.153442loss in batch 61: 0.14299/0.153275loss in batch 62: 0.0253906/0.151245loss in batch 63: 0.642441/0.15892loss in batch 64: 0.0263062/0.156891loss in batch 65: 0.320938/0.159363loss in batch 66: 0.00636292/0.157089loss in batch 67: 0.101456/0.156265loss in batch 68: 1.15172/0.170685loss in batch 69: 0.0643005/0.169174loss in batch 70: 0.0405273/0.167374loss in batch 71: 0.046936/0.165695loss in batch 72: 0.298248/0.167496loss in batch 73: 0.405853/0.170731loss in batch 74: 0.0213318/0.168732loss in batch 75: 0.0443726/0.167099loss in batch 76: 0.197037/0.167496loss in batch 77: 0.034729/0.165787loss in batch 78: 0.159607/0.165695loss in batch 79: 0.111282/0.165039loss in batch 80: 0.039917/0.163483loss in batch 81: 0.425095/0.166672loss in batch 82: 0.232986/0.167465loss in batch 83: 0.0477295/0.166046loss in batch 84: 0.00393677/0.164139loss in batch 85: 0.320389/0.165955loss in batch 86: 0.220306/0.16658loss in batch 87: 0.513885/0.170532loss in batch 88: 0.0337524/0.168991loss in batch 89: 0.338287/0.170883loss in batch 90: 0.517654/0.174683loss in batch 91: 0.257111/0.175583loss in batch 92: 0.685486/0.181061loss in batch 93: 0.0310974/0.179459loss in batch 94: 0.0678101/0.178284loss in batch 95: 0.032196/0.176758loss in batch 96: 0.0370941/0.175339loss in batch 97: 0.0615234/0.174179loss in batch 98: 0.239029/0.17482loss in batch 99: 0.119476/0.174271loss in batch 100: 0.0931396/0.173462loss in batch 101: 0.137268/0.173111loss in batch 102: 0.636322/0.177612loss in batch 103: 0.0315552/0.176208loss in batch 104: 0.0630951/0.175125loss in batch 105: 0.0566101/0.174011loss in batch 106: 0.225296/0.174484loss in batch 107: 0.04039/0.173248loss in batch 108: 0.0354004/0.171982loss in batch 109: 0.783279/0.177536loss in batch 110: 0.0424957/0.176315loss in batch 111: 0.0358124/0.175064loss in batch 112: 0.0166168/0.17366loss in batch 113: 0.0192719/0.172302loss in batch 114: 0.506638/0.175217loss in batch 115: 0.0619812/0.17424loss in batch 116: 0.165634/0.174179loss in batch 117: 0.0252991/0.172913loss in batch 118: 0.53006/0.175903loss in batch 119: 0.157639/0.175766loss in batch 120: 0.0711517/0.174896loss in batch 121: 0.0554352/0.17392loss in batch 122: 0.309525/0.175003loss in batch 123: 0.255737/0.175659loss in batch 124: 0.393356/0.177399loss in batch 125: 0.0792084/0.176636loss in batch 126: 0.193939/0.176773loss in batch 127: 0.17749/0.176773loss in batch 128: 0.00895691/0.175476loss in batch 129: 0.073349/0.174683loss in batch 130: 0.0875092/0.174011loss in batch 131: 0.0948181/0.173416loss in batch 132: 0.0278473/0.172318loss in batch 133: 0.00505066/0.171082loss in batch 134: 0.369751/0.172562loss in batch 135: 0.103348/0.172043loss in batch 136: 0.0329285/0.171036loss in batch 137: 0.473114/0.173203loss in batch 138: 0.28627/0.174026loss in batch 139: 0.0480804/0.173126loss in batch 140: 0.0406342/0.17218loss in batch 141: 0.0330963/0.171219loss in batch 142: 0.245987/0.171722loss in batch 143: 0.190277/0.17186loss in batch 144: 0.0527802/0.171036loss in batch 145: 0.474121/0.173111loss in batch 146: 0.0412445/0.172226loss in batch 147: 0.0819092/0.171616loss in batch 148: 0.0541077/0.170807loss in batch 149: 0.270233/0.171478loss in batch 150: 0.132111/0.171219loss in batch 151: 0.23584/0.171646loss in batch 152: 0.0346375/0.170746loss in batch 153: 0.178207/0.170807loss in batch 154: 0.446823/0.172577loss in batch 155: 0.175369/0.172592loss in batch 156: 0.0516205/0.171829loss in batch 157: 0.0427246/0.171005loss in batch 158: 0.0277252/0.170105loss in batch 159: 0.481506/0.172043loss in batch 160: 0.128571/0.171783loss in batch 161: 0.0843506/0.171249loss in batch 162: 0.0373688/0.170425loss in batch 163: 0.226715/0.170761loss in batch 164: 0.118866/0.170441loss in batch 165: 0.0559845/0.169754loss in batch 166: 0.36557/0.170929loss in batch 167: 0.0569153/0.170258loss in batch 168: 0.391083/0.17157loss in batch 169: 0.0147705/0.170639loss in batch 170: 0.0941467/0.170181loss in batch 171: 0.132248/0.169983loss in batch 172: 0.246201/0.17041loss in batch 173: 0.1539/0.170319loss in batch 174: 0.0386505/0.169571loss in batch 175: 0.0527191/0.1689loss in batch 176: 0.176117/0.168945loss in batch 177: 0.0614166/0.168335loss in batch 178: 0.00614929/0.167435loss in batch 179: 0.0149078/0.166595loss in batch 180: 0.0724182/0.166061loss in batch 181: 0.0944824/0.16568loss in batch 182: 0.151886/0.165604loss in batch 183: 0.0939484/0.165207loss in batch 184: 0.0254364/0.164459loss in batch 185: 0.273972/0.165039loss in batch 186: 1.27995/0.171005loss in batch 187: 0.0331573/0.170273loss in batch 188: 0.0281677/0.16951loss in batch 189: 0.140991/0.169373loss in batch 190: 0.0921021/0.168961loss in batch 191: 0.0541534/0.168365loss in batch 192: 0.21698/0.16861loss in batch 193: 0.174133/0.168655loss in batch 194: 0.217682/0.1689loss in batch 195: 0.00566101/0.16806loss in batch 196: 0.14064/0.167923loss in batch 197: 0.00947571/0.16713loss in batch 198: 0.0399933/0.166489loss in batch 199: 0.277344/0.167038loss in batch 200: 0.0187836/0.166306loss in batch 201: 0.0557861/0.165756loss in batch 202: 0.0253906/0.165054loss in batch 203: 0.823639/0.168289loss in batch 204: 0.701782/0.170898loss in batch 205: 0.100601/0.170563loss in batch 206: 0.050415/0.169968loss in batch 207: 0.00854492/0.169205loss in batch 208: 0.289719/0.169769loss in batch 209: 0.102539/0.169464loss in batch 210: 0.0500641/0.168884loss in batch 211: 0.595734/0.170898loss in batch 212: 0.544525/0.172653
done with epoch 13
train_acc: 0.941315 (401/426)
test loss: 0.544525
acc: 0.944056 (135/143)
loss in batch 0: 0.0815887/0.0815887loss in batch 1: 0.116364/0.0989685loss in batch 2: 0.0325775/0.0768433loss in batch 3: 0.0384216/0.0672302loss in batch 4: 0.0371399/0.0612183loss in batch 5: 0.0542297/0.0600433loss in batch 6: 0.249756/0.0871582loss in batch 7: 0.0982361/0.0885468loss in batch 8: 0.710678/0.157669loss in batch 9: 0.807983/0.222687loss in batch 10: 0.112015/0.212631loss in batch 11: 0.0360107/0.197922loss in batch 12: 0.0249176/0.184616loss in batch 13: 0.170029/0.183563loss in batch 14: 0.00640869/0.171753loss in batch 15: 0.126434/0.16893loss in batch 16: 0.121399/0.166122loss in batch 17: 0.282364/0.172592loss in batch 18: 0.0705261/0.167221loss in batch 19: 0.0385437/0.160782loss in batch 20: 0.11734/0.158722loss in batch 21: 0.31636/0.165878loss in batch 22: 0.000320435/0.158676loss in batch 23: 0.0548401/0.154343loss in batch 24: 0.119003/0.152939loss in batch 25: 0.0684052/0.149689loss in batch 26: 0.0514069/0.146042loss in batch 27: 0.216995/0.148575loss in batch 28: 0.158875/0.148941loss in batch 29: 0.0501556/0.145645loss in batch 30: 0.0503845/0.142563loss in batch 31: 0.137177/0.14241loss in batch 32: 0.148453/0.142578loss in batch 33: 0.0642548/0.140289loss in batch 34: 0.181686/0.141464loss in batch 35: 0.00163269/0.137573loss in batch 36: 0.0272369/0.134598loss in batch 37: 0.0793152/0.133148loss in batch 38: 0.303787/0.137512loss in batch 39: 0.013504/0.134415loss in batch 40: 0.268524/0.13768loss in batch 41: 0.647766/0.149826loss in batch 42: 0.348267/0.154449loss in batch 43: 0.0676727/0.152481loss in batch 44: 0.0615845/0.150452loss in batch 45: 0.304291/0.153809loss in batch 46: 0.21405/0.15509loss in batch 47: 0.257919/0.157227loss in batch 48: 0.117584/0.156418loss in batch 49: 0.0165863/0.153625loss in batch 50: 0.504074/0.160492loss in batch 51: 0.378403/0.164688loss in batch 52: 0.173279/0.164856loss in batch 53: 0.273438/0.166855loss in batch 54: 0.0388489/0.16452loss in batch 55: 0.0802612/0.163025loss in batch 56: 0.304077/0.165497loss in batch 57: 0.0291138/0.163147loss in batch 58: 0.13855/0.16272loss in batch 59: 0.0782623/0.161331loss in batch 60: 0.235886/0.162552loss in batch 61: 0.0159454/0.160172loss in batch 62: 0.23172/0.161316loss in batch 63: 0.0466461/0.159515loss in batch 64: 0.0736542/0.158203loss in batch 65: 0.146774/0.15802loss in batch 66: 0.139816/0.157761loss in batch 67: 0.00378418/0.155487loss in batch 68: 0.146118/0.155365loss in batch 69: 0.026413/0.153503loss in batch 70: 0.098114/0.152725loss in batch 71: 0.0836029/0.151764loss in batch 72: 0.0322266/0.150131loss in batch 73: 0.323151/0.152481loss in batch 74: 0.404556/0.155823loss in batch 75: 0.0207977/0.154068loss in batch 76: 0.261841/0.155457loss in batch 77: 0.439438/0.159103loss in batch 78: 0.525497/0.163727loss in batch 79: 0.0700531/0.162567loss in batch 80: 0.129349/0.162155loss in batch 81: 0.0683594/0.161011loss in batch 82: 0.035965/0.159515loss in batch 83: 0.348816/0.161774loss in batch 84: 0.375/0.164261loss in batch 85: 0.0392609/0.162827loss in batch 86: 0.592117/0.16774loss in batch 87: 0.319321/0.169479loss in batch 88: 0.136917/0.169098loss in batch 89: 0.0467529/0.167755loss in batch 90: 0.0140839/0.166061loss in batch 91: 0.00875854/0.164337loss in batch 92: 0.0187988/0.162781loss in batch 93: 0.102585/0.16214loss in batch 94: 0.0307159/0.160767loss in batch 95: 0.0486603/0.159592loss in batch 96: 1.01923/0.168457loss in batch 97: 0.0820923/0.167572loss in batch 98: 0.0112305/0.165985loss in batch 99: 0.0518799/0.164856loss in batch 100: 0.0982513/0.164185loss in batch 101: 0.294525/0.165482loss in batch 102: 0.133011/0.165161loss in batch 103: 0.117798/0.164703loss in batch 104: 0.0791321/0.163895loss in batch 105: 0.0840149/0.163132loss in batch 106: 0.313583/0.164551loss in batch 107: 0.143951/0.164337loss in batch 108: 0.038208/0.163177loss in batch 109: 0.0350189/0.162033loss in batch 110: 0.0559692/0.161057loss in batch 111: 0.0461121/0.160049loss in batch 112: 0.0677643/0.159225loss in batch 113: 0.227829/0.159836loss in batch 114: 0.0115967/0.158539loss in batch 115: 0.0592499/0.157684loss in batch 116: 0.0496216/0.156769loss in batch 117: 0.0601044/0.155945loss in batch 118: 0.0751953/0.155258loss in batch 119: 0.105118/0.154831loss in batch 120: 0.0752106/0.15419loss in batch 121: 0.336807/0.15567loss in batch 122: 0.0308075/0.154663loss in batch 123: 0.0078125/0.153488loss in batch 124: 0.0838165/0.152939loss in batch 125: 0.17775/0.153122loss in batch 126: 0.371948/0.154846loss in batch 127: 0.377655/0.156586loss in batch 128: 0.0370941/0.15567loss in batch 129: 0.0892944/0.155136loss in batch 130: 0.299454/0.15625loss in batch 131: 0.0557098/0.155487loss in batch 132: 0.279999/0.156433loss in batch 133: 0.361389/0.157959loss in batch 134: 0.0787811/0.157379loss in batch 135: 0.0118561/0.156296loss in batch 136: 0.189636/0.15654loss in batch 137: 0.0365906/0.15567loss in batch 138: 0.164474/0.155731loss in batch 139: 0.0625305/0.15506loss in batch 140: 0.013443/0.154068loss in batch 141: 0.0181427/0.153107loss in batch 142: 0.798813/0.157623loss in batch 143: 0.0178223/0.156662loss in batch 144: 0.403976/0.158356loss in batch 145: 0.441803/0.160294loss in batch 146: 0.220428/0.160706loss in batch 147: 0.558395/0.163406loss in batch 148: 0.0271759/0.162476loss in batch 149: 0.000457764/0.161407loss in batch 150: 0.12648/0.161163loss in batch 151: 0.0220184/0.160263loss in batch 152: 0.443665/0.162094loss in batch 153: 0.0297089/0.161255loss in batch 154: 0.0241852/0.160355loss in batch 155: 0.0491791/0.159653loss in batch 156: 0.00405884/0.158661loss in batch 157: 0.048996/0.157974loss in batch 158: 0.418716/0.159592loss in batch 159: 0.0521393/0.158936loss in batch 160: 0.0244904/0.158096loss in batch 161: 0.0039978/0.15715loss in batch 162: 0.0489044/0.156494loss in batch 163: 0.0970612/0.156128loss in batch 164: 0.0244598/0.155319loss in batch 165: 0.45076/0.157104loss in batch 166: 0.236267/0.157578loss in batch 167: 0.060791/0.157013loss in batch 168: 2.59982/0.171463loss in batch 169: 0.905029/0.175766loss in batch 170: 0.0234375/0.174881loss in batch 171: 0.279129/0.175491loss in batch 172: 0.168198/0.175446loss in batch 173: 0.03302/0.174637loss in batch 174: 0.02005/0.173737loss in batch 175: 0.0747375/0.173172loss in batch 176: 0.155991/0.17308loss in batch 177: 0.0759277/0.172546loss in batch 178: 0.169266/0.172516loss in batch 179: 0.0388641/0.171783loss in batch 180: 0.300766/0.172485loss in batch 181: 0.0746307/0.171951loss in batch 182: 0.0733948/0.171417loss in batch 183: 0.0715485/0.170868loss in batch 184: 0.323257/0.171692loss in batch 185: 0.0224152/0.170898loss in batch 186: 0.191223/0.171005loss in batch 187: 0.262665/0.171478loss in batch 188: 0.0245056/0.1707loss in batch 189: 0.00558472/0.169846loss in batch 190: 0.070343/0.169327loss in batch 191: 0.28891/0.169937loss in batch 192: 0.132233/0.169739loss in batch 193: 0.0296173/0.169037loss in batch 194: 0.408966/0.170258loss in batch 195: 0.183868/0.170319loss in batch 196: 0.0471039/0.169693loss in batch 197: 0.268692/0.170197loss in batch 198: 0.087616/0.169785loss in batch 199: 0.0753632/0.169312loss in batch 200: 0.212631/0.169525loss in batch 201: 0.0465088/0.168915loss in batch 202: 0.218002/0.169159loss in batch 203: 0.0720978/0.168686loss in batch 204: 0.0621948/0.168167loss in batch 205: 0.043808/0.167572loss in batch 206: 0.0782928/0.16713loss in batch 207: 0.100037/0.166809loss in batch 208: 0.167267/0.166809loss in batch 209: 0.00186157/0.166031loss in batch 210: 0.469818/0.167465loss in batch 211: 0.0852661/0.167084loss in batch 212: 0.0833893/0.166687
done with epoch 14
train_acc: 0.948357 (404/426)
test loss: 0.0833893
acc: 0.944056 (135/143)
loss in batch 0: 0.397064/0.397064loss in batch 1: 0.0499878/0.223526loss in batch 2: 0.0418243/0.162964loss in batch 3: 0.0156708/0.126144loss in batch 4: 0.486862/0.198273loss in batch 5: 0.00918579/0.166763loss in batch 6: 0.0140839/0.144943loss in batch 7: 0.14389/0.144821loss in batch 8: 0.546005/0.189392loss in batch 9: 0.0969391/0.180145loss in batch 10: 0.272629/0.188568loss in batch 11: 0.0197296/0.1745loss in batch 12: 0.0131378/0.162064loss in batch 13: 0.0393524/0.153305loss in batch 14: 0.045639/0.146133loss in batch 15: 0.0038147/0.137238loss in batch 16: 0.128311/0.136719loss in batch 17: 0.0218048/0.130325loss in batch 18: 0.58197/0.154099loss in batch 19: 0.138306/0.153305loss in batch 20: 0.0419769/0.14801loss in batch 21: 0.072113/0.144562loss in batch 22: 0.0886841/0.14212loss in batch 23: 0.0462494/0.138138loss in batch 24: 0.3564/0.146866loss in batch 25: 0.0096283/0.141586loss in batch 26: 0.0389709/0.137787loss in batch 27: 0.178741/0.139252loss in batch 28: 0.112747/0.138321loss in batch 29: 0.13472/0.138214loss in batch 30: 0.0222778/0.134476loss in batch 31: 0.321701/0.14032loss in batch 32: 0.0714569/0.138229loss in batch 33: 0.033371/0.135147loss in batch 34: 0.0969238/0.134064loss in batch 35: 0.342285/0.139847loss in batch 36: 0.882233/0.159897loss in batch 37: 0.0891876/0.158051loss in batch 38: 0.506775/0.166992loss in batch 39: 0.0165558/0.163239loss in batch 40: 0.0259552/0.159882loss in batch 41: 0.0552063/0.157394loss in batch 42: 0.326324/0.161331loss in batch 43: 0.0609131/0.159027loss in batch 44: 0.0285034/0.156128loss in batch 45: 0.0457458/0.153732loss in batch 46: 0.253067/0.155853loss in batch 47: 0.161819/0.15596loss in batch 48: 0.155746/0.155975loss in batch 49: 0.0153198/0.153168loss in batch 50: 0.016922/0.150482loss in batch 51: 0.0727844/0.148987loss in batch 52: 0.0298462/0.146744loss in batch 53: 0.201355/0.147751loss in batch 54: 0.0259857/0.145554loss in batch 55: 0.0280304/0.143433loss in batch 56: 0.00933838/0.141083loss in batch 57: 0.269958/0.143311loss in batch 58: 0.175583/0.14386loss in batch 59: 0.152435/0.143997loss in batch 60: 0.00636292/0.141739loss in batch 61: 0.137497/0.141678loss in batch 62: 0.413101/0.145981loss in batch 63: 0.0401611/0.144333loss in batch 64: 0.0379486/0.142685loss in batch 65: 0.0485687/0.141266loss in batch 66: 0.252762/0.142929loss in batch 67: 0.0791931/0.141983loss in batch 68: 0.0923309/0.141281loss in batch 69: 0.0208282/0.139557loss in batch 70: 0.345795/0.142456loss in batch 71: 0.0337982/0.140945loss in batch 72: 0.0348053/0.139496loss in batch 73: 0.317398/0.141907loss in batch 74: 0.183777/0.142456loss in batch 75: 0.0884399/0.141739loss in batch 76: 0.0772705/0.1409loss in batch 77: 0.099762/0.140381loss in batch 78: 0.0797272/0.139618loss in batch 79: 0.111862/0.139267loss in batch 80: 0.182388/0.139801loss in batch 81: 0.127991/0.139664loss in batch 82: 0.0098877/0.138092loss in batch 83: 0.0332336/0.136841loss in batch 84: 0.0382843/0.135681loss in batch 85: 0.117249/0.135483loss in batch 86: 0.0773621/0.134796loss in batch 87: 0.00993347/0.133392loss in batch 88: 0.0480347/0.132416loss in batch 89: 0.264069/0.133881loss in batch 90: 0.36554/0.136429loss in batch 91: 0.150543/0.136581loss in batch 92: 0.0861053/0.136047loss in batch 93: 0.062149/0.135254loss in batch 94: 0.0865936/0.13475loss in batch 95: 0.114899/0.134552loss in batch 96: 0.0339661/0.133499loss in batch 97: 0.0146942/0.132294loss in batch 98: 0.153229/0.132507loss in batch 99: 0.323364/0.134415loss in batch 100: 0.439056/0.137421loss in batch 101: 0.245483/0.138489loss in batch 102: 0.0699615/0.137817loss in batch 103: 0.0400543/0.136871loss in batch 104: 0.0349884/0.13591loss in batch 105: 0.171692/0.136246loss in batch 106: 0.0475464/0.135422loss in batch 107: 0.0199432/0.134354loss in batch 108: 0.37262/0.136536loss in batch 109: 0.585495/0.14061loss in batch 110: 0.257843/0.141678loss in batch 111: 0.200836/0.142197loss in batch 112: 0.341675/0.143967loss in batch 113: 0.911362/0.150696loss in batch 114: 0.199081/0.151108loss in batch 115: 0.0809784/0.150513loss in batch 116: 0.262589/0.151459loss in batch 117: 0.0573578/0.150681loss in batch 118: 0.317886/0.152084loss in batch 119: 0.264084/0.153loss in batch 120: 0.0713501/0.152328loss in batch 121: 0.0413971/0.151428loss in batch 122: 0.0581512/0.150665loss in batch 123: 0.0241089/0.149658loss in batch 124: 0.0533142/0.14888loss in batch 125: 0.0177612/0.147827loss in batch 126: 1.93094/0.16188loss in batch 127: 0.122253/0.161575loss in batch 128: 0.391891/0.163361loss in batch 129: 0.178406/0.163483loss in batch 130: 0.0281372/0.162445loss in batch 131: 0.446564/0.164597loss in batch 132: 0.0105896/0.163422loss in batch 133: 0.235397/0.163971loss in batch 134: 0.344452/0.165298loss in batch 135: 0.34671/0.166641loss in batch 136: 0.0643921/0.165894loss in batch 137: 0.0301971/0.164902loss in batch 138: 0.0706024/0.16423loss in batch 139: 0.148697/0.164124loss in batch 140: 0.0252838/0.163132loss in batch 141: 0.0437927/0.162292loss in batch 142: 0.0234375/0.161331loss in batch 143: 0.0660706/0.160675loss in batch 144: 0.0314941/0.15976loss in batch 145: 0.460526/0.161835loss in batch 146: 0.151169/0.161758loss in batch 147: 0.0142517/0.160751loss in batch 148: 0.409225/0.16243loss in batch 149: 0.0349121/0.161575loss in batch 150: 0.493149/0.163773loss in batch 151: 0.20491/0.164047loss in batch 152: 0.069046/0.163422loss in batch 153: 0.0456238/0.162659loss in batch 154: 0.0646973/0.162018loss in batch 155: 0.0677795/0.161438loss in batch 156: 0.398849/0.162933loss in batch 157: 1.15141/0.169189loss in batch 158: 0.191193/0.169342loss in batch 159: 0.0271606/0.168442loss in batch 160: 0.0940247/0.167984loss in batch 161: 0.0724945/0.167389loss in batch 162: 0.0430603/0.166626loss in batch 163: 0.166214/0.166626loss in batch 164: 0.0942383/0.166183loss in batch 165: 0.447205/0.167877loss in batch 166: 0.0569916/0.167221loss in batch 167: 0.175644/0.167267loss in batch 168: 0.0242157/0.166412loss in batch 169: 0.0212555/0.165558loss in batch 170: 0.151489/0.165482loss in batch 171: 0.0126343/0.164597loss in batch 172: 0.465622/0.166336loss in batch 173: 0.112183/0.166031loss in batch 174: 0.0729675/0.165482loss in batch 175: 0.499161/0.167389loss in batch 176: 0.190063/0.167526loss in batch 177: 0.311478/0.16832loss in batch 178: 0.0142975/0.167465loss in batch 179: 0.0775909/0.166977loss in batch 180: 0.0298309/0.166199loss in batch 181: 0.0433807/0.165543loss in batch 182: 0.021347/0.164749loss in batch 183: 0.488968/0.166504loss in batch 184: 0.252045/0.166977loss in batch 185: 0.436615/0.168427loss in batch 186: 0.00843811/0.167557loss in batch 187: 0.0921478/0.16716loss in batch 188: 0.0625763/0.166611loss in batch 189: 0.0266266/0.165863loss in batch 190: 0.120361/0.165634loss in batch 191: 0.0497742/0.165039loss in batch 192: 0.387543/0.166183loss in batch 193: 0.10051/0.165848loss in batch 194: 0.0412903/0.165207loss in batch 195: 0.434128/0.16658loss in batch 196: 0.225494/0.16687loss in batch 197: 0.0830841/0.166458loss in batch 198: 0.0326996/0.165787loss in batch 199: 0.0216827/0.165054loss in batch 200: 0.0149994/0.164322loss in batch 201: 0.112732/0.164047loss in batch 202: 1.19644/0.169144loss in batch 203: 0.108734/0.168854loss in batch 204: 0.0659637/0.16835loss in batch 205: 0.0224762/0.167648loss in batch 206: 0.0785675/0.167206loss in batch 207: 0.122787/0.166992loss in batch 208: 0.0957642/0.166656loss in batch 209: 0.006073/0.165894loss in batch 210: 0.0384216/0.165283loss in batch 211: 0.0632935/0.16481loss in batch 212: 0.656158/0.167114
done with epoch 15
train_acc: 0.941315 (401/426)
test loss: 0.656158
acc: 0.944056 (135/143)
loss in batch 0: 0.0314636/0.0314636loss in batch 1: 0.105148/0.0682983loss in batch 2: 0.00308228/0.0465698loss in batch 3: 0.0202332/0.039978loss in batch 4: 0.220444/0.0760803loss in batch 5: 0.02948/0.0682983loss in batch 6: 0.510025/0.131409loss in batch 7: 0.303253/0.152893loss in batch 8: 0.0690155/0.14357loss in batch 9: 0.20871/0.150085loss in batch 10: 0.267349/0.160736loss in batch 11: 0.234756/0.166916loss in batch 12: 0.0752716/0.159851loss in batch 13: 0.520264/0.185608loss in batch 14: 0.13063/0.181946loss in batch 15: 0.0153961/0.171539loss in batch 16: 0.0331116/0.163391loss in batch 17: 0.0737152/0.158417loss in batch 18: 0.018692/0.151062loss in batch 19: 0.323074/0.159668loss in batch 20: 0.116577/0.157608loss in batch 21: 0.00614929/0.150726loss in batch 22: 0.142365/0.150345loss in batch 23: 0.0870209/0.147705loss in batch 24: 0.0371246/0.143295loss in batch 25: 0.144409/0.143326loss in batch 26: 0.226959/0.146439loss in batch 27: 0.269775/0.150833loss in batch 28: 0.268616/0.154907loss in batch 29: 0.0895081/0.15271loss in batch 30: 0.954208/0.178574loss in batch 31: 0.471817/0.187744loss in batch 32: 0.0170746/0.182571loss in batch 33: 0.0249176/0.177933loss in batch 34: 0.269058/0.180542loss in batch 35: 1.26204/0.210571loss in batch 36: 0.176224/0.209656loss in batch 37: 0.150314/0.208084loss in batch 38: 0.129562/0.20607loss in batch 39: 0.0312195/0.201691loss in batch 40: 0.0702667/0.198502loss in batch 41: 0.609116/0.208267loss in batch 42: 0.121414/0.206253loss in batch 43: 0.0318146/0.202286loss in batch 44: 0.0454102/0.198792loss in batch 45: 0.761765/0.211044loss in batch 46: 0.213135/0.21109loss in batch 47: 0.144897/0.209702loss in batch 48: 0.0734863/0.206924loss in batch 49: 0.109436/0.204971loss in batch 50: 0.00634766/0.20108loss in batch 51: 0.61496/0.20903loss in batch 52: 0.0347748/0.20575loss in batch 53: 0.00723267/0.202072loss in batch 54: 0.0123138/0.198624loss in batch 55: 0.0286713/0.195587loss in batch 56: 0.219238/0.195999loss in batch 57: 0.0132294/0.192841loss in batch 58: 0.065567/0.190704loss in batch 59: 0.0262756/0.187958loss in batch 60: 0.110458/0.186691loss in batch 61: 0.628479/0.193802loss in batch 62: 0.194565/0.193817loss in batch 63: 0.0849762/0.192123loss in batch 64: 0.0736084/0.190292loss in batch 65: 0.0509949/0.188187loss in batch 66: 0.0495453/0.186127loss in batch 67: 0.175873/0.185974loss in batch 68: 0.0197449/0.183563loss in batch 69: 0.291275/0.185104loss in batch 70: 0.11322/0.184097loss in batch 71: 0.358093/0.186508loss in batch 72: 0.278687/0.187759loss in batch 73: 0.0523376/0.185944loss in batch 74: 0.0324402/0.183884loss in batch 75: 0.104401/0.182831loss in batch 76: 0.156342/0.18251loss in batch 77: 0.0292206/0.180542loss in batch 78: 0.183472/0.180573loss in batch 79: 0.047348/0.178909loss in batch 80: 0.0300293/0.177063loss in batch 81: 0.220367/0.177597loss in batch 82: 0.00411987/0.175507loss in batch 83: 0.15213/0.175232loss in batch 84: 0.0324402/0.173538loss in batch 85: 0.0260315/0.171844loss in batch 86: 0.0894775/0.170883loss in batch 87: 0.00422668/0.168991loss in batch 88: 0.0251007/0.167374loss in batch 89: 0.360107/0.169525loss in batch 90: 0.0452576/0.168152loss in batch 91: 0.094101/0.167343loss in batch 92: 0.0575256/0.166168loss in batch 93: 0.393356/0.168579loss in batch 94: 0.0797577/0.167648loss in batch 95: 0.0567322/0.166504loss in batch 96: 0.170044/0.166519loss in batch 97: 0.29361/0.167831loss in batch 98: 0.00918579/0.166214loss in batch 99: 0.0342407/0.164902loss in batch 100: 0.0289612/0.163544loss in batch 101: 0.0301971/0.162247loss in batch 102: 0.16333/0.162262loss in batch 103: 0.321747/0.163803loss in batch 104: 0.107651/0.163269loss in batch 105: 0.0648956/0.162338loss in batch 106: 0.167206/0.162369loss in batch 107: 0.104568/0.161835loss in batch 108: 0.0272827/0.160599loss in batch 109: 0.000991821/0.159164loss in batch 110: 0.00726318/0.157791loss in batch 111: 0.0533295/0.15686loss in batch 112: 0.0224152/0.15567loss in batch 113: 0.0150757/0.154419loss in batch 114: 0.297974/0.155685loss in batch 115: 0.25885/0.15657loss in batch 116: 0.0718689/0.155838loss in batch 117: 0.130936/0.15564loss in batch 118: 0.0464478/0.154709loss in batch 119: 1.23801/0.163757loss in batch 120: 0.00398254/0.162415loss in batch 121: 0.0284576/0.161331loss in batch 122: 0.0814819/0.160675loss in batch 123: 0.0354309/0.159668loss in batch 124: 0.42691/0.161804loss in batch 125: 0.019577/0.160675loss in batch 126: 0.180542/0.160843loss in batch 127: 0.692154/0.164978loss in batch 128: 0.0504761/0.164108loss in batch 129: 0.0428314/0.163162loss in batch 130: 0.0423279/0.162247loss in batch 131: 0.0267792/0.161209loss in batch 132: 0.0203247/0.160156loss in batch 133: 0.0479584/0.159317loss in batch 134: 0.076767/0.158707loss in batch 135: 0.256149/0.159424loss in batch 136: 0.145248/0.159317loss in batch 137: 0.227615/0.159821loss in batch 138: 0.0234222/0.158844loss in batch 139: 0.0329285/0.157944loss in batch 140: 0.205322/0.158279loss in batch 141: 0.0317993/0.157379loss in batch 142: 0.0207672/0.156433loss in batch 143: 0.0864868/0.15593loss in batch 144: 0.0118256/0.154953loss in batch 145: 0.107849/0.154617loss in batch 146: 0.228561/0.155121loss in batch 147: 0.0591125/0.15448loss in batch 148: 0.0421906/0.153717loss in batch 149: 0.0244598/0.152863loss in batch 150: 0.00291443/0.151855loss in batch 151: 0.221161/0.152328loss in batch 152: 0.0234985/0.151489loss in batch 153: 0.297958/0.152435loss in batch 154: 0.011673/0.15152loss in batch 155: 0.154541/0.151535loss in batch 156: 0.597366/0.154373loss in batch 157: 0.410553/0.156006loss in batch 158: 0.0244904/0.155167loss in batch 159: 1.02087/0.160583loss in batch 160: 0.0565033/0.159927loss in batch 161: 0.333755/0.161011loss in batch 162: 0.32164/0.162003loss in batch 163: 0.173126/0.162064loss in batch 164: 0.0289917/0.161255loss in batch 165: 0.370804/0.162521loss in batch 166: 0.0411224/0.161804loss in batch 167: 0.132904/0.161636loss in batch 168: 0.284088/0.162354loss in batch 169: 0.0475159/0.161682loss in batch 170: 0.100372/0.161316loss in batch 171: 0.0476532/0.16066loss in batch 172: 0.0520477/0.160034loss in batch 173: 0.0882416/0.159622loss in batch 174: 0.0675659/0.159088loss in batch 175: 0.0675507/0.158569loss in batch 176: 0.153549/0.158539loss in batch 177: 0.0373077/0.157852loss in batch 178: 0.363205/0.158997loss in batch 179: 0.106583/0.158722loss in batch 180: 0.204376/0.158966loss in batch 181: 0.990021/0.163544loss in batch 182: 0.0609436/0.162979loss in batch 183: 0.142044/0.162857loss in batch 184: 0.296158/0.163589loss in batch 185: 0.00117493/0.162704loss in batch 186: 0.101151/0.162384loss in batch 187: 0.0336304/0.161697loss in batch 188: 0.131897/0.16153loss in batch 189: 0.107758/0.16124loss in batch 190: 1.03519/0.165833loss in batch 191: 0.21904/0.166107loss in batch 192: 0.000915527/0.165253loss in batch 193: 0.103317/0.164932loss in batch 194: 0.0544891/0.164368loss in batch 195: 0.0678406/0.163879loss in batch 196: 0.0401917/0.163254loss in batch 197: 0.0388641/0.162613loss in batch 198: 0.0170135/0.161896loss in batch 199: 0.236847/0.162262loss in batch 200: 0.0492554/0.161697loss in batch 201: 0.0971222/0.161377loss in batch 202: 0.0362091/0.160751loss in batch 203: 0.50563/0.162445loss in batch 204: 0.406647/0.163635loss in batch 205: 0.0624084/0.163162loss in batch 206: 0.151917/0.163086loss in batch 207: 0.327286/0.163895loss in batch 208: 0.108856/0.16362loss in batch 209: 0.0141907/0.162918loss in batch 210: 0.0243683/0.162247loss in batch 211: 0.0362091/0.161667loss in batch 212: 0.0394135/0.161072
done with epoch 16
train_acc: 0.950704 (405/426)
test loss: 0.0394135
acc: 0.951049 (136/143)
loss in batch 0: 0.195511/0.195511loss in batch 1: 0.0914459/0.143478loss in batch 2: 0.151321/0.146088loss in batch 3: 0.147858/0.146545loss in batch 4: 0.0601959/0.129257loss in batch 5: 0.245667/0.148666loss in batch 6: 0.00718689/0.128448loss in batch 7: 0.0399475/0.117386loss in batch 8: 0.0351868/0.108261loss in batch 9: 0.0487823/0.10231loss in batch 10: 0.0249176/0.0952759loss in batch 11: 0.105789/0.0961456loss in batch 12: 0.0186768/0.0901794loss in batch 13: 0.492401/0.118912loss in batch 14: 0.0825653/0.116501loss in batch 15: 0.03508/0.111404loss in batch 16: 0.0594177/0.108353loss in batch 17: 0.0446167/0.104813loss in batch 18: 0.519943/0.126663loss in batch 19: 1.57343/0.19899loss in batch 20: 0.116486/0.195068loss in batch 21: 0.241898/0.197205loss in batch 22: 0.0281982/0.18985loss in batch 23: 0.00157166/0.182007loss in batch 24: 0.0205994/0.175552loss in batch 25: 0.128006/0.173721loss in batch 26: 0.0362549/0.168625loss in batch 27: 0.0197144/0.163315loss in batch 28: 0.0118256/0.158081loss in batch 29: 0.0410309/0.154175loss in batch 30: 0.0461731/0.150696loss in batch 31: 0.0228729/0.146713loss in batch 32: 0.0205994/0.142883loss in batch 33: 0.114532/0.142059loss in batch 34: 0.00814819/0.138214loss in batch 35: 0.145096/0.138412loss in batch 36: 0.363281/0.144485loss in batch 37: 0.0298615/0.141464loss in batch 38: 0.0215607/0.138397loss in batch 39: 0.0897064/0.137177loss in batch 40: 1.15211/0.161942loss in batch 41: 0.0733032/0.159836loss in batch 42: 0.351501/0.164291loss in batch 43: 0.305893/0.167496loss in batch 44: 0.0475922/0.164841loss in batch 45: 0.089798/0.163208loss in batch 46: 0.03685/0.160522loss in batch 47: 0.168442/0.16069loss in batch 48: 0.207672/0.161652loss in batch 49: 0.0176239/0.158768loss in batch 50: 0.0633087/0.156891loss in batch 51: 0.369171/0.16098loss in batch 52: 0.102142/0.159851loss in batch 53: 0.654037/0.169022loss in batch 54: 0.299805/0.171387loss in batch 55: 0.0153503/0.16861loss in batch 56: 0.162613/0.168503loss in batch 57: 0.242538/0.169769loss in batch 58: 0.027832/0.167374loss in batch 59: 0.0167542/0.164871loss in batch 60: 0.0604553/0.163162loss in batch 61: 0.0239258/0.160904loss in batch 62: 0.119339/0.160248loss in batch 63: 0.0843811/0.159058loss in batch 64: 0.11673/0.158417loss in batch 65: 0.0557404/0.156845loss in batch 66: 0.120956/0.156326loss in batch 67: 0.064621/0.154968loss in batch 68: 0.466248/0.159485loss in batch 69: 0.0267334/0.157593loss in batch 70: 0.0317078/0.155807loss in batch 71: 0.0387268/0.154175loss in batch 72: 0.0376892/0.152588loss in batch 73: 0.0404663/0.151077loss in batch 74: 1.02522/0.162735loss in batch 75: 0.0214539/0.160873loss in batch 76: 0.0401917/0.159302loss in batch 77: 0.0507202/0.157913loss in batch 78: 0.0877686/0.157028loss in batch 79: 0.0126953/0.155212loss in batch 80: 0.041153/0.153809loss in batch 81: 0.0350342/0.152359loss in batch 82: 0.104065/0.151779loss in batch 83: 0.0831604/0.150955loss in batch 84: 0.0412598/0.149673loss in batch 85: 0.456223/0.153244loss in batch 86: 0.0579681/0.152145loss in batch 87: 0.239227/0.153137loss in batch 88: 0.0266571/0.151718loss in batch 89: 0.0927429/0.151047loss in batch 90: 0.0226593/0.149643loss in batch 91: 0.0292206/0.148346loss in batch 92: 0.0384827/0.147141loss in batch 93: 0.139801/0.147079loss in batch 94: 0.0623322/0.146179loss in batch 95: 0.185715/0.146591loss in batch 96: 0.232925/0.147491loss in batch 97: 0.0618744/0.146622loss in batch 98: 0.583344/0.151031loss in batch 99: 0.115005/0.15065loss in batch 100: 0.389023/0.153015loss in batch 101: 0.0209351/0.151733loss in batch 102: 0.147247/0.151688loss in batch 103: 0.0324707/0.150528loss in batch 104: 0.0349426/0.149429loss in batch 105: 0.0241089/0.148254loss in batch 106: 0.0246735/0.14711loss in batch 107: 0.178604/0.1474loss in batch 108: 0.239441/0.148239loss in batch 109: 0.265976/0.149307loss in batch 110: 0.129562/0.149124loss in batch 111: 0.132111/0.148972loss in batch 112: 0.155746/0.149033loss in batch 113: 0.356171/0.150848loss in batch 114: 0.0145721/0.149673loss in batch 115: 0.525177/0.152908loss in batch 116: 0.0647125/0.152145loss in batch 117: 0.0406952/0.151215loss in batch 118: 0.0917664/0.150711loss in batch 119: 0.428757/0.153015loss in batch 120: 0.0830994/0.152435loss in batch 121: 0.0529938/0.151627loss in batch 122: 0.0270081/0.15062loss in batch 123: 0.499359/0.153427loss in batch 124: 0.0526276/0.152618loss in batch 125: 0.0256958/0.151611loss in batch 126: 0.269836/0.152557loss in batch 127: 0.0724792/0.151917loss in batch 128: 0.0267181/0.150955loss in batch 129: 0.060379/0.150253loss in batch 130: 0.146698/0.150223loss in batch 131: 0.516449/0.153loss in batch 132: 0.0461426/0.152191loss in batch 133: 0.00846863/0.151138loss in batch 134: 0.150345/0.151123loss in batch 135: 0.289063/0.15213loss in batch 136: 0.392059/0.153885loss in batch 137: 0.161362/0.153946loss in batch 138: 0.138718/0.153824loss in batch 139: 0.122849/0.15361loss in batch 140: 0.0244598/0.152695loss in batch 141: 0.0285797/0.15181loss in batch 142: 0.289658/0.152771loss in batch 143: 0.333954/0.154037loss in batch 144: 0.297089/0.155029loss in batch 145: 0.0848846/0.154556loss in batch 146: 0.0233002/0.153656loss in batch 147: 0.299896/0.154648loss in batch 148: 0.229416/0.155136loss in batch 149: 0.0992279/0.15477loss in batch 150: 0.0667572/0.15419loss in batch 151: 0.0433807/0.153458loss in batch 152: 0.0114288/0.152527loss in batch 153: 0.0912018/0.15213loss in batch 154: 0.161728/0.152191loss in batch 155: 0.0175781/0.151321loss in batch 156: 0.0154572/0.150467loss in batch 157: 0.0349121/0.14975loss in batch 158: 0.0975647/0.149399loss in batch 159: 0.287079/0.150269loss in batch 160: 0.342896/0.151459loss in batch 161: 0.0765533/0.151001loss in batch 162: 0.25148/0.151627loss in batch 163: 0.0518951/0.151016loss in batch 164: 0.235855/0.151535loss in batch 165: 0.00328064/0.150635loss in batch 166: 0.03862/0.149963loss in batch 167: 0.0358734/0.149277loss in batch 168: 0.0392303/0.148636loss in batch 169: 0.0304565/0.147934loss in batch 170: 0.337357/0.149033loss in batch 171: 0.0378265/0.148407loss in batch 172: 0.0182343/0.147644loss in batch 173: 0.439575/0.149323loss in batch 174: 0.162445/0.149399loss in batch 175: 0.4991/0.151382loss in batch 176: 0.038269/0.150742loss in batch 177: 0.0492249/0.150177loss in batch 178: 0.272141/0.150848loss in batch 179: 0.00675964/0.150055loss in batch 180: 0.143616/0.150024loss in batch 181: 0.05896/0.149521loss in batch 182: 0.124527/0.149384loss in batch 183: 0.175766/0.149536loss in batch 184: 0.0660248/0.149078loss in batch 185: 0.88858/0.153061loss in batch 186: 0.255066/0.153595loss in batch 187: 0.421768/0.155014loss in batch 188: 0.0295105/0.154358loss in batch 189: 0.169632/0.154434loss in batch 190: 0.383057/0.15564loss in batch 191: 1.34789/0.161835loss in batch 192: 0.191422/0.162003loss in batch 193: 0.0360565/0.161346loss in batch 194: 0.0661163/0.160873loss in batch 195: 0.104324/0.160583loss in batch 196: 0.00906372/0.159805loss in batch 197: 0.00195313/0.158997loss in batch 198: 0.0246887/0.158325loss in batch 199: 0.0824432/0.157944loss in batch 200: 0.000671387/0.157181loss in batch 201: 0.0969543/0.156876loss in batch 202: 0.234192/0.157257loss in batch 203: 0.133163/0.157135loss in batch 204: 0.477066/0.158691loss in batch 205: 0.0402679/0.158112loss in batch 206: 0.285431/0.158737loss in batch 207: 0.116974/0.158539loss in batch 208: 0.335938/0.159378loss in batch 209: 0.0694427/0.158966loss in batch 210: 0.383942/0.160019loss in batch 211: 0.17337/0.16008loss in batch 212: 0.0367889/0.1595
done with epoch 17
train_acc: 0.941315 (401/426)
test loss: 0.0367889
acc: 0.937063 (134/143)
loss in batch 0: 0.124939/0.124939loss in batch 1: 0.0230713/0.0740051loss in batch 2: 0.498322/0.215439loss in batch 3: 0.0284882/0.168716loss in batch 4: 0.0326843/0.141495loss in batch 5: 0.0371399/0.1241loss in batch 6: 0.0333252/0.11113loss in batch 7: 0.182632/0.120071loss in batch 8: 0.0677643/0.114273loss in batch 9: 0.100708/0.112915loss in batch 10: 0.00320435/0.102936loss in batch 11: 0.0321655/0.0970306loss in batch 12: 0.0389252/0.0925598loss in batch 13: 0.254059/0.104095loss in batch 14: 0.05896/0.101089loss in batch 15: 0.0470123/0.0977173loss in batch 16: 0.0726624/0.0962372loss in batch 17: 0.30188/0.107666loss in batch 18: 0.0311127/0.103638loss in batch 19: 0.388214/0.117859loss in batch 20: 0.099762/0.117004loss in batch 21: 0.138214/0.117966loss in batch 22: 0.0830994/0.116455loss in batch 23: 0.317963/0.124847loss in batch 24: 0.0335693/0.121185loss in batch 25: 0.0705261/0.119247loss in batch 26: 0.00941467/0.115189loss in batch 27: 0.133881/0.115845loss in batch 28: 0.0616913/0.113983loss in batch 29: 0.321671/0.120895loss in batch 30: 0.0760803/0.119446loss in batch 31: 0.465485/0.130264loss in batch 32: 0.0294952/0.127213loss in batch 33: 0.355667/0.133926loss in batch 34: 0.354263/0.140228loss in batch 35: 0.0453339/0.137589loss in batch 36: 0.0445709/0.135071loss in batch 37: 0.168396/0.135956loss in batch 38: 0.119431/0.135529loss in batch 39: 0.19342/0.136978loss in batch 40: 0.417816/0.143829loss in batch 41: 0.375717/0.149338loss in batch 42: 0.158096/0.149567loss in batch 43: 0.0760803/0.147888loss in batch 44: 0.610703/0.158173loss in batch 45: 0.0162659/0.15509loss in batch 46: 0.00468445/0.151886loss in batch 47: 0.102356/0.150848loss in batch 48: 0.0203552/0.148178loss in batch 49: 0.422836/0.153687loss in batch 50: 0.0452271/0.15155loss in batch 51: 0.039505/0.149399loss in batch 52: 0.257401/0.151428loss in batch 53: 0.00735474/0.148773loss in batch 54: 0.0273743/0.146576loss in batch 55: 0.132874/0.146317loss in batch 56: 0.171692/0.146759loss in batch 57: 0.0586243/0.145248loss in batch 58: 0.00688171/0.142899loss in batch 59: 0.676544/0.151779loss in batch 60: 0.0167084/0.149567loss in batch 61: 0.0795898/0.148453loss in batch 62: 0.154114/0.148529loss in batch 63: 0.0246582/0.146591loss in batch 64: 1.12302/0.161636loss in batch 65: 0.0194855/0.15947loss in batch 66: 0.0364532/0.157639loss in batch 67: 0.100876/0.156799loss in batch 68: 0.209915/0.157578loss in batch 69: 0.368393/0.160583loss in batch 70: 0.109192/0.159851loss in batch 71: 0.0414886/0.158218loss in batch 72: 0.0580292/0.156845loss in batch 73: 0.170059/0.157013loss in batch 74: 0.496078/0.161545loss in batch 75: 0.0170135/0.159637loss in batch 76: 0.0647888/0.158401loss in batch 77: 0.544952/0.163376loss in batch 78: 0.501968/0.167664loss in batch 79: 0.00363159/0.165604loss in batch 80: 0.257904/0.166733loss in batch 81: 0.1026/0.165955loss in batch 82: 0.353607/0.168213loss in batch 83: 0.0494385/0.166809loss in batch 84: 0.757889/0.173752loss in batch 85: 1.21132/0.185822loss in batch 86: 0.0115356/0.183823loss in batch 87: 0.131607/0.183228loss in batch 88: 0.054657/0.181778loss in batch 89: 0.324249/0.183365loss in batch 90: 0.0522003/0.181931loss in batch 91: 0.0755768/0.180771loss in batch 92: 0.067688/0.17955loss in batch 93: 0.0995636/0.178696loss in batch 94: 0.00915527/0.17691loss in batch 95: 0.138306/0.176514loss in batch 96: 0.0218658/0.174927loss in batch 97: 0.455154/0.17778loss in batch 98: 0.0649872/0.176636loss in batch 99: 0.015976/0.175034loss in batch 100: 0.0249634/0.173538loss in batch 101: 0.345932/0.175232loss in batch 102: 0.104019/0.174545loss in batch 103: 0.168655/0.1745loss in batch 104: 0.0222473/0.17305loss in batch 105: 0.00505066/0.171463loss in batch 106: 0.03685/0.170197loss in batch 107: 0.133545/0.169861loss in batch 108: 0.216125/0.170288loss in batch 109: 0.11702/0.1698loss in batch 110: 0.107086/0.169235loss in batch 111: 0.0384674/0.168076loss in batch 112: 0.554123/0.171478loss in batch 113: 0.0200195/0.170151loss in batch 114: 0.443954/0.172531loss in batch 115: 0.0166473/0.171188loss in batch 116: 0.0362549/0.170044loss in batch 117: 0.00875854/0.168671loss in batch 118: 0.0335083/0.167526loss in batch 119: 0.0291901/0.166382loss in batch 120: 0.044281/0.165359loss in batch 121: 0.0474396/0.164413loss in batch 122: 0.0563812/0.163528loss in batch 123: 0.212555/0.16391loss in batch 124: 0.378143/0.165634loss in batch 125: 0.215195/0.166031loss in batch 126: 0.225693/0.166504loss in batch 127: 0.0413208/0.165512loss in batch 128: 0.216843/0.165924loss in batch 129: 0.774063/0.170609loss in batch 130: 0.0467529/0.169647loss in batch 131: 0.0913086/0.169067loss in batch 132: 0.125443/0.168732loss in batch 133: 0.0211792/0.167633loss in batch 134: 0.0217285/0.16655loss in batch 135: 0.0186005/0.165466loss in batch 136: 0.138596/0.165268loss in batch 137: 0.0943451/0.164749loss in batch 138: 0.0541534/0.163956loss in batch 139: 0.0254822/0.162979loss in batch 140: 0.0354919/0.162064loss in batch 141: 0.00396729/0.16095loss in batch 142: 0.0625153/0.160263loss in batch 143: 0.361954/0.161667loss in batch 144: 0.00590515/0.160599loss in batch 145: 0.229904/0.161072loss in batch 146: 0.203598/0.161346loss in batch 147: 0.100967/0.160934loss in batch 148: 0.0332031/0.16008loss in batch 149: 0.105347/0.159729loss in batch 150: 0.402435/0.161316loss in batch 151: 0.0469818/0.160583loss in batch 152: 0.071991/0.159988loss in batch 153: 0.0959778/0.159592loss in batch 154: 0.0718994/0.159012loss in batch 155: 0.00746155/0.158051loss in batch 156: 0.0278931/0.157227loss in batch 157: 0.0847473/0.156754loss in batch 158: 0.106903/0.156433loss in batch 159: 0.074295/0.15593loss in batch 160: 0.030365/0.155136loss in batch 161: 0.0467529/0.15448loss in batch 162: 0.0476532/0.153839loss in batch 163: 0.242294/0.154358loss in batch 164: 0.319717/0.155365loss in batch 165: 0.0726776/0.154861loss in batch 166: 0.203873/0.155151loss in batch 167: 0.0248108/0.154373loss in batch 168: 0.0430603/0.153717loss in batch 169: 0.0404968/0.153061loss in batch 170: 0.287537/0.153854loss in batch 171: 0.456421/0.155609loss in batch 172: 0.0137177/0.154785loss in batch 173: 0.147598/0.154739loss in batch 174: 0.13588/0.154633loss in batch 175: 0.0952454/0.154297loss in batch 176: 0.0792999/0.15387loss in batch 177: 0.856354/0.157822loss in batch 178: 0.00274658/0.156952loss in batch 179: 0.070816/0.156479loss in batch 180: 0.123856/0.156296loss in batch 181: 0.213806/0.156616loss in batch 182: 0.328461/0.157547loss in batch 183: 0.00224304/0.156708loss in batch 184: 0.104919/0.156418loss in batch 185: 0.0122681/0.155655loss in batch 186: 0.00889587/0.154861loss in batch 187: 0.00222778/0.154053loss in batch 188: 0.183868/0.154221loss in batch 189: 0.026062/0.153534loss in batch 190: 0.0450439/0.152985loss in batch 191: 0.27179/0.153595loss in batch 192: 1.56131/0.160889loss in batch 193: 0.0945587/0.160538loss in batch 194: 0.0794983/0.160141loss in batch 195: 0.0338135/0.15947loss in batch 196: 0.0146332/0.158752loss in batch 197: 0.00263977/0.157959loss in batch 198: 0.0759583/0.157547loss in batch 199: 0.0369415/0.156937loss in batch 200: 0.249222/0.15741loss in batch 201: 0.035141/0.156799loss in batch 202: 0.00215149/0.156021loss in batch 203: 0.234985/0.156418loss in batch 204: 0.347504/0.157349loss in batch 205: 0.00465393/0.156601loss in batch 206: 0.340118/0.157486loss in batch 207: 0.337585/0.158371loss in batch 208: 0.117752/0.158173loss in batch 209: 0.0251617/0.157547loss in batch 210: 0.0328522/0.156952loss in batch 211: 0.0337524/0.156357loss in batch 212: 0.0205383/0.155731
done with epoch 18
train_acc: 0.950704 (405/426)
test loss: 0.0205383
acc: 0.944056 (135/143)
loss in batch 0: 0.157928/0.157928loss in batch 1: 0.0566559/0.107285loss in batch 2: 0.101227/0.10527loss in batch 3: 0.0621948/0.0944977loss in batch 4: 0.060318/0.0876617loss in batch 5: 0.00740051/0.0742798loss in batch 6: 0.74791/0.170517loss in batch 7: 0.00422668/0.149734loss in batch 8: 0.026001/0.135986loss in batch 9: 0.269608/0.149353loss in batch 10: 0.116089/0.146317loss in batch 11: 0.0245667/0.136185loss in batch 12: 0.0800629/0.131851loss in batch 13: 0.074234/0.127747loss in batch 14: 0.272141/0.137375loss in batch 15: 0.0116425/0.129517loss in batch 16: 0.0359955/0.124008loss in batch 17: 0.0658112/0.120773loss in batch 18: 0.319183/0.131226loss in batch 19: 0.0642242/0.127869loss in batch 20: 0.115295/0.127274loss in batch 21: 0.0397797/0.123291loss in batch 22: 0.0620728/0.120636loss in batch 23: 0.0391388/0.117233loss in batch 24: 0.0665131/0.115204loss in batch 25: 0.034317/0.112106loss in batch 26: 0.0305023/0.109085loss in batch 27: 0.269348/0.114807loss in batch 28: 0.0375366/0.112137loss in batch 29: 0.220139/0.115738loss in batch 30: 0.0720215/0.114319loss in batch 31: 0.0107117/0.111099loss in batch 32: 0.0605164/0.109543loss in batch 33: 0.028717/0.107178loss in batch 34: 0.450424/0.116989loss in batch 35: 0.223251/0.119934loss in batch 36: 0.099823/0.119385loss in batch 37: 0.00906372/0.116486loss in batch 38: 0.0248108/0.114136loss in batch 39: 0.588486/0.125992loss in batch 40: 0.363846/0.13179loss in batch 41: 0.0248413/0.129257loss in batch 42: 0.0471497/0.12735loss in batch 43: 0.00204468/0.124481loss in batch 44: 0.0678864/0.12323loss in batch 45: 0.177948/0.12442loss in batch 46: 0.0346222/0.122513loss in batch 47: 0.0206909/0.120392loss in batch 48: 0.174362/0.121506loss in batch 49: 0.438431/0.127838loss in batch 50: 0.0555573/0.126419loss in batch 51: 0.0375061/0.12471loss in batch 52: 0.0268555/0.122849loss in batch 53: 0.14827/0.123322loss in batch 54: 0.106659/0.123032loss in batch 55: 0.0826874/0.122314loss in batch 56: 0.321655/0.125793loss in batch 57: 1.74281/0.153687loss in batch 58: 0.0352631/0.151672loss in batch 59: 0.0832062/0.150528loss in batch 60: 0.00828552/0.148193loss in batch 61: 0.0187073/0.146118loss in batch 62: 0.0640564/0.144806loss in batch 63: 0.435074/0.149353loss in batch 64: 0.138367/0.14917loss in batch 65: 0.526794/0.154907loss in batch 66: 0.116013/0.154327loss in batch 67: 0.2435/0.155624loss in batch 68: 0.13739/0.15538loss in batch 69: 0.0113373/0.153305loss in batch 70: 0.0860748/0.152359loss in batch 71: 0.0540466/0.150986loss in batch 72: 0.039444/0.14946loss in batch 73: 0.0414124/0.14801loss in batch 74: 0.0234375/0.146347loss in batch 75: 0.0111389/0.144562loss in batch 76: 0.0479584/0.143311loss in batch 77: 0.298096/0.145294loss in batch 78: 0.269119/0.146866loss in batch 79: 0.0982056/0.146255loss in batch 80: 0.132538/0.146088loss in batch 81: 0.0900574/0.145416loss in batch 82: 0.0298615/0.144012loss in batch 83: 0.0119324/0.142441loss in batch 84: 0.0508423/0.141357loss in batch 85: 0.00535583/0.139786loss in batch 86: 0.280548/0.141403loss in batch 87: 0.573792/0.146301loss in batch 88: 0.0475616/0.145203loss in batch 89: 0.227081/0.146103loss in batch 90: 0.0167999/0.144684loss in batch 91: 0.0529633/0.143707loss in batch 92: 0.576431/0.148346loss in batch 93: 0.229141/0.149216loss in batch 94: 0.0305481/0.147964loss in batch 95: 0.00935364/0.146515loss in batch 96: 0.00868225/0.145096loss in batch 97: 0.881927/0.152618loss in batch 98: 0.0160675/0.151245loss in batch 99: 0.0944519/0.150665loss in batch 100: 0.0132751/0.149307loss in batch 101: 0.0827026/0.148651loss in batch 102: 0.416077/0.151245loss in batch 103: 0.399399/0.153625loss in batch 104: 0.0159149/0.152313loss in batch 105: 0.0977325/0.15181loss in batch 106: 0.159897/0.151886loss in batch 107: 0.166458/0.152008loss in batch 108: 0.114471/0.151672loss in batch 109: 0.310715/0.153122loss in batch 110: 0.095459/0.152603loss in batch 111: 0.0427094/0.151611loss in batch 112: 0.983597/0.158981loss in batch 113: 0.237732/0.159668loss in batch 114: 0.470245/0.162369loss in batch 115: 0.016098/0.161118loss in batch 116: 0.296829/0.162277loss in batch 117: 0.223251/0.162781loss in batch 118: 0.380173/0.164612loss in batch 119: 0.343948/0.166122loss in batch 120: 0.335892/0.167526loss in batch 121: 0.0435791/0.166504loss in batch 122: 0.395691/0.168365loss in batch 123: 0.208313/0.168686loss in batch 124: 0.0270386/0.167557loss in batch 125: 0.134171/0.167297loss in batch 126: 0.0435486/0.166321loss in batch 127: 0.250488/0.166962loss in batch 128: 0.382843/0.16864loss in batch 129: 0.021698/0.167511loss in batch 130: 0.00642395/0.166275loss in batch 131: 0.0791779/0.165634loss in batch 132: 0.0209656/0.16452loss in batch 133: 0.110321/0.164139loss in batch 134: 0.100952/0.163666loss in batch 135: 0.0550079/0.162872loss in batch 136: 0.0032959/0.161697loss in batch 137: 0.148804/0.161606loss in batch 138: 0.0357971/0.160706loss in batch 139: 0.0808563/0.160141loss in batch 140: 0.142548/0.160004loss in batch 141: 0.232193/0.160522loss in batch 142: 1.01387/0.166473loss in batch 143: 0.0923157/0.16597loss in batch 144: 0.346146/0.167221loss in batch 145: 0.0186005/0.166183loss in batch 146: 0.877457/0.171036loss in batch 147: 0.0274658/0.170059loss in batch 148: 0.0777283/0.169434loss in batch 149: 0.250397/0.169983loss in batch 150: 0.0482483/0.169174loss in batch 151: 0.0700073/0.168518loss in batch 152: 0.0517426/0.167755loss in batch 153: 0.669998/0.171021loss in batch 154: 0.0189667/0.170029loss in batch 155: 0.101608/0.169601loss in batch 156: 0.0301971/0.168701loss in batch 157: 0.0284729/0.167831loss in batch 158: 0.0392761/0.167023loss in batch 159: 0.510452/0.169159loss in batch 160: 0.189468/0.169296loss in batch 161: 0.0128784/0.16832loss in batch 162: 0.00567627/0.167328loss in batch 163: 0.118088/0.167023loss in batch 164: 0.0497131/0.166306loss in batch 165: 0.0213013/0.165436loss in batch 166: 0.181168/0.165527loss in batch 167: 0.0313263/0.164734loss in batch 168: 0.119003/0.164459loss in batch 169: 0.201004/0.164673loss in batch 170: 0.04599/0.163986loss in batch 171: 0.0242462/0.163177loss in batch 172: 0.0228119/0.162354loss in batch 173: 0.115845/0.162094loss in batch 174: 0.209656/0.162369loss in batch 175: 0.06987/0.161835loss in batch 176: 0.0317841/0.161102loss in batch 177: 0.137924/0.16098loss in batch 178: 0.00897217/0.160126loss in batch 179: 0.191833/0.160309loss in batch 180: 0.495865/0.162155loss in batch 181: 0.0131226/0.161346loss in batch 182: 0.0649872/0.160812loss in batch 183: 0.0712891/0.160324loss in batch 184: 0.0326385/0.159622loss in batch 185: 0.278442/0.160263loss in batch 186: 0.00242615/0.159439loss in batch 187: 0.0639038/0.15892loss in batch 188: 0.519882/0.160828loss in batch 189: 0.0447388/0.160217loss in batch 190: 0.391235/0.161438loss in batch 191: 0.0465088/0.160828loss in batch 192: 0.680283/0.163528loss in batch 193: 0.072403/0.163055loss in batch 194: 0.0148926/0.162292loss in batch 195: 0.0122833/0.16153loss in batch 196: 0.00593567/0.160736loss in batch 197: 0.00239563/0.159927loss in batch 198: 0.042038/0.159332loss in batch 199: 0.0133972/0.158615loss in batch 200: 0.136047/0.158508loss in batch 201: 0.0605621/0.15802loss in batch 202: 0.00337219/0.157257loss in batch 203: 0.00949097/0.156525loss in batch 204: 0.110962/0.156311loss in batch 205: 0.157974/0.156311loss in batch 206: 0.14241/0.15625loss in batch 207: 0.0118866/0.155548loss in batch 208: 0.0611877/0.15509loss in batch 209: 0.0075531/0.154388loss in batch 210: 0.0417786/0.15387loss in batch 211: 0.0348358/0.153305loss in batch 212: 0.0121613/0.152649
done with epoch 19
train_acc: 0.955399 (407/426)
test loss: 0.0121613
acc: 0.944056 (135/143)
[0.13472, -0.0676117, -0.0740204, -0.12677, -0.0221252, -0.0340118, -0.112793, -0.016037, -0.00137329, -0.00297546, -0.319183, -0.121506, -0.00701904, -0.339371, -0.441986, 0.0677948, -0.0596466, 0.0100098, 0.0410614, 0.000762939, 0.398056, 0.0614777, -0.19458, -0.0355835, 0.00947571, -0.039093, -0.0105743, 0.0665588, -0.0527191, 0.0184784, -0.0120087, 0.0335999, -0.36908, 0.234222, -0.00164795, 0.258652, -0.108841, 0.11879, -0.109207, 0.253967, 0.814056, -0.0078125, 0.596161, -0.0050354, -0.432053, 0.000762939, -0.00120544, -0.0727386, -0.0128632, 0.0862274, 0.0442047, 0.130356, 0.0637207, -0.04245, -0.0592499, -0.0174103, -0.0171509, -0.0088501, -0.0513611, 0.000717163, 0.372147, 0.175156, -0.0112, -0.0327454, 0.0153656, -0.0460358, 0, 0.010376, 0.00759888, -0.00892639, -0.421188, 0.0256348, -0.0472565, 0.507706, 0.00422668, -0.0557709, -0.00862122, -0.175156, -0.0157318, -0.001297, 0.186401, 0.00062561, 0.0254974, -0.0110626, 0.0840454, -0.045166, -0.0187225, -0.00294495, 0.0484772, 6.10352e-05, -0.00622559, 0.539703, -0.466003, 0.00439453, -0.00454712, -0.0437469, 0.0132904, -0.556, -0.0346832, -0.0186157, -0.00170898, -0.071106, -0.0774994, -0.197647, 0.0496979, -0.0106812, 0.00352478, -0.161667, 0.667664, 0.679581, -0.047348, 0.00675964, 0.0429077, -0.31723, -0.0136414, -0.0360413, -0.340332, -0.0172882, -0.0492859, -0.127884, -0.0291748, -0.0271759, 0.241287, -0.109116, 0.00389099, -0.162521, -0.235123, -0.0281372, -0.0154572, -0.00430298, 0.11792, -0.00337219, -0.0140076, -0.255615, -0.161423, -0.0105286, -0.249084, 0.538681, 7.62939e-05, -0.0093689, -0.0887146, -0.0358124, 0.000701904]
Compiler: ./compile.py breast_logistic
	8484 dabits of replicated gfp left
	2022 dabits of replicated gfp left
2 threads spent a total of 548.879 seconds (400.029 MB, 1367614 rounds) on the online phase, 103.57 seconds (3677.61 MB, 91492 rounds) on the preprocessing/offline phase, and 654.924 seconds idling.
Join timer: 0 653668
Finish timer: 0.0571524
Join timer: 1 642742
Finish timer: 0.0571524
Communication details (rounds in parallel threads counted double):
Passing around 399.747 MB in 1367596 rounds, taking 183.615 seconds
Receiving directly 3677.61 MB in 45746 rounds, taking 15.2775 seconds
Sending directly 3677.89 MB in 45764 rounds, taking 8.36269 seconds
CPU time = 731.009 (overall core time)
The following benchmarks are including preprocessing (offline phase).
Time = 653.726 seconds 
Data sent = 4077.64 MB in ~1459106 rounds (party 0 only)
Global data sent = 12232.4 MB (all parties)
Actual cost of program:
  Type int
     229817398           Bits
This program might benefit from some protocol options.
Consider adding the following at the beginning of your code:
	program.use_trunc_pr = True
	program.use_edabit(True)
This virtual machine was compiled with GCC. Recompile with 'CXX = clang++' in 'CONFIG.mine' for optimal performance.
Command line: replicated-field-party.x --ip-file-name /HOST -p 0 -v breast_logistic
