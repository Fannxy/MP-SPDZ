Using statistical security parameter 40
Trying to run 64-bit computation
Current working directory: "/"
Current working directory: "/"
Using SGD
done with epoch 0done with epoch 1done with epoch 2done with epoch 3done with epoch 4done with epoch 5done with epoch 6done with epoch 7done with epoch 8done with epoch 9done with epoch 10done with epoch 11done with epoch 12done with epoch 13done with epoch 14done with epoch 15done with epoch 16done with epoch 17done with epoch 18done with epoch 19
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Using SGD
loss in batch 0: 0.622375/0.622375loss in batch 1: 0.922775/0.772583loss in batch 2: 0.612762/0.719315loss in batch 3: 0.690475/0.712097loss in batch 4: 0.67099/0.703873loss in batch 5: 0.626465/0.690979loss in batch 6: 0.773483/0.702759loss in batch 7: 0.629761/0.693649loss in batch 8: 0.788376/0.704163loss in batch 9: 0.697784/0.703522loss in batch 10: 0.68927/0.702225loss in batch 11: 0.66893/0.699463loss in batch 12: 0.633041/0.694351loss in batch 13: 0.590515/0.68692loss in batch 14: 0.652084/0.684616loss in batch 15: 0.715652/0.686554loss in batch 16: 0.566467/0.679489loss in batch 17: 0.674408/0.679199loss in batch 18: 0.551392/0.67247loss in batch 19: 0.625641/0.670135loss in batch 20: 0.691956/0.671173loss in batch 21: 0.708511/0.672867loss in batch 22: 0.474655/0.664246loss in batch 23: 0.70253/0.665848loss in batch 24: 0.75209/0.669296loss in batch 25: 0.926071/0.679169loss in batch 26: 0.700119/0.679947loss in batch 27: 0.391724/0.669662loss in batch 28: 0.467941/0.662704loss in batch 29: 0.698608/0.663895loss in batch 30: 0.93634/0.672684loss in batch 31: 0.751221/0.67514loss in batch 32: 0.811798/0.679276loss in batch 33: 0.501694/0.674057loss in batch 34: 0.554016/0.670624loss in batch 35: 0.565948/0.667725loss in batch 36: 0.560043/0.66481loss in batch 37: 0.511597/0.660782loss in batch 38: 0.66394/0.660858loss in batch 39: 0.66217/0.660904loss in batch 40: 0.531662/0.657745loss in batch 41: 0.497681/0.653915loss in batch 42: 0.630188/0.653381loss in batch 43: 0.816086/0.657074loss in batch 44: 0.701813/0.658081loss in batch 45: 0.397156/0.652405loss in batch 46: 0.819077/0.655945loss in batch 47: 0.671463/0.656265loss in batch 48: 0.741333/0.658005loss in batch 49: 0.711365/0.659073loss in batch 50: 0.503311/0.656021loss in batch 51: 0.571762/0.654388loss in batch 52: 0.518951/0.65184loss in batch 53: 0.658066/0.651947loss in batch 54: 0.504776/0.649277loss in batch 55: 0.613647/0.648636loss in batch 56: 0.638748/0.648468loss in batch 57: 0.592209/0.647507loss in batch 58: 0.472885/0.644547loss in batch 59: 0.593292/0.643677loss in batch 60: 0.533417/0.641876loss in batch 61: 0.553314/0.640442loss in batch 62: 0.503433/0.638275loss in batch 63: 0.559525/0.637054loss in batch 64: 0.615021/0.636703loss in batch 65: 0.428452/0.63356loss in batch 66: 0.558594/0.632431loss in batch 67: 0.45787/0.629868loss in batch 68: 0.431671/0.626984loss in batch 69: 0.56076/0.626038loss in batch 70: 0.633057/0.626144loss in batch 71: 0.662628/0.626648loss in batch 72: 0.448044/0.624191loss in batch 73: 0.671692/0.624847loss in batch 74: 0.634491/0.624969loss in batch 75: 0.394791/0.621948loss in batch 76: 0.727631/0.623322loss in batch 77: 0.545074/0.622314loss in batch 78: 0.375671/0.619202loss in batch 79: 0.442368/0.616989loss in batch 80: 0.362823/0.613846loss in batch 81: 0.645599/0.614227loss in batch 82: 0.371826/0.611313loss in batch 83: 0.447723/0.609375loss in batch 84: 0.48378/0.607895loss in batch 85: 0.314301/0.604462loss in batch 86: 0.801056/0.606735loss in batch 87: 0.602448/0.606689loss in batch 88: 0.362457/0.603928loss in batch 89: 0.634933/0.604294loss in batch 90: 0.628937/0.604553loss in batch 91: 0.644501/0.60498loss in batch 92: 0.345978/0.602203loss in batch 93: 0.71347/0.603394loss in batch 94: 0.507568/0.602386loss in batch 95: 0.519424/0.601501loss in batch 96: 0.595947/0.601456loss in batch 97: 0.655823/0.602005loss in batch 98: 0.4039/0.600006loss in batch 99: 0.394836/0.597961loss in batch 100: 0.525772/0.597229loss in batch 101: 0.483719/0.59613loss in batch 102: 0.566299/0.59584loss in batch 103: 0.499161/0.594925loss in batch 104: 0.496872/0.593979loss in batch 105: 0.420181/0.592346loss in batch 106: 0.444534/0.590958loss in batch 107: 0.590591/0.590958loss in batch 108: 0.564697/0.590714loss in batch 109: 0.507904/0.589966loss in batch 110: 0.248856/0.586884loss in batch 111: 0.420258/0.585388loss in batch 112: 0.442108/0.584137loss in batch 113: 0.542221/0.583755loss in batch 114: 0.401459/0.582184loss in batch 115: 0.418655/0.580765loss in batch 116: 0.474243/0.579849loss in batch 117: 0.553696/0.579636loss in batch 118: 0.690781/0.580566loss in batch 119: 0.271439/0.577988loss in batch 120: 0.510986/0.577438loss in batch 121: 0.411255/0.57608loss in batch 122: 0.327408/0.574051loss in batch 123: 0.44606/0.573029loss in batch 124: 0.785843/0.574738loss in batch 125: 0.77092/0.576279loss in batch 126: 0.251038/0.573715loss in batch 127: 0.237549/0.571091loss in batch 128: 0.612106/0.571411loss in batch 129: 0.73584/0.572678loss in batch 130: 0.385742/0.571259loss in batch 131: 0.508041/0.57077loss in batch 132: 0.297379/0.568726loss in batch 133: 0.812561/0.570541loss in batch 134: 0.720795/0.571655loss in batch 135: 0.738495/0.572876loss in batch 136: 0.685837/0.5737loss in batch 137: 0.504944/0.573196loss in batch 138: 0.629364/0.573608loss in batch 139: 0.444992/0.572693loss in batch 140: 0.360413/0.571182loss in batch 141: 0.310791/0.569351loss in batch 142: 0.270096/0.567261loss in batch 143: 0.372681/0.565903loss in batch 144: 0.568146/0.565918loss in batch 145: 0.382034/0.564651loss in batch 146: 0.404221/0.563568loss in batch 147: 0.336792/0.562027loss in batch 148: 0.776138/0.563461loss in batch 149: 0.776566/0.564896loss in batch 150: 0.483917/0.564362loss in batch 151: 0.664703/0.565033loss in batch 152: 0.651154/0.565582loss in batch 153: 0.513/0.565247loss in batch 154: 0.547302/0.565125loss in batch 155: 0.701279/0.565994loss in batch 156: 0.464554/0.565353loss in batch 157: 0.570831/0.565384loss in batch 158: 0.230713/0.563278loss in batch 159: 0.42923/0.562439loss in batch 160: 0.321762/0.560944loss in batch 161: 0.299591/0.559326loss in batch 162: 0.360168/0.558121loss in batch 163: 0.350327/0.556854loss in batch 164: 0.252136/0.554993loss in batch 165: 0.336624/0.55368loss in batch 166: 0.404434/0.552795loss in batch 167: 0.279282/0.551163loss in batch 168: 0.290024/0.549622loss in batch 169: 0.79718/0.551071loss in batch 170: 0.269379/0.549438loss in batch 171: 0.331421/0.548157loss in batch 172: 0.324524/0.54686loss in batch 173: 0.203964/0.544891loss in batch 174: 0.246017/0.543198loss in batch 175: 0.561188/0.543289loss in batch 176: 0.229599/0.541519loss in batch 177: 0.391724/0.540665loss in batch 178: 0.946045/0.542938loss in batch 179: 0.511612/0.54277loss in batch 180: 0.706512/0.543671loss in batch 181: 0.479691/0.54332loss in batch 182: 0.498611/0.543076loss in batch 183: 0.5634/0.543182loss in batch 184: 0.209442/0.541382loss in batch 185: 0.50827/0.541214loss in batch 186: 0.561905/0.541306loss in batch 187: 0.275238/0.539886loss in batch 188: 0.582489/0.540131loss in batch 189: 0.577347/0.540314loss in batch 190: 0.577896/0.540512loss in batch 191: 0.373322/0.539658loss in batch 192: 0.327896/0.538559loss in batch 193: 0.283768/0.537231loss in batch 194: 0.333572/0.536194loss in batch 195: 0.262207/0.53479loss in batch 196: 0.296417/0.533585loss in batch 197: 0.407455/0.532959loss in batch 198: 0.49144/0.532745loss in batch 199: 0.251511/0.531326loss in batch 200: 0.673294/0.532028loss in batch 201: 0.344986/0.531113loss in batch 202: 0.46756/0.530792loss in batch 203: 0.262253/0.529495loss in batch 204: 0.388397/0.528793loss in batch 205: 0.985931/0.531021loss in batch 206: 0.560333/0.531158loss in batch 207: 0.424393/0.53064loss in batch 208: 0.413254/0.530075loss in batch 209: 0.322098/0.529083loss in batch 210: 0.245407/0.52774loss in batch 211: 0.355804/0.526932loss in batch 212: 0.402023/0.526337
done with epoch 0
train_acc: 0.779343 (332/426)
test loss: 0.402022
acc: 0.923075 (132/143)
loss in batch 0: 0.519104/0.519104loss in batch 1: 0.457809/0.488464loss in batch 2: 0.428955/0.468613loss in batch 3: 0.311722/0.429398loss in batch 4: 0.548859/0.453278loss in batch 5: 0.425781/0.448715loss in batch 6: 0.303192/0.427917loss in batch 7: 0.300385/0.411972loss in batch 8: 0.561234/0.428558loss in batch 9: 0.564636/0.442169loss in batch 10: 0.543762/0.451401loss in batch 11: 0.312576/0.439835loss in batch 12: 0.574661/0.450195loss in batch 13: 0.27684/0.43782loss in batch 14: 0.533936/0.444229loss in batch 15: 0.251434/0.432175loss in batch 16: 0.457031/0.43364loss in batch 17: 0.485168/0.436508loss in batch 18: 0.404587/0.43483loss in batch 19: 0.359055/0.431046loss in batch 20: 0.533768/0.435928loss in batch 21: 0.427536/0.435547loss in batch 22: 0.302887/0.429779loss in batch 23: 0.377029/0.427582loss in batch 24: 0.288055/0.422012loss in batch 25: 0.718918/0.433426loss in batch 26: 0.38353/0.431564loss in batch 27: 0.578064/0.436798loss in batch 28: 0.449539/0.437241loss in batch 29: 0.217422/0.429916loss in batch 30: 0.36322/0.427765loss in batch 31: 0.483215/0.429504loss in batch 32: 0.293671/0.425385loss in batch 33: 0.30011/0.421707loss in batch 34: 0.635147/0.427795loss in batch 35: 0.638/0.43364loss in batch 36: 0.570419/0.437332loss in batch 37: 0.584641/0.441208loss in batch 38: 0.499756/0.442703loss in batch 39: 0.465286/0.443268loss in batch 40: 0.836777/0.452866loss in batch 41: 0.356476/0.450577loss in batch 42: 0.204285/0.444855loss in batch 43: 0.292801/0.441391loss in batch 44: 0.505569/0.44281loss in batch 45: 0.299881/0.439713loss in batch 46: 0.369705/0.438217loss in batch 47: 0.286926/0.435074loss in batch 48: 0.350616/0.43335loss in batch 49: 0.277496/0.430222loss in batch 50: 0.356628/0.428787loss in batch 51: 0.11232/0.422699loss in batch 52: 0.715546/0.428223loss in batch 53: 0.295334/0.425751loss in batch 54: 0.270065/0.422928loss in batch 55: 0.4375/0.423203loss in batch 56: 0.257843/0.420288loss in batch 57: 0.319717/0.418564loss in batch 58: 0.244324/0.415604loss in batch 59: 0.636856/0.419296loss in batch 60: 0.284241/0.417084loss in batch 61: 0.522552/0.418777loss in batch 62: 0.162888/0.414719loss in batch 63: 0.26796/0.412415loss in batch 64: 0.544327/0.414459loss in batch 65: 0.533752/0.416275loss in batch 66: 0.432571/0.416504loss in batch 67: 0.337372/0.415344loss in batch 68: 0.392395/0.415009loss in batch 69: 0.160187/0.411377loss in batch 70: 0.458771/0.412033loss in batch 71: 0.371948/0.411484loss in batch 72: 0.422852/0.411636loss in batch 73: 0.532776/0.413269loss in batch 74: 0.29715/0.411728loss in batch 75: 0.409119/0.411697loss in batch 76: 0.376892/0.41124loss in batch 77: 0.470779/0.412003loss in batch 78: 0.245071/0.409882loss in batch 79: 0.367783/0.409363loss in batch 80: 0.432419/0.409653loss in batch 81: 0.35405/0.408966loss in batch 82: 0.58992/0.411148loss in batch 83: 0.138031/0.407898loss in batch 84: 0.501373/0.408997loss in batch 85: 0.432053/0.409271loss in batch 86: 0.276627/0.407745loss in batch 87: 0.113937/0.404404loss in batch 88: 0.777527/0.4086loss in batch 89: 0.132858/0.405518loss in batch 90: 0.264542/0.403976loss in batch 91: 0.5215/0.405258loss in batch 92: 0.292267/0.404053loss in batch 93: 0.298294/0.402908loss in batch 94: 0.230698/0.401093loss in batch 95: 0.337143/0.400452loss in batch 96: 0.242752/0.398804loss in batch 97: 0.542114/0.400284loss in batch 98: 0.138062/0.397629loss in batch 99: 0.234039/0.395981loss in batch 100: 0.22673/0.394318loss in batch 101: 0.119064/0.391617loss in batch 102: 0.318954/0.390915loss in batch 103: 0.142822/0.388519loss in batch 104: 0.1362/0.386124loss in batch 105: 0.477753/0.386993loss in batch 106: 0.187744/0.385132loss in batch 107: 0.328354/0.384598loss in batch 108: 0.27626/0.383606loss in batch 109: 0.330292/0.383118loss in batch 110: 0.177979/0.381287loss in batch 111: 0.346573/0.380966loss in batch 112: 0.260391/0.379883loss in batch 113: 0.249146/0.378754loss in batch 114: 0.213104/0.377304loss in batch 115: 0.396103/0.377472loss in batch 116: 0.471741/0.378281loss in batch 117: 0.569397/0.379883loss in batch 118: 0.276016/0.379028loss in batch 119: 0.219955/0.377701loss in batch 120: 0.188812/0.376129loss in batch 121: 0.247559/0.375076loss in batch 122: 0.0840149/0.372711loss in batch 123: 0.23494/0.371613loss in batch 124: 0.318115/0.37117loss in batch 125: 0.326477/0.370819loss in batch 126: 0.163254/0.369186loss in batch 127: 0.278839/0.368484loss in batch 128: 0.929428/0.372818loss in batch 129: 0.617722/0.37471loss in batch 130: 0.290985/0.374069loss in batch 131: 0.447372/0.374634loss in batch 132: 0.234619/0.373566loss in batch 133: 0.408279/0.37384loss in batch 134: 0.191635/0.372498loss in batch 135: 0.178162/0.371048loss in batch 136: 0.325653/0.370728loss in batch 137: 0.374313/0.370758loss in batch 138: 0.161194/0.369247loss in batch 139: 0.582687/0.370773loss in batch 140: 0.0839233/0.368729loss in batch 141: 0.283325/0.368134loss in batch 142: 0.380524/0.36821loss in batch 143: 0.248169/0.367386loss in batch 144: 0.664017/0.369431loss in batch 145: 0.296402/0.368927loss in batch 146: 0.172409/0.367584loss in batch 147: 0.208572/0.366516loss in batch 148: 0.261414/0.365814loss in batch 149: 0.22023/0.364853loss in batch 150: 0.408401/0.365143loss in batch 151: 0.12822/0.363586loss in batch 152: 0.124557/0.362015loss in batch 153: 0.223663/0.361115loss in batch 154: 0.462952/0.361755loss in batch 155: 0.113922/0.360184loss in batch 156: 0.148239/0.358826loss in batch 157: 0.9767/0.362747loss in batch 158: 0.159821/0.361465loss in batch 159: 0.371414/0.361526loss in batch 160: 0.247299/0.360809loss in batch 161: 0.150574/0.359528loss in batch 162: 0.143646/0.3582loss in batch 163: 0.962921/0.361877loss in batch 164: 0.348083/0.361801loss in batch 165: 0.271393/0.361267loss in batch 166: 0.141632/0.35994loss in batch 167: 0.153976/0.358719loss in batch 168: 0.216995/0.35788loss in batch 169: 0.607712/0.359344loss in batch 170: 0.618271/0.36087loss in batch 171: 0.126099/0.359482loss in batch 172: 0.213776/0.358658loss in batch 173: 0.611603/0.360107loss in batch 174: 0.572617/0.361328loss in batch 175: 0.192459/0.360367loss in batch 176: 0.188553/0.359406loss in batch 177: 0.29068/0.358994loss in batch 178: 0.531647/0.35997loss in batch 179: 0.268158/0.359467loss in batch 180: 0.367798/0.359512loss in batch 181: 0.276169/0.359055loss in batch 182: 0.406082/0.359314loss in batch 183: 0.246048/0.358688loss in batch 184: 0.228134/0.357971loss in batch 185: 0.195099/0.357117loss in batch 186: 0.124664/0.355865loss in batch 187: 0.462326/0.35643loss in batch 188: 0.542999/0.357422loss in batch 189: 0.193436/0.356552loss in batch 190: 0.216873/0.35582loss in batch 191: 0.100494/0.354492loss in batch 192: 0.232727/0.353867loss in batch 193: 0.244049/0.353302loss in batch 194: 0.11644/0.352081loss in batch 195: 0.121307/0.350906loss in batch 196: 0.129349/0.349777loss in batch 197: 0.746109/0.351776loss in batch 198: 0.315216/0.351608loss in batch 199: 0.560059/0.352646loss in batch 200: 0.770096/0.354721loss in batch 201: 0.374283/0.354813loss in batch 202: 0.284042/0.354462loss in batch 203: 0.20282/0.353714loss in batch 204: 0.299652/0.353455loss in batch 205: 0.324203/0.353317loss in batch 206: 0.196243/0.35257loss in batch 207: 0.642563/0.353958loss in batch 208: 0.560333/0.354935loss in batch 209: 0.0992126/0.353729loss in batch 210: 0.473206/0.354294loss in batch 211: 0.484665/0.354904loss in batch 212: 0.299438/0.35463
done with epoch 1
train_acc: 0.908451 (387/426)
test loss: 0.299438
acc: 0.944054 (135/143)
loss in batch 0: 0.392242/0.392242loss in batch 1: 0.201462/0.29686loss in batch 2: 0.225052/0.272919loss in batch 3: 0.231735/0.262619loss in batch 4: 0.215347/0.253159loss in batch 5: 0.219421/0.247543loss in batch 6: 0.378769/0.266296loss in batch 7: 0.0788574/0.242859loss in batch 8: 0.237717/0.242294loss in batch 9: 0.282974/0.246353loss in batch 10: 0.359207/0.256607loss in batch 11: 0.301529/0.260361loss in batch 12: 0.250107/0.259567loss in batch 13: 0.530472/0.278931loss in batch 14: 0.197052/0.273453loss in batch 15: 0.2155/0.269836loss in batch 16: 0.26973/0.269836loss in batch 17: 0.312622/0.272202loss in batch 18: 0.198914/0.268341loss in batch 19: 0.296951/0.269775loss in batch 20: 0.381439/0.275101loss in batch 21: 0.121567/0.268112loss in batch 22: 0.101654/0.26088loss in batch 23: 0.432861/0.268051loss in batch 24: 0.36853/0.272064loss in batch 25: 0.168304/0.268066loss in batch 26: 0.163223/0.264191loss in batch 27: 0.195114/0.261719loss in batch 28: 0.209229/0.259903loss in batch 29: 0.394241/0.264389loss in batch 30: 0.210846/0.26268loss in batch 31: 0.278107/0.263138loss in batch 32: 0.24353/0.262558loss in batch 33: 0.497559/0.26947loss in batch 34: 0.52951/0.276886loss in batch 35: 0.234512/0.275711loss in batch 36: 0.0926056/0.270767loss in batch 37: 0.502365/0.276871loss in batch 38: 0.214645/0.275269loss in batch 39: 0.109924/0.271133loss in batch 40: 0.132858/0.267761loss in batch 41: 0.197311/0.266083loss in batch 42: 0.624542/0.274414loss in batch 43: 0.290329/0.27478loss in batch 44: 0.331131/0.276031loss in batch 45: 0.156555/0.273438loss in batch 46: 0.168106/0.271194loss in batch 47: 0.174408/0.26918loss in batch 48: 0.139404/0.266525loss in batch 49: 0.268768/0.266586loss in batch 50: 0.51741/0.271484loss in batch 51: 0.18277/0.269791loss in batch 52: 0.0929413/0.266449loss in batch 53: 0.77092/0.275787loss in batch 54: 0.176788/0.273987loss in batch 55: 0.1008/0.270889loss in batch 56: 0.200623/0.269669loss in batch 57: 0.189545/0.26828loss in batch 58: 0.297546/0.268784loss in batch 59: 0.215363/0.267883loss in batch 60: 0.180222/0.266464loss in batch 61: 0.388748/0.268433loss in batch 62: 0.121857/0.266098loss in batch 63: 0.478333/0.269409loss in batch 64: 0.0348053/0.265808loss in batch 65: 0.333511/0.26683loss in batch 66: 0.314911/0.267563loss in batch 67: 0.269455/0.267578loss in batch 68: 0.270752/0.267624loss in batch 69: 0.101685/0.265244loss in batch 70: 0.138855/0.263474loss in batch 71: 0.918365/0.272568loss in batch 72: 0.163895/0.271088loss in batch 73: 1.08034/0.282013loss in batch 74: 0.750259/0.288269loss in batch 75: 0.329803/0.288803loss in batch 76: 0.196167/0.287613loss in batch 77: 0.278107/0.287491loss in batch 78: 0.286469/0.287476loss in batch 79: 0.248856/0.286987loss in batch 80: 0.179077/0.28566loss in batch 81: 0.185104/0.284424loss in batch 82: 0.378403/0.285568loss in batch 83: 0.26152/0.285278loss in batch 84: 0.301529/0.285461loss in batch 85: 0.141403/0.283783loss in batch 86: 0.397491/0.285095loss in batch 87: 0.210953/0.284256loss in batch 88: 0.404495/0.285599loss in batch 89: 0.35498/0.286377loss in batch 90: 0.10611/0.284409loss in batch 91: 0.215179/0.283646loss in batch 92: 0.397263/0.284866loss in batch 93: 0.475418/0.286896loss in batch 94: 0.185959/0.285828loss in batch 95: 0.300888/0.28598loss in batch 96: 0.882187/0.29213loss in batch 97: 0.124054/0.290421loss in batch 98: 0.105942/0.288559loss in batch 99: 0.212814/0.287796loss in batch 100: 0.39624/0.288864loss in batch 101: 0.370621/0.289673loss in batch 102: 0.589706/0.292587loss in batch 103: 0.471771/0.294312loss in batch 104: 0.420303/0.295517loss in batch 105: 0.457611/0.297028loss in batch 106: 0.448639/0.298462loss in batch 107: 0.144165/0.297028loss in batch 108: 0.125427/0.295456loss in batch 109: 0.309631/0.295578loss in batch 110: 0.107086/0.293884loss in batch 111: 0.288651/0.293839loss in batch 112: 0.308105/0.293961loss in batch 113: 0.389755/0.2948loss in batch 114: 0.172546/0.293747loss in batch 115: 0.309891/0.293884loss in batch 116: 0.120117/0.292404loss in batch 117: 0.122665/0.290955loss in batch 118: 0.494858/0.292664loss in batch 119: 0.305206/0.29277loss in batch 120: 0.57103/0.295074loss in batch 121: 0.419952/0.296097loss in batch 122: 0.234985/0.295609loss in batch 123: 0.170624/0.294586loss in batch 124: 0.303772/0.294678loss in batch 125: 0.487839/0.296204loss in batch 126: 0.658951/0.299057loss in batch 127: 0.291229/0.298996loss in batch 128: 0.161224/0.297928loss in batch 129: 0.657745/0.30069loss in batch 130: 0.176773/0.299744loss in batch 131: 0.109421/0.298309loss in batch 132: 0.145721/0.297165loss in batch 133: 0.193832/0.296387loss in batch 134: 0.444061/0.297485loss in batch 135: 0.133804/0.296265loss in batch 136: 0.238403/0.295853loss in batch 137: 0.117981/0.294556loss in batch 138: 0.22612/0.294083loss in batch 139: 0.21402/0.293503loss in batch 140: 0.787994/0.296997loss in batch 141: 0.139236/0.295898loss in batch 142: 0.127594/0.294708loss in batch 143: 0.178711/0.293915loss in batch 144: 0.103394/0.292603loss in batch 145: 0.108246/0.291336loss in batch 146: 0.129959/0.290237loss in batch 147: 0.166077/0.289398loss in batch 148: 0.127243/0.288315loss in batch 149: 0.0807953/0.286926loss in batch 150: 0.213837/0.286453loss in batch 151: 0.270203/0.286346loss in batch 152: 0.635345/0.28862loss in batch 153: 0.235077/0.288269loss in batch 154: 0.0648193/0.286835loss in batch 155: 0.704361/0.289505loss in batch 156: 0.0627899/0.288055loss in batch 157: 0.505981/0.289444loss in batch 158: 0.0856018/0.288162loss in batch 159: 0.171082/0.28743loss in batch 160: 0.226303/0.287048loss in batch 161: 0.214111/0.286591loss in batch 162: 0.375458/0.28714loss in batch 163: 0.210281/0.286682loss in batch 164: 0.498978/0.287964loss in batch 165: 0.135742/0.287048loss in batch 166: 0.0428772/0.285583loss in batch 167: 0.411987/0.286346loss in batch 168: 0.171402/0.28566loss in batch 169: 0.106903/0.284592loss in batch 170: 0.142212/0.283768loss in batch 171: 0.406128/0.284485loss in batch 172: 0.288895/0.2845loss in batch 173: 0.317947/0.284698loss in batch 174: 0.594284/0.286469loss in batch 175: 1.06467/0.290894loss in batch 176: 0.553879/0.292374loss in batch 177: 0.118622/0.291397loss in batch 178: 0.166092/0.290695loss in batch 179: 0.340576/0.290985loss in batch 180: 0.477066/0.292007loss in batch 181: 0.293869/0.292023loss in batch 182: 0.28447/0.291962loss in batch 183: 0.550461/0.293381loss in batch 184: 0.0753632/0.292191loss in batch 185: 0.859634/0.295258loss in batch 186: 0.409836/0.295853loss in batch 187: 0.52652/0.297089loss in batch 188: 0.205902/0.2966loss in batch 189: 0.197128/0.296082loss in batch 190: 0.282791/0.296021loss in batch 191: 0.17627/0.295395loss in batch 192: 0.417419/0.296021loss in batch 193: 0.125397/0.295151loss in batch 194: 0.122513/0.294266loss in batch 195: 0.205399/0.293808loss in batch 196: 0.516205/0.294937loss in batch 197: 0.278931/0.294861loss in batch 198: 0.521744/0.29599loss in batch 199: 0.428162/0.296646loss in batch 200: 0.329803/0.296814loss in batch 201: 0.533966/0.297989loss in batch 202: 0.150467/0.297256loss in batch 203: 0.221634/0.29689loss in batch 204: 0.46228/0.297699loss in batch 205: 0.255432/0.297485loss in batch 206: 0.142456/0.296753loss in batch 207: 0.222153/0.296387loss in batch 208: 0.154846/0.295715loss in batch 209: 0.168076/0.295105loss in batch 210: 0.249405/0.294891loss in batch 211: 0.486786/0.295792loss in batch 212: 0.186172/0.295288
done with epoch 2
train_acc: 0.908451 (387/426)
test loss: 0.186172
acc: 0.937061 (134/143)
loss in batch 0: 0.124451/0.124451loss in batch 1: 0.254028/0.18924loss in batch 2: 0.119324/0.165939loss in batch 3: 0.12175/0.154892loss in batch 4: 0.38147/0.200195loss in batch 5: 0.119202/0.186707loss in batch 6: 0.149292/0.181351loss in batch 7: 0.1436/0.176636loss in batch 8: 0.115341/0.16983loss in batch 9: 0.403244/0.193161loss in batch 10: 0.120743/0.186584loss in batch 11: 0.371994/0.202042loss in batch 12: 0.0831909/0.192902loss in batch 13: 0.288834/0.199738loss in batch 14: 0.209183/0.200378loss in batch 15: 0.0661774/0.191986loss in batch 16: 0.347229/0.201111loss in batch 17: 0.473633/0.216248loss in batch 18: 0.247543/0.217911loss in batch 19: 0.498199/0.231918loss in batch 20: 0.34668/0.237396loss in batch 21: 0.261261/0.23848loss in batch 22: 0.933868/0.268692loss in batch 23: 0.349274/0.272064loss in batch 24: 0.284241/0.272552loss in batch 25: 0.192703/0.26947loss in batch 26: 0.236984/0.26828loss in batch 27: 0.434311/0.2742loss in batch 28: 0.101151/0.26825loss in batch 29: 0.225571/0.266815loss in batch 30: 0.234665/0.265793loss in batch 31: 0.129745/0.26152loss in batch 32: 0.17012/0.258759loss in batch 33: 0.61911/0.269363loss in batch 34: 0.218475/0.267899loss in batch 35: 0.178482/0.265411loss in batch 36: 0.198364/0.263596loss in batch 37: 0.381119/0.266693loss in batch 38: 0.695007/0.277679loss in batch 39: 0.16452/0.274841loss in batch 40: 0.218353/0.273468loss in batch 41: 0.107056/0.269501loss in batch 42: 0.254715/0.269165loss in batch 43: 0.271271/0.269211loss in batch 44: 0.195892/0.267578loss in batch 45: 0.205597/0.266251loss in batch 46: 0.311676/0.267212loss in batch 47: 0.420044/0.270386loss in batch 48: 0.113892/0.267197loss in batch 49: 0.124542/0.264343loss in batch 50: 0.105576/0.26123loss in batch 51: 0.494843/0.265717loss in batch 52: 0.163666/0.263794loss in batch 53: 0.34639/0.26532loss in batch 54: 0.280472/0.265594loss in batch 55: 0.146225/0.263474loss in batch 56: 0.0306091/0.259384loss in batch 57: 0.311081/0.260269loss in batch 58: 0.0563507/0.256821loss in batch 59: 0.206131/0.255966loss in batch 60: 0.0809937/0.253113loss in batch 61: 0.181229/0.251953loss in batch 62: 0.577667/0.257111loss in batch 63: 0.268158/0.257278loss in batch 64: 0.163757/0.255844loss in batch 65: 0.0683289/0.253006loss in batch 66: 0.0630035/0.250183loss in batch 67: 0.437256/0.252914loss in batch 68: 0.316772/0.253845loss in batch 69: 0.307571/0.254623loss in batch 70: 0.156799/0.25325loss in batch 71: 0.260101/0.253326loss in batch 72: 0.0759583/0.250916loss in batch 73: 0.173981/0.249863loss in batch 74: 0.184494/0.248993loss in batch 75: 0.412079/0.251144loss in batch 76: 0.0344543/0.248322loss in batch 77: 0.180954/0.247467loss in batch 78: 0.234467/0.247299loss in batch 79: 0.588379/0.251556loss in batch 80: 0.168869/0.250549loss in batch 81: 0.0920715/0.248596loss in batch 82: 0.238464/0.248489loss in batch 83: 0.632629/0.253052loss in batch 84: 0.208984/0.252533loss in batch 85: 0.090683/0.250656loss in batch 86: 0.129105/0.249252loss in batch 87: 0.138153/0.248001loss in batch 88: 0.300827/0.248596loss in batch 89: 0.0603943/0.246506loss in batch 90: 0.980255/0.254562loss in batch 91: 0.126877/0.253174loss in batch 92: 0.241684/0.253052loss in batch 93: 0.0943451/0.251373loss in batch 94: 0.525604/0.254257loss in batch 95: 0.190094/0.253586loss in batch 96: 0.647873/0.257645loss in batch 97: 0.173447/0.256775loss in batch 98: 0.316498/0.257385loss in batch 99: 0.203506/0.256851loss in batch 100: 0.199631/0.256287loss in batch 101: 0.143555/0.255173loss in batch 102: 0.305573/0.255661loss in batch 103: 0.111221/0.254272loss in batch 104: 0.360138/0.255295loss in batch 105: 0.227936/0.255035loss in batch 106: 0.181229/0.254333loss in batch 107: 0.07547/0.252686loss in batch 108: 0.238358/0.252563loss in batch 109: 1.10532/0.2603loss in batch 110: 0.0989227/0.25885loss in batch 111: 0.147125/0.257858loss in batch 112: 0.208145/0.257416loss in batch 113: 0.126038/0.256271loss in batch 114: 0.1203/0.255081loss in batch 115: 0.319321/0.255646loss in batch 116: 0.451797/0.257309loss in batch 117: 0.0649261/0.255676loss in batch 118: 0.410843/0.256989loss in batch 119: 0.112732/0.255783loss in batch 120: 0.105621/0.254532loss in batch 121: 0.757156/0.258667loss in batch 122: 0.234375/0.258469loss in batch 123: 0.212494/0.258102loss in batch 124: 0.390778/0.259155loss in batch 125: 0.565353/0.261581loss in batch 126: 0.344467/0.262238loss in batch 127: 0.253555/0.262161loss in batch 128: 0.201431/0.261703loss in batch 129: 0.10112/0.260468loss in batch 130: 0.185486/0.259888loss in batch 131: 0.276489/0.260025loss in batch 132: 0.363434/0.260788loss in batch 133: 0.258636/0.260788loss in batch 134: 0.439362/0.2621loss in batch 135: 0.444473/0.263443loss in batch 136: 0.0900269/0.262177loss in batch 137: 0.0429535/0.26059loss in batch 138: 0.125122/0.259613loss in batch 139: 0.326019/0.260086loss in batch 140: 0.543671/0.2621loss in batch 141: 0.393875/0.263016loss in batch 142: 0.264816/0.263031loss in batch 143: 0.11322/0.261993loss in batch 144: 0.366547/0.262726loss in batch 145: 0.0753937/0.261429loss in batch 146: 0.366959/0.262161loss in batch 147: 0.0858612/0.260956loss in batch 148: 0.0878601/0.259796loss in batch 149: 0.158875/0.259125loss in batch 150: 0.130035/0.258286loss in batch 151: 0.233765/0.258102loss in batch 152: 0.0450287/0.256714loss in batch 153: 0.69223/0.259537loss in batch 154: 0.845047/0.263321loss in batch 155: 0.116928/0.26239loss in batch 156: 0.287842/0.262558loss in batch 157: 0.380875/0.263306loss in batch 158: 0.198242/0.262878loss in batch 159: 0.178375/0.26236loss in batch 160: 0.301514/0.262604loss in batch 161: 0.161026/0.261978loss in batch 162: 0.126923/0.261154loss in batch 163: 0.0624847/0.259933loss in batch 164: 0.0555267/0.258698loss in batch 165: 0.183304/0.25824loss in batch 166: 0.572861/0.260132loss in batch 167: 0.175369/0.259613loss in batch 168: 0.130997/0.25885loss in batch 169: 0.15358/0.25824loss in batch 170: 0.840988/0.261642loss in batch 171: 0.237564/0.261505loss in batch 172: 0.48555/0.262802loss in batch 173: 0.308197/0.263062loss in batch 174: 0.157547/0.262466loss in batch 175: 0.34996/0.26297loss in batch 176: 0.0358582/0.261673loss in batch 177: 0.312149/0.261948loss in batch 178: 0.470459/0.263123loss in batch 179: 0.462662/0.264236loss in batch 180: 0.283447/0.264343loss in batch 181: 0.120895/0.26355loss in batch 182: 0.205658/0.263245loss in batch 183: 0.976593/0.26712loss in batch 184: 0.0562286/0.265976loss in batch 185: 0.154068/0.265366loss in batch 186: 0.289917/0.265503loss in batch 187: 0.580246/0.267181loss in batch 188: 0.270035/0.267197loss in batch 189: 0.634705/0.269119loss in batch 190: 0.164093/0.26857loss in batch 191: 0.148315/0.267944loss in batch 192: 0.157242/0.26738loss in batch 193: 0.271408/0.267395loss in batch 194: 0.101883/0.266541loss in batch 195: 0.120575/0.265808loss in batch 196: 0.203827/0.265488loss in batch 197: 0.125443/0.264786loss in batch 198: 0.554001/0.266235loss in batch 199: 0.230164/0.266052loss in batch 200: 0.0910187/0.265182loss in batch 201: 0.255081/0.265121loss in batch 202: 0.0978241/0.264313loss in batch 203: 0.260071/0.264297loss in batch 204: 0.112457/0.26355loss in batch 205: 0.285431/0.263641loss in batch 206: 0.23497/0.263519loss in batch 207: 0.076889/0.262619loss in batch 208: 0.0558472/0.261627loss in batch 209: 0.319122/0.261902loss in batch 210: 0.0489655/0.260895loss in batch 211: 0.110168/0.260178loss in batch 212: 0.0448761/0.259155
done with epoch 3
train_acc: 0.941315 (401/426)
test loss: 0.044876
acc: 0.909089 (130/143)
loss in batch 0: 0.589722/0.589722loss in batch 1: 0.0665131/0.328125loss in batch 2: 0.13913/0.265121loss in batch 3: 0.0925598/0.221985loss in batch 4: 0.26181/0.22995loss in batch 5: 0.317444/0.244537loss in batch 6: 0.149811/0.230988loss in batch 7: 0.179352/0.224548loss in batch 8: 0.411179/0.245285loss in batch 9: 0.364761/0.257233loss in batch 10: 0.134064/0.246033loss in batch 11: 0.0605316/0.230576loss in batch 12: 0.190964/0.227524loss in batch 13: 0.0845947/0.217316loss in batch 14: 0.0582123/0.206711loss in batch 15: 0.0838318/0.19902loss in batch 16: 0.164902/0.197021loss in batch 17: 0.106461/0.191986loss in batch 18: 0.118347/0.18811loss in batch 19: 0.21936/0.189682loss in batch 20: 0.0976257/0.185287loss in batch 21: 0.0935364/0.181122loss in batch 22: 0.123413/0.178619loss in batch 23: 0.333054/0.185043loss in batch 24: 0.145218/0.183456loss in batch 25: 0.303497/0.18808loss in batch 26: 0.777435/0.2099loss in batch 27: 0.132324/0.207123loss in batch 28: 0.118454/0.204071loss in batch 29: 0.575073/0.216431loss in batch 30: 0.106964/0.212906loss in batch 31: 0.0857086/0.208939loss in batch 32: 0.266602/0.210693loss in batch 33: 0.150864/0.208923loss in batch 34: 0.278137/0.210892loss in batch 35: 0.221375/0.211182loss in batch 36: 0.508667/0.219223loss in batch 37: 0.329865/0.222137loss in batch 38: 0.214172/0.221939loss in batch 39: 0.183853/0.220993loss in batch 40: 0.199829/0.220474loss in batch 41: 0.559967/0.228561loss in batch 42: 0.168091/0.227142loss in batch 43: 0.0945129/0.224121loss in batch 44: 0.127197/0.221985loss in batch 45: 0.167038/0.220779loss in batch 46: 0.0366669/0.216873loss in batch 47: 0.314987/0.218918loss in batch 48: 0.027359/0.214996loss in batch 49: 0.0800934/0.212311loss in batch 50: 0.106537/0.21022loss in batch 51: 0.574539/0.217224loss in batch 52: 0.336456/0.219482loss in batch 53: 0.554535/0.225693loss in batch 54: 0.255356/0.226227loss in batch 55: 0.156372/0.224991loss in batch 56: 0.0694122/0.22226loss in batch 57: 0.430298/0.22583loss in batch 58: 0.048111/0.222824loss in batch 59: 0.178879/0.222107loss in batch 60: 0.218094/0.222031loss in batch 61: 0.17157/0.221207loss in batch 62: 0.329254/0.222931loss in batch 63: 0.0662231/0.220474loss in batch 64: 0.121445/0.218964loss in batch 65: 0.0920105/0.217041loss in batch 66: 0.259247/0.217667loss in batch 67: 0.285751/0.218658loss in batch 68: 0.0695496/0.216507loss in batch 69: 0.0292816/0.213821loss in batch 70: 0.256439/0.214432loss in batch 71: 0.121689/0.213135loss in batch 72: 0.168243/0.212524loss in batch 73: 0.465393/0.215942loss in batch 74: 0.0630951/0.213898loss in batch 75: 0.107925/0.212509loss in batch 76: 0.21814/0.212585loss in batch 77: 0.130112/0.211517loss in batch 78: 0.116348/0.210312loss in batch 79: 1.44139/0.225723loss in batch 80: 0.139389/0.224655loss in batch 81: 0.0688629/0.222748loss in batch 82: 0.433304/0.225281loss in batch 83: 0.51088/0.228683loss in batch 84: 0.62709/0.233368loss in batch 85: 0.154358/0.232452loss in batch 86: 0.101807/0.230957loss in batch 87: 0.0668182/0.229095loss in batch 88: 0.231293/0.229111loss in batch 89: 0.174469/0.2285loss in batch 90: 0.0549164/0.226593loss in batch 91: 0.576385/0.230392loss in batch 92: 0.216736/0.230255loss in batch 93: 0.112793/0.229004loss in batch 94: 0.17247/0.228394loss in batch 95: 0.379929/0.22998loss in batch 96: 0.124527/0.228897loss in batch 97: 0.390411/0.230545loss in batch 98: 0.0719299/0.228943loss in batch 99: 0.12767/0.227921loss in batch 100: 0.247101/0.228119loss in batch 101: 0.224152/0.228088loss in batch 102: 0.134003/0.227158loss in batch 103: 0.731522/0.232025loss in batch 104: 0.148865/0.231232loss in batch 105: 0.0601959/0.229599loss in batch 106: 0.424957/0.23143loss in batch 107: 0.0748444/0.22998loss in batch 108: 0.248337/0.230148loss in batch 109: 0.290359/0.230698loss in batch 110: 0.68602/0.234802loss in batch 111: 0.186203/0.234375loss in batch 112: 0.0683289/0.232895loss in batch 113: 0.347733/0.233902loss in batch 114: 0.386337/0.235229loss in batch 115: 0.144287/0.234451loss in batch 116: 0.0950165/0.233261loss in batch 117: 0.0546112/0.23175loss in batch 118: 0.0635986/0.230331loss in batch 119: 0.423294/0.231949loss in batch 120: 0.539505/0.234482loss in batch 121: 0.375565/0.235641loss in batch 122: 0.102936/0.234558loss in batch 123: 0.105148/0.233521loss in batch 124: 0.342377/0.234375loss in batch 125: 0.105392/0.233368loss in batch 126: 0.878784/0.238449loss in batch 127: 0.190506/0.238068loss in batch 128: 0.551743/0.240494loss in batch 129: 0.0799408/0.239273loss in batch 130: 0.314514/0.239853loss in batch 131: 0.437531/0.241333loss in batch 132: 0.127258/0.240479loss in batch 133: 0.187668/0.240082loss in batch 134: 0.353226/0.240921loss in batch 135: 0.300735/0.241364loss in batch 136: 0.0430756/0.239914loss in batch 137: 0.174225/0.239441loss in batch 138: 0.0437927/0.238037loss in batch 139: 0.206894/0.237808loss in batch 140: 1.21558/0.244736loss in batch 141: 0.444962/0.246155loss in batch 142: 0.0279083/0.244629loss in batch 143: 0.464203/0.246155loss in batch 144: 0.119034/0.245285loss in batch 145: 0.137024/0.244537loss in batch 146: 0.0570221/0.243256loss in batch 147: 0.281891/0.243515loss in batch 148: 0.180893/0.243088loss in batch 149: 0.0225983/0.241638loss in batch 150: 0.146637/0.240997loss in batch 151: 0.159958/0.240479loss in batch 152: 0.223618/0.240356loss in batch 153: 0.643478/0.242981loss in batch 154: 0.229477/0.242889loss in batch 155: 0.0963135/0.241943loss in batch 156: 0.120102/0.241165loss in batch 157: 0.266769/0.241333loss in batch 158: 0.152008/0.240784loss in batch 159: 0.442444/0.242035loss in batch 160: 0.49733/0.243622loss in batch 161: 0.389252/0.244507loss in batch 162: 0.240051/0.244476loss in batch 163: 0.130417/0.24379loss in batch 164: 0.0938721/0.242889loss in batch 165: 0.0965881/0.242004loss in batch 166: 0.0529633/0.240875loss in batch 167: 0.684952/0.243515loss in batch 168: 0.109512/0.242722loss in batch 169: 0.131012/0.242065loss in batch 170: 0.210449/0.241882loss in batch 171: 0.0591278/0.240814loss in batch 172: 0.231293/0.240768loss in batch 173: 0.302094/0.241119loss in batch 174: 0.159576/0.240662loss in batch 175: 0.1203/0.23996loss in batch 176: 1.23409/0.245575loss in batch 177: 0.671616/0.247971loss in batch 178: 0.517853/0.249481loss in batch 179: 0.178467/0.249084loss in batch 180: 0.0535889/0.248001loss in batch 181: 0.171738/0.247589loss in batch 182: 0.0477295/0.24649loss in batch 183: 0.30661/0.246826loss in batch 184: 0.133911/0.246216loss in batch 185: 0.434753/0.247238loss in batch 186: 0.296326/0.247482loss in batch 187: 0.334396/0.247955loss in batch 188: 0.187286/0.247635loss in batch 189: 0.104889/0.246872loss in batch 190: 0.182343/0.246552loss in batch 191: 0.0998383/0.245789loss in batch 192: 0.157669/0.245331loss in batch 193: 0.157089/0.244873loss in batch 194: 0.120789/0.244232loss in batch 195: 0.228714/0.244156loss in batch 196: 0.113113/0.243484loss in batch 197: 0.145782/0.242981loss in batch 198: 0.409424/0.243835loss in batch 199: 0.446075/0.244843loss in batch 200: 0.0699463/0.243958loss in batch 201: 0.0449829/0.242981loss in batch 202: 0.352921/0.243515loss in batch 203: 0.14296/0.243042loss in batch 204: 0.109436/0.242386loss in batch 205: 0.0758514/0.241577loss in batch 206: 0.0355682/0.240585loss in batch 207: 0.18013/0.240295loss in batch 208: 0.118042/0.2397loss in batch 209: 0.445755/0.240692loss in batch 210: 0.15741/0.24028loss in batch 211: 0.0844879/0.239563loss in batch 212: 0.0254822/0.238556
done with epoch 4
train_acc: 0.93662 (399/426)
test loss: 0.0254821
acc: 0.944054 (135/143)
loss in batch 0: 0.628769/0.628769loss in batch 1: 0.16423/0.3965loss in batch 2: 0.0997314/0.297577loss in batch 3: 0.689377/0.395538loss in batch 4: 0.0747223/0.33136loss in batch 5: 0.305511/0.327057loss in batch 6: 0.17186/0.304886loss in batch 7: 0.382034/0.314529loss in batch 8: 0.0450134/0.284576loss in batch 9: 0.204712/0.276596loss in batch 10: 0.0572205/0.256653loss in batch 11: 0.61937/0.28688loss in batch 12: 0.0593567/0.269379loss in batch 13: 0.0750732/0.255508loss in batch 14: 0.113968/0.246063loss in batch 15: 0.343735/0.252167loss in batch 16: 0.0977631/0.243088loss in batch 17: 0.160446/0.238495loss in batch 18: 0.124771/0.232513loss in batch 19: 0.505081/0.24614loss in batch 20: 0.243317/0.246002loss in batch 21: 0.574921/0.260956loss in batch 22: 0.692673/0.279724loss in batch 23: 0.0383911/0.269669loss in batch 24: 0.0883026/0.262421loss in batch 25: 0.652054/0.27739loss in batch 26: 0.0934296/0.270584loss in batch 27: 0.0525665/0.262802loss in batch 28: 0.151367/0.258957loss in batch 29: 0.080307/0.253006loss in batch 30: 0.0660858/0.246964loss in batch 31: 0.18811/0.245132loss in batch 32: 0.153412/0.242355loss in batch 33: 0.113876/0.238571loss in batch 34: 0.158463/0.236282loss in batch 35: 0.312149/0.238403loss in batch 36: 0.197128/0.237274loss in batch 37: 0.524765/0.244843loss in batch 38: 0.114853/0.241516loss in batch 39: 0.45015/0.246735loss in batch 40: 0.45575/0.251831loss in batch 41: 0.129944/0.248917loss in batch 42: 0.042572/0.244125loss in batch 43: 0.052887/0.239777loss in batch 44: 0.0917053/0.236496loss in batch 45: 0.167313/0.234985loss in batch 46: 0.130173/0.232758loss in batch 47: 0.0497284/0.228943loss in batch 48: 0.0675964/0.225647loss in batch 49: 0.75061/0.236145loss in batch 50: 0.199936/0.235428loss in batch 51: 0.438416/0.239349loss in batch 52: 0.0837402/0.236404loss in batch 53: 0.0728607/0.233383loss in batch 54: 0.597809/0.23999loss in batch 55: 0.489075/0.244446loss in batch 56: 0.11853/0.242233loss in batch 57: 0.119812/0.240128loss in batch 58: 0.462875/0.243896loss in batch 59: 0.23407/0.243744loss in batch 60: 0.0503235/0.24057loss in batch 61: 0.19339/0.239807loss in batch 62: 0.0562592/0.236893loss in batch 63: 0.169586/0.23584loss in batch 64: 0.174881/0.234909loss in batch 65: 0.137604/0.233429loss in batch 66: 0.111008/0.231598loss in batch 67: 0.150009/0.230408loss in batch 68: 0.0870209/0.228333loss in batch 69: 0.285995/0.229156loss in batch 70: 0.150818/0.228058loss in batch 71: 0.0931854/0.226181loss in batch 72: 0.598541/0.231277loss in batch 73: 0.129059/0.229904loss in batch 74: 0.0198822/0.227097loss in batch 75: 0.303925/0.228104loss in batch 76: 0.0819092/0.226212loss in batch 77: 0.210709/0.226013loss in batch 78: 0.0354614/0.223587loss in batch 79: 0.483383/0.226852loss in batch 80: 0.0622559/0.224823loss in batch 81: 0.0845032/0.223099loss in batch 82: 0.143784/0.222153loss in batch 83: 0.101517/0.220718loss in batch 84: 0.0579529/0.218796loss in batch 85: 0.169891/0.218231loss in batch 86: 0.151855/0.217453loss in batch 87: 0.172104/0.216949loss in batch 88: 0.157806/0.216278loss in batch 89: 0.105453/0.215042loss in batch 90: 0.145157/0.214279loss in batch 91: 0.0895996/0.212936loss in batch 92: 0.0853577/0.211563loss in batch 93: 0.845413/0.218292loss in batch 94: 0.435318/0.220581loss in batch 95: 0.09552/0.219284loss in batch 96: 0.132553/0.218384loss in batch 97: 0.115677/0.217346loss in batch 98: 0.150482/0.21666loss in batch 99: 0.218521/0.21669loss in batch 100: 0.136505/0.215881loss in batch 101: 0.145828/0.215195loss in batch 102: 0.284454/0.215866loss in batch 103: 0.802856/0.221527loss in batch 104: 1.05615/0.229477loss in batch 105: 0.476044/0.231781loss in batch 106: 0.0797119/0.230377loss in batch 107: 0.0393524/0.228607loss in batch 108: 0.440704/0.230545loss in batch 109: 0.108032/0.229431loss in batch 110: 0.27002/0.229813loss in batch 111: 0.0358734/0.228073loss in batch 112: 0.132706/0.227234loss in batch 113: 0.0783386/0.225922loss in batch 114: 0.136185/0.225143loss in batch 115: 0.0865479/0.223938loss in batch 116: 0.223694/0.223953loss in batch 117: 0.128815/0.223129loss in batch 118: 0.477371/0.225266loss in batch 119: 0.420929/0.226913loss in batch 120: 0.424332/0.228531loss in batch 121: 0.334137/0.229401loss in batch 122: 0.606598/0.232468loss in batch 123: 0.173141/0.231995loss in batch 124: 0.218689/0.231873loss in batch 125: 0.0376282/0.230347loss in batch 126: 0.314926/0.231003loss in batch 127: 0.181549/0.230606loss in batch 128: 0.0925903/0.229553loss in batch 129: 0.13826/0.228851loss in batch 130: 0.0991974/0.227859loss in batch 131: 0.35762/0.228851loss in batch 132: 0.104843/0.227905loss in batch 133: 0.181473/0.227554loss in batch 134: 0.0722809/0.22641loss in batch 135: 0.0701294/0.225266loss in batch 136: 0.0852966/0.224243loss in batch 137: 0.382278/0.225388loss in batch 138: 0.149429/0.224838loss in batch 139: 0.306107/0.225433loss in batch 140: 0.0636292/0.224274loss in batch 141: 0.294617/0.224777loss in batch 142: 0.103622/0.223907loss in batch 143: 0.437683/0.225418loss in batch 144: 0.269608/0.225708loss in batch 145: 0.0746155/0.224686loss in batch 146: 0.0830688/0.223709loss in batch 147: 0.108734/0.222931loss in batch 148: 0.000411987/0.221451loss in batch 149: 0.147369/0.220947loss in batch 150: 0.160324/0.220535loss in batch 151: 0.0432434/0.219376loss in batch 152: 0.54747/0.221527loss in batch 153: 0.149597/0.221054loss in batch 154: 0.292709/0.221527loss in batch 155: 0.119522/0.220871loss in batch 156: 0.11615/0.2202loss in batch 157: 0.282562/0.220581loss in batch 158: 0.160919/0.220215loss in batch 159: 0.143509/0.219742loss in batch 160: 0.100784/0.218994loss in batch 161: 0.0536194/0.217972loss in batch 162: 0.190079/0.217804loss in batch 163: 0.0719452/0.216919loss in batch 164: 0.16098/0.216583loss in batch 165: 0.215729/0.216583loss in batch 166: 0.146851/0.216156loss in batch 167: 0.120468/0.215576loss in batch 168: 0.434814/0.216888loss in batch 169: 0.302994/0.217392loss in batch 170: 0.0762634/0.216568loss in batch 171: 0.344727/0.217316loss in batch 172: 0.226379/0.217361loss in batch 173: 0.319305/0.217941loss in batch 174: 0.0812988/0.217178loss in batch 175: 0.0945435/0.216476loss in batch 176: 0.0957031/0.21579loss in batch 177: 1.29181/0.221832loss in batch 178: 0.420654/0.222946loss in batch 179: 0.0978394/0.22226loss in batch 180: 0.0257874/0.221161loss in batch 181: 0.178589/0.220932loss in batch 182: 1.12347/0.225861loss in batch 183: 0.652313/0.22818loss in batch 184: 0.32016/0.228683loss in batch 185: 0.0636139/0.227798loss in batch 186: 0.098877/0.227112loss in batch 187: 0.0785675/0.226303loss in batch 188: 0.0512085/0.225388loss in batch 189: 0.236038/0.225449loss in batch 190: 0.235092/0.225494loss in batch 191: 0.153702/0.225113loss in batch 192: 0.147079/0.224701loss in batch 193: 0.462082/0.225937loss in batch 194: 0.140381/0.22551loss in batch 195: 0.2444/0.225586loss in batch 196: 0.420059/0.226578loss in batch 197: 0.104904/0.225967loss in batch 198: 0.0957336/0.225311loss in batch 199: 0.30629/0.225723loss in batch 200: 0.422409/0.226685loss in batch 201: 0.412292/0.227615loss in batch 202: 0.215225/0.227554loss in batch 203: 0.130081/0.227081loss in batch 204: 0.149994/0.2267loss in batch 205: 0.499527/0.228012loss in batch 206: 0.116653/0.227478loss in batch 207: 0.121429/0.226974loss in batch 208: 0.0795135/0.226273loss in batch 209: 0.0854492/0.225601loss in batch 210: 0.0635529/0.224823loss in batch 211: 0.130493/0.22438loss in batch 212: 0.337036/0.224915
done with epoch 5
train_acc: 0.929577 (396/426)
test loss: 0.337035
acc: 0.937061 (134/143)
loss in batch 0: 0.166946/0.166946loss in batch 1: 0.44455/0.305756loss in batch 2: 0.190491/0.267319loss in batch 3: 0.200516/0.250626loss in batch 4: 0.0184479/0.204193loss in batch 5: 0.334961/0.225983loss in batch 6: 0.454391/0.258621loss in batch 7: 0.139816/0.243759loss in batch 8: 0.859955/0.312225loss in batch 9: 0.175339/0.298538loss in batch 10: 0.0701752/0.277771loss in batch 11: 0.33783/0.282776loss in batch 12: 0.370605/0.289536loss in batch 13: 0.418777/0.298782loss in batch 14: 0.553192/0.315735loss in batch 15: 0.376511/0.319534loss in batch 16: 0.130447/0.308411loss in batch 17: 0.0275269/0.292816loss in batch 18: 0.470505/0.30217loss in batch 19: 0.12764/0.293442loss in batch 20: 0.0874023/0.28363loss in batch 21: 0.176834/0.278778loss in batch 22: 0.172897/0.274155loss in batch 23: 0.185822/0.270477loss in batch 24: 0.0208282/0.260498loss in batch 25: 0.0642242/0.252945loss in batch 26: 0.0919189/0.246994loss in batch 27: 0.558167/0.258102loss in batch 28: 0.373901/0.2621loss in batch 29: 0.15593/0.258545loss in batch 30: 0.500366/0.266357loss in batch 31: 0.0771637/0.260437loss in batch 32: 0.133041/0.256577loss in batch 33: 0.172852/0.25412loss in batch 34: 0.0847015/0.249283loss in batch 35: 0.191116/0.247665loss in batch 36: 0.513443/0.254837loss in batch 37: 0.0900574/0.250504loss in batch 38: 0.0697479/0.24588loss in batch 39: 0.142639/0.243286loss in batch 40: 0.140961/0.240784loss in batch 41: 0.340195/0.243164loss in batch 42: 0.0568542/0.238831loss in batch 43: 0.107864/0.235855loss in batch 44: 0.0509796/0.23175loss in batch 45: 0.0865173/0.228577loss in batch 46: 0.040451/0.224579loss in batch 47: 0.0552673/0.221069loss in batch 48: 0.199142/0.220612loss in batch 49: 0.0867157/0.217941loss in batch 50: 0.307999/0.219696loss in batch 51: 0.0738983/0.216888loss in batch 52: 0.389999/0.220154loss in batch 53: 0.311951/0.221863loss in batch 54: 0.129532/0.220184loss in batch 55: 0.0992432/0.218018loss in batch 56: 0.224976/0.21814loss in batch 57: 0.421722/0.221649loss in batch 58: 0.0655518/0.219009loss in batch 59: 0.0766907/0.216629loss in batch 60: 0.185226/0.216125loss in batch 61: 0.0774078/0.213882loss in batch 62: 0.0736542/0.211655loss in batch 63: 0.128326/0.210342loss in batch 64: 0.108124/0.208771loss in batch 65: 0.0431213/0.206268loss in batch 66: 0.272736/0.20726loss in batch 67: 0.0593414/0.205093loss in batch 68: 0.147461/0.204254loss in batch 69: 0.82637/0.213135loss in batch 70: 0.161896/0.212433loss in batch 71: 0.312195/0.213806loss in batch 72: 0.238724/0.214157loss in batch 73: 0.311157/0.215454loss in batch 74: 0.446365/0.218536loss in batch 75: 0.301804/0.219635loss in batch 76: 0.294495/0.220612loss in batch 77: 0.161865/0.219864loss in batch 78: 0.387115/0.22197loss in batch 79: 0.146744/0.221039loss in batch 80: 0.124695/0.219849loss in batch 81: 0.0824585/0.21817loss in batch 82: 0.37851/0.220093loss in batch 83: 0.268127/0.220673loss in batch 84: 0.248932/0.220993loss in batch 85: 0.673584/0.226257loss in batch 86: 0.011261/0.223801loss in batch 87: 0.388016/0.225647loss in batch 88: 0.108917/0.22435loss in batch 89: 0.374115/0.226013loss in batch 90: 1.52249/0.240265loss in batch 91: 0.154953/0.239334loss in batch 92: 0.0819702/0.23764loss in batch 93: 0.307663/0.238373loss in batch 94: 0.102951/0.236954loss in batch 95: 0.0491638/0.235001loss in batch 96: 0.465912/0.237381loss in batch 97: 0.402298/0.239059loss in batch 98: 0.141678/0.238083loss in batch 99: 0.26741/0.238388loss in batch 100: 0.348709/0.239471loss in batch 101: 0.140442/0.238495loss in batch 102: 0.177628/0.237915loss in batch 103: 0.142349/0.236984loss in batch 104: 0.172531/0.236374loss in batch 105: 0.0746155/0.234848loss in batch 106: 0.103622/0.233612loss in batch 107: 0.437515/0.235519loss in batch 108: 0.0597839/0.233887loss in batch 109: 0.0887909/0.232574loss in batch 110: 0.0450745/0.230896loss in batch 111: 0.772766/0.235733loss in batch 112: 0.0837708/0.23439loss in batch 113: 0.262177/0.234619loss in batch 114: 0.167465/0.234039loss in batch 115: 0.113693/0.233002loss in batch 116: 0.0466919/0.231415loss in batch 117: 0.0921326/0.23024loss in batch 118: 0.319016/0.230972loss in batch 119: 0.13475/0.230179loss in batch 120: 0.212936/0.230026loss in batch 121: 0.0527954/0.228577loss in batch 122: 0.203857/0.228378loss in batch 123: 0.0894012/0.227264loss in batch 124: 0.259247/0.227509loss in batch 125: 0.113373/0.226608loss in batch 126: 0.495361/0.228729loss in batch 127: 0.0482635/0.22731loss in batch 128: 0.111389/0.22641loss in batch 129: 0.0661926/0.225189loss in batch 130: 0.071579/0.224014loss in batch 131: 0.227386/0.22403loss in batch 132: 0.472137/0.225906loss in batch 133: 0.00579834/0.224258loss in batch 134: 0.457672/0.225998loss in batch 135: 0.449005/0.227615loss in batch 136: 1.09792/0.233978loss in batch 137: 0.355698/0.234863loss in batch 138: 0.0874786/0.23381loss in batch 139: 0.0525513/0.232513loss in batch 140: 0.150909/0.231934loss in batch 141: 0.281372/0.232269loss in batch 142: 0.263382/0.232498loss in batch 143: 0.107773/0.231628loss in batch 144: 0.208847/0.231476loss in batch 145: 0.145432/0.230865loss in batch 146: 0.327026/0.231537loss in batch 147: 0.0578461/0.230362loss in batch 148: 0.0802917/0.229355loss in batch 149: 0.0693665/0.228287loss in batch 150: 0.227219/0.228271loss in batch 151: 0.164825/0.227859loss in batch 152: 0.639801/0.23056loss in batch 153: 0.0651855/0.229492loss in batch 154: 0.0652313/0.228424loss in batch 155: 0.0639648/0.227371loss in batch 156: 0.180374/0.227066loss in batch 157: 0.149994/0.226578loss in batch 158: 0.180298/0.226288loss in batch 159: 0.276367/0.226608loss in batch 160: 0.100235/0.225815loss in batch 161: 0.0816803/0.22493loss in batch 162: 0.0370026/0.22377loss in batch 163: 0.0786896/0.222885loss in batch 164: 0.0580444/0.221893loss in batch 165: 0.121445/0.221283loss in batch 166: 0.800415/0.224747loss in batch 167: 0.0910187/0.223969loss in batch 168: 0.0518036/0.222946loss in batch 169: 0.152969/0.222519loss in batch 170: 0.264633/0.222778loss in batch 171: 0.343323/0.22348loss in batch 172: 0.154678/0.223083loss in batch 173: 0.0426941/0.222046loss in batch 174: 0.028244/0.220932loss in batch 175: 0.037323/0.219894loss in batch 176: 0.926254/0.223877loss in batch 177: 0.0502625/0.2229loss in batch 178: 0.0502472/0.221939loss in batch 179: 0.269409/0.222214loss in batch 180: 0.0523987/0.221268loss in batch 181: 0.0753632/0.220474loss in batch 182: 0.0820007/0.219711loss in batch 183: 0.0638885/0.218857loss in batch 184: 0.0597992/0.218002loss in batch 185: 0.092926/0.217331loss in batch 186: 0.0584869/0.216492loss in batch 187: 0.0579987/0.215637loss in batch 188: 0.0383453/0.214691loss in batch 189: 0.225037/0.214752loss in batch 190: 0.147064/0.214401loss in batch 191: 0.0966187/0.213791loss in batch 192: 0.0358582/0.21286loss in batch 193: 0.0621033/0.212082loss in batch 194: 0.315552/0.212616loss in batch 195: 0.0486908/0.211777loss in batch 196: 0.0396881/0.210907loss in batch 197: 0.35524/0.211624loss in batch 198: 0.12793/0.211212loss in batch 199: 0.143173/0.210876loss in batch 200: 0.0263367/0.209961loss in batch 201: 0.0345459/0.209076loss in batch 202: 0.158554/0.208832loss in batch 203: 0.0741425/0.208176loss in batch 204: 1.48112/0.214386loss in batch 205: 0.0630341/0.213654loss in batch 206: 0.176285/0.21347loss in batch 207: 0.022522/0.212555loss in batch 208: 0.121719/0.212112loss in batch 209: 0.0292969/0.211243loss in batch 210: 0.0921021/0.210678loss in batch 211: 0.0491791/0.20993loss in batch 212: 0.0415344/0.209122
done with epoch 6
train_acc: 0.941315 (401/426)
test loss: 0.0415343
acc: 0.930068 (133/143)
loss in batch 0: 0.180038/0.180038loss in batch 1: 0.137558/0.158798loss in batch 2: 0.0619659/0.126526loss in batch 3: 0.192596/0.143051loss in batch 4: 0.36795/0.188019loss in batch 5: 0.306854/0.207825loss in batch 6: 0.127914/0.196411loss in batch 7: 0.289856/0.208099loss in batch 8: 0.0369263/0.189072loss in batch 9: 0.055191/0.17569loss in batch 10: 0.205109/0.17836loss in batch 11: 0.17485/0.17807loss in batch 12: 0.222107/0.181458loss in batch 13: 0.0987549/0.175537loss in batch 14: 0.172943/0.175369loss in batch 15: 0.0335388/0.166519loss in batch 16: 0.254562/0.171692loss in batch 17: 0.363037/0.182327loss in batch 18: 0.101822/0.178085loss in batch 19: 0.3526/0.186813loss in batch 20: 0.203979/0.187622loss in batch 21: 0.0341492/0.180649loss in batch 22: 0.0377808/0.174438loss in batch 23: 0.120865/0.172211loss in batch 24: 0.28215/0.176605loss in batch 25: 1.29678/0.219696loss in batch 26: 0.0385437/0.212967loss in batch 27: 0.266174/0.214874loss in batch 28: 0.109604/0.211243loss in batch 29: 0.474091/0.220016loss in batch 30: 0.382156/0.225235loss in batch 31: 0.190308/0.224152loss in batch 32: 0.511108/0.232834loss in batch 33: 0.0571136/0.227676loss in batch 34: 1.14461/0.253876loss in batch 35: 0.545654/0.261978loss in batch 36: 0.0536194/0.256348loss in batch 37: 0.248276/0.256134loss in batch 38: 0.0584259/0.251068loss in batch 39: 0.0178986/0.245239loss in batch 40: 0.0743256/0.241058loss in batch 41: 0.266861/0.241684loss in batch 42: 0.894958/0.256882loss in batch 43: 0.0823975/0.252914loss in batch 44: 0.482773/0.258011loss in batch 45: 0.113022/0.254868loss in batch 46: 0.189392/0.253479loss in batch 47: 0.00692749/0.248337loss in batch 48: 0.166046/0.246658loss in batch 49: 0.182846/0.245377loss in batch 50: 0.188782/0.244263loss in batch 51: 0.414337/0.247543loss in batch 52: 0.0730286/0.244247loss in batch 53: 0.338028/0.245987loss in batch 54: 0.409424/0.248962loss in batch 55: 0.317932/0.250183loss in batch 56: 0.285355/0.250809loss in batch 57: 0.0716248/0.247711loss in batch 58: 0.337296/0.249222loss in batch 59: 0.127411/0.247208loss in batch 60: 0.0951843/0.244705loss in batch 61: 0.0333252/0.241302loss in batch 62: 0.026413/0.2379loss in batch 63: 0.0955658/0.235672loss in batch 64: 0.208496/0.23526loss in batch 65: 0.288162/0.236053loss in batch 66: 0.0515594/0.233292loss in batch 67: 0.0664673/0.23085loss in batch 68: 0.0694122/0.228516loss in batch 69: 0.113205/0.226868loss in batch 70: 0.0519714/0.224396loss in batch 71: 0.0813599/0.222412loss in batch 72: 0.142639/0.221313loss in batch 73: 0.0571289/0.219101loss in batch 74: 0.0523376/0.216873loss in batch 75: 0.429474/0.219666loss in batch 76: 0.101517/0.21814loss in batch 77: 0.147095/0.217224loss in batch 78: 0.0201263/0.214737loss in batch 79: 0.0914459/0.213181loss in batch 80: 0.161484/0.21254loss in batch 81: 0.198349/0.212387loss in batch 82: 0.145859/0.211563loss in batch 83: 0.05867/0.209747loss in batch 84: 0.237854/0.210083loss in batch 85: 0.43161/0.212662loss in batch 86: 0.123108/0.211624loss in batch 87: 0.0429077/0.209702loss in batch 88: 0.622711/0.214355loss in batch 89: 0.0634918/0.212677loss in batch 90: 0.132751/0.211792loss in batch 91: 0.171112/0.211365loss in batch 92: 0.0294342/0.209396loss in batch 93: 0.0538177/0.207748loss in batch 94: 0.0733643/0.206345loss in batch 95: 0.0479279/0.204681loss in batch 96: 0.088562/0.203476loss in batch 97: 0.25174/0.203979loss in batch 98: 0.390198/0.205856loss in batch 99: 0.0467224/0.204269loss in batch 100: 0.0180206/0.202423loss in batch 101: 0.247452/0.202866loss in batch 102: 0.0556335/0.201447loss in batch 103: 0.19194/0.20134loss in batch 104: 0.0849762/0.200226loss in batch 105: 0.129379/0.19957loss in batch 106: 0.231491/0.19986loss in batch 107: 0.10498/0.198975loss in batch 108: 0.775208/0.204285loss in batch 109: 0.276871/0.204941loss in batch 110: 0.108398/0.204056loss in batch 111: 0.16478/0.203705loss in batch 112: 0.111496/0.202896loss in batch 113: 0.215408/0.203003loss in batch 114: 0.0589447/0.201752loss in batch 115: 0.0635986/0.200562loss in batch 116: 0.0998688/0.199707loss in batch 117: 0.0429535/0.19838loss in batch 118: 0.0538177/0.197159loss in batch 119: 0.18161/0.197021loss in batch 120: 0.050827/0.195831loss in batch 121: 0.126358/0.195251loss in batch 122: 0.338943/0.196426loss in batch 123: 0.0791626/0.195465loss in batch 124: 0.850754/0.200714loss in batch 125: 0.121231/0.200089loss in batch 126: 0.0387115/0.198822loss in batch 127: 0.173813/0.198608loss in batch 128: 0.67189/0.202286loss in batch 129: 0.0746613/0.201309loss in batch 130: 0.0397491/0.200073loss in batch 131: 0.417862/0.201721loss in batch 132: 0.085495/0.200851loss in batch 133: 0.314514/0.201691loss in batch 134: 0.137131/0.201218loss in batch 135: 0.135513/0.200729loss in batch 136: 0.0900726/0.199936loss in batch 137: 0.0960999/0.199188loss in batch 138: 0.0299377/0.197952loss in batch 139: 0.106567/0.197311loss in batch 140: 0.297195/0.198013loss in batch 141: 0.024353/0.196793loss in batch 142: 0.288742/0.197433loss in batch 143: 0.114365/0.196854loss in batch 144: 0.0302429/0.195709loss in batch 145: 0.351776/0.196777loss in batch 146: 1.01651/0.202347loss in batch 147: 0.0566864/0.20137loss in batch 148: 0.077301/0.200531loss in batch 149: 0.144897/0.200165loss in batch 150: 0.30864/0.200882loss in batch 151: 0.0352783/0.199799loss in batch 152: 0.116852/0.199249loss in batch 153: 0.0843353/0.198502loss in batch 154: 0.0352173/0.197449loss in batch 155: 0.036911/0.196426loss in batch 156: 0.104889/0.195847loss in batch 157: 0.127914/0.195404loss in batch 158: 0.623154/0.198105loss in batch 159: 0.0757294/0.197342loss in batch 160: 0.0365753/0.196335loss in batch 161: 0.480286/0.19809loss in batch 162: 0.486206/0.19986loss in batch 163: 0.0572357/0.19899loss in batch 164: 0.0723877/0.198227loss in batch 165: 0.119003/0.197739loss in batch 166: 0.388962/0.198883loss in batch 167: 0.10791/0.198349loss in batch 168: 0.329651/0.199112loss in batch 169: 0.0645447/0.198334loss in batch 170: 0.0448914/0.197433loss in batch 171: 0.033844/0.196487loss in batch 172: 0.29071/0.197037loss in batch 173: 0.467407/0.198578loss in batch 174: 0.0614319/0.1978loss in batch 175: 0.564178/0.19989loss in batch 176: 0.8974/0.203812loss in batch 177: 0.257187/0.204132loss in batch 178: 0.0486145/0.203247loss in batch 179: 0.0735626/0.20253loss in batch 180: 0.18689/0.202454loss in batch 181: 0.0504456/0.201614loss in batch 182: 0.123825/0.201187loss in batch 183: 0.0376282/0.200287loss in batch 184: 0.353058/0.201126loss in batch 185: 0.18399/0.201035loss in batch 186: 0.0617828/0.200287loss in batch 187: 0.104752/0.199768loss in batch 188: 0.119766/0.199356loss in batch 189: 0.0613251/0.198624loss in batch 190: 0.31282/0.199234loss in batch 191: 0.0679779/0.198547loss in batch 192: 0.233246/0.19873loss in batch 193: 0.0955353/0.198181loss in batch 194: 0.113831/0.197754loss in batch 195: 0.0660706/0.197083loss in batch 196: 0.663666/0.199448loss in batch 197: 0.0648041/0.198776loss in batch 198: 0.286179/0.199219loss in batch 199: 0.542633/0.200928loss in batch 200: 0.0876312/0.200363loss in batch 201: 0.0654297/0.199707loss in batch 202: 0.0245514/0.198837loss in batch 203: 0.128693/0.198486loss in batch 204: 0.102737/0.198029loss in batch 205: 0.155884/0.19783loss in batch 206: 0.0829315/0.197266loss in batch 207: 0.20961/0.197327loss in batch 208: 0.529602/0.198914loss in batch 209: 0.396698/0.19986loss in batch 210: 0.298355/0.200317loss in batch 211: 0.763687/0.202988loss in batch 212: 0.105133/0.202515
done with epoch 7
train_acc: 0.938967 (400/426)
test loss: 0.105133
acc: 0.944054 (135/143)
loss in batch 0: 0.0526886/0.0526886loss in batch 1: 0.0570374/0.0548553loss in batch 2: 0.0326843/0.0474701loss in batch 3: 0.0429535/0.0463409loss in batch 4: 0.0443878/0.0459442loss in batch 5: 0.150574/0.063385loss in batch 6: 0.0675201/0.0639801loss in batch 7: 0.677689/0.140686loss in batch 8: 0.0366821/0.129135loss in batch 9: 0.290207/0.145248loss in batch 10: 0.823578/0.206909loss in batch 11: 0.200272/0.20636loss in batch 12: 0.18161/0.204453loss in batch 13: 0.0738831/0.195114loss in batch 14: 0.0901794/0.188126loss in batch 15: 0.0648956/0.180435loss in batch 16: 0.206726/0.181976loss in batch 17: 0.0234833/0.173172loss in batch 18: 0.210373/0.175125loss in batch 19: 0.0738983/0.170074loss in batch 20: 0.0706635/0.165344loss in batch 21: 0.441559/0.177902loss in batch 22: 0.101151/0.174561loss in batch 23: 0.15683/0.173813loss in batch 24: 0.0328827/0.168182loss in batch 25: 0.355255/0.175369loss in batch 26: 0.608093/0.191391loss in batch 27: 0.0869598/0.187668loss in batch 28: 0.0799255/0.18396loss in batch 29: 0.426056/0.192017loss in batch 30: 0.0561218/0.187637loss in batch 31: 0.0649567/0.183807loss in batch 32: 0.325226/0.188095loss in batch 33: 0.0224152/0.183212loss in batch 34: 0.422699/0.190063loss in batch 35: 0.0496674/0.186157loss in batch 36: 0.0856018/0.183441loss in batch 37: 0.0895233/0.180969loss in batch 38: 0.139679/0.179916loss in batch 39: 0.253372/0.181747loss in batch 40: 0.168472/0.181427loss in batch 41: 0.392227/0.186447loss in batch 42: 0.0363007/0.182953loss in batch 43: 0.36171/0.187012loss in batch 44: 0.115097/0.185425loss in batch 45: 0.0636139/0.18277loss in batch 46: 0.263977/0.184494loss in batch 47: 0.211304/0.185059loss in batch 48: 0.24791/0.18634loss in batch 49: 0.103561/0.184692loss in batch 50: 0.49733/0.190811loss in batch 51: 0.258423/0.192108loss in batch 52: 0.0868835/0.190125loss in batch 53: 0.231857/0.190887loss in batch 54: 0.182693/0.19075loss in batch 55: 0.19194/0.190765loss in batch 56: 0.33963/0.193375loss in batch 57: 0.358932/0.196243loss in batch 58: 0.46817/0.200836loss in batch 59: 0.102066/0.199203loss in batch 60: 0.0527039/0.196793loss in batch 61: 0.0734406/0.194809loss in batch 62: 0.130234/0.193787loss in batch 63: 0.0501251/0.191544loss in batch 64: 0.271072/0.192764loss in batch 65: 0.571045/0.198502loss in batch 66: 0.0354767/0.19606loss in batch 67: 0.436829/0.1996loss in batch 68: 0.59726/0.205368loss in batch 69: 0.373306/0.207764loss in batch 70: 0.169006/0.20723loss in batch 71: 0.217514/0.207352loss in batch 72: 0.122513/0.206192loss in batch 73: 0.0602417/0.204224loss in batch 74: 0.123154/0.20314loss in batch 75: 0.165817/0.202652loss in batch 76: 0.0938263/0.201248loss in batch 77: 0.711823/0.207779loss in batch 78: 0.0612183/0.205933loss in batch 79: 0.2108/0.205994loss in batch 80: 0.098175/0.204651loss in batch 81: 0.21933/0.204849loss in batch 82: 0.162872/0.20433loss in batch 83: 0.332092/0.205856loss in batch 84: 0.113083/0.204758loss in batch 85: 0.100113/0.203552loss in batch 86: 0.0576935/0.201874loss in batch 87: 0.0597382/0.200256loss in batch 88: 0.146423/0.199646loss in batch 89: 0.41124/0.201996loss in batch 90: 0.104218/0.200928loss in batch 91: 0.0754395/0.199554loss in batch 92: 0.15654/0.199112loss in batch 93: 0.262802/0.199783loss in batch 94: 0.117523/0.198914loss in batch 95: 0.0416412/0.197281loss in batch 96: 0.0334167/0.195587loss in batch 97: 0.0717926/0.194321loss in batch 98: 0.220917/0.194595loss in batch 99: 0.0375671/0.193024loss in batch 100: 0.143982/0.192535loss in batch 101: 1.30341/0.20343loss in batch 102: 0.0326843/0.201767loss in batch 103: 0.0679016/0.200485loss in batch 104: 0.495758/0.203293loss in batch 105: 0.32843/0.204483loss in batch 106: 0.0337524/0.202881loss in batch 107: 0.117065/0.202087loss in batch 108: 0.0641937/0.200806loss in batch 109: 0.168442/0.200516loss in batch 110: 0.0603943/0.199265loss in batch 111: 0.837738/0.204956loss in batch 112: 0.0852509/0.203903loss in batch 113: 0.103973/0.203018loss in batch 114: 0.144119/0.202515loss in batch 115: 0.111404/0.201736loss in batch 116: 0.263672/0.202255loss in batch 117: 0.207108/0.202301loss in batch 118: 0.361755/0.203629loss in batch 119: 0.314682/0.204559loss in batch 120: 0.0984039/0.20369loss in batch 121: 0.0791321/0.202667loss in batch 122: 0.0390778/0.20134loss in batch 123: 1.11366/0.208694loss in batch 124: 0.0240784/0.207214loss in batch 125: 0.0505524/0.205963loss in batch 126: 0.0842743/0.205017loss in batch 127: 0.152084/0.204605loss in batch 128: 0.289413/0.205261loss in batch 129: 0.301224/0.205994loss in batch 130: 0.0340576/0.204681loss in batch 131: 0.0812531/0.203751loss in batch 132: 0.500763/0.205994loss in batch 133: 0.0674438/0.204956loss in batch 134: 0.345749/0.205994loss in batch 135: 0.102905/0.205246loss in batch 136: 0.0273285/0.203934loss in batch 137: 0.281082/0.204483loss in batch 138: 0.0668793/0.203506loss in batch 139: 0.0887604/0.202682loss in batch 140: 0.0510559/0.201614loss in batch 141: 0.300446/0.202301loss in batch 142: 0.671707/0.205597loss in batch 143: 0.325073/0.206421loss in batch 144: 0.129242/0.205887loss in batch 145: 0.113663/0.205246loss in batch 146: 0.0413513/0.204147loss in batch 147: 0.425598/0.205643loss in batch 148: 0.228088/0.205795loss in batch 149: 0.0593262/0.204819loss in batch 150: 0.491165/0.206696loss in batch 151: 0.123428/0.206161loss in batch 152: 0.353363/0.207123loss in batch 153: 0.029892/0.205978loss in batch 154: 0.150742/0.205612loss in batch 155: 0.0984802/0.204926loss in batch 156: 0.0302734/0.203827loss in batch 157: 0.237671/0.204025loss in batch 158: 0.0409088/0.203003loss in batch 159: 0.120789/0.202484loss in batch 160: 0.113785/0.201935loss in batch 161: 0.683426/0.20491loss in batch 162: 0.13707/0.204498loss in batch 163: 0.036911/0.203461loss in batch 164: 0.0312195/0.202423loss in batch 165: 0.0363464/0.201431loss in batch 166: 0.0296478/0.200394loss in batch 167: 0.264938/0.200775loss in batch 168: 0.0679626/0.199997loss in batch 169: 0.0381317/0.199036loss in batch 170: 0.303375/0.199661loss in batch 171: 0.000915527/0.198502loss in batch 172: 0.103607/0.197952loss in batch 173: 0.0575409/0.197144loss in batch 174: 0.078186/0.196472loss in batch 175: 1.29851/0.202728loss in batch 176: 0.446243/0.204102loss in batch 177: 0.0525208/0.203262loss in batch 178: 0.117371/0.202759loss in batch 179: 0.130615/0.202362loss in batch 180: 0.0646973/0.201614loss in batch 181: 0.0470581/0.20076loss in batch 182: 0.0253754/0.199799loss in batch 183: 0.055603/0.199005loss in batch 184: 0.252167/0.199295loss in batch 185: 0.384445/0.200302loss in batch 186: 0.57164/0.202286loss in batch 187: 0.0375061/0.201416loss in batch 188: 0.194458/0.201385loss in batch 189: 0.0106354/0.200363loss in batch 190: 0.0555725/0.199615loss in batch 191: 0.24205/0.199829loss in batch 192: 0.231232/0.199997loss in batch 193: 0.212875/0.200073loss in batch 194: 0.0451355/0.199265loss in batch 195: 0.410751/0.200333loss in batch 196: 0.0475464/0.19957loss in batch 197: 0.243912/0.199799loss in batch 198: 0.103058/0.19931loss in batch 199: 0.028656/0.198456loss in batch 200: 0.185516/0.198395loss in batch 201: 0.118912/0.197983loss in batch 202: 0.0515137/0.197266loss in batch 203: 0.0995941/0.196793loss in batch 204: 0.0219574/0.195953loss in batch 205: 0.0106506/0.195038loss in batch 206: 0.0753784/0.194458loss in batch 207: 0.397018/0.195435loss in batch 208: 0.00737/0.194534loss in batch 209: 0.169495/0.194412loss in batch 210: 0.135559/0.194138loss in batch 211: 0.0359497/0.19339loss in batch 212: 0.302719/0.193909
done with epoch 8
train_acc: 0.941315 (401/426)
test loss: 0.302718
acc: 0.937061 (134/143)
loss in batch 0: 0.256226/0.256226loss in batch 1: 0.148148/0.202179loss in batch 2: 0.0350342/0.146469loss in batch 3: 0.389267/0.207169loss in batch 4: 0.129639/0.191666loss in batch 5: 0.316055/0.212402loss in batch 6: 0.267319/0.22023loss in batch 7: 0.564499/0.263275loss in batch 8: 0.270798/0.264114loss in batch 9: 0.0751038/0.245209loss in batch 10: 0.0875092/0.230865loss in batch 11: 0.154495/0.224503loss in batch 12: 0.550476/0.249588loss in batch 13: 0.128738/0.240936loss in batch 14: 0.0482635/0.228104loss in batch 15: 0.147751/0.223083loss in batch 16: 0.468781/0.237534loss in batch 17: 0.105728/0.230225loss in batch 18: 0.19252/0.228226loss in batch 19: 0.0540619/0.219528loss in batch 20: 0.192184/0.218216loss in batch 21: 0.143936/0.214844loss in batch 22: 0.0275574/0.206696loss in batch 23: 0.295364/0.210388loss in batch 24: 0.0453339/0.203781loss in batch 25: 0.724243/0.223816loss in batch 26: 0.362335/0.228943loss in batch 27: 0.0218353/0.221542loss in batch 28: 0.327957/0.22522loss in batch 29: 0.0391693/0.219025loss in batch 30: 0.0117645/0.212326loss in batch 31: 0.032547/0.206711loss in batch 32: 0.310104/0.209854loss in batch 33: 0.0696411/0.205719loss in batch 34: 0.0102234/0.200134loss in batch 35: 0.0661316/0.196411loss in batch 36: 0.0649567/0.192856loss in batch 37: 0.739975/0.20726loss in batch 38: 0.0310974/0.202728loss in batch 39: 0.0697479/0.199417loss in batch 40: 0.0253906/0.195175loss in batch 41: 0.574356/0.204193loss in batch 42: 0.0679626/0.201035loss in batch 43: 0.181641/0.200592loss in batch 44: 0.0358887/0.19693loss in batch 45: 0.0505524/0.193741loss in batch 46: 0.10202/0.191788loss in batch 47: 0.0636597/0.189117loss in batch 48: 0.0822296/0.186935loss in batch 49: 0.0557709/0.184326loss in batch 50: 0.0710449/0.182098loss in batch 51: 0.0440216/0.179443loss in batch 52: 0.156021/0.179001loss in batch 53: 0.111328/0.17775loss in batch 54: 0.0791473/0.175949loss in batch 55: 0.123871/0.175034loss in batch 56: 0.0821686/0.173386loss in batch 57: 0.070343/0.171616loss in batch 58: 0.0780792/0.170044loss in batch 59: 0.335281/0.172791loss in batch 60: 0.162537/0.172623loss in batch 61: 0.101685/0.171478loss in batch 62: 0.265686/0.172974loss in batch 63: 0.0469055/0.171005loss in batch 64: 0.0263977/0.168777loss in batch 65: 0.0799103/0.167435loss in batch 66: 0.0561523/0.165771loss in batch 67: 0.730301/0.174072loss in batch 68: 0.0633698/0.172455loss in batch 69: 0.435898/0.176239loss in batch 70: 0.125198/0.175522loss in batch 71: 0.225021/0.176208loss in batch 72: 0.28923/0.17775loss in batch 73: 0.351593/0.180099loss in batch 74: 0.119125/0.179291loss in batch 75: 1.05554/0.190826loss in batch 76: 0.910919/0.200165loss in batch 77: 0.404495/0.202774loss in batch 78: 0.129913/0.201859loss in batch 79: 0.128662/0.200943loss in batch 80: 0.281265/0.201935loss in batch 81: 0.0449524/0.200027loss in batch 82: 0.264267/0.200806loss in batch 83: 0.0807648/0.199371loss in batch 84: 0.0207977/0.197266loss in batch 85: 0.0101471/0.195099loss in batch 86: 0.0787964/0.193756loss in batch 87: 0.570374/0.198044loss in batch 88: 0.326874/0.199478loss in batch 89: 0.30661/0.200668loss in batch 90: 0.105118/0.199615loss in batch 91: 0.214233/0.199783loss in batch 92: 0.62442/0.204346loss in batch 93: 0.303894/0.205414loss in batch 94: 0.020401/0.203461loss in batch 95: 0.166214/0.203079loss in batch 96: 0.0194702/0.201172loss in batch 97: 0.0288696/0.199417loss in batch 98: 0.100937/0.198425loss in batch 99: 0.11235/0.197571loss in batch 100: 0.15625/0.197159loss in batch 101: 0.128342/0.196472loss in batch 102: 0.17952/0.196304loss in batch 103: 0.0443878/0.194855loss in batch 104: 0.154373/0.194473loss in batch 105: 0.186905/0.194397loss in batch 106: 0.256622/0.194977loss in batch 107: 0.189911/0.194931loss in batch 108: 0.149475/0.194519loss in batch 109: 0.16716/0.194275loss in batch 110: 0.924103/0.200836loss in batch 111: 0.25267/0.201309loss in batch 112: 0.0253296/0.199753loss in batch 113: 0.0118713/0.198105loss in batch 114: 0.186371/0.197998loss in batch 115: 0.263565/0.198563loss in batch 116: 0.116226/0.197861loss in batch 117: 0.0422668/0.196548loss in batch 118: 0.0742035/0.195511loss in batch 119: 0.134918/0.195007loss in batch 120: 0.0228577/0.193588loss in batch 121: 0.696976/0.197708loss in batch 122: 0.120895/0.197098loss in batch 123: 0.0345917/0.195786loss in batch 124: 0.171005/0.195572loss in batch 125: 0.0735016/0.194611loss in batch 126: 0.426544/0.196442loss in batch 127: 0.174393/0.196259loss in batch 128: 0.141129/0.195847loss in batch 129: 0.21524/0.195984loss in batch 130: 0.0280304/0.194702loss in batch 131: 0.0604553/0.19368loss in batch 132: 0.0975494/0.192963loss in batch 133: 0.120926/0.192429loss in batch 134: 0.0288849/0.191223loss in batch 135: 0.632553/0.194458loss in batch 136: 0.295471/0.195206loss in batch 137: 0.043335/0.194092loss in batch 138: 0.031311/0.192932loss in batch 139: 0.401077/0.194412loss in batch 140: 0.224335/0.194626loss in batch 141: 0.0595398/0.193665loss in batch 142: 0.317642/0.194534loss in batch 143: 0.315521/0.195374loss in batch 144: 0.446609/0.197113loss in batch 145: 1.43797/0.205612loss in batch 146: 0.0290375/0.204422loss in batch 147: 0.155701/0.204071loss in batch 148: 0.108231/0.20343loss in batch 149: 0.297302/0.204071loss in batch 150: 0.151184/0.203705loss in batch 151: 0.099472/0.203033loss in batch 152: 0.20816/0.203064loss in batch 153: 0.039917/0.201996loss in batch 154: 0.371078/0.203094loss in batch 155: 0.0774078/0.202286loss in batch 156: 0.170273/0.202087loss in batch 157: 0.00457764/0.200836loss in batch 158: 0.032196/0.199783loss in batch 159: 0.0298157/0.198715loss in batch 160: 0.0632629/0.197876loss in batch 161: 0.00721741/0.196686loss in batch 162: 0.0422058/0.19574loss in batch 163: 0.412369/0.197067loss in batch 164: 0.115005/0.196564loss in batch 165: 0.0349426/0.195602loss in batch 166: 0.347275/0.196503loss in batch 167: 0.0849152/0.195831loss in batch 168: 0.0895538/0.195206loss in batch 169: 0.0901184/0.194595loss in batch 170: 0.150436/0.194321loss in batch 171: 0.176514/0.194229loss in batch 172: 0.100357/0.19368loss in batch 173: 0.0943756/0.193115loss in batch 174: 0.262268/0.193512loss in batch 175: 0.0202332/0.19252loss in batch 176: 0.0415039/0.191681loss in batch 177: 0.053009/0.190887loss in batch 178: 0.0232391/0.189957loss in batch 179: 0.477905/0.191559loss in batch 180: 0.102371/0.191071loss in batch 181: 0.232605/0.191299loss in batch 182: 0.0372009/0.190445loss in batch 183: 0.0842133/0.18988loss in batch 184: 0.17836/0.189819loss in batch 185: 0.323929/0.190536loss in batch 186: 0.0501251/0.189774loss in batch 187: 0.0710144/0.189148loss in batch 188: 0.205063/0.18924loss in batch 189: 0.100876/0.188766loss in batch 190: 0.893784/0.192459loss in batch 191: 0.332779/0.193192loss in batch 192: 0.104874/0.192734loss in batch 193: 0.438568/0.194loss in batch 194: 0.0113831/0.193069loss in batch 195: 0.0326385/0.192245loss in batch 196: 0.0307617/0.191422loss in batch 197: 0.00434875/0.190475loss in batch 198: 0.0630341/0.189835loss in batch 199: 0.0152893/0.18898loss in batch 200: 0.0335236/0.188202loss in batch 201: 0.0284882/0.187393loss in batch 202: 0.0294952/0.18663loss in batch 203: 0.0479584/0.185959loss in batch 204: 0.506409/0.1875loss in batch 205: 0.391388/0.188492loss in batch 206: 0.0555878/0.187851loss in batch 207: 0.0953827/0.187408loss in batch 208: 0.315353/0.188019loss in batch 209: 0.43483/0.189194loss in batch 210: 0.337173/0.189896loss in batch 211: 0.0807495/0.189377loss in batch 212: 0.0514526/0.188736
done with epoch 9
train_acc: 0.943662 (402/426)
test loss: 0.0514525
acc: 0.944054 (135/143)
loss in batch 0: 0.104279/0.104279loss in batch 1: 0.231644/0.167953loss in batch 2: 0.204147/0.180023loss in batch 3: 0.0673676/0.151871loss in batch 4: 0.235443/0.168579loss in batch 5: 0.367142/0.201675loss in batch 6: 0.049469/0.179916loss in batch 7: 0.0948639/0.169296loss in batch 8: 0.185593/0.171097loss in batch 9: 0.287781/0.18277loss in batch 10: 0.463165/0.208267loss in batch 11: 0.352097/0.220245loss in batch 12: 0.334412/0.229034loss in batch 13: 2.11484/0.363739loss in batch 14: 0.182632/0.351654loss in batch 15: 0.0784302/0.334579loss in batch 16: 0.0560455/0.318192loss in batch 17: 0.0528412/0.303452loss in batch 18: 1.1015/0.345459loss in batch 19: 0.139572/0.335159loss in batch 20: 0.359756/0.336334loss in batch 21: 0.316345/0.335434loss in batch 22: 0.440643/0.339996loss in batch 23: 0.0635986/0.328476loss in batch 24: 0.052063/0.317413loss in batch 25: 0.0320435/0.306458loss in batch 26: 0.282623/0.305573loss in batch 27: 0.0598297/0.296799loss in batch 28: 0.495834/0.303665loss in batch 29: 0.0256042/0.294388loss in batch 30: 0.025177/0.285706loss in batch 31: 0.153717/0.28157loss in batch 32: 0.0121002/0.273407loss in batch 33: 0.0297546/0.266251loss in batch 34: 0.435791/0.271088loss in batch 35: 0.330688/0.272736loss in batch 36: 0.110184/0.268341loss in batch 37: 4.57764e-05/0.261292loss in batch 38: 0.821381/0.27565loss in batch 39: 0.157898/0.272705loss in batch 40: 0.263611/0.272491loss in batch 41: 0.199509/0.270737loss in batch 42: 0.0560913/0.265762loss in batch 43: 0.252457/0.265457loss in batch 44: 0.0298309/0.260223loss in batch 45: 0.0668182/0.256012loss in batch 46: 0.272659/0.256378loss in batch 47: 0.1521/0.254196loss in batch 48: 0.00378418/0.249084loss in batch 49: 0.0128326/0.24437loss in batch 50: 0.205429/0.243591loss in batch 51: 0.12027/0.241226loss in batch 52: 0.0353394/0.23735loss in batch 53: 0.0547943/0.233963loss in batch 54: 0.0180817/0.230026loss in batch 55: 0.139236/0.228424loss in batch 56: 0.0830841/0.225876loss in batch 57: 0.0940552/0.223587loss in batch 58: 0.0796356/0.221146loss in batch 59: 0.261322/0.221832loss in batch 60: 0.0164337/0.21846loss in batch 61: 0.30127/0.219788loss in batch 62: 0.158112/0.218811loss in batch 63: 0.206696/0.218628loss in batch 64: 0.123703/0.217148loss in batch 65: 0.0815582/0.215103loss in batch 66: 0.262207/0.215805loss in batch 67: 0.0858002/0.213898loss in batch 68: 0.0527344/0.211563loss in batch 69: 0.439743/0.214813loss in batch 70: 0.212967/0.214798loss in batch 71: 0.254364/0.215332loss in batch 72: 0.0801697/0.213501loss in batch 73: 0.0410004/0.211166loss in batch 74: 0.663864/0.217194loss in batch 75: 0.357498/0.21904loss in batch 76: 0.058136/0.216949loss in batch 77: 0.0563812/0.21489loss in batch 78: 0.194183/0.214645loss in batch 79: 0.525223/0.218506loss in batch 80: 0.102264/0.217087loss in batch 81: 0.27887/0.217834loss in batch 82: 0.00842285/0.215317loss in batch 83: 0.0527191/0.213364loss in batch 84: 0.0291138/0.211212loss in batch 85: 0.232254/0.211456loss in batch 86: 0.582153/0.215714loss in batch 87: 0.0351868/0.213669loss in batch 88: 0.0597076/0.211929loss in batch 89: 0.0717316/0.210373loss in batch 90: 0.142822/0.209625loss in batch 91: 0.0664368/0.208084loss in batch 92: 1.15118/0.218216loss in batch 93: 0.143906/0.217422loss in batch 94: 0.0276947/0.215424loss in batch 95: 0.0352783/0.213562loss in batch 96: 0.0542145/0.211914loss in batch 97: 0.138931/0.211166loss in batch 98: 0.0494232/0.209534loss in batch 99: 0.452057/0.21196loss in batch 100: 0.068924/0.210541loss in batch 101: 0.104507/0.209503loss in batch 102: 0.0887299/0.208328loss in batch 103: 0.069397/0.206985loss in batch 104: 0.0291748/0.205292loss in batch 105: 0.113251/0.204437loss in batch 106: 0.275375/0.205093loss in batch 107: 0.0196686/0.203384loss in batch 108: 0.133804/0.202728loss in batch 109: 0.207367/0.202774loss in batch 110: 0.152557/0.202316loss in batch 111: 0.170258/0.202042loss in batch 112: 0.0665436/0.200851loss in batch 113: 0.0126038/0.199188loss in batch 114: 0.0842896/0.198196loss in batch 115: 0.0576477/0.196976loss in batch 116: 0.0699615/0.195892loss in batch 117: 0.699036/0.20015loss in batch 118: 0.0204468/0.198654loss in batch 119: 0.0519714/0.197418loss in batch 120: 0.439102/0.199417loss in batch 121: 0.0832977/0.198471loss in batch 122: 0.31485/0.199417loss in batch 123: 0.052124/0.198242loss in batch 124: 0.0203705/0.196808loss in batch 125: 0.0509033/0.195648loss in batch 126: 0.0145111/0.194229loss in batch 127: 0.484528/0.196487loss in batch 128: 0.168579/0.196274loss in batch 129: 0.0537109/0.195175loss in batch 130: 0.204514/0.195251loss in batch 131: 0.102341/0.194534loss in batch 132: 0.320389/0.195496loss in batch 133: 0.20369/0.195557loss in batch 134: 0.0319824/0.194351loss in batch 135: 0.0103455/0.192993loss in batch 136: 0.341568/0.194077loss in batch 137: 0.0500336/0.193024loss in batch 138: 0.0241394/0.191818loss in batch 139: 0.0945282/0.191116loss in batch 140: 0.0735016/0.190277loss in batch 141: 0.106049/0.189697loss in batch 142: 0.102371/0.189087loss in batch 143: 0.146179/0.188782loss in batch 144: 0.091507/0.18811loss in batch 145: 0.0160065/0.186935loss in batch 146: 0.0758972/0.186172loss in batch 147: 0.0937042/0.185547loss in batch 148: 0.246994/0.185959loss in batch 149: 0.574905/0.188568loss in batch 150: 0.0399323/0.187576loss in batch 151: 0.0190887/0.186462loss in batch 152: 0.26947/0.187012loss in batch 153: 0.242065/0.187363loss in batch 154: 0.0494537/0.186462loss in batch 155: 0.0582428/0.185654loss in batch 156: 0.0186157/0.184586loss in batch 157: 0.0210724/0.183563loss in batch 158: 0.14003/0.183273loss in batch 159: 0.309372/0.184082loss in batch 160: 0.128571/0.183731loss in batch 161: 0.00367737/0.182617loss in batch 162: 0.0392456/0.181732loss in batch 163: 0.0695648/0.181046loss in batch 164: 0.105896/0.180603loss in batch 165: 0.0857391/0.180023loss in batch 166: 0.0612335/0.179306loss in batch 167: 0.0626678/0.178619loss in batch 168: 0.504715/0.180542loss in batch 169: 0.0684052/0.179886loss in batch 170: 0.0126648/0.178909loss in batch 171: 0.284851/0.179535loss in batch 172: 0.496628/0.181366loss in batch 173: 0.157516/0.181213loss in batch 174: 0.148895/0.181046loss in batch 175: 0.0460968/0.180283loss in batch 176: 0.456253/0.181824loss in batch 177: 0.42424/0.183197loss in batch 178: 0.481735/0.18486loss in batch 179: 0.315918/0.185593loss in batch 180: 0.542313/0.187561loss in batch 181: 0.162445/0.187424loss in batch 182: 0.171616/0.187332loss in batch 183: 0.250259/0.187683loss in batch 184: 0.092804/0.187164loss in batch 185: 0.0976715/0.186676loss in batch 186: 0.566711/0.188705loss in batch 187: 0.0565186/0.188004loss in batch 188: 0.456039/0.189438loss in batch 189: 0.833084/0.19281loss in batch 190: 0.142517/0.192551loss in batch 191: 0.100082/0.192062loss in batch 192: 0.0200043/0.191177loss in batch 193: 0.054657/0.190475loss in batch 194: 0.326721/0.191177loss in batch 195: 0.0461578/0.190445loss in batch 196: 0.00460815/0.189499loss in batch 197: 0.0534515/0.188812loss in batch 198: 0.0388184/0.188049loss in batch 199: 0.0970154/0.187592loss in batch 200: 0.00863647/0.186707loss in batch 201: 0.156921/0.186554loss in batch 202: 0.178986/0.186523loss in batch 203: 0.0506744/0.185867loss in batch 204: 0.207779/0.185959loss in batch 205: 0.0817719/0.185455loss in batch 206: 0.0698547/0.184906loss in batch 207: 0.233276/0.185135loss in batch 208: 0.0535736/0.184494loss in batch 209: 0.175034/0.184464loss in batch 210: 0.0310211/0.183731loss in batch 211: 0.0762482/0.183212loss in batch 212: 0.0276642/0.18248
done with epoch 10
train_acc: 0.941315 (401/426)
test loss: 0.0276641
acc: 0.937061 (134/143)
loss in batch 0: 0.0825043/0.0825043loss in batch 1: 0.0901031/0.0863037loss in batch 2: 0.244339/0.138977loss in batch 3: 0.0578918/0.118698loss in batch 4: 0.0607605/0.107117loss in batch 5: 0.0496368/0.0975342loss in batch 6: 0.614624/0.171402loss in batch 7: 0.0958252/0.161957loss in batch 8: 0.0814056/0.153015loss in batch 9: 0.0904541/0.146759loss in batch 10: 0.510269/0.17981loss in batch 11: 0.11235/0.174179loss in batch 12: 0.551666/0.203217loss in batch 13: 0.0858917/0.194824loss in batch 14: 0.047699/0.185028loss in batch 15: 0.0983276/0.179611loss in batch 16: 0.0379028/0.171265loss in batch 17: 0.0431519/0.164154loss in batch 18: 0.382568/0.175659loss in batch 19: 0.217545/0.17775loss in batch 20: 0.280624/0.182648loss in batch 21: 0.0543671/0.176819loss in batch 22: 0.301453/0.182236loss in batch 23: 0.470932/0.19426loss in batch 24: 0.13858/0.192032loss in batch 25: 0.297287/0.196075loss in batch 26: 0.0471954/0.190567loss in batch 27: 0.109619/0.187683loss in batch 28: 0.0141754/0.181702loss in batch 29: 0.501205/0.192337loss in batch 30: 0.00820923/0.186401loss in batch 31: 0.0405121/0.181854loss in batch 32: 0.046814/0.177765loss in batch 33: 0.0773468/0.174805loss in batch 34: 0.0580597/0.171463loss in batch 35: 0.319489/0.175583loss in batch 36: 0.473526/0.18364loss in batch 37: 0.0545502/0.180222loss in batch 38: 0.0849304/0.177795loss in batch 39: 0.113937/0.176193loss in batch 40: 0.290939/0.178986loss in batch 41: 0.178925/0.178986loss in batch 42: 0.266159/0.181015loss in batch 43: 0.0461731/0.177963loss in batch 44: 0.220581/0.178909loss in batch 45: 0.0259094/0.175583loss in batch 46: 0.200775/0.176102loss in batch 47: 0.134537/0.175247loss in batch 48: 0.15538/0.174835loss in batch 49: 0.0246735/0.171829loss in batch 50: 0.249725/0.173355loss in batch 51: 0.0621796/0.171234loss in batch 52: 0.122635/0.170319loss in batch 53: 0.166885/0.170242loss in batch 54: 0.0615845/0.168259loss in batch 55: 0.15239/0.167984loss in batch 56: 0.0190277/0.165375loss in batch 57: 0.281128/0.167374loss in batch 58: 0.0106964/0.164719loss in batch 59: 0.447433/0.169434loss in batch 60: 0.0667419/0.16774loss in batch 61: 0.469177/0.172592loss in batch 62: 0.443298/0.176895loss in batch 63: 0.933167/0.188721loss in batch 64: 0.0371399/0.186386loss in batch 65: 0.0554504/0.184402loss in batch 66: 0.0862427/0.182938loss in batch 67: 0.015686/0.180466loss in batch 68: 0.193832/0.180664loss in batch 69: 0.0191498/0.17836loss in batch 70: 0.324768/0.180435loss in batch 71: 0.192612/0.180588loss in batch 72: 0.488647/0.184814loss in batch 73: 0.0659332/0.183197loss in batch 74: 0.0477753/0.181396loss in batch 75: 0.281723/0.182709loss in batch 76: 0.251282/0.183624loss in batch 77: 0.235306/0.18428loss in batch 78: 0.238235/0.184952loss in batch 79: 1.15504/0.197083loss in batch 80: 0.12767/0.196228loss in batch 81: 0.00978088/0.193939loss in batch 82: 0.789978/0.201141loss in batch 83: 0.347458/0.202866loss in batch 84: 0.000961304/0.2005loss in batch 85: 0.0448761/0.1987loss in batch 86: 0.0291901/0.196747loss in batch 87: 0.106476/0.195724loss in batch 88: 0.136139/0.195038loss in batch 89: 0.419632/0.19754loss in batch 90: 0.126923/0.196762loss in batch 91: 0.068573/0.195374loss in batch 92: 0.17952/0.19519loss in batch 93: 0.177719/0.195023loss in batch 94: 0.074646/0.193756loss in batch 95: 0.114044/0.192917loss in batch 96: 0.0798035/0.191757loss in batch 97: 0.0120544/0.189926loss in batch 98: 0.06073/0.188614loss in batch 99: 1.17091/0.198441loss in batch 100: 0.466904/0.201096loss in batch 101: 0.119751/0.200287loss in batch 102: 0.107819/0.199402loss in batch 103: 0.0312195/0.197784loss in batch 104: 0.197235/0.197769loss in batch 105: 0.0397186/0.196274loss in batch 106: 0.0288849/0.194717loss in batch 107: 0.214691/0.194901loss in batch 108: 1.45348/0.206467loss in batch 109: 0.0275269/0.204834loss in batch 110: 0.158829/0.204407loss in batch 111: 0.0267181/0.20282loss in batch 112: 0.0606689/0.201569loss in batch 113: 0.139038/0.201019loss in batch 114: 0.0670471/0.199844loss in batch 115: 0.0185547/0.198288loss in batch 116: 0.0681305/0.197174loss in batch 117: 0.4711/0.199493loss in batch 118: 0.348587/0.200745loss in batch 119: 0.0764771/0.199722loss in batch 120: 0.328629/0.200775loss in batch 121: 0.0142822/0.199249loss in batch 122: 0.340225/0.200394loss in batch 123: 0.16124/0.200089loss in batch 124: 0.332611/0.201141loss in batch 125: 0.125366/0.200546loss in batch 126: 0.0493774/0.199356loss in batch 127: 0.0428162/0.19812loss in batch 128: 0.234894/0.198425loss in batch 129: 0.0593567/0.197342loss in batch 130: 0.0515442/0.196228loss in batch 131: 0.423096/0.197952loss in batch 132: 0.192612/0.197922loss in batch 133: 0.101837/0.197189loss in batch 134: 0.0929871/0.196426loss in batch 135: 0.105331/0.19574loss in batch 136: 0.13028/0.195282loss in batch 137: 0.208984/0.195374loss in batch 138: 0.479584/0.197418loss in batch 139: 0.42775/0.199066loss in batch 140: 0.0224457/0.197815loss in batch 141: 0.0144043/0.196533loss in batch 142: 0.40535/0.197983loss in batch 143: 0.363113/0.199127loss in batch 144: 0.0565491/0.198151loss in batch 145: 0.197266/0.198135loss in batch 146: 0.0239716/0.19696loss in batch 147: 0.24556/0.197281loss in batch 148: 0.148361/0.19696loss in batch 149: 0.369003/0.198105loss in batch 150: 0.150162/0.197769loss in batch 151: 0.205688/0.197845loss in batch 152: 0.0236206/0.196686loss in batch 153: 0.112869/0.196152loss in batch 154: 0.026535/0.195068loss in batch 155: 0.0341644/0.194031loss in batch 156: 0.0834503/0.193329loss in batch 157: 0.066864/0.19252loss in batch 158: 0.0411072/0.191559loss in batch 159: 0.0409393/0.190628loss in batch 160: 0.411499/0.192001loss in batch 161: 0.316559/0.192764loss in batch 162: 0.140839/0.192444loss in batch 163: 0.00410461/0.191299loss in batch 164: 0.0928497/0.190704loss in batch 165: 0.0766754/0.190018loss in batch 166: 0.160599/0.18985loss in batch 167: 0.12236/0.189438loss in batch 168: 0.239685/0.189728loss in batch 169: 0.0747223/0.189056loss in batch 170: 0.0429993/0.188202loss in batch 171: 0.0230103/0.187256loss in batch 172: 0.0641937/0.186523loss in batch 173: 0.315506/0.187271loss in batch 174: 0.327881/0.18808loss in batch 175: 0.0379333/0.187225loss in batch 176: 0.1129/0.186813loss in batch 177: 0.304031/0.187469loss in batch 178: 0.00526428/0.186447loss in batch 179: 0.052536/0.185699loss in batch 180: 0.116486/0.185333loss in batch 181: 0.0286713/0.184464loss in batch 182: 0.169022/0.184372loss in batch 183: 0.077774/0.183792loss in batch 184: 0.0498962/0.18306loss in batch 185: 0.129776/0.182785loss in batch 186: 0.0371399/0.182007loss in batch 187: 0.0404053/0.181244loss in batch 188: 0.0757141/0.180695loss in batch 189: 0.265747/0.181137loss in batch 190: 0.0408783/0.180405loss in batch 191: 0.0548248/0.179749loss in batch 192: 0.188858/0.17981loss in batch 193: 0.0221405/0.178986loss in batch 194: 0.0658112/0.178406loss in batch 195: 0.296356/0.179016loss in batch 196: 0.508987/0.180695loss in batch 197: 0.458557/0.182098loss in batch 198: 0.0413055/0.181381loss in batch 199: 0.0482635/0.180725loss in batch 200: 0.156342/0.180603loss in batch 201: 0.422684/0.181793loss in batch 202: 0.372299/0.182724loss in batch 203: 0.088562/0.182266loss in batch 204: 0.362015/0.183151loss in batch 205: 0.0474396/0.18248loss in batch 206: 0.172379/0.182449loss in batch 207: 0.025589/0.181686loss in batch 208: 0.0171509/0.180893loss in batch 209: 0.125061/0.180634loss in batch 210: 0.0135956/0.17984loss in batch 211: 0.0585632/0.17926loss in batch 212: 0.0475159/0.17865
done with epoch 11
train_acc: 0.946009 (403/426)
test loss: 0.0475158
acc: 0.944054 (135/143)
loss in batch 0: 0.0433655/0.0433655loss in batch 1: 0.12207/0.0827179loss in batch 2: 0.0167694/0.06073loss in batch 3: 0.195282/0.0943604loss in batch 4: 0.198318/0.115158loss in batch 5: 0.16835/0.124023loss in batch 6: 0.0730743/0.116745loss in batch 7: 0.0542908/0.108948loss in batch 8: 0.0123596/0.0982208loss in batch 9: 0.0637054/0.0947723loss in batch 10: 0.0688477/0.0924072loss in batch 11: 0.0275116/0.0869904loss in batch 12: 0.102005/0.08815loss in batch 13: 0.148117/0.0924377loss in batch 14: 0.317993/0.107468loss in batch 15: 0.095047/0.106705loss in batch 16: 0.0283356/0.102081loss in batch 17: 0.491318/0.123703loss in batch 18: 0.0179138/0.118149loss in batch 19: 0.0645294/0.115463loss in batch 20: 0.632843/0.140091loss in batch 21: 0.108521/0.138672loss in batch 22: 0.42598/0.151154loss in batch 23: 0.105881/0.149261loss in batch 24: 0.112991/0.147827loss in batch 25: 0.0697174/0.144806loss in batch 26: 0.0810699/0.142456loss in batch 27: 0.262604/0.146744loss in batch 28: 0.117096/0.145721loss in batch 29: 0.0606842/0.142883loss in batch 30: 0.0583496/0.140152loss in batch 31: 0.0353088/0.136871loss in batch 32: 0.0324402/0.133713loss in batch 33: 0.0255432/0.130524loss in batch 34: 0.0634003/0.128616loss in batch 35: 0.050705/0.12645loss in batch 36: 0.705063/0.14209loss in batch 37: 0.26062/0.145203loss in batch 38: 0.341812/0.150253loss in batch 39: 0.00704956/0.146667loss in batch 40: 0.0140228/0.143448loss in batch 41: 0.0435028/0.141052loss in batch 42: 0.0174866/0.138184loss in batch 43: 0.0646515/0.13652loss in batch 44: 0.272568/0.139526loss in batch 45: 0.0520782/0.137634loss in batch 46: 0.502777/0.145416loss in batch 47: 0.133499/0.145157loss in batch 48: 0.163895/0.145538loss in batch 49: 0.0155945/0.142944loss in batch 50: 0.011734/0.140366loss in batch 51: 0.246948/0.14241loss in batch 52: 0.0606842/0.140869loss in batch 53: 1.53261/0.166656loss in batch 54: 0.499847/0.172699loss in batch 55: 0.148911/0.172287loss in batch 56: 0.311264/0.174728loss in batch 57: 0.0347443/0.172302loss in batch 58: 0.461243/0.1772loss in batch 59: 1.26961/0.195404loss in batch 60: 0.085022/0.193588loss in batch 61: 0.158813/0.193039loss in batch 62: 0.0597382/0.190918loss in batch 63: 0.136948/0.190079loss in batch 64: 0.10376/0.188751loss in batch 65: 0.5439/0.194122loss in batch 66: 0.175064/0.193848loss in batch 67: 0.262604/0.194855loss in batch 68: 0.0667725/0.193008loss in batch 69: 0.0722961/0.191284loss in batch 70: 0.183075/0.191162loss in batch 71: 0.0364532/0.189011loss in batch 72: 0.126907/0.188156loss in batch 73: 0.288513/0.189529loss in batch 74: 0.305084/0.191071loss in batch 75: 0.0791626/0.18959loss in batch 76: 0.1633/0.189255loss in batch 77: 0.0235443/0.187119loss in batch 78: 0.15892/0.186752loss in batch 79: 0.0133209/0.184601loss in batch 80: 0.0695648/0.183167loss in batch 81: 0.172943/0.183044loss in batch 82: 0.186707/0.18309loss in batch 83: 0.0378723/0.181366loss in batch 84: 0.0314941/0.179596loss in batch 85: 0.680771/0.18544loss in batch 86: 0.0463562/0.183838loss in batch 87: 0.0274506/0.182068loss in batch 88: 0.117569/0.181335loss in batch 89: 0.323166/0.182907loss in batch 90: 0.0168762/0.181091loss in batch 91: 0.171783/0.180984loss in batch 92: 0.053772/0.179611loss in batch 93: 0.340652/0.18132loss in batch 94: 0.328094/0.182877loss in batch 95: 0.0484619/0.181473loss in batch 96: 0.0301056/0.179916loss in batch 97: 0.759033/0.185822loss in batch 98: 0.484695/0.188828loss in batch 99: 0.0395813/0.187347loss in batch 100: 0.103622/0.186508loss in batch 101: 0.0354614/0.185043loss in batch 102: 0.0651398/0.183868loss in batch 103: 0.0970306/0.183044loss in batch 104: 0.0151215/0.181442loss in batch 105: 0.0138397/0.179855loss in batch 106: 0.0719299/0.178848loss in batch 107: 0.213043/0.179169loss in batch 108: 0.4832/0.181961loss in batch 109: 0.331604/0.183319loss in batch 110: 0.218582/0.183624loss in batch 111: 0.231171/0.184052loss in batch 112: 0.03862/0.18277loss in batch 113: 0.129562/0.182297loss in batch 114: 0.0306091/0.180984loss in batch 115: 0.0312958/0.179688loss in batch 116: 0.126038/0.179245loss in batch 117: 0.238983/0.179733loss in batch 118: 0.0654907/0.178772loss in batch 119: 0.0793152/0.177948loss in batch 120: 0.14473/0.177673loss in batch 121: 0.0190887/0.176376loss in batch 122: 0.107407/0.175827loss in batch 123: 0.26709/0.176544loss in batch 124: 0.294968/0.17749loss in batch 125: 0.0255432/0.1763loss in batch 126: 0.0286407/0.17514loss in batch 127: 0.0558014/0.174194loss in batch 128: 0.0939789/0.173584loss in batch 129: 0.536896/0.176376loss in batch 130: 0.114166/0.175888loss in batch 131: 0.183502/0.175949loss in batch 132: 0.0464172/0.174973loss in batch 133: 0.0171356/0.173813loss in batch 134: 0.350647/0.175125loss in batch 135: 0.0459137/0.174164loss in batch 136: 1.42252/0.183273loss in batch 137: 0.453232/0.185226loss in batch 138: 0.146683/0.184952loss in batch 139: 0.0765991/0.184189loss in batch 140: 0.0776825/0.183426loss in batch 141: 0.266205/0.184021loss in batch 142: 0.627319/0.187103loss in batch 143: 0.213531/0.187286loss in batch 144: 0.0925751/0.18663loss in batch 145: 0.0888367/0.185974loss in batch 146: 0.0863953/0.185287loss in batch 147: 0.391144/0.186676loss in batch 148: 0.00238037/0.18544loss in batch 149: 0.100586/0.184875loss in batch 150: 0.0552521/0.184021loss in batch 151: 0.00892639/0.182861loss in batch 152: 0.292755/0.183594loss in batch 153: 0.0559387/0.18277loss in batch 154: 0.205124/0.182907loss in batch 155: 0.258804/0.183395loss in batch 156: 0.410629/0.18483loss in batch 157: 0.0502014/0.18399loss in batch 158: 0.251984/0.184402loss in batch 159: 0.0113373/0.183334loss in batch 160: 0.0866241/0.182739loss in batch 161: 0.434219/0.18428loss in batch 162: 0.0697327/0.183578loss in batch 163: 0.101563/0.183075loss in batch 164: 0.355652/0.184128loss in batch 165: 0.016571/0.183121loss in batch 166: 0.0291138/0.18219loss in batch 167: 0.0145721/0.181198loss in batch 168: 0.0358429/0.180344loss in batch 169: 0.146057/0.18013loss in batch 170: 0.0749664/0.17952loss in batch 171: 0.0211334/0.178604loss in batch 172: 0.0190277/0.177673loss in batch 173: 0.117401/0.177338loss in batch 174: 0.271698/0.177872loss in batch 175: 0.162857/0.17778loss in batch 176: 0.0664673/0.177155loss in batch 177: 0.0563812/0.176483loss in batch 178: 0.698364/0.179398loss in batch 179: 0.108932/0.179001loss in batch 180: 0.0679932/0.178375loss in batch 181: 0.396164/0.179581loss in batch 182: 0.0507965/0.178879loss in batch 183: 0.0450592/0.178162loss in batch 184: 0.108673/0.17778loss in batch 185: 0.109665/0.177399loss in batch 186: 0.311356/0.178116loss in batch 187: 0.0246887/0.177307loss in batch 188: 0.237778/0.177628loss in batch 189: 0.0139313/0.176773loss in batch 190: 0.0722046/0.176224loss in batch 191: 0.0293274/0.175461loss in batch 192: 0.0505219/0.174805loss in batch 193: 0.119446/0.17453loss in batch 194: 0.169739/0.1745loss in batch 195: 0.260071/0.174942loss in batch 196: 0.277679/0.175461loss in batch 197: 0.271576/0.175934loss in batch 198: 0.342804/0.176773loss in batch 199: 0.0145874/0.175964loss in batch 200: 0.206573/0.176132loss in batch 201: 0.169266/0.176086loss in batch 202: 0.0565796/0.175507loss in batch 203: 0.184189/0.175537loss in batch 204: 0.164032/0.175491loss in batch 205: 0.121094/0.175217loss in batch 206: 0.618332/0.177368loss in batch 207: 0.0303955/0.176666loss in batch 208: 0.138046/0.176468loss in batch 209: 0.0448456/0.175842loss in batch 210: 0.0798492/0.175385loss in batch 211: 0.0348969/0.174728loss in batch 212: 0.024231/0.174011
done with epoch 12
train_acc: 0.946009 (403/426)
test loss: 0.0242309
acc: 0.937061 (134/143)
loss in batch 0: 0.0435028/0.0435028loss in batch 1: 0.0266876/0.0350952loss in batch 2: 0.0843506/0.0515137loss in batch 3: 0.0578156/0.0530853loss in batch 4: 0.157394/0.0739441loss in batch 5: 0.350845/0.120087loss in batch 6: 0.0739136/0.113495loss in batch 7: 0.0935822/0.111008loss in batch 8: 0.0417633/0.103317loss in batch 9: 0.0106506/0.0940552loss in batch 10: 0.0208435/0.0874023loss in batch 11: 0.0471344/0.0840302loss in batch 12: 0.0504456/0.0814514loss in batch 13: 0.0991821/0.0827179loss in batch 14: 0.0298157/0.0791931loss in batch 15: 0.0343628/0.0763855loss in batch 16: 0.0655365/0.0757599loss in batch 17: 0.260452/0.0860138loss in batch 18: 0.577942/0.111908loss in batch 19: 0.0389099/0.108261loss in batch 20: 0.351089/0.119827loss in batch 21: 0.314056/0.128662loss in batch 22: 0.492569/0.14447loss in batch 23: 0.0106964/0.138901loss in batch 24: 0.0570374/0.13562loss in batch 25: 0.0140228/0.130951loss in batch 26: 0.00404358/0.126251loss in batch 27: 0.0286407/0.122757loss in batch 28: 0.060791/0.120621loss in batch 29: 0.0210876/0.11731loss in batch 30: 0.305115/0.123367loss in batch 31: 0.0363464/0.120651loss in batch 32: 0.155182/0.121689loss in batch 33: 0.150543/0.122543loss in batch 34: 0.246674/0.126083loss in batch 35: 0.0178375/0.123077loss in batch 36: 0.0805511/0.121933loss in batch 37: 0.766144/0.138885loss in batch 38: 0.0453186/0.136475loss in batch 39: 0.00411987/0.133179loss in batch 40: 0.0393982/0.130875loss in batch 41: 0.110626/0.130402loss in batch 42: 0.151871/0.130905loss in batch 43: 0.193268/0.132324loss in batch 44: 0.131042/0.132294loss in batch 45: 0.165771/0.133011loss in batch 46: 0.405228/0.138809loss in batch 47: 0.0349731/0.136642loss in batch 48: 0.0807495/0.135513loss in batch 49: 0.0286255/0.133377loss in batch 50: 0.113129/0.132965loss in batch 51: 0.0288849/0.130966loss in batch 52: 0.0406036/0.129272loss in batch 53: 0.0586853/0.12796loss in batch 54: 0.0219269/0.126038loss in batch 55: 0.678574/0.135895loss in batch 56: 0.0625/0.134613loss in batch 57: 0.0234375/0.13269loss in batch 58: 0.624161/0.141022loss in batch 59: 0.123123/0.140717loss in batch 60: 0.694687/0.149796loss in batch 61: 0.512222/0.155655loss in batch 62: 0.0628815/0.154175loss in batch 63: 0.0647583/0.152786loss in batch 64: 0.0796814/0.151657loss in batch 65: 0.366959/0.154922loss in batch 66: 0.0517731/0.153381loss in batch 67: 0.449539/0.15773loss in batch 68: 0.15741/0.15773loss in batch 69: 0.0227356/0.155792loss in batch 70: 0.308456/0.157944loss in batch 71: 0.688232/0.165329loss in batch 72: 0.504654/0.169968loss in batch 73: 0.159668/0.16983loss in batch 74: 0.616638/0.175797loss in batch 75: 1.13474/0.188416loss in batch 76: 0.125854/0.187592loss in batch 77: 0.0671692/0.18605loss in batch 78: 0.425842/0.189087loss in batch 79: 0.0784454/0.187698loss in batch 80: 0.0426025/0.185898loss in batch 81: 0.0843964/0.184662loss in batch 82: 0.0404816/0.182938loss in batch 83: 0.0925751/0.181854loss in batch 84: 0.192337/0.181976loss in batch 85: 0.143463/0.181534loss in batch 86: 0.00390625/0.179489loss in batch 87: 0.600693/0.18428loss in batch 88: 0.0524139/0.1828loss in batch 89: 0.052002/0.181351loss in batch 90: 0.0738983/0.180161loss in batch 91: 0.182388/0.180191loss in batch 92: 0.121902/0.17955loss in batch 93: 0.327911/0.181137loss in batch 94: 0.0119781/0.179367loss in batch 95: 0.259445/0.180191loss in batch 96: 0.871918/0.187317loss in batch 97: 0.0115509/0.185516loss in batch 98: 0.590698/0.189621loss in batch 99: 0.0566406/0.188293loss in batch 100: 0.0658417/0.187073loss in batch 101: 0.0752869/0.185989loss in batch 102: 0.0918274/0.185059loss in batch 103: 0.0372162/0.183655loss in batch 104: 0.121262/0.18306loss in batch 105: 0.0449829/0.181747loss in batch 106: 0.0827484/0.180817loss in batch 107: 0.392059/0.182785loss in batch 108: 0.102692/0.182053loss in batch 109: 0.0711365/0.181046loss in batch 110: 0.189392/0.181107loss in batch 111: 0.2733/0.181931loss in batch 112: 0.0997009/0.181213loss in batch 113: 0.0828552/0.180344loss in batch 114: 0.0415192/0.179138loss in batch 115: 0.0255737/0.177811loss in batch 116: 0.556976/0.181061loss in batch 117: 0.0485535/0.179932loss in batch 118: 0.00694275/0.178467loss in batch 119: 0.234528/0.178955loss in batch 120: 0.0562286/0.177933loss in batch 121: 0.0290527/0.176712loss in batch 122: 0.16835/0.176651loss in batch 123: 0.105423/0.176071loss in batch 124: 0.123077/0.175644loss in batch 125: 0.0357361/0.174545loss in batch 126: 0.0347137/0.173431loss in batch 127: 0.119171/0.173004loss in batch 128: 0.014801/0.171783loss in batch 129: 0.0427551/0.170792loss in batch 130: 0.46994/0.173065loss in batch 131: 1.32379/0.181793loss in batch 132: 0.0937042/0.181122loss in batch 133: 0.295578/0.181992loss in batch 134: 0.169601/0.181885loss in batch 135: 0.0853424/0.181183loss in batch 136: 0.173141/0.181122loss in batch 137: 0.0753326/0.180359loss in batch 138: 0.0482788/0.179413loss in batch 139: 0.000366211/0.178131loss in batch 140: 0.0124969/0.176956loss in batch 141: 0.221069/0.177261loss in batch 142: 0.0625305/0.176453loss in batch 143: 0.167145/0.176407loss in batch 144: 0.0952606/0.175842loss in batch 145: 0.0126953/0.174713loss in batch 146: 0.0526581/0.173889loss in batch 147: 0.266769/0.174515loss in batch 148: 0.315277/0.175461loss in batch 149: 0.0262146/0.174469loss in batch 150: 0.611145/0.177353loss in batch 151: 0.048584/0.176514loss in batch 152: 0.0497131/0.175674loss in batch 153: 0.0687714/0.174988loss in batch 154: 0.320892/0.175934loss in batch 155: 0.108932/0.175507loss in batch 156: 0.0384827/0.174637loss in batch 157: 0.244034/0.175064loss in batch 158: 0.0512543/0.174286loss in batch 159: 0.0283203/0.173386loss in batch 160: 0.0125427/0.172379loss in batch 161: 0.156387/0.172272loss in batch 162: 0.0177917/0.171341loss in batch 163: 0.0171204/0.170395loss in batch 164: 0.0600128/0.169724loss in batch 165: 0.113785/0.169373loss in batch 166: 0.0531616/0.168686loss in batch 167: 0.0302887/0.167862loss in batch 168: 0.226563/0.168213loss in batch 169: 0.0435333/0.167465loss in batch 170: 0.0845184/0.166992loss in batch 171: 1.46475/0.174545loss in batch 172: 0.201675/0.174698loss in batch 173: 0.32428/0.175568loss in batch 174: 0.0649414/0.174927loss in batch 175: 0.138092/0.174713loss in batch 176: 0.291946/0.175369loss in batch 177: 0.0649719/0.174759loss in batch 178: 0.0945435/0.174316loss in batch 179: 0.267044/0.17482loss in batch 180: 0.0781097/0.174286loss in batch 181: 0.047287/0.173599loss in batch 182: 0.0765533/0.173065loss in batch 183: 0.0227509/0.172256loss in batch 184: 0.49498/0.173981loss in batch 185: 0.0945129/0.173553loss in batch 186: 0.0873566/0.173096loss in batch 187: 0.075119/0.172577loss in batch 188: 0.144394/0.172424loss in batch 189: 0.0276337/0.171661loss in batch 190: 0.467575/0.173218loss in batch 191: 0.00621033/0.172348loss in batch 192: 0.0590363/0.171768loss in batch 193: 0.0902863/0.171341loss in batch 194: 0.0244141/0.170593loss in batch 195: 0.0143433/0.169785loss in batch 196: 0.0193939/0.169022loss in batch 197: 0.0220032/0.168289loss in batch 198: 0.0667572/0.16777loss in batch 199: 0.0447388/0.16716loss in batch 200: 0.140854/0.167038loss in batch 201: 0.0549622/0.166473loss in batch 202: 0.294464/0.167099loss in batch 203: 0.0065155/0.166321loss in batch 204: 0.385574/0.167389loss in batch 205: 0.175751/0.167435loss in batch 206: 0.178558/0.16748loss in batch 207: 0.269028/0.167969loss in batch 208: 0.1819/0.16803loss in batch 209: 0.0167236/0.167313loss in batch 210: 0.448196/0.16864loss in batch 211: 0.0251923/0.167969loss in batch 212: 0.611542/0.170059
done with epoch 13
train_acc: 0.943662 (402/426)
test loss: 0.61154
acc: 0.944054 (135/143)
loss in batch 0: 0.0600433/0.0600433loss in batch 1: 1.20428/0.632156loss in batch 2: 0.532532/0.598953loss in batch 3: 0.0544739/0.46283loss in batch 4: 0.0214996/0.374557loss in batch 5: 0.225525/0.349716loss in batch 6: 0.0893707/0.312546loss in batch 7: 0.207443/0.299393loss in batch 8: 0.352493/0.305298loss in batch 9: 0.0102386/0.275803loss in batch 10: 0.0788574/0.257889loss in batch 11: 0.00868225/0.237122loss in batch 12: 0.476196/0.255508loss in batch 13: 0.0767975/0.242737loss in batch 14: 0.0289307/0.228485loss in batch 15: 0.115433/0.221436loss in batch 16: 0.230743/0.22197loss in batch 17: 0.418549/0.232895loss in batch 18: 0.0721283/0.224426loss in batch 19: 0.0414429/0.215271loss in batch 20: 0.107162/0.210129loss in batch 21: 0.0323944/0.202057loss in batch 22: 0.140808/0.199402loss in batch 23: 0.355759/0.205902loss in batch 24: 0.240631/0.207291loss in batch 25: 0.073288/0.202133loss in batch 26: 0.0239105/0.195541loss in batch 27: 0.158112/0.194199loss in batch 28: 0.599197/0.20816loss in batch 29: 0.0841522/0.204025loss in batch 30: 0.210114/0.204224loss in batch 31: 0.0279388/0.198715loss in batch 32: 0.0722809/0.194885loss in batch 33: 0.28244/0.197479loss in batch 34: 0.104004/0.194794loss in batch 35: 0.226959/0.195694loss in batch 36: 0.0426941/0.191559loss in batch 37: 0.111176/0.189453loss in batch 38: 0.0858002/0.186783loss in batch 39: 0.300217/0.189621loss in batch 40: 0.16716/0.189056loss in batch 41: 0.207474/0.189514loss in batch 42: 0.0414734/0.186066loss in batch 43: 1.00308/0.204636loss in batch 44: 0.0896149/0.202072loss in batch 45: 0.43927/0.207245loss in batch 46: 0.00526428/0.202942loss in batch 47: 0.0957184/0.200699loss in batch 48: 0.297104/0.202682loss in batch 49: 0.0786896/0.200195loss in batch 50: 0.130692/0.198822loss in batch 51: 0.354721/0.201828loss in batch 52: 0.200104/0.201797loss in batch 53: 0.0539093/0.199066loss in batch 54: 0.0421143/0.196198loss in batch 55: 0.116806/0.194778loss in batch 56: 0.454163/0.199341loss in batch 57: 0.104843/0.197708loss in batch 58: 0.225067/0.198166loss in batch 59: 0.0275421/0.195328loss in batch 60: 0.124115/0.194153loss in batch 61: 0.0536346/0.191879loss in batch 62: 0.261322/0.192993loss in batch 63: 0.0892029/0.19136loss in batch 64: 0.134232/0.190491loss in batch 65: 0.0265808/0.188004loss in batch 66: 0.440216/0.191772loss in batch 67: 1.38068/0.209259loss in batch 68: 0.436966/0.212555loss in batch 69: 0.0548553/0.210297loss in batch 70: 0.0491638/0.208038loss in batch 71: 0.0125885/0.205322loss in batch 72: 0.327469/0.206985loss in batch 73: 1.35458/0.222504loss in batch 74: 0.06604/0.220413loss in batch 75: 0.0323334/0.217941loss in batch 76: 0.114258/0.216599loss in batch 77: 0.0305328/0.214203loss in batch 78: 0.479095/0.21756loss in batch 79: 0.0850067/0.215897loss in batch 80: 0.0393524/0.213715loss in batch 81: 0.126511/0.212662loss in batch 82: 0.0690155/0.210922loss in batch 83: 0.0784302/0.209351loss in batch 84: 0.204132/0.20929loss in batch 85: 0.0605774/0.207565loss in batch 86: 0.00299072/0.205215loss in batch 87: 0.0494385/0.203445loss in batch 88: 0.0378723/0.201569loss in batch 89: 0.0640259/0.200043loss in batch 90: 0.451523/0.20282loss in batch 91: 0.222733/0.203018loss in batch 92: 0.0242157/0.201111loss in batch 93: 0.0636749/0.199661loss in batch 94: 0.0957184/0.198547loss in batch 95: 0.37677/0.200409loss in batch 96: 0.0647278/0.199005loss in batch 97: 0.022995/0.197205loss in batch 98: 0.133835/0.196579loss in batch 99: 0.128693/0.195892loss in batch 100: 0.366547/0.197586loss in batch 101: 0.213211/0.197739loss in batch 102: 0.0591583/0.196381loss in batch 103: 0.039856/0.194885loss in batch 104: 0.0164337/0.193192loss in batch 105: 0.229065/0.193527loss in batch 106: 0.153824/0.193161loss in batch 107: 0.0768433/0.192078loss in batch 108: 0.0643616/0.190903loss in batch 109: 0.00460815/0.189209loss in batch 110: 0.00762939/0.187576loss in batch 111: 0.313141/0.188705loss in batch 112: 0.510162/0.191544loss in batch 113: 0.0421295/0.190231loss in batch 114: 0.0751801/0.189224loss in batch 115: 0.0448914/0.187988loss in batch 116: 0.0300598/0.186646loss in batch 117: 0.08255/0.18576loss in batch 118: 0.0202789/0.184372loss in batch 119: 0.0166779/0.182968loss in batch 120: 0.025528/0.181671loss in batch 121: 0.200043/0.181824loss in batch 122: 0.0699768/0.180908loss in batch 123: 0.085144/0.180145loss in batch 124: 0.00387573/0.178726loss in batch 125: 0.654312/0.182495loss in batch 126: 0.0310516/0.181305loss in batch 127: 0.066803/0.18042loss in batch 128: 0.0336151/0.179276loss in batch 129: 0.0782471/0.178497loss in batch 130: 0.0235138/0.177307loss in batch 131: 0.388977/0.178909loss in batch 132: 0.264328/0.179565loss in batch 133: 0.499573/0.181946loss in batch 134: 0.0409546/0.180908loss in batch 135: 0.08284/0.180191loss in batch 136: 0.0162354/0.178986loss in batch 137: 0.239578/0.179428loss in batch 138: 0.0538025/0.178528loss in batch 139: 0.145279/0.178284loss in batch 140: 0.0698547/0.177521loss in batch 141: 0.0226593/0.176422loss in batch 142: 0.263763/0.177032loss in batch 143: 0.0767059/0.176346loss in batch 144: 0.233902/0.176743loss in batch 145: 0.0684662/0.17598loss in batch 146: 0.110123/0.175537loss in batch 147: 0.318314/0.176514loss in batch 148: 0.0466614/0.175644loss in batch 149: 0.0450287/0.174774loss in batch 150: 0.0371704/0.173859loss in batch 151: 0.515427/0.176102loss in batch 152: 0.312347/0.176987loss in batch 153: 0.0105591/0.175919loss in batch 154: 0.0239105/0.174927loss in batch 155: 0.0658569/0.17424loss in batch 156: 0.0811768/0.173645loss in batch 157: 0.0147247/0.172623loss in batch 158: 0.55748/0.175064loss in batch 159: 0.0174255/0.174072loss in batch 160: 0.00543213/0.173035loss in batch 161: 0.0714111/0.172394loss in batch 162: 0.282623/0.17308loss in batch 163: 0.381577/0.174332loss in batch 164: 0.042038/0.173538loss in batch 165: 0.0273132/0.172668loss in batch 166: 0.0767059/0.172089loss in batch 167: 0.234192/0.172455loss in batch 168: 0.0989838/0.172028loss in batch 169: 0.11084/0.171661loss in batch 170: 0.0728912/0.171082loss in batch 171: 0.0749207/0.170532loss in batch 172: 0.0176392/0.169647loss in batch 173: 0.218628/0.169922loss in batch 174: 0.118774/0.169632loss in batch 175: 0.00292969/0.168686loss in batch 176: 0.0799866/0.168182loss in batch 177: 0.0846405/0.167709loss in batch 178: 0.0270996/0.166931loss in batch 179: 0.301285/0.167679loss in batch 180: 0.00404358/0.166763loss in batch 181: 0.216858/0.167053loss in batch 182: 0.293289/0.16774loss in batch 183: 0.0636292/0.16716loss in batch 184: 0.192612/0.167313loss in batch 185: 0.0538635/0.166702loss in batch 186: 0.179367/0.166763loss in batch 187: 0.0899506/0.166367loss in batch 188: 0.00782776/0.165512loss in batch 189: 0.530228/0.167435loss in batch 190: 0.0679779/0.166916loss in batch 191: 0.049942/0.166306loss in batch 192: 0.0928955/0.165924loss in batch 193: 0.170441/0.165939loss in batch 194: 0.037796/0.165298loss in batch 195: 0.138779/0.165161loss in batch 196: 0.519547/0.166962loss in batch 197: 0.105835/0.166656loss in batch 198: 0.0648041/0.166138loss in batch 199: 0.0129852/0.165359loss in batch 200: 0.443588/0.166763loss in batch 201: 0.0439758/0.166153loss in batch 202: 0.0590668/0.165619loss in batch 203: 0.00961304/0.164856loss in batch 204: 0.143921/0.164749loss in batch 205: 0.0597076/0.164246loss in batch 206: 0.0585938/0.163727loss in batch 207: 0.746811/0.166534loss in batch 208: 0.0415192/0.165939loss in batch 209: 0.460541/0.167343loss in batch 210: 0.036972/0.166718loss in batch 211: 0.16597/0.166718loss in batch 212: 0.212143/0.166931
done with epoch 14
train_acc: 0.93662 (399/426)
test loss: 0.212143
acc: 0.944054 (135/143)
loss in batch 0: 0.132721/0.132721loss in batch 1: 0.817215/0.474976loss in batch 2: 0.0728455/0.340927loss in batch 3: 0.0359039/0.264664loss in batch 4: 0.0616302/0.22406loss in batch 5: 0.141068/0.210236loss in batch 6: 0.0161591/0.182495loss in batch 7: 0.112808/0.173782loss in batch 8: 0.198074/0.176483loss in batch 9: 0.164612/0.175308loss in batch 10: 0.051651/0.164063loss in batch 11: 0.149872/0.162888loss in batch 12: 0.0120239/0.151276loss in batch 13: 0.130386/0.14978loss in batch 14: 0.000213623/0.139816loss in batch 15: 0.0569763/0.134628loss in batch 16: 0.315292/0.145264loss in batch 17: 0.12117/0.143921loss in batch 18: 0.439362/0.15947loss in batch 19: 0.215881/0.162292loss in batch 20: 0.538086/0.180191loss in batch 21: 0.048111/0.174179loss in batch 22: 0.123001/0.171967loss in batch 23: 0.170563/0.171906loss in batch 24: 0.0142822/0.165604loss in batch 25: 0.355789/0.172913loss in batch 26: 0.0694885/0.169083loss in batch 27: 0.0333405/0.164246loss in batch 28: 0.123138/0.162811loss in batch 29: 0.00601196/0.157593loss in batch 30: 0.0731812/0.154861loss in batch 31: 0.199951/0.156281loss in batch 32: 0.0273132/0.152374loss in batch 33: 0.23114/0.154694loss in batch 34: 0.0738068/0.152374loss in batch 35: 0.356964/0.158066loss in batch 36: 0.24411/0.16037loss in batch 37: 0.0144043/0.15654loss in batch 38: 0.00274658/0.152588loss in batch 39: 0.0158081/0.14917loss in batch 40: 0.119308/0.148453loss in batch 41: 0.0602112/0.146347loss in batch 42: 0.0215912/0.143448loss in batch 43: 0.066925/0.141708loss in batch 44: 0.247269/0.144043loss in batch 45: 0.270309/0.146805loss in batch 46: 0.0310516/0.144348loss in batch 47: 0.0673676/0.142731loss in batch 48: 0.242477/0.14476loss in batch 49: 0.33844/0.148636loss in batch 50: 0.19133/0.149475loss in batch 51: 0.141754/0.149323loss in batch 52: 0.0383148/0.147232loss in batch 53: 0.436737/0.152588loss in batch 54: 0.0657349/0.151016loss in batch 55: 0.0343628/0.148926loss in batch 56: 0.0662537/0.147476loss in batch 57: 0.693878/0.156891loss in batch 58: 0.0240021/0.154648loss in batch 59: 0.0239563/0.152466loss in batch 60: 0.0767975/0.15123loss in batch 61: 0.0213013/0.149139loss in batch 62: 0.0786896/0.14801loss in batch 63: 0.00553894/0.145782loss in batch 64: 0.0318756/0.144028loss in batch 65: 0.0825806/0.143112loss in batch 66: 0.0958405/0.142395loss in batch 67: 0.470764/0.147232loss in batch 68: 0.0561371/0.145905loss in batch 69: 0.0924835/0.145142loss in batch 70: 0.0424194/0.143692loss in batch 71: 0.010849/0.141846loss in batch 72: 0.0697021/0.140869loss in batch 73: 0.146011/0.14093loss in batch 74: 0.0111847/0.139206loss in batch 75: 0.0298004/0.137756loss in batch 76: 0.033493/0.136414loss in batch 77: 0.209732/0.137344loss in batch 78: 0.0188293/0.135849loss in batch 79: 0.0548706/0.134842loss in batch 80: 0.037262/0.133636loss in batch 81: 0.550751/0.138718loss in batch 82: 0.0671234/0.137863loss in batch 83: 0.121353/0.137665loss in batch 84: 0.034729/0.136444loss in batch 85: 0.0320892/0.135254loss in batch 86: 0.117569/0.135025loss in batch 87: 0.0384979/0.133942loss in batch 88: 0.0593872/0.133102loss in batch 89: 0.143356/0.133209loss in batch 90: 0.0708618/0.132523loss in batch 91: 0.0133057/0.131226loss in batch 92: 0.0262146/0.130096loss in batch 93: 0.123734/0.130035loss in batch 94: 0.282318/0.131653loss in batch 95: 0.330658/0.133713loss in batch 96: 0.0225525/0.132568loss in batch 97: 0.424667/0.135559loss in batch 98: 0.059906/0.134781loss in batch 99: 0.0401154/0.133835loss in batch 100: 0.173416/0.134232loss in batch 101: 0.0292206/0.133194loss in batch 102: 0.00790405/0.131989loss in batch 103: 0.0863647/0.131546loss in batch 104: 0.336029/0.133484loss in batch 105: 0.049118/0.132706loss in batch 106: 0.105179/0.132446loss in batch 107: 0.0354767/0.131546loss in batch 108: 0.0260315/0.130585loss in batch 109: 0.107864/0.130356loss in batch 110: 0.0816803/0.129929loss in batch 111: 0.0908813/0.129578loss in batch 112: 0.444855/0.13237loss in batch 113: 0.0276947/0.131454loss in batch 114: 0.125519/0.131393loss in batch 115: 0.367462/0.133438loss in batch 116: 0.0136108/0.132416loss in batch 117: 0.249634/0.133408loss in batch 118: 0.0391388/0.132599loss in batch 119: 0.039917/0.131836loss in batch 120: 0.0747681/0.131363loss in batch 121: 0.0106964/0.130386loss in batch 122: 0.0113831/0.12941loss in batch 123: 0.013443/0.128479loss in batch 124: 0.562241/0.131943loss in batch 125: 0.187012/0.132385loss in batch 126: 0.0117645/0.131439loss in batch 127: 0.402756/0.133545loss in batch 128: 0.0449371/0.132874loss in batch 129: 0.0856628/0.132507loss in batch 130: 0.304413/0.13382loss in batch 131: 0.0789185/0.133408loss in batch 132: 0.0381317/0.132675loss in batch 133: 0.0167847/0.131821loss in batch 134: 0.0630035/0.131302loss in batch 135: 0.0374908/0.13063loss in batch 136: 0.0751953/0.130219loss in batch 137: 0.197159/0.130692loss in batch 138: 0.0665131/0.130249loss in batch 139: 0.027771/0.129501loss in batch 140: 0.0274963/0.128769loss in batch 141: 0.402542/0.130722loss in batch 142: 0.0838776/0.130371loss in batch 143: 0.044754/0.129791loss in batch 144: 0.664566/0.133484loss in batch 145: 0.0579987/0.132965loss in batch 146: 1.23808/0.140472loss in batch 147: 0.0858612/0.140106loss in batch 148: 0.00737/0.139206loss in batch 149: 0.185577/0.139526loss in batch 150: 0.0324249/0.138809loss in batch 151: 0.0476379/0.138214loss in batch 152: 0.163239/0.138382loss in batch 153: 0.0243683/0.13765loss in batch 154: 0.00300598/0.136765loss in batch 155: 0.0823364/0.136414loss in batch 156: 0.0539246/0.135895loss in batch 157: 0.0869446/0.13559loss in batch 158: 0.0850983/0.135269loss in batch 159: 0.0768433/0.134903loss in batch 160: 0.0120239/0.13414loss in batch 161: 0.0854034/0.13385loss in batch 162: 0.108658/0.133682loss in batch 163: 0.584641/0.136444loss in batch 164: 0.0432281/0.135864loss in batch 165: 0.425522/0.137619loss in batch 166: 0.25354/0.138306loss in batch 167: 0.20491/0.138702loss in batch 168: 0.0574188/0.138214loss in batch 169: 0.306793/0.139221loss in batch 170: 0.053772/0.138718loss in batch 171: 0.149948/0.138794loss in batch 172: 0.0487976/0.138275loss in batch 173: 0.0671234/0.137848loss in batch 174: 0.102753/0.13765loss in batch 175: 0.0508575/0.137161loss in batch 176: 0.0523071/0.136673loss in batch 177: 0.0942078/0.136429loss in batch 178: 0.0762939/0.136108loss in batch 179: 0.475052/0.137985loss in batch 180: 0.0732727/0.137634loss in batch 181: 0.961029/0.142151loss in batch 182: 0.160934/0.142258loss in batch 183: 0.00488281/0.141525loss in batch 184: 0.0450134/0.140976loss in batch 185: 0.0196838/0.140335loss in batch 186: 0.0449371/0.139832loss in batch 187: 0.052063/0.139374loss in batch 188: 0.420242/0.140854loss in batch 189: 0.157669/0.140945loss in batch 190: 0.605804/0.143372loss in batch 191: 0.0426636/0.142838loss in batch 192: 0.177231/0.143036loss in batch 193: 0.0803833/0.142685loss in batch 194: 0.150482/0.142731loss in batch 195: 0.0895691/0.142471loss in batch 196: 0.212326/0.142822loss in batch 197: 0.256622/0.143402loss in batch 198: 0.487534/0.145126loss in batch 199: 0.0175934/0.144501loss in batch 200: 1.52299/0.151352loss in batch 201: 0.15773/0.151367loss in batch 202: 0.0968323/0.151108loss in batch 203: 0.313782/0.151917loss in batch 204: 0.254608/0.152405loss in batch 205: 0.0252075/0.151794loss in batch 206: 1.58914/0.158737loss in batch 207: 0.0545654/0.158249loss in batch 208: 0.237595/0.158615loss in batch 209: 1.38113/0.164444loss in batch 210: 0.254974/0.164871loss in batch 211: 0.104935/0.164581loss in batch 212: 0.146622/0.164505
done with epoch 15
train_acc: 0.943662 (402/426)
test loss: 0.146621
acc: 0.937061 (134/143)
loss in batch 0: 0.332214/0.332214loss in batch 1: 0.0330811/0.182648loss in batch 2: 0.100815/0.155365loss in batch 3: 0.013855/0.11998loss in batch 4: 0.0261383/0.101212loss in batch 5: 0.0376129/0.0906067loss in batch 6: 0.0427094/0.0837708loss in batch 7: 0.249115/0.104446loss in batch 8: 0.582916/0.157608loss in batch 9: 0.753967/0.217239loss in batch 10: 0.0265961/0.199921loss in batch 11: 0.382065/0.215088loss in batch 12: 0.0920563/0.205627loss in batch 13: 0.184296/0.204102loss in batch 14: 0.111801/0.197952loss in batch 15: 0.31778/0.205429loss in batch 16: 0.0329132/0.195282loss in batch 17: 0.131104/0.191727loss in batch 18: 0.142105/0.189117loss in batch 19: 0.511292/0.205215loss in batch 20: 0.0670013/0.198639loss in batch 21: 0.0553894/0.192123loss in batch 22: 0.123611/0.189148loss in batch 23: 0.0961609/0.185272loss in batch 24: 0.171204/0.184708loss in batch 25: 0.094635/0.181244loss in batch 26: 0.00521851/0.174728loss in batch 27: 0.0374146/0.16983loss in batch 28: 0.322357/0.175079loss in batch 29: 0.109985/0.172913loss in batch 30: 0.0529938/0.169037loss in batch 31: 0.0567932/0.165527loss in batch 32: 0.319962/0.170212loss in batch 33: 0.234772/0.172119loss in batch 34: 0.0147095/0.167618loss in batch 35: 0.0358734/0.163971loss in batch 36: 0.00601196/0.159683loss in batch 37: 0.0288849/0.15625loss in batch 38: 0.0361481/0.153168loss in batch 39: 0.0196838/0.149826loss in batch 40: 0.435379/0.156799loss in batch 41: 0.0372772/0.153946loss in batch 42: 0.0463562/0.151459loss in batch 43: 0.128525/0.150925loss in batch 44: 0.00411987/0.147659loss in batch 45: 0.0335236/0.145187loss in batch 46: 0.0590515/0.143356loss in batch 47: 0.0203705/0.140778loss in batch 48: 0.0511932/0.138962loss in batch 49: 0.028183/0.136749loss in batch 50: 0.118607/0.136383loss in batch 51: 0.0210876/0.134171loss in batch 52: 0.228424/0.135956loss in batch 53: 0.0506439/0.134384loss in batch 54: 0.0679779/0.133163loss in batch 55: 0.0524292/0.131714loss in batch 56: 0.198349/0.132889loss in batch 57: 0.268799/0.135223loss in batch 58: 0.0660858/0.134064loss in batch 59: 0.106918/0.133606loss in batch 60: 0.0515747/0.132263loss in batch 61: 0.0645752/0.131165loss in batch 62: 0.0251617/0.129486loss in batch 63: 0.00497437/0.127548loss in batch 64: 0.0126343/0.125778loss in batch 65: 0.0263367/0.124268loss in batch 66: 0.0273132/0.122818loss in batch 67: 0.0410309/0.121613loss in batch 68: 0.312332/0.12439loss in batch 69: 0.0775909/0.123718loss in batch 70: 0.0601349/0.122818loss in batch 71: 0.0567474/0.121902loss in batch 72: 0.00778198/0.120346loss in batch 73: 0.241348/0.121979loss in batch 74: 0.00364685/0.120392loss in batch 75: 0.0574036/0.119568loss in batch 76: 0.0887909/0.119171loss in batch 77: 0.0393982/0.118149loss in batch 78: 1.77171/0.139069loss in batch 79: 0.041275/0.137848loss in batch 80: 0.291824/0.139755loss in batch 81: 0.0332031/0.138443loss in batch 82: 0.0845947/0.137817loss in batch 83: 0.456772/0.141602loss in batch 84: 0.0300598/0.140289loss in batch 85: 0.0356293/0.139084loss in batch 86: 0.0193481/0.137695loss in batch 87: 0.275879/0.139267loss in batch 88: 0.327179/0.141388loss in batch 89: 0.0760498/0.140656loss in batch 90: 0.284027/0.142227loss in batch 91: 0.0199585/0.140915loss in batch 92: 0.0297394/0.139709loss in batch 93: 0.0706024/0.138977loss in batch 94: 0.0171204/0.13768loss in batch 95: 0.818405/0.144775loss in batch 96: 0.0784912/0.144104loss in batch 97: 0.0565338/0.143204loss in batch 98: 0.0377655/0.142136loss in batch 99: 0.309067/0.143799loss in batch 100: 0.0440674/0.142822loss in batch 101: 0.0828552/0.142227loss in batch 102: 0.0107269/0.140961loss in batch 103: 0.442093/0.143845loss in batch 104: 0.0144806/0.142609loss in batch 105: 0.285614/0.143967loss in batch 106: 0.792236/0.150024loss in batch 107: 0.266968/0.151108loss in batch 108: 0.0570221/0.150238loss in batch 109: 0.213593/0.150818loss in batch 110: 0.00985718/0.149551loss in batch 111: 0.174576/0.149765loss in batch 112: 0.047699/0.148865loss in batch 113: 0.0833282/0.1483loss in batch 114: 0.0303497/0.147278loss in batch 115: 0.0572968/0.146484loss in batch 116: 0.0256958/0.145462loss in batch 117: 0.0766144/0.144882loss in batch 118: 0.0173798/0.143799loss in batch 119: 0.241562/0.144623loss in batch 120: 0.0572815/0.143906loss in batch 121: 0.0130463/0.142822loss in batch 122: 0.157349/0.142944loss in batch 123: 0.0368347/0.14209loss in batch 124: 0.0577698/0.141418loss in batch 125: 0.413406/0.14357loss in batch 126: 0.16185/0.143723loss in batch 127: 0.0557861/0.143021loss in batch 128: 1.28825/0.151917loss in batch 129: 0.547867/0.154953loss in batch 130: 0.096283/0.15451loss in batch 131: 0.0753021/0.153915loss in batch 132: 0.660873/0.157715loss in batch 133: 0.35881/0.159225loss in batch 134: 0.0259399/0.158234loss in batch 135: 0.0492706/0.157425loss in batch 136: 0.00114441/0.156296loss in batch 137: 0.561401/0.159225loss in batch 138: 0.150406/0.159164loss in batch 139: 0.0820465/0.158615loss in batch 140: 0.0204773/0.157639loss in batch 141: 0.298141/0.15863loss in batch 142: 0.140076/0.158493loss in batch 143: 0.0536652/0.157776loss in batch 144: 0.0274353/0.15686loss in batch 145: 0.195709/0.157135loss in batch 146: 0.403976/0.158813loss in batch 147: 0.422974/0.160583loss in batch 148: 0.0200653/0.159653loss in batch 149: 0.118607/0.159378loss in batch 150: 0.550003/0.161957loss in batch 151: 0.0305939/0.161102loss in batch 152: 0.0731201/0.160522loss in batch 153: 0.509033/0.162781loss in batch 154: 0.0139771/0.161819loss in batch 155: 0.0530853/0.161133loss in batch 156: 0.0527496/0.160446loss in batch 157: 0.0274658/0.159607loss in batch 158: 0.0496674/0.158905loss in batch 159: 0.0085144/0.157959loss in batch 160: 0.479126/0.159958loss in batch 161: 0.353561/0.161148loss in batch 162: 0.0659637/0.160583loss in batch 163: 0.0692596/0.160019loss in batch 164: 0.338028/0.161102loss in batch 165: 0.0837708/0.160629loss in batch 166: 0.0241699/0.159805loss in batch 167: 0.359726/0.161011loss in batch 168: 0.575546/0.163452loss in batch 169: 0.104279/0.163101loss in batch 170: 0.271347/0.163742loss in batch 171: 0.174713/0.163803loss in batch 172: 0.0078125/0.162903loss in batch 173: 0.0198822/0.162079loss in batch 174: 0.0129089/0.161224loss in batch 175: 0.0224762/0.160431loss in batch 176: 0.416855/0.16188loss in batch 177: 0.144058/0.161789loss in batch 178: 0.0975952/0.161438loss in batch 179: 0.00315857/0.160553loss in batch 180: 0.0138092/0.159729loss in batch 181: 0.285873/0.160446loss in batch 182: 0.903687/0.164505loss in batch 183: 0.0788422/0.164032loss in batch 184: 0.0631409/0.163498loss in batch 185: 0.018158/0.162689loss in batch 186: 0.0513306/0.162109loss in batch 187: 0.0671692/0.161606loss in batch 188: 0.0457153/0.160995loss in batch 189: 0.323792/0.161835loss in batch 190: 0.0731201/0.161392loss in batch 191: 0.0518341/0.160812loss in batch 192: 0.0935211/0.160461loss in batch 193: 0.0607605/0.159943loss in batch 194: 0.0334167/0.159302loss in batch 195: 0.0185089/0.158569loss in batch 196: 0.2341/0.158966loss in batch 197: 0.14888/0.158905loss in batch 198: 0.143417/0.158829loss in batch 199: 0.0794678/0.158447loss in batch 200: 0.198975/0.158646loss in batch 201: 0.181488/0.158752loss in batch 202: 0.263046/0.159271loss in batch 203: 0.0895538/0.15892loss in batch 204: 0.39296/0.16008loss in batch 205: 0.110001/0.159821loss in batch 206: 0.0928955/0.1595loss in batch 207: 0.0812378/0.159134loss in batch 208: 0.148041/0.159073loss in batch 209: 0.133896/0.158951loss in batch 210: 0.162476/0.158966loss in batch 211: 0.293213/0.159592loss in batch 212: 0.197235/0.159775
done with epoch 16
train_acc: 0.955399 (407/426)
test loss: 0.197235
acc: 0.944054 (135/143)
loss in batch 0: 0.0296326/0.0296326loss in batch 1: 0.0353241/0.0324707loss in batch 2: 0.0291901/0.0313873loss in batch 3: 0.0176086/0.0279388loss in batch 4: 0.0353394/0.0294189loss in batch 5: 0.406418/0.0922394loss in batch 6: 0.0771332/0.0900879loss in batch 7: 0.0121918/0.0803528loss in batch 8: 0.0446625/0.0764008loss in batch 9: 0.0593872/0.0746765loss in batch 10: 0.00624084/0.0684662loss in batch 11: 0.137115/0.0741882loss in batch 12: 0.0636139/0.0733643loss in batch 13: 0.514099/0.104858loss in batch 14: 6.10352e-05/0.0978699loss in batch 15: 0.0826263/0.0969238loss in batch 16: 0.572876/0.124908loss in batch 17: 0.0313568/0.11972loss in batch 18: 0.0764008/0.117447loss in batch 19: 0.0352478/0.113327loss in batch 20: 0.244064/0.119553loss in batch 21: 0.25679/0.125793loss in batch 22: 0.0734711/0.12352loss in batch 23: 0.371994/0.133865loss in batch 24: 0.0406952/0.130142loss in batch 25: 0.214127/0.133377loss in batch 26: 0.0201874/0.129181loss in batch 27: 0.030365/0.125656loss in batch 28: 0.0847473/0.124237loss in batch 29: 0.190781/0.126465loss in batch 30: 1.17419/0.160248loss in batch 31: 0.0499878/0.156815loss in batch 32: 0.0402374/0.153275loss in batch 33: 0.254623/0.156265loss in batch 34: 0.0285339/0.152603loss in batch 35: 0.223816/0.154587loss in batch 36: 0.082016/0.152618loss in batch 37: 0.0440826/0.14978loss in batch 38: 0.173981/0.150391loss in batch 39: 0.139465/0.150116loss in batch 40: 0.0113983/0.146744loss in batch 41: 0.0382233/0.14415loss in batch 42: 0.0613861/0.142227loss in batch 43: 0.18483/0.143188loss in batch 44: 0.01297/0.140289loss in batch 45: 0.312851/0.144058loss in batch 46: 0.153915/0.144272loss in batch 47: 0.447845/0.150574loss in batch 48: 0.0262299/0.148041loss in batch 49: 0.0214233/0.145508loss in batch 50: 0.0196838/0.143051loss in batch 51: 0.000259399/0.140305loss in batch 52: 0.040741/0.138428loss in batch 53: 0.427292/0.143768loss in batch 54: 0.0364532/0.141815loss in batch 55: 0.0752563/0.140625loss in batch 56: 0.0193634/0.138504loss in batch 57: 0.392776/0.142899loss in batch 58: 0.0964661/0.142105loss in batch 59: 0.0523682/0.14061loss in batch 60: 0.0321808/0.138824loss in batch 61: 0.122772/0.138565loss in batch 62: 0.380829/0.14241loss in batch 63: 0.00424194/0.140259loss in batch 64: 0.000854492/0.138107loss in batch 65: 0.0370941/0.136581loss in batch 66: 0.0240631/0.134903loss in batch 67: 0.0387573/0.133484loss in batch 68: 0.074646/0.132629loss in batch 69: 0.0423279/0.131348loss in batch 70: 0.0362549/0.130005loss in batch 71: 0.0632324/0.129074loss in batch 72: 0.310333/0.131561loss in batch 73: 0.0631866/0.13063loss in batch 74: 0.0294952/0.129288loss in batch 75: 0.225876/0.130554loss in batch 76: 0.868011/0.140137loss in batch 77: 0.00344849/0.138382loss in batch 78: 0.192215/0.139069loss in batch 79: 0.0543976/0.138loss in batch 80: 0.0571747/0.137009loss in batch 81: 0.0384216/0.135803loss in batch 82: 0.564468/0.140976loss in batch 83: 0.0493317/0.139877loss in batch 84: 0.051651/0.13884loss in batch 85: 0.0423126/0.137726loss in batch 86: 0.443375/0.141235loss in batch 87: 0.430832/0.144516loss in batch 88: 0.0233765/0.143173loss in batch 89: 0.0138702/0.141724loss in batch 90: 0.118103/0.141464loss in batch 91: 0.142761/0.141495loss in batch 92: 0.0604095/0.14061loss in batch 93: 0.142136/0.14064loss in batch 94: 0.0192108/0.139343loss in batch 95: 0.0768585/0.138702loss in batch 96: 0.0221558/0.137497loss in batch 97: 0.0125885/0.13623loss in batch 98: 0.347/0.138336loss in batch 99: 0.135971/0.138321loss in batch 100: 0.0328064/0.137283loss in batch 101: 0.0332794/0.136261loss in batch 102: 0.00682068/0.13501loss in batch 103: 0.054306/0.134232loss in batch 104: 0.0220795/0.133163loss in batch 105: 0.162857/0.133438loss in batch 106: 0.00759888/0.132263loss in batch 107: 0.127716/0.132233loss in batch 108: 0.101822/0.131958loss in batch 109: 0.159164/0.132187loss in batch 110: 0.0581665/0.131531loss in batch 111: 0.12738/0.131485loss in batch 112: 0.0817566/0.131042loss in batch 113: 0.38269/0.133255loss in batch 114: 0.025116/0.132309loss in batch 115: 0.352829/0.134216loss in batch 116: 0.0650482/0.133621loss in batch 117: 0.135513/0.133636loss in batch 118: 0.432632/0.136154loss in batch 119: 0.0339966/0.1353loss in batch 120: 0.0115356/0.134277loss in batch 121: 0.141068/0.134338loss in batch 122: 0.244217/0.135239loss in batch 123: 0.0733185/0.134735loss in batch 124: 0.0871124/0.134354loss in batch 125: 0.20723/0.134918loss in batch 126: 0.425201/0.137222loss in batch 127: 0.0892639/0.136826loss in batch 128: 0.27948/0.137955loss in batch 129: 0.11525/0.137772loss in batch 130: 0.0267944/0.136917loss in batch 131: 0.785309/0.14183loss in batch 132: 0.0183258/0.1409loss in batch 133: 0.0101166/0.139923loss in batch 134: 0.0172577/0.139008loss in batch 135: 0.524963/0.141861loss in batch 136: 0.0650787/0.141296loss in batch 137: 0.187561/0.141632loss in batch 138: 1.58107/0.151993loss in batch 139: 0.0693054/0.151398loss in batch 140: 0.0557556/0.150726loss in batch 141: 0.366211/0.152237loss in batch 142: 0.0751953/0.151688loss in batch 143: 0.285126/0.152618loss in batch 144: 0.0419617/0.151855loss in batch 145: 0.201813/0.152191loss in batch 146: 0.376038/0.153732loss in batch 147: 0.105408/0.153397loss in batch 148: 0.201126/0.153717loss in batch 149: 0.0574799/0.153076loss in batch 150: 0.0132294/0.152145loss in batch 151: 0.119949/0.151947loss in batch 152: 0.074173/0.151428loss in batch 153: 0.196304/0.151718loss in batch 154: 0.173492/0.151855loss in batch 155: 0.0934448/0.151489loss in batch 156: 0.508881/0.153763loss in batch 157: 0.0870514/0.153351loss in batch 158: 0.161987/0.153397loss in batch 159: 0.207489/0.153732loss in batch 160: 0.0738983/0.153229loss in batch 161: 0.0898285/0.152847loss in batch 162: 0.00392151/0.151947loss in batch 163: 0.0390015/0.151245loss in batch 164: 0.0208435/0.150467loss in batch 165: 0.269363/0.151169loss in batch 166: 0.0184937/0.150375loss in batch 167: 0.113297/0.150146loss in batch 168: 0.202637/0.150467loss in batch 169: 0.43959/0.152161loss in batch 170: 0.00567627/0.151321loss in batch 171: 0.275284/0.152039loss in batch 172: 0.0071106/0.151184loss in batch 173: 0.0207062/0.150452loss in batch 174: 0.288345/0.15123loss in batch 175: 0.0350952/0.150574loss in batch 176: 0.194199/0.150818loss in batch 177: 0.0180206/0.15007loss in batch 178: 0.0766449/0.149658loss in batch 179: 0.156387/0.149689loss in batch 180: 0.0622711/0.149216loss in batch 181: 0.0114899/0.148453loss in batch 182: 0.0123749/0.14772loss in batch 183: 0.0157471/0.147003loss in batch 184: 0.449524/0.148636loss in batch 185: 0.0520935/0.148102loss in batch 186: 0.180176/0.148285loss in batch 187: 0.236725/0.148758loss in batch 188: 0.0333862/0.148148loss in batch 189: 0.09729/0.147873loss in batch 190: 0.371826/0.149048loss in batch 191: 1.83853/0.157852loss in batch 192: 0.121689/0.157669loss in batch 193: 0.343185/0.158615loss in batch 194: 0.617844/0.16098loss in batch 195: 0.325546/0.161819loss in batch 196: 0.0320435/0.161148loss in batch 197: 0.0236969/0.160461loss in batch 198: 0.0797577/0.160049loss in batch 199: 0.0328064/0.159424loss in batch 200: 0.0460815/0.158859loss in batch 201: 0.33786/0.159744loss in batch 202: 0.193893/0.159912loss in batch 203: 0.133499/0.159775loss in batch 204: 0.0496674/0.159241loss in batch 205: 0.00611877/0.158493loss in batch 206: 0.314606/0.159256loss in batch 207: 0.1642/0.159286loss in batch 208: 0.261917/0.159775loss in batch 209: 0.145554/0.159714loss in batch 210: 0.156677/0.159683loss in batch 211: 0.214157/0.159943loss in batch 212: 0.133102/0.159821
done with epoch 17
train_acc: 0.948357 (404/426)
test loss: 0.133102
acc: 0.937061 (134/143)
loss in batch 0: 0.0817413/0.0817413loss in batch 1: 0.163315/0.122528loss in batch 2: 0.383942/0.209656loss in batch 3: 0.385468/0.253616loss in batch 4: 0.0413513/0.211166loss in batch 5: 0.0817413/0.18959loss in batch 6: 0.470459/0.229706loss in batch 7: 0.0717621/0.209976loss in batch 8: 0.308014/0.220871loss in batch 9: 0.269318/0.225708loss in batch 10: 0.0807343/0.21254loss in batch 11: 0.509003/0.237244loss in batch 12: 0.201248/0.234467loss in batch 13: 0.120712/0.226334loss in batch 14: 0.0224152/0.212738loss in batch 15: 0.0310211/0.201385loss in batch 16: 0.605606/0.225159loss in batch 17: 0.0111389/0.213272loss in batch 18: 0.0571442/0.205063loss in batch 19: 0.028183/0.196213loss in batch 20: 0.0380707/0.18869loss in batch 21: 0.0227051/0.181152loss in batch 22: 0.0644989/0.176071loss in batch 23: 0.115723/0.173553loss in batch 24: 0.0112152/0.167068loss in batch 25: 0.21373/0.168854loss in batch 26: 0.200851/0.170044loss in batch 27: 0.0772552/0.166733loss in batch 28: 0.31105/0.171707loss in batch 29: 0.0417328/0.167374loss in batch 30: 0.674026/0.183716loss in batch 31: 0.0119934/0.17836loss in batch 32: 0.0331421/0.17395loss in batch 33: 0.0297852/0.169708loss in batch 34: 0.168243/0.169678loss in batch 35: 0.0391541/0.166046loss in batch 36: 0.00532532/0.161697loss in batch 37: 0.0507507/0.158783loss in batch 38: 0.347229/0.16362loss in batch 39: 0.085495/0.161667loss in batch 40: 0.00410461/0.157822loss in batch 41: 0.398056/0.163544loss in batch 42: 0.386383/0.168716loss in batch 43: 0.0116425/0.165146loss in batch 44: 0.00747681/0.161652loss in batch 45: 0.0246124/0.158661loss in batch 46: 0.128601/0.15802loss in batch 47: 0.0603485/0.155975loss in batch 48: 0.227539/0.157455loss in batch 49: 0.0452271/0.155212loss in batch 50: 0.139969/0.154907loss in batch 51: 0.0321808/0.152542loss in batch 52: 0.0815582/0.151199loss in batch 53: 0.155807/0.151291loss in batch 54: 0.409409/0.155991loss in batch 55: 0.0207062/0.153564loss in batch 56: 0.0816956/0.152313loss in batch 57: 0.136612/0.152039loss in batch 58: 0.0198212/0.149796loss in batch 59: 0.191589/0.150482loss in batch 60: 0.00154114/0.148041loss in batch 61: 0.0531769/0.146515loss in batch 62: 0.138855/0.146393loss in batch 63: 0.0457916/0.144821loss in batch 64: 0.0196533/0.142899loss in batch 65: 0.0574036/0.141602loss in batch 66: 0.225311/0.142853loss in batch 67: 0.0575867/0.141602loss in batch 68: 0.296021/0.143845loss in batch 69: 0.00743103/0.141891loss in batch 70: 1.89323/0.16655loss in batch 71: 0.144836/0.16626loss in batch 72: 0.0533752/0.164703loss in batch 73: 0.0801849/0.163574loss in batch 74: 0.0507507/0.162064loss in batch 75: 0.0779266/0.160965loss in batch 76: 0.3797/0.163788loss in batch 77: 0.0850372/0.162796loss in batch 78: 0.0198517/0.16098loss in batch 79: 0.0166168/0.15918loss in batch 80: 0.0460358/0.157776loss in batch 81: 0.0299377/0.156219loss in batch 82: 0.0767365/0.155273loss in batch 83: 0.100571/0.154602loss in batch 84: 0.00856018/0.152893loss in batch 85: 0.0190582/0.151337loss in batch 86: 0.0085907/0.149704loss in batch 87: 0.157761/0.149796loss in batch 88: 0.00474548/0.148163loss in batch 89: 0.365143/0.150558loss in batch 90: 0.0569/0.149536loss in batch 91: 0.0578003/0.148544loss in batch 92: 0.101532/0.148026loss in batch 93: 0.0663605/0.147171loss in batch 94: 0.39621/0.14978loss in batch 95: 0.0377808/0.148636loss in batch 96: 0.00160217/0.14711loss in batch 97: 0.0500031/0.146118loss in batch 98: 0.00897217/0.14473loss in batch 99: 0.0212402/0.143494loss in batch 100: 0.0648346/0.142715loss in batch 101: 0.227631/0.143539loss in batch 102: 0.0574951/0.142715loss in batch 103: 0.132233/0.142609loss in batch 104: 0.271866/0.143845loss in batch 105: 0.0030365/0.142517loss in batch 106: 0.0505066/0.141647loss in batch 107: 0.0178223/0.140503loss in batch 108: 0.150482/0.14061loss in batch 109: 0.0488586/0.139771loss in batch 110: 0.693787/0.14476loss in batch 111: 0.0236359/0.143677loss in batch 112: 0.0935822/0.143234loss in batch 113: 0.0597687/0.142502loss in batch 114: 0.0410461/0.141617loss in batch 115: 0.255508/0.142593loss in batch 116: 0.0344849/0.141678loss in batch 117: 0.00816345/0.140533loss in batch 118: 0.0483246/0.139771loss in batch 119: 0.0200806/0.138763loss in batch 120: 0.0261993/0.137833loss in batch 121: 0.04422/0.13707loss in batch 122: 0.0100555/0.136047loss in batch 123: 0.139389/0.136078loss in batch 124: 0.0245209/0.135178loss in batch 125: 0.0948639/0.134857loss in batch 126: 1.31482/0.14415loss in batch 127: 0.014389/0.143127loss in batch 128: 0.12233/0.142975loss in batch 129: 0.0738373/0.142441loss in batch 130: 0.00621033/0.141403loss in batch 131: 0.088623/0.141006loss in batch 132: 0.655228/0.144867loss in batch 133: 0.409653/0.146835loss in batch 134: 0.246674/0.147583loss in batch 135: 0.0951233/0.147202loss in batch 136: 1.99155/0.16066loss in batch 137: 0.0677948/0.159988loss in batch 138: 0.181351/0.160141loss in batch 139: 0.0900879/0.159653loss in batch 140: 0.0552063/0.158905loss in batch 141: 0.0485535/0.158127loss in batch 142: 0.0203247/0.157166loss in batch 143: 0.052002/0.156433loss in batch 144: 0.401428/0.158112loss in batch 145: 0.0179596/0.15715loss in batch 146: 0.0235138/0.15625loss in batch 147: 0.0500946/0.155533loss in batch 148: 0.0506897/0.154816loss in batch 149: 0.291916/0.155731loss in batch 150: 0.32225/0.156845loss in batch 151: 0.0194702/0.155945loss in batch 152: 0.0845184/0.155472loss in batch 153: 0.0257568/0.154633loss in batch 154: 0.129242/0.154465loss in batch 155: 0.0153351/0.153564loss in batch 156: 0.00738525/0.152634loss in batch 157: 0.196991/0.152924loss in batch 158: 0.0609741/0.152344loss in batch 159: 0.22226/0.152786loss in batch 160: 0.0718536/0.152283loss in batch 161: 0.0867767/0.151871loss in batch 162: 0.27977/0.152664loss in batch 163: 0.102539/0.152359loss in batch 164: 0.0967865/0.152023loss in batch 165: 0.00907898/0.151154loss in batch 166: 0.0261078/0.150406loss in batch 167: 0.0243683/0.149658loss in batch 168: 0.103714/0.149384loss in batch 169: 0.439011/0.151093loss in batch 170: 0.249802/0.151672loss in batch 171: 0.267593/0.152344loss in batch 172: 0.286972/0.153122loss in batch 173: 0.0302582/0.15242loss in batch 174: 0.171783/0.152527loss in batch 175: 0.482697/0.154404loss in batch 176: 0.228928/0.154816loss in batch 177: 0.121704/0.154633loss in batch 178: 0.012085/0.153839loss in batch 179: 0.0870209/0.153458loss in batch 180: 0.383331/0.154739loss in batch 181: 0.428528/0.156235loss in batch 182: 0.384888/0.157486loss in batch 183: 0.0179138/0.156738loss in batch 184: 0.141129/0.156647loss in batch 185: 0.0353088/0.155991loss in batch 186: 0.0339355/0.155334loss in batch 187: 0.0804901/0.154938loss in batch 188: 0.175781/0.15506loss in batch 189: 0.0922241/0.154724loss in batch 190: 0.0302429/0.154083loss in batch 191: 0.142609/0.154022loss in batch 192: 0.148148/0.153992loss in batch 193: 0.180298/0.154114loss in batch 194: 0.281967/0.15477loss in batch 195: 0.0608826/0.154297loss in batch 196: 0.142578/0.154236loss in batch 197: 0.377655/0.155365loss in batch 198: 0.471695/0.156952loss in batch 199: 0.104858/0.156693loss in batch 200: 0.0694885/0.156265loss in batch 201: 0.078064/0.155884loss in batch 202: 0.646484/0.158295loss in batch 203: 0.0328674/0.157669loss in batch 204: 0.0326385/0.157059loss in batch 205: 0.0154266/0.156372loss in batch 206: 0.243134/0.156784loss in batch 207: 0.540283/0.158646loss in batch 208: 0.0144806/0.157959loss in batch 209: 0.274033/0.158508loss in batch 210: 0.0890503/0.158173loss in batch 211: 0.0143433/0.157486loss in batch 212: 0.010498/0.156815
done with epoch 18
train_acc: 0.948357 (404/426)
test loss: 0.010498
acc: 0.951047 (136/143)
loss in batch 0: 0.0683746/0.0683746loss in batch 1: 0.03479/0.0515747loss in batch 2: 0.0283356/0.0438385loss in batch 3: 0.104446/0.0589905loss in batch 4: 0.0797424/0.0631409loss in batch 5: 0.0501556/0.0609741loss in batch 6: 0.00999451/0.0536957loss in batch 7: 0.0996246/0.059433loss in batch 8: 0.35228/0.09198loss in batch 9: 0.110275/0.093811loss in batch 10: 0.0177307/0.0868835loss in batch 11: 0.00720215/0.080246loss in batch 12: 0.0451202/0.0775452loss in batch 13: 0.342575/0.0964661loss in batch 14: 0.00567627/0.0904236loss in batch 15: 0.0863342/0.0901642loss in batch 16: 0.00547791/0.0851898loss in batch 17: 0.0533752/0.0834045loss in batch 18: 0.634705/0.112442loss in batch 19: 0.352356/0.124435loss in batch 20: 0.2771/0.131699loss in batch 21: 0.302414/0.139465loss in batch 22: 0.336853/0.148041loss in batch 23: 0.0351257/0.143326loss in batch 24: 1.56853/0.200348loss in batch 25: 0.43602/0.209396loss in batch 26: 0.0441589/0.203278loss in batch 27: 0.0205383/0.196762loss in batch 28: 0.0118103/0.190384loss in batch 29: 0.0250397/0.184875loss in batch 30: 0.0812836/0.181519loss in batch 31: 0.00856018/0.176132loss in batch 32: 0.0178375/0.171326loss in batch 33: 0.0526581/0.167831loss in batch 34: 0.0661621/0.164932loss in batch 35: 0.0951843/0.162994loss in batch 36: 0.0941467/0.161133loss in batch 37: 0.196213/0.162064loss in batch 38: 0.0339661/0.158783loss in batch 39: 0.707901/0.172501loss in batch 40: 0.560196/0.181961loss in batch 41: 0.037384/0.178513loss in batch 42: 0.0136719/0.174683loss in batch 43: 0.0764465/0.17244loss in batch 44: 0.200577/0.17308loss in batch 45: 0.0289459/0.169937loss in batch 46: 0.421844/0.175293loss in batch 47: 0.0832825/0.173386loss in batch 48: 0.15094/0.172928loss in batch 49: 0.106934/0.1716loss in batch 50: 0.0360718/0.168945loss in batch 51: 0.0361481/0.166397loss in batch 52: 0.0795746/0.164764loss in batch 53: 0.0501556/0.162643loss in batch 54: 0.0677948/0.160904loss in batch 55: 0.280151/0.16304loss in batch 56: 0.0853577/0.161682loss in batch 57: 0.0731812/0.160156loss in batch 58: 0.128128/0.159607loss in batch 59: 0.0404053/0.157623loss in batch 60: 0.490372/0.163086loss in batch 61: 0.120575/0.162384loss in batch 62: 0.0317993/0.160324loss in batch 63: 0.0348053/0.158356loss in batch 64: 0.0477448/0.156662loss in batch 65: 0.133926/0.156311loss in batch 66: 0.0581512/0.154846loss in batch 67: 0.180328/0.155212loss in batch 68: 0.0900269/0.154266loss in batch 69: 0.430954/0.158234loss in batch 70: 0.249878/0.159515loss in batch 71: 0.0233917/0.157639loss in batch 72: 0.491974/0.162216loss in batch 73: 0.00735474/0.16011loss in batch 74: 0.0865631/0.159134loss in batch 75: 0.416092/0.162521loss in batch 76: 0.0190277/0.160645loss in batch 77: 0.0620117/0.159393loss in batch 78: 0.00666809/0.157455loss in batch 79: 0.0416107/0.156006loss in batch 80: 0.0884705/0.155167loss in batch 81: 0.17627/0.155426loss in batch 82: 0.260391/0.156693loss in batch 83: 0.038269/0.155273loss in batch 84: 0.0214844/0.153702loss in batch 85: 0.0260162/0.152237loss in batch 86: 0.343567/0.154434loss in batch 87: 0.212219/0.15509loss in batch 88: 0.157684/0.155106loss in batch 89: 0.0214081/0.153625loss in batch 90: 0.011261/0.152054loss in batch 91: 0.368042/0.154404loss in batch 92: 0.0468903/0.153259loss in batch 93: 0.0385132/0.152039loss in batch 94: 0.171082/0.152222loss in batch 95: 0.0328369/0.151001loss in batch 96: 0.350235/0.15303loss in batch 97: 0.0200653/0.151688loss in batch 98: 0.0156097/0.150314loss in batch 99: 0.0727539/0.149536loss in batch 100: 0.0685272/0.148727loss in batch 101: 0.0354462/0.147629loss in batch 102: 0.0749359/0.146912loss in batch 103: 0.018158/0.145676loss in batch 104: 0.109039/0.145325loss in batch 105: 0.0174103/0.144119loss in batch 106: 0.0310516/0.143066loss in batch 107: 0.0727692/0.142426loss in batch 108: 0.00360107/0.141144loss in batch 109: 0.0119019/0.139969loss in batch 110: 0.39653/0.142288loss in batch 111: 0.681763/0.147095loss in batch 112: 0.0177612/0.14595loss in batch 113: 0.575745/0.149719loss in batch 114: 0.0817719/0.149124loss in batch 115: 0.0527649/0.1483loss in batch 116: 0.0304413/0.147293loss in batch 117: 0.148254/0.147308loss in batch 118: 0.0524292/0.146515loss in batch 119: 0.0494385/0.145691loss in batch 120: 0.0932007/0.145264loss in batch 121: 0.00839233/0.144135loss in batch 122: 0.370941/0.145996loss in batch 123: 0.135971/0.145889loss in batch 124: 0.0102844/0.144821loss in batch 125: 0.0463562/0.144028loss in batch 126: 0.121597/0.14386loss in batch 127: 0.00997925/0.142807loss in batch 128: 0.104858/0.142532loss in batch 129: 0.0484161/0.141785loss in batch 130: 0.45723/0.144196loss in batch 131: 0.0730743/0.143661loss in batch 132: 0.0267792/0.142776loss in batch 133: 0.257645/0.143631loss in batch 134: 0.0200806/0.142731loss in batch 135: 0.0393982/0.141968loss in batch 136: 0.0231781/0.141098loss in batch 137: 0.17572/0.141342loss in batch 138: 0.562149/0.144379loss in batch 139: 0.0508575/0.143707loss in batch 140: 0.0858917/0.143295loss in batch 141: 0.438995/0.145386loss in batch 142: 0.0929413/0.14502loss in batch 143: 0.037796/0.144272loss in batch 144: 0.0574341/0.143661loss in batch 145: 0.151276/0.143723loss in batch 146: 0.489609/0.146088loss in batch 147: 0.218796/0.146576loss in batch 148: 0.229477/0.147125loss in batch 149: 0.0787811/0.146667loss in batch 150: 0.0641785/0.146118loss in batch 151: 0.0280457/0.14534loss in batch 152: 0.0137482/0.144501loss in batch 153: 0.0505371/0.143875loss in batch 154: 0.0689087/0.143387loss in batch 155: 0.585022/0.146225loss in batch 156: 0.276688/0.147064loss in batch 157: 0.163162/0.147156loss in batch 158: 0.429672/0.148926loss in batch 159: 0.222305/0.149384loss in batch 160: 0.068222/0.14888loss in batch 161: 0.00379944/0.147995loss in batch 162: 0.00804138/0.147125loss in batch 163: 0.264938/0.147858loss in batch 164: 0.145813/0.147842loss in batch 165: 0.145752/0.147812loss in batch 166: 0.0241394/0.147095loss in batch 167: 0.00854492/0.146255loss in batch 168: 0.0358887/0.145599loss in batch 169: 0.172195/0.145767loss in batch 170: 0.0228577/0.14505loss in batch 171: 0.556473/0.147446loss in batch 172: 0.0103607/0.146652loss in batch 173: 1.21672/0.152802loss in batch 174: 0.0314941/0.1521loss in batch 175: 0.130386/0.151978loss in batch 176: 0.0906067/0.151627loss in batch 177: 0.188812/0.15184loss in batch 178: 0.0254211/0.151123loss in batch 179: 0.0275421/0.150436loss in batch 180: 0.152649/0.150452loss in batch 181: 1.05298/0.155411loss in batch 182: 0.0342102/0.154755loss in batch 183: 0.152863/0.154755loss in batch 184: 0.0297241/0.154068loss in batch 185: 0.0296478/0.153412loss in batch 186: 0.0057373/0.152603loss in batch 187: 0.0320282/0.151978loss in batch 188: 0.0202026/0.15126loss in batch 189: 0.0412598/0.150681loss in batch 190: 0.00718689/0.149933loss in batch 191: 0.117615/0.14978loss in batch 192: 0.106735/0.149551loss in batch 193: 0.203156/0.149826loss in batch 194: 0.0163269/0.149139loss in batch 195: 0.0689392/0.148743loss in batch 196: 0.0341949/0.148163loss in batch 197: 0.151443/0.148163loss in batch 198: 0.0302887/0.147583loss in batch 199: 0.00857544/0.146881loss in batch 200: 0.0941925/0.146606loss in batch 201: 0.243378/0.147095loss in batch 202: 0.188599/0.147308loss in batch 203: 0.046814/0.14682loss in batch 204: 0.179276/0.146973loss in batch 205: 0.0165253/0.146332loss in batch 206: 0.040451/0.145828loss in batch 207: 0.0442505/0.145325loss in batch 208: 0.716568/0.148071loss in batch 209: 0.0380096/0.147537loss in batch 210: 1.37953/0.153381loss in batch 211: 0.0445709/0.152863loss in batch 212: 0.0154877/0.152222
done with epoch 19
train_acc: 0.948357 (404/426)
test loss: 0.0154876
acc: 0.944054 (135/143)
[0.15686, -0.0595398, -0.0624084, -0.105606, -0.0192261, -0.0316772, -0.0918579, -0.0142365, -0.00100708, -0.00260925, -0.275696, -0.100494, -0.0057373, -0.296661, -0.403961, 0.07724, -0.0463409, 0.0126495, 0.0468903, 0.000930786, 0.435638, 0.0775146, -0.166077, -0.0305786, 0.0128326, -0.0358276, -0.00923157, 0.088562, -0.0455627, 0.0219269, -0.00982666, 0.0434265, -0.342545, 0.270691, -0.00138855, 0.277802, -0.099823, 0.135178, -0.0953369, 0.269882, 0.823257, -0.00662231, 0.639206, -0.00419617, -0.394333, 0.000961304, -0.000991821, -0.0607452, -0.0122528, 0.102966, 0.0578308, 0.147247, 0.072403, -0.0396729, -0.0506134, -0.015625, -0.0141907, -0.00700378, -0.0447693, 0.000930786, 0.383408, 0.182755, -0.0101166, -0.0306549, 0.0227051, -0.03685, 0, 0.0135193, 0.00804138, -0.00752258, -0.383041, 0.031662, -0.0411835, 0.529144, 0.0045166, -0.0473022, -0.00732422, -0.157028, -0.0135956, -0.000946045, 0.218765, 0.000656128, 0.0303955, -0.00979614, 0.0955353, -0.0414734, -0.0160675, -0.00256348, 0.0542297, 9.15527e-05, -0.0052948, 0.568802, -0.412186, 0.00524902, -0.00393677, -0.0382233, 0.0163269, -0.516907, -0.0300446, -0.0156708, -0.00105286, -0.0609589, -0.0684052, -0.165009, 0.056839, -0.00737, 0.0043335, -0.129456, 0.700638, 0.702835, -0.0389404, 0.00743103, 0.0505829, -0.254364, -0.0117035, -0.0323486, -0.307556, -0.0139771, -0.0429993, -0.114197, -0.0268097, -0.0243683, 0.261032, -0.096817, 0.00439453, -0.141937, -0.22464, -0.0242615, -0.0133362, -0.00366211, 0.135437, -0.00311279, -0.0113678, -0.213852, -0.137054, -0.00907898, -0.216278, 0.576141, 0.00012207, -0.0068512, -0.0786285, -0.029541, 0.000808716]
Compiler: ./compile.py -R 64 breast_logistic
	899 triples of Z2^64 left
	53 dabits of Z2^64 left
	360 triples of Z2^64 left
	212 dabits of Z2^64 left
2 threads spent a total of 912.465 seconds (376.243 MB, 3110106 rounds) on the online phase, 935.161 seconds (24042.1 MB, 502175 rounds) on the preprocessing/offline phase, and 1855.59 seconds idling.
Join timer: 0 1.85156e+06
Finish timer: 0.14554
Join timer: 1 1.83212e+06
Finish timer: 0.14554
Communication details (rounds in parallel threads counted double):
Exchanging one-to-one 11384.9 MB in 66717 rounds, taking 32.1458 seconds
Receiving directly 376.243 MB in 1555053 rounds, taking 248.805 seconds
Receiving one-to-one 14239.5 MB in 217729 rounds, taking 30.3677 seconds
Sending directly 376.243 MB in 1555053 rounds, taking 104.358 seconds
Sending one-to-one 12657.3 MB in 217729 rounds, taking 9.37269 seconds
CPU time = 1833.89 (overall core time)
The following benchmarks are including preprocessing (offline phase).
Time = 1851.7 seconds 
Data sent = 24418.4 MB in ~3612281 rounds (party 0 only)
Global data sent = 50419 MB (all parties)
Actual cost of program:
  Type int
      22233741        Triples
     131855095           Bits
This program might benefit from some protocol options.
Consider adding the following at the beginning of your code:
	program.use_trunc_pr = True
	program.use_split(2)
This virtual machine was compiled with GCC. Recompile with 'CXX = clang++' in 'CONFIG.mine' for optimal performance.
Command line: semi2k-party.x -N 2 -e --ip-file-name /HOST -p 0 -v breast_logistic
