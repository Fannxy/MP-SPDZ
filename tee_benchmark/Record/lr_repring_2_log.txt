Using statistical security parameter 40
Trying to run 64-bit computation
Current working directory: "/"
Current working directory: "/"
Using SGD
done with epoch 0done with epoch 1done with epoch 2done with epoch 3done with epoch 4done with epoch 5done with epoch 6done with epoch 7done with epoch 8done with epoch 9done with epoch 10done with epoch 11done with epoch 12done with epoch 13done with epoch 14done with epoch 15done with epoch 16done with epoch 17done with epoch 18done with epoch 19
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
Using SGD
loss in batch 0: 0.773544/0.773544loss in batch 1: 0.874435/0.82399loss in batch 2: 0.750671/0.799561loss in batch 3: 0.833237/0.807968loss in batch 4: 0.748077/0.79599loss in batch 5: 0.706284/0.781036loss in batch 6: 0.663666/0.764282loss in batch 7: 0.864395/0.776779loss in batch 8: 0.729828/0.771561loss in batch 9: 0.559967/0.750412loss in batch 10: 0.681656/0.744171loss in batch 11: 0.726898/0.742722loss in batch 12: 0.786667/0.746094loss in batch 13: 0.655823/0.739639loss in batch 14: 0.687744/0.736191loss in batch 15: 0.69899/0.733871loss in batch 16: 0.485443/0.719254loss in batch 17: 0.705338/0.718475loss in batch 18: 0.64064/0.714386loss in batch 19: 0.684036/0.71286loss in batch 20: 0.660431/0.710358loss in batch 21: 0.825821/0.715607loss in batch 22: 0.64032/0.712341loss in batch 23: 0.54274/0.705276loss in batch 24: 1.14169/0.722733loss in batch 25: 0.750565/0.723816loss in batch 26: 0.613327/0.719711loss in batch 27: 0.682419/0.718369loss in batch 28: 0.850739/0.722946loss in batch 29: 0.550659/0.717194loss in batch 30: 0.699905/0.716644loss in batch 31: 0.691574/0.715866loss in batch 32: 0.580933/0.711777loss in batch 33: 0.424774/0.703339loss in batch 34: 0.717133/0.70372loss in batch 35: 0.665863/0.702667loss in batch 36: 0.519653/0.697723loss in batch 37: 0.7686/0.6996loss in batch 38: 0.608414/0.69725loss in batch 39: 0.59082/0.694595loss in batch 40: 0.671967/0.694031loss in batch 41: 0.564682/0.690964loss in batch 42: 0.579956/0.688385loss in batch 43: 0.682068/0.688232loss in batch 44: 0.567047/0.685547loss in batch 45: 0.645981/0.684677loss in batch 46: 0.634674/0.683609loss in batch 47: 0.682037/0.683594loss in batch 48: 0.506912/0.679993loss in batch 49: 0.444229/0.675262loss in batch 50: 0.553436/0.672882loss in batch 51: 0.459106/0.668777loss in batch 52: 0.423599/0.664139loss in batch 53: 0.31871/0.65773loss in batch 54: 0.653076/0.657654loss in batch 55: 0.402725/0.653091loss in batch 56: 1.03485/0.65979loss in batch 57: 0.759445/0.66153loss in batch 58: 0.665756/0.661591loss in batch 59: 0.31076/0.655731loss in batch 60: 1.08989/0.662872loss in batch 61: 0.680878/0.663147loss in batch 62: 0.267273/0.656876loss in batch 63: 0.575165/0.655594loss in batch 64: 0.636337/0.655289loss in batch 65: 0.301147/0.649933loss in batch 66: 0.501312/0.647705loss in batch 67: 0.847351/0.65065loss in batch 68: 0.428436/0.64743loss in batch 69: 0.696686/0.648132loss in batch 70: 0.355988/0.644012loss in batch 71: 0.603958/0.643463loss in batch 72: 0.577301/0.642548loss in batch 73: 0.589783/0.641846loss in batch 74: 0.490631/0.639832loss in batch 75: 0.641205/0.639832loss in batch 76: 0.40419/0.63678loss in batch 77: 0.532562/0.635437loss in batch 78: 0.650162/0.635635loss in batch 79: 0.366837/0.632278loss in batch 80: 0.64505/0.632431loss in batch 81: 0.472824/0.630478loss in batch 82: 0.50235/0.628937loss in batch 83: 0.795715/0.63092loss in batch 84: 0.472046/0.629059loss in batch 85: 0.701141/0.629898loss in batch 86: 0.540024/0.62886loss in batch 87: 0.471436/0.627075loss in batch 88: 0.609528/0.626877loss in batch 89: 0.619598/0.626801loss in batch 90: 0.564972/0.626114loss in batch 91: 0.411163/0.623779loss in batch 92: 0.563324/0.623138loss in batch 93: 0.628571/0.623184loss in batch 94: 0.519943/0.622101loss in batch 95: 0.511185/0.620941loss in batch 96: 0.572571/0.620453loss in batch 97: 1.02908/0.624619loss in batch 98: 0.344513/0.62178loss in batch 99: 0.550964/0.621078loss in batch 100: 1.01369/0.624969loss in batch 101: 0.51886/0.623917loss in batch 102: 0.45816/0.622314loss in batch 103: 0.387558/0.620056loss in batch 104: 0.408173/0.618042loss in batch 105: 0.545227/0.617355loss in batch 106: 0.399078/0.615311loss in batch 107: 0.380249/0.613144loss in batch 108: 0.601791/0.613037loss in batch 109: 0.359558/0.610718loss in batch 110: 0.876846/0.613129loss in batch 111: 0.554306/0.61261loss in batch 112: 0.332733/0.610123loss in batch 113: 0.863846/0.61235loss in batch 114: 0.47876/0.611191loss in batch 115: 0.618622/0.611252loss in batch 116: 0.621841/0.611343loss in batch 117: 0.538422/0.610718loss in batch 118: 0.548691/0.610199loss in batch 119: 0.370728/0.608215loss in batch 120: 0.688171/0.608871loss in batch 121: 0.568314/0.608536loss in batch 122: 0.379898/0.606689loss in batch 123: 0.513565/0.605927loss in batch 124: 0.449371/0.604675loss in batch 125: 0.491516/0.603775loss in batch 126: 0.527267/0.603165loss in batch 127: 0.487656/0.602264loss in batch 128: 0.276474/0.599747loss in batch 129: 0.360855/0.597916loss in batch 130: 0.413666/0.596497loss in batch 131: 0.65387/0.596939loss in batch 132: 0.552277/0.596603loss in batch 133: 0.424225/0.595322loss in batch 134: 0.736343/0.596359loss in batch 135: 0.633072/0.596619loss in batch 136: 0.542007/0.596237loss in batch 137: 0.310608/0.594162loss in batch 138: 0.365601/0.592514loss in batch 139: 0.63768/0.592834loss in batch 140: 0.312775/0.590851loss in batch 141: 0.453735/0.58989loss in batch 142: 0.340729/0.58815loss in batch 143: 0.427139/0.587021loss in batch 144: 0.23912/0.584625loss in batch 145: 0.295792/0.582642loss in batch 146: 0.693085/0.583389loss in batch 147: 0.458511/0.58255loss in batch 148: 0.329468/0.580841loss in batch 149: 0.269257/0.578781loss in batch 150: 0.636612/0.579163loss in batch 151: 0.494919/0.578613loss in batch 152: 0.648972/0.579071loss in batch 153: 0.543884/0.578842loss in batch 154: 0.252045/0.576736loss in batch 155: 0.81955/0.578278loss in batch 156: 0.282501/0.576416loss in batch 157: 0.483994/0.575821loss in batch 158: 0.322113/0.574219loss in batch 159: 0.591568/0.574326loss in batch 160: 0.675095/0.574951loss in batch 161: 0.43779/0.574097loss in batch 162: 0.431427/0.573242loss in batch 163: 0.336121/0.571793loss in batch 164: 0.449036/0.571045loss in batch 165: 0.281982/0.569305loss in batch 166: 0.523788/0.569031loss in batch 167: 0.355225/0.567764loss in batch 168: 0.412125/0.566833loss in batch 169: 0.508331/0.566498loss in batch 170: 0.237228/0.56456loss in batch 171: 0.345886/0.563293loss in batch 172: 0.356216/0.562103loss in batch 173: 0.416702/0.561264loss in batch 174: 0.294052/0.559738loss in batch 175: 0.444443/0.559082loss in batch 176: 0.266449/0.557434loss in batch 177: 0.466415/0.556915loss in batch 178: 0.340668/0.55571loss in batch 179: 0.298813/0.554291loss in batch 180: 0.566513/0.554352loss in batch 181: 0.736115/0.555344loss in batch 182: 0.249405/0.55368loss in batch 183: 0.362549/0.552643loss in batch 184: 0.671829/0.553284loss in batch 185: 0.357742/0.552231loss in batch 186: 0.514801/0.552017loss in batch 187: 0.249908/0.55043loss in batch 188: 0.824478/0.551865loss in batch 189: 0.370636/0.550919loss in batch 190: 0.197586/0.549072loss in batch 191: 0.409119/0.54834loss in batch 192: 0.321136/0.547165loss in batch 193: 0.382141/0.54631loss in batch 194: 0.864883/0.547958loss in batch 195: 0.298477/0.546677loss in batch 196: 0.224487/0.545044loss in batch 197: 0.236572/0.543488loss in batch 198: 0.666855/0.544098loss in batch 199: 0.678482/0.544769loss in batch 200: 0.329422/0.543701loss in batch 201: 0.334564/0.542664loss in batch 202: 0.389267/0.541916loss in batch 203: 0.360809/0.541016loss in batch 204: 0.362747/0.540146loss in batch 205: 0.581253/0.540344loss in batch 206: 0.384842/0.539597loss in batch 207: 0.836151/0.541031loss in batch 208: 0.664063/0.541611loss in batch 209: 0.378754/0.540848loss in batch 210: 0.223511/0.539337loss in batch 211: 0.667343/0.539932loss in batch 212: 0.352371/0.539063
done with epoch 0
train_acc: 0.748826 (319/426)
test loss: 0.35237
acc: 0.860138 (123/143)
loss in batch 0: 0.269073/0.269073loss in batch 1: 0.561127/0.4151loss in batch 2: 0.318695/0.382965loss in batch 3: 0.203506/0.338089loss in batch 4: 0.619751/0.394424loss in batch 5: 0.317963/0.381699loss in batch 6: 0.515381/0.400787loss in batch 7: 0.313553/0.389877loss in batch 8: 0.583435/0.411392loss in batch 9: 0.146591/0.384903loss in batch 10: 0.332535/0.380142loss in batch 11: 0.508575/0.390854loss in batch 12: 0.952377/0.434036loss in batch 13: 0.506119/0.439194loss in batch 14: 0.209885/0.423904loss in batch 15: 0.443466/0.425125loss in batch 16: 0.758057/0.444702loss in batch 17: 0.598694/0.453262loss in batch 18: 0.198502/0.43985loss in batch 19: 0.213684/0.428543loss in batch 20: 0.880875/0.450089loss in batch 21: 0.291382/0.442871loss in batch 22: 0.24472/0.434265loss in batch 23: 0.165527/0.423065loss in batch 24: 0.579422/0.429321loss in batch 25: 0.42424/0.429123loss in batch 26: 0.63205/0.43663loss in batch 27: 0.286804/0.43129loss in batch 28: 0.393509/0.429993loss in batch 29: 0.9953/0.448822loss in batch 30: 0.431808/0.448273loss in batch 31: 0.24617/0.441956loss in batch 32: 0.266663/0.436661loss in batch 33: 0.291641/0.432388loss in batch 34: 0.578766/0.436569loss in batch 35: 0.372467/0.434784loss in batch 36: 0.248032/0.429749loss in batch 37: 0.493835/0.431427loss in batch 38: 0.22287/0.426086loss in batch 39: 0.33551/0.423813loss in batch 40: 0.651733/0.429382loss in batch 41: 0.372864/0.428024loss in batch 42: 0.399536/0.427368loss in batch 43: 0.333328/0.425232loss in batch 44: 0.527481/0.427505loss in batch 45: 0.513382/0.429367loss in batch 46: 0.409042/0.42894loss in batch 47: 0.189117/0.423935loss in batch 48: 0.181061/0.418991loss in batch 49: 0.474457/0.42009loss in batch 50: 0.667175/0.424927loss in batch 51: 0.34967/0.423492loss in batch 52: 0.252533/0.420273loss in batch 53: 0.212158/0.416412loss in batch 54: 0.288712/0.414093loss in batch 55: 0.224335/0.410706loss in batch 56: 0.62529/0.414459loss in batch 57: 0.577118/0.417282loss in batch 58: 0.369049/0.416443loss in batch 59: 0.57103/0.419037loss in batch 60: 0.394821/0.418625loss in batch 61: 0.358444/0.417664loss in batch 62: 0.312408/0.416loss in batch 63: 0.245392/0.413315loss in batch 64: 0.632599/0.416702loss in batch 65: 0.189224/0.413239loss in batch 66: 0.282349/0.411301loss in batch 67: 0.356644/0.410492loss in batch 68: 0.241241/0.408035loss in batch 69: 0.637405/0.411316loss in batch 70: 0.190781/0.408203loss in batch 71: 0.337433/0.407242loss in batch 72: 0.231552/0.404816loss in batch 73: 0.212906/0.402222loss in batch 74: 0.312317/0.401016loss in batch 75: 0.309097/0.399826loss in batch 76: 0.236237/0.397705loss in batch 77: 0.488342/0.398849loss in batch 78: 0.223648/0.396637loss in batch 79: 0.559372/0.398666loss in batch 80: 0.365982/0.39827loss in batch 81: 0.608521/0.400833loss in batch 82: 0.378281/0.400574loss in batch 83: 0.672714/0.403793loss in batch 84: 0.269119/0.402206loss in batch 85: 0.69165/0.405594loss in batch 86: 0.372101/0.405197loss in batch 87: 0.183014/0.402679loss in batch 88: 0.305115/0.401581loss in batch 89: 0.683884/0.404709loss in batch 90: 0.70755/0.408051loss in batch 91: 0.380096/0.40773loss in batch 92: 0.375549/0.407394loss in batch 93: 0.253235/0.405746loss in batch 94: 0.192154/0.403503loss in batch 95: 0.311554/0.402542loss in batch 96: 0.102814/0.399445loss in batch 97: 0.49762/0.400467loss in batch 98: 0.275253/0.3992loss in batch 99: 0.362427/0.398819loss in batch 100: 0.405884/0.398895loss in batch 101: 0.275696/0.39769loss in batch 102: 0.371674/0.39743loss in batch 103: 0.476151/0.398178loss in batch 104: 0.728867/0.401337loss in batch 105: 0.283264/0.400223loss in batch 106: 0.338928/0.399658loss in batch 107: 0.50264/0.400604loss in batch 108: 0.391922/0.400528loss in batch 109: 0.50264/0.401443loss in batch 110: 0.360443/0.401077loss in batch 111: 0.415985/0.401215loss in batch 112: 0.248245/0.399872loss in batch 113: 0.322098/0.399185loss in batch 114: 0.363052/0.398865loss in batch 115: 0.836594/0.402634loss in batch 116: 0.316513/0.401901loss in batch 117: 0.247589/0.400589loss in batch 118: 0.34758/0.400146loss in batch 119: 0.603973/0.40184loss in batch 120: 0.183243/0.40004loss in batch 121: 0.224915/0.398621loss in batch 122: 0.266739/0.397537loss in batch 123: 0.363525/0.397263loss in batch 124: 0.325943/0.396683loss in batch 125: 0.152054/0.394745loss in batch 126: 0.299469/0.394012loss in batch 127: 0.219666/0.392639loss in batch 128: 0.170624/0.39093loss in batch 129: 0.425552/0.391174loss in batch 130: 0.166122/0.389465loss in batch 131: 0.161072/0.387741loss in batch 132: 0.241501/0.386642loss in batch 133: 0.282547/0.385864loss in batch 134: 0.395462/0.385941loss in batch 135: 0.414307/0.386139loss in batch 136: 0.397537/0.386215loss in batch 137: 0.246536/0.385208loss in batch 138: 0.130356/0.383377loss in batch 139: 0.315384/0.382889loss in batch 140: 0.242188/0.381897loss in batch 141: 0.098053/0.379898loss in batch 142: 0.238312/0.378906loss in batch 143: 0.250153/0.378006loss in batch 144: 0.188904/0.376709loss in batch 145: 0.415695/0.376968loss in batch 146: 0.187958/0.375687loss in batch 147: 0.324829/0.375336loss in batch 148: 0.268646/0.374634loss in batch 149: 0.187317/0.373383loss in batch 150: 0.238281/0.372482loss in batch 151: 0.0888672/0.370621loss in batch 152: 0.638016/0.37236loss in batch 153: 0.162537/0.371002loss in batch 154: 0.121582/0.369385loss in batch 155: 0.79216/0.372101loss in batch 156: 0.518082/0.373032loss in batch 157: 0.132385/0.371506loss in batch 158: 0.717392/0.373672loss in batch 159: 0.263763/0.373001loss in batch 160: 0.793823/0.37561loss in batch 161: 0.130249/0.3741loss in batch 162: 0.492218/0.374832loss in batch 163: 0.132233/0.373337loss in batch 164: 0.291718/0.372849loss in batch 165: 0.298157/0.372391loss in batch 166: 0.440277/0.372803loss in batch 167: 0.250244/0.37207loss in batch 168: 0.155273/0.370789loss in batch 169: 0.795135/0.373291loss in batch 170: 0.48819/0.373962loss in batch 171: 0.26442/0.373322loss in batch 172: 0.325439/0.373047loss in batch 173: 0.406677/0.373245loss in batch 174: 0.271942/0.372665loss in batch 175: 0.316086/0.372345loss in batch 176: 0.333206/0.372116loss in batch 177: 0.190536/0.371109loss in batch 178: 0.279556/0.37059loss in batch 179: 0.365189/0.37056loss in batch 180: 0.190979/0.369568loss in batch 181: 0.493347/0.370255loss in batch 182: 0.686813/0.371979loss in batch 183: 0.418472/0.372223loss in batch 184: 0.698685/0.373993loss in batch 185: 0.321152/0.373703loss in batch 186: 0.503525/0.374405loss in batch 187: 0.184601/0.373398loss in batch 188: 0.238312/0.372681loss in batch 189: 0.188919/0.371704loss in batch 190: 0.317474/0.371429loss in batch 191: 0.237442/0.370728loss in batch 192: 0.36615/0.370697loss in batch 193: 0.399414/0.37085loss in batch 194: 0.573318/0.371887loss in batch 195: 0.249405/0.371277loss in batch 196: 0.32663/0.371048loss in batch 197: 0.346512/0.370911loss in batch 198: 0.506088/0.371597loss in batch 199: 0.217072/0.370819loss in batch 200: 0.194977/0.369949loss in batch 201: 0.266678/0.369431loss in batch 202: 0.206924/0.368637loss in batch 203: 0.402863/0.368805loss in batch 204: 0.158737/0.367783loss in batch 205: 0.510178/0.368469loss in batch 206: 0.804794/0.370575loss in batch 207: 0.282516/0.370163loss in batch 208: 0.289703/0.369766loss in batch 209: 0.311066/0.369492loss in batch 210: 0.316513/0.369232loss in batch 211: 0.370621/0.369247loss in batch 212: 0.168503/0.368301
done with epoch 1
train_acc: 0.892019 (380/426)
test loss: 0.168502
acc: 0.916082 (131/143)
loss in batch 0: 0.423859/0.423859loss in batch 1: 0.220612/0.322235loss in batch 2: 0.372818/0.339096loss in batch 3: 0.339493/0.339188loss in batch 4: 0.815308/0.434418loss in batch 5: 0.346771/0.419815loss in batch 6: 0.289856/0.401245loss in batch 7: 0.380524/0.398651loss in batch 8: 0.0619202/0.361252loss in batch 9: 0.138992/0.33902loss in batch 10: 0.156723/0.322449loss in batch 11: 0.404617/0.329285loss in batch 12: 0.972839/0.378799loss in batch 13: 0.86528/0.413544loss in batch 14: 0.261642/0.403412loss in batch 15: 0.27562/0.395432loss in batch 16: 0.68158/0.412262loss in batch 17: 0.574051/0.421249loss in batch 18: 0.403488/0.420319loss in batch 19: 0.162979/0.407455loss in batch 20: 0.100067/0.392807loss in batch 21: 0.228806/0.385361loss in batch 22: 0.590836/0.394302loss in batch 23: 0.198593/0.386139loss in batch 24: 0.304916/0.382889loss in batch 25: 0.187225/0.375366loss in batch 26: 0.187103/0.368393loss in batch 27: 0.17128/0.361359loss in batch 28: 0.20282/0.355881loss in batch 29: 0.359299/0.356003loss in batch 30: 0.529358/0.361588loss in batch 31: 0.157104/0.355194loss in batch 32: 0.126373/0.348267loss in batch 33: 0.168182/0.342972loss in batch 34: 0.229248/0.339706loss in batch 35: 0.259613/0.337494loss in batch 36: 0.0590668/0.329971loss in batch 37: 0.572876/0.336365loss in batch 38: 0.126862/0.330978loss in batch 39: 0.0866089/0.324875loss in batch 40: 0.287003/0.323959loss in batch 41: 0.384995/0.325409loss in batch 42: 0.0636139/0.319321loss in batch 43: 0.13533/0.31514loss in batch 44: 0.224594/0.313126loss in batch 45: 0.0898285/0.308273loss in batch 46: 0.330017/0.308746loss in batch 47: 0.67511/0.31636loss in batch 48: 0.390411/0.317871loss in batch 49: 0.189789/0.315308loss in batch 50: 0.525284/0.319427loss in batch 51: 0.167801/0.316513loss in batch 52: 0.467072/0.319366loss in batch 53: 0.235641/0.31781loss in batch 54: 0.682434/0.324432loss in batch 55: 0.205032/0.322311loss in batch 56: 0.178665/0.319794loss in batch 57: 0.288239/0.319244loss in batch 58: 0.459702/0.321625loss in batch 59: 0.419098/0.323242loss in batch 60: 0.0824738/0.319305loss in batch 61: 0.180283/0.317062loss in batch 62: 0.11261/0.313812loss in batch 63: 0.59201/0.318161loss in batch 64: 0.393524/0.319321loss in batch 65: 0.246109/0.318207loss in batch 66: 0.179626/0.316147loss in batch 67: 0.171707/0.314011loss in batch 68: 0.0926361/0.310806loss in batch 69: 0.253586/0.309998loss in batch 70: 0.494751/0.312592loss in batch 71: 0.492844/0.315094loss in batch 72: 0.835556/0.322235loss in batch 73: 0.228287/0.320969loss in batch 74: 0.343338/0.321259loss in batch 75: 0.187454/0.319504loss in batch 76: 0.17363/0.317596loss in batch 77: 0.424072/0.31897loss in batch 78: 0.359818/0.319473loss in batch 79: 0.77507/0.32518loss in batch 80: 0.296112/0.324814loss in batch 81: 0.22644/0.323624loss in batch 82: 0.107742/0.32103loss in batch 83: 0.244904/0.320114loss in batch 84: 0.42337/0.32132loss in batch 85: 0.408661/0.322342loss in batch 86: 0.202255/0.320969loss in batch 87: 0.121246/0.31868loss in batch 88: 0.270645/0.318146loss in batch 89: 0.42131/0.31929loss in batch 90: 0.113129/0.317032loss in batch 91: 0.232819/0.316116loss in batch 92: 0.23085/0.315201loss in batch 93: 0.204346/0.314026loss in batch 94: 0.868576/0.319855loss in batch 95: 0.171036/0.318298loss in batch 96: 0.251907/0.317612loss in batch 97: 0.181335/0.316238loss in batch 98: 0.327606/0.31636loss in batch 99: 0.106888/0.314255loss in batch 100: 0.2939/0.314056loss in batch 101: 0.357178/0.314468loss in batch 102: 0.321991/0.314545loss in batch 103: 0.520691/0.316528loss in batch 104: 0.50705/0.318344loss in batch 105: 0.204483/0.317276loss in batch 106: 0.675842/0.320633loss in batch 107: 0.245056/0.319916loss in batch 108: 0.0374908/0.317337loss in batch 109: 0.303955/0.3172loss in batch 110: 0.227509/0.316406loss in batch 111: 0.205704/0.315414loss in batch 112: 0.125046/0.313721loss in batch 113: 0.122009/0.312042loss in batch 114: 0.387772/0.312714loss in batch 115: 0.0848389/0.310745loss in batch 116: 0.133102/0.309219loss in batch 117: 0.720505/0.312698loss in batch 118: 0.673706/0.315735loss in batch 119: 0.368942/0.316177loss in batch 120: 0.132034/0.314667loss in batch 121: 0.107513/0.312973loss in batch 122: 0.340714/0.313187loss in batch 123: 0.218307/0.312424loss in batch 124: 0.203827/0.311554loss in batch 125: 0.486267/0.312943loss in batch 126: 0.227036/0.312256loss in batch 127: 0.192932/0.31134loss in batch 128: 0.379944/0.311859loss in batch 129: 0.170074/0.310776loss in batch 130: 0.486481/0.312119loss in batch 131: 0.262314/0.311737loss in batch 132: 0.103027/0.310165loss in batch 133: 0.222549/0.309509loss in batch 134: 0.306335/0.309494loss in batch 135: 0.522369/0.31105loss in batch 136: 0.535004/0.312698loss in batch 137: 0.158157/0.311584loss in batch 138: 0.17012/0.310562loss in batch 139: 0.139008/0.309326loss in batch 140: 0.417313/0.310104loss in batch 141: 0.582245/0.312012loss in batch 142: 0.754333/0.315109loss in batch 143: 0.307968/0.315048loss in batch 144: 0.824371/0.318573loss in batch 145: 0.148544/0.317413loss in batch 146: 0.231628/0.316818loss in batch 147: 0.147308/0.315674loss in batch 148: 0.319473/0.315689loss in batch 149: 0.315262/0.315704loss in batch 150: 0.19487/0.314896loss in batch 151: 0.180939/0.314026loss in batch 152: 0.102585/0.312637loss in batch 153: 0.365005/0.312973loss in batch 154: 0.274902/0.312729loss in batch 155: 0.586578/0.314484loss in batch 156: 0.353088/0.314728loss in batch 157: 0.217636/0.314117loss in batch 158: 0.186356/0.313309loss in batch 159: 0.197006/0.312592loss in batch 160: 0.17598/0.311737loss in batch 161: 0.0965118/0.31041loss in batch 162: 0.23082/0.309921loss in batch 163: 0.0827484/0.308533loss in batch 164: 0.29512/0.308456loss in batch 165: 0.626694/0.310364loss in batch 166: 0.128311/0.30928loss in batch 167: 0.420471/0.309937loss in batch 168: 0.133804/0.308899loss in batch 169: 0.245987/0.308517loss in batch 170: 0.635208/0.31044loss in batch 171: 0.249908/0.310089loss in batch 172: 0.166519/0.309265loss in batch 173: 0.147217/0.308319loss in batch 174: 0.144012/0.307388loss in batch 175: 0.60968/0.309113loss in batch 176: 0.299576/0.309052loss in batch 177: 0.628555/0.310852loss in batch 178: 0.176773/0.310104loss in batch 179: 0.0905457/0.308884loss in batch 180: 0.281937/0.308731loss in batch 181: 0.12468/0.307724loss in batch 182: 0.270996/0.307526loss in batch 183: 0.276718/0.307358loss in batch 184: 0.300079/0.307312loss in batch 185: 0.138702/0.306412loss in batch 186: 0.196533/0.305817loss in batch 187: 0.942993/0.309204loss in batch 188: 0.342392/0.309387loss in batch 189: 0.0763397/0.308151loss in batch 190: 0.256668/0.307877loss in batch 191: 0.200439/0.307327loss in batch 192: 0.614685/0.308929loss in batch 193: 0.596771/0.310394loss in batch 194: 0.275482/0.310226loss in batch 195: 0.169464/0.309494loss in batch 196: 0.159927/0.308746loss in batch 197: 0.149139/0.307938loss in batch 198: 0.264221/0.307724loss in batch 199: 0.176147/0.307068loss in batch 200: 0.0965881/0.306015loss in batch 201: 0.26062/0.305786loss in batch 202: 0.130722/0.304932loss in batch 203: 0.189758/0.304367loss in batch 204: 0.21991/0.303955loss in batch 205: 0.207352/0.303482loss in batch 206: 0.301392/0.303482loss in batch 207: 0.189301/0.302933loss in batch 208: 0.24733/0.302658loss in batch 209: 0.296371/0.302628loss in batch 210: 0.969193/0.305771loss in batch 211: 0.233353/0.30545loss in batch 212: 0.252441/0.305191
done with epoch 2
train_acc: 0.908451 (387/426)
test loss: 0.252441
acc: 0.916082 (131/143)
loss in batch 0: 0.0749207/0.0749207loss in batch 1: 0.170319/0.12262loss in batch 2: 0.189087/0.144775loss in batch 3: 1.0439/0.369553loss in batch 4: 0.185196/0.332672loss in batch 5: 0.368286/0.338608loss in batch 6: 0.198456/0.318604loss in batch 7: 0.276566/0.313354loss in batch 8: 0.134705/0.293488loss in batch 9: 0.162903/0.280441loss in batch 10: 0.577667/0.307465loss in batch 11: 0.118286/0.291687loss in batch 12: 0.226807/0.286697loss in batch 13: 0.18364/0.279343loss in batch 14: 0.187485/0.273209loss in batch 15: 0.327957/0.276627loss in batch 16: 0.237198/0.274307loss in batch 17: 0.322311/0.276978loss in batch 18: 0.199799/0.272919loss in batch 19: 0.187393/0.268646loss in batch 20: 0.171753/0.264023loss in batch 21: 0.196075/0.260941loss in batch 22: 0.482086/0.270554loss in batch 23: 0.210907/0.268066loss in batch 24: 0.146851/0.263229loss in batch 25: 0.473602/0.271301loss in batch 26: 0.124176/0.265869loss in batch 27: 0.141724/0.261429loss in batch 28: 0.171494/0.258316loss in batch 29: 0.308823/0.26001loss in batch 30: 0.243942/0.259506loss in batch 31: 0.455185/0.265594loss in batch 32: 0.180725/0.263031loss in batch 33: 0.980164/0.284134loss in batch 34: 0.0898438/0.278564loss in batch 35: 0.0628815/0.272583loss in batch 36: 0.297302/0.273254loss in batch 37: 0.0908051/0.268448loss in batch 38: 0.547058/0.275589loss in batch 39: 0.102325/0.271255loss in batch 40: 0.17865/0.269012loss in batch 41: 0.389145/0.271866loss in batch 42: 0.199936/0.270187loss in batch 43: 0.151337/0.267487loss in batch 44: 0.233582/0.266739loss in batch 45: 0.319351/0.267883loss in batch 46: 0.165131/0.265701loss in batch 47: 0.182693/0.263962loss in batch 48: 0.496857/0.268723loss in batch 49: 0.202972/0.26741loss in batch 50: 0.199081/0.266068loss in batch 51: 0.201416/0.264816loss in batch 52: 0.628601/0.271698loss in batch 53: 0.348022/0.273102loss in batch 54: 0.202118/0.271805loss in batch 55: 0.0809021/0.268402loss in batch 56: 0.769211/0.277191loss in batch 57: 0.426193/0.279755loss in batch 58: 0.536148/0.284103loss in batch 59: 0.128571/0.281509loss in batch 60: 0.0518341/0.27774loss in batch 61: 0.324951/0.278519loss in batch 62: 0.379471/0.280106loss in batch 63: 0.151016/0.278091loss in batch 64: 0.294312/0.278336loss in batch 65: 0.322205/0.279007loss in batch 66: 0.183884/0.277588loss in batch 67: 0.483139/0.280609loss in batch 68: 0.10051/0.278loss in batch 69: 0.112839/0.275635loss in batch 70: 0.681488/0.281357loss in batch 71: 0.0985107/0.278809loss in batch 72: 0.320206/0.279373loss in batch 73: 0.223022/0.278625loss in batch 74: 0.0238342/0.275223loss in batch 75: 0.248047/0.274872loss in batch 76: 0.106155/0.272675loss in batch 77: 0.0867462/0.270279loss in batch 78: 0.400986/0.271942loss in batch 79: 0.145126/0.270355loss in batch 80: 0.257492/0.270203loss in batch 81: 0.0464325/0.267471loss in batch 82: 0.136032/0.265884loss in batch 83: 0.534378/0.269073loss in batch 84: 0.18364/0.268082loss in batch 85: 0.236877/0.267715loss in batch 86: 0.32814/0.268417loss in batch 87: 0.112549/0.266632loss in batch 88: 0.120667/0.264999loss in batch 89: 0.707779/0.269913loss in batch 90: 0.457092/0.271973loss in batch 91: 0.163422/0.270798loss in batch 92: 0.126892/0.269241loss in batch 93: 0.179489/0.268295loss in batch 94: 0.118118/0.266724loss in batch 95: 0.282303/0.266876loss in batch 96: 0.193954/0.266113loss in batch 97: 0.141617/0.264847loss in batch 98: 0.279114/0.264999loss in batch 99: 0.249039/0.264847loss in batch 100: 0.171188/0.263901loss in batch 101: 0.154602/0.262848loss in batch 102: 0.373352/0.263901loss in batch 103: 0.251083/0.263779loss in batch 104: 0.17012/0.262894loss in batch 105: 0.160248/0.261917loss in batch 106: 0.0633087/0.260086loss in batch 107: 0.194885/0.259476loss in batch 108: 0.136887/0.258347loss in batch 109: 0.113678/0.257034loss in batch 110: 0.0639191/0.255295loss in batch 111: 0.236404/0.255112loss in batch 112: 0.165924/0.254333loss in batch 113: 0.434967/0.25592loss in batch 114: 0.0522461/0.25415loss in batch 115: 0.195313/0.253632loss in batch 116: 0.336899/0.254349loss in batch 117: 0.590042/0.257202loss in batch 118: 0.312469/0.25766loss in batch 119: 0.199112/0.257172loss in batch 120: 0.207062/0.25676loss in batch 121: 0.324188/0.257309loss in batch 122: 0.109177/0.256104loss in batch 123: 0.981476/0.261963loss in batch 124: 0.413635/0.263168loss in batch 125: 0.169067/0.262421loss in batch 126: 0.127914/0.261368loss in batch 127: 0.0613098/0.259796loss in batch 128: 0.0693207/0.258316loss in batch 129: 0.29776/0.258621loss in batch 130: 0.289093/0.25885loss in batch 131: 0.328568/0.259399loss in batch 132: 0.0808105/0.258041loss in batch 133: 0.145248/0.257202loss in batch 134: 0.171921/0.256577loss in batch 135: 0.348587/0.257248loss in batch 136: 0.763718/0.260941loss in batch 137: 0.530426/0.262909loss in batch 138: 0.034256/0.261246loss in batch 139: 0.299667/0.26152loss in batch 140: 0.248703/0.261429loss in batch 141: 0.287704/0.261612loss in batch 142: 0.130371/0.260696loss in batch 143: 0.13678/0.259842loss in batch 144: 0.737427/0.263138loss in batch 145: 0.269058/0.263184loss in batch 146: 0.0819092/0.261948loss in batch 147: 0.0286865/0.260361loss in batch 148: 0.102768/0.259308loss in batch 149: 0.240265/0.259186loss in batch 150: 0.592041/0.261398loss in batch 151: 0.0742645/0.260147loss in batch 152: 0.07164/0.258911loss in batch 153: 0.117508/0.257996loss in batch 154: 0.358444/0.258667loss in batch 155: 0.210373/0.258347loss in batch 156: 0.239731/0.258224loss in batch 157: 0.362762/0.258881loss in batch 158: 0.0623779/0.257645loss in batch 159: 0.138321/0.256912loss in batch 160: 0.213837/0.256638loss in batch 161: 0.0998383/0.255661loss in batch 162: 0.0521088/0.254425loss in batch 163: 0.183548/0.253983loss in batch 164: 0.0892639/0.252991loss in batch 165: 0.508911/0.254532loss in batch 166: 0.383194/0.255295loss in batch 167: 0.0852661/0.254288loss in batch 168: 0.128265/0.253555loss in batch 169: 0.140717/0.252884loss in batch 170: 0.129013/0.252151loss in batch 171: 0.488876/0.25354loss in batch 172: 0.229507/0.253387loss in batch 173: 0.993439/0.257645loss in batch 174: 0.708466/0.260223loss in batch 175: 0.364243/0.260818loss in batch 176: 0.312332/0.261108loss in batch 177: 0.124344/0.26033loss in batch 178: 0.449707/0.261398loss in batch 179: 0.413818/0.262238loss in batch 180: 0.0936737/0.261322loss in batch 181: 1.15125/0.26619loss in batch 182: 0.631866/0.268204loss in batch 183: 0.342545/0.268616loss in batch 184: 0.23999/0.268448loss in batch 185: 0.235611/0.268265loss in batch 186: 0.0883331/0.267303loss in batch 187: 0.0792084/0.266312loss in batch 188: 0.543732/0.267776loss in batch 189: 0.439087/0.268677loss in batch 190: 0.144928/0.268021loss in batch 191: 0.0948944/0.26712loss in batch 192: 0.131927/0.266434loss in batch 193: 0.0137482/0.265121loss in batch 194: 0.139709/0.264481loss in batch 195: 0.325439/0.264786loss in batch 196: 0.179672/0.264359loss in batch 197: 0.263412/0.264359loss in batch 198: 0.348969/0.264786loss in batch 199: 0.153122/0.264221loss in batch 200: 0.492065/0.265366loss in batch 201: 0.115768/0.264618loss in batch 202: 0.445206/0.265503loss in batch 203: 0.165527/0.265015loss in batch 204: 0.198227/0.264679loss in batch 205: 0.0814819/0.263794loss in batch 206: 0.0904541/0.26297loss in batch 207: 0.742783/0.265259loss in batch 208: 0.926849/0.268433loss in batch 209: 0.306381/0.268616loss in batch 210: 0.167999/0.268143loss in batch 211: 0.160187/0.267624loss in batch 212: 0.332047/0.267929
done with epoch 3
train_acc: 0.920188 (392/426)
test loss: 0.332046
acc: 0.923075 (132/143)
loss in batch 0: 0.08461/0.08461loss in batch 1: 0.185486/0.13504loss in batch 2: 0.842255/0.370773loss in batch 3: 0.394485/0.376709loss in batch 4: 0.0770569/0.316772loss in batch 5: 0.146408/0.288376loss in batch 6: 0.146927/0.268188loss in batch 7: 0.581345/0.307312loss in batch 8: 0.620377/0.342102loss in batch 9: 0.365005/0.344391loss in batch 10: 0.124756/0.324432loss in batch 11: 0.154266/0.310242loss in batch 12: 0.128494/0.296265loss in batch 13: 0.798721/0.332169loss in batch 14: 0.344116/0.332947loss in batch 15: 0.312576/0.33168loss in batch 16: 0.404984/0.335983loss in batch 17: 0.158218/0.326111loss in batch 18: 0.246155/0.321915loss in batch 19: 0.175354/0.314575loss in batch 20: 0.166687/0.307541loss in batch 21: 0.231812/0.304092loss in batch 22: 0.335068/0.30545loss in batch 23: 0.454041/0.31163loss in batch 24: 0.281891/0.31044loss in batch 25: 0.247818/0.308029loss in batch 26: 0.195816/0.303879loss in batch 27: 0.0937653/0.296371loss in batch 28: 0.0992889/0.289581loss in batch 29: 0.227325/0.287506loss in batch 30: 0.128082/0.282364loss in batch 31: 0.125916/0.277481loss in batch 32: 0.461395/0.283051loss in batch 33: 0.299088/0.283508loss in batch 34: 0.175278/0.280411loss in batch 35: 0.0513916/0.274063loss in batch 36: 0.150513/0.270721loss in batch 37: 0.191559/0.268631loss in batch 38: 0.086319/0.263962loss in batch 39: 0.205994/0.262512loss in batch 40: 0.364914/0.265015loss in batch 41: 0.202652/0.263519loss in batch 42: 1.02469/0.281235loss in batch 43: 0.127182/0.27774loss in batch 44: 0.284302/0.277863loss in batch 45: 0.175262/0.27565loss in batch 46: 0.395004/0.278183loss in batch 47: 0.0335846/0.273087loss in batch 48: 0.141052/0.270386loss in batch 49: 0.515137/0.275284loss in batch 50: 0.0814514/0.271484loss in batch 51: 0.107132/0.268326loss in batch 52: 0.15741/0.26622loss in batch 53: 0.126389/0.263641loss in batch 54: 0.501663/0.267975loss in batch 55: 0.367462/0.269745loss in batch 56: 0.274506/0.269836loss in batch 57: 0.126129/0.267349loss in batch 58: 0.21347/0.266434loss in batch 59: 0.687515/0.273453loss in batch 60: 0.170288/0.271774loss in batch 61: 0.226181/0.271042loss in batch 62: 0.410751/0.273239loss in batch 63: 0.471771/0.276352loss in batch 64: 0.229721/0.275635loss in batch 65: 0.170074/0.274048loss in batch 66: 0.179581/0.272629loss in batch 67: 0.109085/0.270218loss in batch 68: 0.0841522/0.267532loss in batch 69: 0.0445862/0.264328loss in batch 70: 0.151123/0.262741loss in batch 71: 0.258041/0.26268loss in batch 72: 0.219513/0.262085loss in batch 73: 0.697327/0.26796loss in batch 74: 0.103363/0.265778loss in batch 75: 0.161148/0.264389loss in batch 76: 0.0869293/0.262085loss in batch 77: 0.0750732/0.259689loss in batch 78: 0.085434/0.257492loss in batch 79: 0.099823/0.255524loss in batch 80: 0.0404053/0.252869loss in batch 81: 0.161713/0.251755loss in batch 82: 0.255356/0.251785loss in batch 83: 0.125366/0.25029loss in batch 84: 0.0427246/0.247849loss in batch 85: 0.117188/0.246323loss in batch 86: 0.564957/0.249985loss in batch 87: 0.340454/0.251022loss in batch 88: 0.111588/0.249451loss in batch 89: 0.350525/0.25058loss in batch 90: 0.0785675/0.248688loss in batch 91: 0.271484/0.248932loss in batch 92: 0.0769501/0.247086loss in batch 93: 0.116928/0.245697loss in batch 94: 0.247147/0.245712loss in batch 95: 0.0618744/0.243805loss in batch 96: 0.350098/0.244904loss in batch 97: 0.0785065/0.243195loss in batch 98: 0.15271/0.242279loss in batch 99: 0.0449829/0.240311loss in batch 100: 0.582504/0.243698loss in batch 101: 0.302032/0.244278loss in batch 102: 0.225174/0.244095loss in batch 103: 0.0785217/0.242493loss in batch 104: 0.472137/0.244675loss in batch 105: 0.23027/0.244553loss in batch 106: 0.107971/0.243271loss in batch 107: 0.126495/0.242188loss in batch 108: 0.710693/0.24649loss in batch 109: 0.111389/0.245255loss in batch 110: 0.346115/0.246155loss in batch 111: 0.309113/0.246735loss in batch 112: 0.625717/0.250076loss in batch 113: 0.0615845/0.248428loss in batch 114: 0.0659485/0.246841loss in batch 115: 0.13591/0.24588loss in batch 116: 0.0851288/0.244507loss in batch 117: 0.143692/0.243652loss in batch 118: 0.0705872/0.242203loss in batch 119: 0.120438/0.241196loss in batch 120: 0.232513/0.241119loss in batch 121: 0.250732/0.241196loss in batch 122: 0.378128/0.24231loss in batch 123: 0.18103/0.241806loss in batch 124: 0.776169/0.246094loss in batch 125: 0.264206/0.246231loss in batch 126: 0.335541/0.246933loss in batch 127: 1.53192/0.256973loss in batch 128: 0.685516/0.2603loss in batch 129: 0.13559/0.259338loss in batch 130: 0.15094/0.258499loss in batch 131: 0.0664368/0.25705loss in batch 132: 0.496887/0.258865loss in batch 133: 0.208084/0.258484loss in batch 134: 0.0788116/0.257156loss in batch 135: 0.262222/0.257187loss in batch 136: 0.110199/0.256119loss in batch 137: 0.15274/0.255371loss in batch 138: 0.0518646/0.253906loss in batch 139: 0.12085/0.252945loss in batch 140: 0.416306/0.254105loss in batch 141: 0.119888/0.253174loss in batch 142: 0.242142/0.253098loss in batch 143: 1.10558/0.259003loss in batch 144: 0.184433/0.258484loss in batch 145: 0.0825653/0.257294loss in batch 146: 0.327545/0.257767loss in batch 147: 0.127472/0.256882loss in batch 148: 0.35405/0.257538loss in batch 149: 0.101913/0.2565loss in batch 150: 0.278412/0.256653loss in batch 151: 0.136444/0.255844loss in batch 152: 0.131165/0.255035loss in batch 153: 0.227814/0.254868loss in batch 154: 0.088562/0.253784loss in batch 155: 0.125732/0.25296loss in batch 156: 0.281631/0.253143loss in batch 157: 0.203033/0.252823loss in batch 158: 0.132339/0.252075loss in batch 159: 0.166931/0.251541loss in batch 160: 0.336914/0.252075loss in batch 161: 0.098999/0.251129loss in batch 162: 0.250076/0.251129loss in batch 163: 0.172974/0.250641loss in batch 164: 0.236588/0.250565loss in batch 165: 0.039917/0.249298loss in batch 166: 0.304916/0.249619loss in batch 167: 0.402328/0.250534loss in batch 168: 0.0180817/0.249161loss in batch 169: 0.415115/0.250122loss in batch 170: 0.0320282/0.248856loss in batch 171: 0.32457/0.249298loss in batch 172: 0.249863/0.249298loss in batch 173: 0.189575/0.248962loss in batch 174: 0.283691/0.249146loss in batch 175: 0.319534/0.249557loss in batch 176: 0.16684/0.249084loss in batch 177: 0.358627/0.24971loss in batch 178: 0.391815/0.250504loss in batch 179: 0.571671/0.252289loss in batch 180: 0.0587921/0.251221loss in batch 181: 0.068161/0.250214loss in batch 182: 0.337738/0.250687loss in batch 183: 0.0962372/0.249847loss in batch 184: 0.15683/0.249344loss in batch 185: 0.0834808/0.248459loss in batch 186: 0.277908/0.248611loss in batch 187: 0.157791/0.248123loss in batch 188: 0.253571/0.248154loss in batch 189: 0.160202/0.247681loss in batch 190: 0.409698/0.24855loss in batch 191: 0.234695/0.248474loss in batch 192: 0.216034/0.248306loss in batch 193: 0.119797/0.247635loss in batch 194: 0.216537/0.247482loss in batch 195: 0.108978/0.24678loss in batch 196: 0.157761/0.246307loss in batch 197: 0.0406647/0.245285loss in batch 198: 0.110245/0.244598loss in batch 199: 0.0332642/0.243546loss in batch 200: 0.47261/0.244675loss in batch 201: 0.270218/0.244812loss in batch 202: 0.654678/0.246826loss in batch 203: 0.146942/0.246338loss in batch 204: 0.0849304/0.24556loss in batch 205: 0.212723/0.245392loss in batch 206: 0.294418/0.245621loss in batch 207: 0.0340881/0.244614loss in batch 208: 0.41925/0.245453loss in batch 209: 0.0689545/0.244614loss in batch 210: 0.329712/0.24501loss in batch 211: 0.197281/0.244797loss in batch 212: 0.0508728/0.243866
done with epoch 4
train_acc: 0.929577 (396/426)
test loss: 0.0508727
acc: 0.937061 (134/143)
loss in batch 0: 0.241089/0.241089loss in batch 1: 0.128525/0.184799loss in batch 2: 0.0356293/0.135086loss in batch 3: 0.169952/0.143799loss in batch 4: 1.194/0.353836loss in batch 5: 0.313782/0.347153loss in batch 6: 0.561005/0.377716loss in batch 7: 0.106918/0.343872loss in batch 8: 0.0171204/0.307556loss in batch 9: 0.138626/0.290665loss in batch 10: 0.111481/0.274368loss in batch 11: 0.107513/0.260468loss in batch 12: 0.633652/0.289185loss in batch 13: 0.0383453/0.271271loss in batch 14: 0.11705/0.260971loss in batch 15: 0.0915833/0.250397loss in batch 16: 0.217117/0.248428loss in batch 17: 0.174622/0.244339loss in batch 18: 0.204514/0.242233loss in batch 19: 0.599289/0.260086loss in batch 20: 0.122849/0.253555loss in batch 21: 0.803711/0.278564loss in batch 22: 0.141922/0.272614loss in batch 23: 0.277985/0.272842loss in batch 24: 0.666031/0.288574loss in batch 25: 0.0770111/0.280426loss in batch 26: 0.0655518/0.272476loss in batch 27: 0.715347/0.2883loss in batch 28: 0.35025/0.290421loss in batch 29: 0.0976257/0.284012loss in batch 30: 0.12326/0.278824loss in batch 31: 0.380188/0.281982loss in batch 32: 0.31041/0.282852loss in batch 33: 0.323944/0.284058loss in batch 34: 0.120514/0.279373loss in batch 35: 0.129562/0.275223loss in batch 36: 0.172256/0.27243loss in batch 37: 0.160721/0.269485loss in batch 38: 0.398285/0.272797loss in batch 39: 0.202789/0.271042loss in batch 40: 0.376099/0.273621loss in batch 41: 0.0085907/0.267303loss in batch 42: 0.10675/0.263565loss in batch 43: 0.104126/0.259949loss in batch 44: 0.0621948/0.255554loss in batch 45: 0.160706/0.253494loss in batch 46: 0.0665436/0.249496loss in batch 47: 0.295227/0.250458loss in batch 48: 0.552628/0.256638loss in batch 49: 0.0271606/0.252045loss in batch 50: 0.858322/0.263931loss in batch 51: 0.226212/0.263199loss in batch 52: 0.298615/0.26387loss in batch 53: 0.163147/0.262009loss in batch 54: 0.00793457/0.257385loss in batch 55: 0.335388/0.258774loss in batch 56: 0.253693/0.258682loss in batch 57: 0.217987/0.257996loss in batch 58: 0.50621/0.262192loss in batch 59: 0.394775/0.264404loss in batch 60: 0.168243/0.262833loss in batch 61: 0.18161/0.26152loss in batch 62: 0.753937/0.269333loss in batch 63: 0.324265/0.270187loss in batch 64: 0.181274/0.268814loss in batch 65: 0.180099/0.267487loss in batch 66: 0.13324/0.265488loss in batch 67: 0.156326/0.26387loss in batch 68: 0.418671/0.266129loss in batch 69: 0.50473/0.269516loss in batch 70: 0.089035/0.266983loss in batch 71: 0.0900421/0.264526loss in batch 72: 0.297073/0.264984loss in batch 73: 0.174316/0.263748loss in batch 74: 0.381561/0.26532loss in batch 75: 0.050827/0.262497loss in batch 76: 0.0284424/0.259445loss in batch 77: 0.0599976/0.256897loss in batch 78: 0.355103/0.258148loss in batch 79: 0.196198/0.25737loss in batch 80: 0.072052/0.255081loss in batch 81: 0.156479/0.253876loss in batch 82: 0.0429077/0.251328loss in batch 83: 0.205322/0.250793loss in batch 84: 0.253571/0.250809loss in batch 85: 0.238037/0.250671loss in batch 86: 0.32077/0.251465loss in batch 87: 0.0906525/0.249649loss in batch 88: 0.0665894/0.247589loss in batch 89: 0.147858/0.24649loss in batch 90: 0.211487/0.246094loss in batch 91: 0.0925446/0.244431loss in batch 92: 0.2341/0.244324loss in batch 93: 0.44574/0.24646loss in batch 94: 0.0771484/0.24469loss in batch 95: 0.553177/0.247894loss in batch 96: 0.0428925/0.245773loss in batch 97: 0.0374298/0.243668loss in batch 98: 0.473236/0.245972loss in batch 99: 0.319931/0.246719loss in batch 100: 0.0624695/0.244888loss in batch 101: 0.218552/0.244629loss in batch 102: 0.1689/0.243896loss in batch 103: 0.563995/0.246979loss in batch 104: 0.108292/0.245651loss in batch 105: 0.0732117/0.244034loss in batch 106: 0.602173/0.247375loss in batch 107: 0.0864105/0.24588loss in batch 108: 0.0179901/0.243805loss in batch 109: 0.0496979/0.242035loss in batch 110: 0.276352/0.24234loss in batch 111: 0.0542145/0.240662loss in batch 112: 0.322418/0.241379loss in batch 113: 0.181732/0.24086loss in batch 114: 0.122116/0.239822loss in batch 115: 0.230591/0.239746loss in batch 116: 0.151398/0.238998loss in batch 117: 0.197052/0.238632loss in batch 118: 0.535385/0.241135loss in batch 119: 0.315216/0.241745loss in batch 120: 0.0766144/0.240387loss in batch 121: 0.0556183/0.238861loss in batch 122: 0.57373/0.241592loss in batch 123: 0.138535/0.240753loss in batch 124: 0.20372/0.240463loss in batch 125: 1.15805/0.247742loss in batch 126: 0.0093689/0.245865loss in batch 127: 0.306976/0.246338loss in batch 128: 0.479141/0.248154loss in batch 129: 0.0782318/0.246841loss in batch 130: 0.117523/0.24585loss in batch 131: 0.0737152/0.244553loss in batch 132: 0.13324/0.243713loss in batch 133: 0.0668335/0.242386loss in batch 134: 0.247375/0.242432loss in batch 135: 0.0527649/0.241043loss in batch 136: 0.24382/0.241058loss in batch 137: 0.289108/0.241409loss in batch 138: 0.426529/0.242737loss in batch 139: 0.233917/0.242676loss in batch 140: 0.143951/0.241974loss in batch 141: 0.121353/0.241119loss in batch 142: 0.13472/0.240372loss in batch 143: 0.383881/0.241379loss in batch 144: 0.0424042/0.240005loss in batch 145: 0.0303802/0.238556loss in batch 146: 0.0709229/0.237427loss in batch 147: 0.0922852/0.236435loss in batch 148: 0.196747/0.236176loss in batch 149: 0.0521698/0.234955loss in batch 150: 0.141479/0.234329loss in batch 151: 0.228302/0.234299loss in batch 152: 0.157303/0.233795loss in batch 153: 0.0647278/0.232697loss in batch 154: 0.372177/0.233582loss in batch 155: 0.298355/0.234009loss in batch 156: 0.121826/0.233307loss in batch 157: 0.2742/0.233551loss in batch 158: 0.393097/0.234558loss in batch 159: 0.0600739/0.233459loss in batch 160: 0.0301208/0.232208loss in batch 161: 0.356903/0.232971loss in batch 162: 0.0502777/0.231842loss in batch 163: 0.0878601/0.230972loss in batch 164: 0.203842/0.230804loss in batch 165: 0.0553894/0.229752loss in batch 166: 0.0789948/0.228836loss in batch 167: 0.0393524/0.227722loss in batch 168: 1.00136/0.2323loss in batch 169: 0.0261841/0.231094loss in batch 170: 0.0843658/0.230225loss in batch 171: 0.141174/0.229706loss in batch 172: 0.105316/0.228989loss in batch 173: 0.178329/0.228714loss in batch 174: 0.0908966/0.227921loss in batch 175: 0.509048/0.229523loss in batch 176: 0.502228/0.231064loss in batch 177: 0.0756683/0.230179loss in batch 178: 0.0647736/0.229248loss in batch 179: 0.299957/0.22966loss in batch 180: 0.0796204/0.228821loss in batch 181: 0.0565643/0.227875loss in batch 182: 0.811737/0.231064loss in batch 183: 0.0531769/0.230087loss in batch 184: 0.0515747/0.229141loss in batch 185: 0.215454/0.229065loss in batch 186: 0.142288/0.228592loss in batch 187: 0.0344391/0.227554loss in batch 188: 0.22142/0.227524loss in batch 189: 0.257782/0.227692loss in batch 190: 0.32431/0.228195loss in batch 191: 0.749619/0.230911loss in batch 192: 0.047287/0.22995loss in batch 193: 0.036499/0.228958loss in batch 194: 0.255234/0.229095loss in batch 195: 0.0826416/0.228348loss in batch 196: 0.27774/0.228592loss in batch 197: 0.0163422/0.227524loss in batch 198: 0.182114/0.227295loss in batch 199: 0.107559/0.2267loss in batch 200: 0.126556/0.226196loss in batch 201: 0.297516/0.226563loss in batch 202: 0.122299/0.226028loss in batch 203: 0.243073/0.226135loss in batch 204: 0.14003/0.225708loss in batch 205: 0.697174/0.227982loss in batch 206: 0.454636/0.22908loss in batch 207: 0.320969/0.229538loss in batch 208: 0.0780487/0.228806loss in batch 209: 0.159668/0.228485loss in batch 210: 0.449417/0.229523loss in batch 211: 0.586761/0.231201loss in batch 212: 0.054718/0.230377
done with epoch 5
train_acc: 0.92723 (395/426)
test loss: 0.0547179
acc: 0.930068 (133/143)
loss in batch 0: 0.0934753/0.0934753loss in batch 1: 0.248764/0.171112loss in batch 2: 0.0220947/0.121445loss in batch 3: 0.115829/0.120041loss in batch 4: 0.0335083/0.102737loss in batch 5: 0.342194/0.142654loss in batch 6: 0.0205841/0.125198loss in batch 7: 0.13652/0.126617loss in batch 8: 0.258972/0.141327loss in batch 9: 0.12561/0.139755loss in batch 10: 0.614426/0.182907loss in batch 11: 0.133743/0.178818loss in batch 12: 0.527618/0.205643loss in batch 13: 0.25827/0.209396loss in batch 14: 0.530746/0.23082loss in batch 15: 0.526184/0.249283loss in batch 16: 0.242279/0.248871loss in batch 17: 0.107162/0.240997loss in batch 18: 0.291916/0.243683loss in batch 19: 0.103973/0.236694loss in batch 20: 0.0302277/0.226868loss in batch 21: 0.0537567/0.218994loss in batch 22: 0.151901/0.21608loss in batch 23: 0.181137/0.214615loss in batch 24: 0.0846863/0.209412loss in batch 25: 0.0915833/0.20488loss in batch 26: 0.124237/0.201904loss in batch 27: 1.12547/0.234894loss in batch 28: 0.106461/0.230469loss in batch 29: 0.105698/0.226303loss in batch 30: 0.143326/0.223618loss in batch 31: 0.378876/0.22847loss in batch 32: 0.210541/0.227936loss in batch 33: 0.119354/0.224731loss in batch 34: 0.139877/0.222321loss in batch 35: 0.202026/0.221756loss in batch 36: 0.212616/0.221497loss in batch 37: 0.0335999/0.216553loss in batch 38: 0.216187/0.216537loss in batch 39: 0.372604/0.220459loss in batch 40: 0.888138/0.236725loss in batch 41: 0.0911713/0.233276loss in batch 42: 0.834915/0.247269loss in batch 43: 0.282043/0.248047loss in batch 44: 0.050415/0.243668loss in batch 45: 0.110046/0.240753loss in batch 46: 0.580612/0.247986loss in batch 47: 0.100235/0.244904loss in batch 48: 0.0755463/0.241455loss in batch 49: 0.0692596/0.238022loss in batch 50: 0.351089/0.240219loss in batch 51: 0.12178/0.237946loss in batch 52: 0.0792694/0.234955loss in batch 53: 0.138657/0.23317loss in batch 54: 0.49968/0.238007loss in batch 55: 0.00614929/0.233887loss in batch 56: 0.621918/0.240677loss in batch 57: 0.0879364/0.238052loss in batch 58: 0.648972/0.24501loss in batch 59: 0.299316/0.245926loss in batch 60: 0.00845337/0.24202loss in batch 61: 0.10321/0.239777loss in batch 62: 0.205719/0.239243loss in batch 63: 0.57724/0.244537loss in batch 64: 0.304077/0.245453loss in batch 65: 0.613373/0.251022loss in batch 66: 0.0871124/0.248566loss in batch 67: 0.180695/0.247574loss in batch 68: 0.333359/0.248825loss in batch 69: 0.0641785/0.246185loss in batch 70: 0.35379/0.247696loss in batch 71: 0.190979/0.246902loss in batch 72: 0.222549/0.246582loss in batch 73: 0.204987/0.246002loss in batch 74: 0.0542145/0.243454loss in batch 75: 0.231461/0.243301loss in batch 76: 0.164459/0.242264loss in batch 77: 0.528717/0.245941loss in batch 78: 0.202759/0.245392loss in batch 79: 0.506195/0.248657loss in batch 80: 0.36113/0.250046loss in batch 81: 0.166992/0.249023loss in batch 82: 0.277695/0.249374loss in batch 83: 0.0714569/0.247269loss in batch 84: 0.282761/0.247681loss in batch 85: 0.0589142/0.245483loss in batch 86: 0.0293121/0.242996loss in batch 87: 0.283691/0.243469loss in batch 88: 0.0329742/0.241104loss in batch 89: 0.136887/0.239944loss in batch 90: 0.0755157/0.238129loss in batch 91: 0.249863/0.238266loss in batch 92: 0.226471/0.238129loss in batch 93: 0.176529/0.237473loss in batch 94: 0.121353/0.236252loss in batch 95: 0.271027/0.236618loss in batch 96: 0.0887756/0.235092loss in batch 97: 0.138977/0.234116loss in batch 98: 0.38295/0.235611loss in batch 99: 0.179626/0.235046loss in batch 100: 0.0380707/0.233093loss in batch 101: 0.0832367/0.231628loss in batch 102: 0.102448/0.230377loss in batch 103: 0.0577545/0.228729loss in batch 104: 0.0875397/0.227386loss in batch 105: 0.195435/0.227066loss in batch 106: 0.397537/0.228668loss in batch 107: 0.56427/0.231766loss in batch 108: 0.270386/0.232132loss in batch 109: 0.081955/0.230774loss in batch 110: 0.0605927/0.229233loss in batch 111: 0.052597/0.227661loss in batch 112: 0.055481/0.226135loss in batch 113: 0.0856171/0.224899loss in batch 114: 0.155975/0.224304loss in batch 115: 0.307785/0.225021loss in batch 116: 0.125122/0.224167loss in batch 117: 0.172241/0.223724loss in batch 118: 0.0997467/0.222687loss in batch 119: 0.0653229/0.221375loss in batch 120: 0.0879364/0.220276loss in batch 121: 0.0588531/0.218948loss in batch 122: 0.214249/0.218918loss in batch 123: 0.204803/0.218796loss in batch 124: 0.757965/0.223114loss in batch 125: 0.0645294/0.221863loss in batch 126: 0.166016/0.221405loss in batch 127: 0.12149/0.220627loss in batch 128: 0.107834/0.219757loss in batch 129: 0.128296/0.219055loss in batch 130: 0.0787659/0.217987loss in batch 131: 0.427887/0.219574loss in batch 132: 0.477127/0.221512loss in batch 133: 0.231293/0.221573loss in batch 134: 0.0621643/0.220398loss in batch 135: 0.159027/0.21994loss in batch 136: 0.10405/0.219101loss in batch 137: 0.0967102/0.218216loss in batch 138: 0.223785/0.218262loss in batch 139: 0.309784/0.218918loss in batch 140: 0.121475/0.218216loss in batch 141: 0.324524/0.218964loss in batch 142: 0.0553436/0.217819loss in batch 143: 0.151459/0.217361loss in batch 144: 0.1259/0.216721loss in batch 145: 0.297546/0.217285loss in batch 146: 0.100571/0.216492loss in batch 147: 0.0106354/0.215103loss in batch 148: 0.153122/0.214691loss in batch 149: 0.0457916/0.213562loss in batch 150: 0.0812225/0.212677loss in batch 151: 0.0610504/0.211685loss in batch 152: 0.175308/0.211441loss in batch 153: 0.0711365/0.210541loss in batch 154: 0.104294/0.209839loss in batch 155: 0.0435333/0.208771loss in batch 156: 0.146469/0.208389loss in batch 157: 0.214828/0.208435loss in batch 158: 0.0358887/0.207352loss in batch 159: 0.10347/0.206696loss in batch 160: 0.0475464/0.205704loss in batch 161: 0.0258331/0.20459loss in batch 162: 0.0671692/0.203751loss in batch 163: 0.120789/0.203247loss in batch 164: 0.0164185/0.202103loss in batch 165: 0.0435638/0.201172loss in batch 166: 0.298355/0.201752loss in batch 167: 0.0413818/0.200775loss in batch 168: 0.310455/0.201431loss in batch 169: 1.59947/0.209656loss in batch 170: 0.60585/0.211975loss in batch 171: 0.0427094/0.210999loss in batch 172: 0.0542145/0.210083loss in batch 173: 0.456024/0.211502loss in batch 174: 0.0555878/0.210602loss in batch 175: 0.239456/0.21077loss in batch 176: 0.526459/0.212555loss in batch 177: 0.526428/0.214325loss in batch 178: 0.645477/0.216736loss in batch 179: 0.448181/0.218002loss in batch 180: 0.565796/0.21994loss in batch 181: 0.136398/0.219482loss in batch 182: 0.0624084/0.218613loss in batch 183: 0.108963/0.218018loss in batch 184: 0.24115/0.21814loss in batch 185: 0.0818787/0.217407loss in batch 186: 0.573578/0.219315loss in batch 187: 0.0448303/0.218384loss in batch 188: 0.0371552/0.217438loss in batch 189: 0.124313/0.216934loss in batch 190: 0.257401/0.217148loss in batch 191: 0.504288/0.218643loss in batch 192: 0.165436/0.218369loss in batch 193: 0.170212/0.218124loss in batch 194: 0.155487/0.217789loss in batch 195: 0.208588/0.217758loss in batch 196: 0.192032/0.217636loss in batch 197: 0.0580902/0.216812loss in batch 198: 0.125351/0.216354loss in batch 199: 0.046051/0.2155loss in batch 200: 0.772888/0.218277loss in batch 201: 0.0751495/0.217575loss in batch 202: 0.215912/0.21756loss in batch 203: 0.308746/0.218002loss in batch 204: 0.0609894/0.217255loss in batch 205: 0.751343/0.219833loss in batch 206: 0.131836/0.219406loss in batch 207: 0.283539/0.219727loss in batch 208: 0.0335236/0.218826loss in batch 209: 0.463257/0.220001loss in batch 210: 0.0948334/0.219406loss in batch 211: 0.0576324/0.218643loss in batch 212: 0.0471497/0.217834
done with epoch 6
train_acc: 0.924883 (394/426)
test loss: 0.0471496
acc: 0.923075 (132/143)
loss in batch 0: 0.135788/0.135788loss in batch 1: 0.0237732/0.0797729loss in batch 2: 0.006073/0.0552063loss in batch 3: 0.290192/0.113953loss in batch 4: 0.162048/0.123566loss in batch 5: 0.380951/0.166473loss in batch 6: 0.372574/0.195908loss in batch 7: 0.365326/0.217102loss in batch 8: 0.052002/0.198746loss in batch 9: 0.085907/0.187454loss in batch 10: 0.372101/0.204254loss in batch 11: 0.097168/0.195328loss in batch 12: 0.132462/0.190491loss in batch 13: 0.204987/0.191513loss in batch 14: 0.246521/0.19519loss in batch 15: 0.295929/0.201477loss in batch 16: 0.0810394/0.194397loss in batch 17: 0.331696/0.202026loss in batch 18: 0.14537/0.199051loss in batch 19: 0.265686/0.202377loss in batch 20: 0.182861/0.201447loss in batch 21: 0.493423/0.214722loss in batch 22: 0.228897/0.215332loss in batch 23: 0.189697/0.214264loss in batch 24: 0.0951996/0.209503loss in batch 25: 0.158905/0.20755loss in batch 26: 0.0419617/0.201416loss in batch 27: 0.0801392/0.197098loss in batch 28: 0.659729/0.213043loss in batch 29: 0.274979/0.215103loss in batch 30: 1.2645/0.248962loss in batch 31: 0.108826/0.244583loss in batch 32: 0.0748901/0.239441loss in batch 33: 0.0556183/0.234039loss in batch 34: 0.932205/0.253983loss in batch 35: 0.187683/0.252136loss in batch 36: 0.259155/0.252335loss in batch 37: 0.0641479/0.247375loss in batch 38: 0.392197/0.251099loss in batch 39: 0.245911/0.250961loss in batch 40: 0.216049/0.250122loss in batch 41: 0.575378/0.257858loss in batch 42: 0.0756531/0.253616loss in batch 43: 0.0782928/0.249634loss in batch 44: 0.849854/0.26297loss in batch 45: 0.0561523/0.258469loss in batch 46: 0.12236/0.255585loss in batch 47: 0.157654/0.25354loss in batch 48: 0.0828857/0.250061loss in batch 49: 0.160721/0.248276loss in batch 50: 0.369049/0.250641loss in batch 51: 0.0776825/0.247314loss in batch 52: 0.0552521/0.243698loss in batch 53: 0.108261/0.24118loss in batch 54: 0.308121/0.242401loss in batch 55: 0.259888/0.242706loss in batch 56: 0.458862/0.246506loss in batch 57: 0.167404/0.245132loss in batch 58: 0.432083/0.248306loss in batch 59: 0.165131/0.246918loss in batch 60: 0.0692444/0.244003loss in batch 61: 0.456451/0.247421loss in batch 62: 0.367325/0.249344loss in batch 63: 0.133301/0.247528loss in batch 64: 0.16333/0.246231loss in batch 65: 0.457718/0.249435loss in batch 66: 0.336136/0.250732loss in batch 67: 0.455856/0.253738loss in batch 68: 0.34964/0.255127loss in batch 69: 0.311722/0.255936loss in batch 70: 0.416046/0.258194loss in batch 71: 0.162399/0.256866loss in batch 72: 0.328125/0.257843loss in batch 73: 0.10791/0.255814loss in batch 74: 1.30492/0.269791loss in batch 75: 0.198669/0.26886loss in batch 76: 0.121536/0.266953loss in batch 77: 0.00198364/0.26355loss in batch 78: 0.896881/0.271576loss in batch 79: 0.0391083/0.268661loss in batch 80: 0.536026/0.271973loss in batch 81: 0.262161/0.271851loss in batch 82: 0.128006/0.270111loss in batch 83: 0.0992889/0.268082loss in batch 84: 0.0802765/0.265869loss in batch 85: 0.0408783/0.26326loss in batch 86: 0.0921478/0.261292loss in batch 87: 0.275482/0.261459loss in batch 88: 0.336365/0.262283loss in batch 89: 0.54538/0.265442loss in batch 90: 0.3992/0.266907loss in batch 91: 0.204193/0.266235loss in batch 92: 0.546021/0.269226loss in batch 93: 0.141403/0.267883loss in batch 94: 0.189194/0.267059loss in batch 95: 0.214325/0.266495loss in batch 96: 0.0410919/0.264175loss in batch 97: 0.557907/0.267166loss in batch 98: 0.0263519/0.26474loss in batch 99: 0.34137/0.265518loss in batch 100: 0.341736/0.266251loss in batch 101: 0.333176/0.266922loss in batch 102: 0.218552/0.266434loss in batch 103: 0.631912/0.269958loss in batch 104: 0.176895/0.269073loss in batch 105: 0.0157013/0.266678loss in batch 106: 0.0138855/0.264328loss in batch 107: 0.0410461/0.262253loss in batch 108: 0.298889/0.262589loss in batch 109: 0.456253/0.264359loss in batch 110: 0.140488/0.263229loss in batch 111: 0.0672913/0.26149loss in batch 112: 0.0674438/0.259766loss in batch 113: 0.0808411/0.258209loss in batch 114: 0.136536/0.257141loss in batch 115: 0.125854/0.256012loss in batch 116: 0.0743866/0.254456loss in batch 117: 0.0652771/0.252853loss in batch 118: 0.342545/0.253616loss in batch 119: 0.403931/0.254852loss in batch 120: 0.0898895/0.253494loss in batch 121: 0.0717773/0.252014loss in batch 122: 0.127762/0.250992loss in batch 123: 0.0587463/0.249451loss in batch 124: 0.0489655/0.247849loss in batch 125: 0.042572/0.246216loss in batch 126: 0.0449677/0.244629loss in batch 127: 0.0119019/0.242813loss in batch 128: 0.0881195/0.241608loss in batch 129: 0.134064/0.240784loss in batch 130: 0.351852/0.241638loss in batch 131: 0.120377/0.240707loss in batch 132: 0.156754/0.240082loss in batch 133: 0.0596313/0.238724loss in batch 134: 0.0133972/0.237061loss in batch 135: 0.316818/0.23764loss in batch 136: 0.422638/0.238998loss in batch 137: 0.054657/0.237656loss in batch 138: 0.147446/0.237015loss in batch 139: 0.131241/0.236267loss in batch 140: 0.0556641/0.23497loss in batch 141: 0.0883942/0.233948loss in batch 142: 0.142502/0.233307loss in batch 143: 0.488983/0.235092loss in batch 144: 0.0438232/0.233765loss in batch 145: 0.128174/0.233047loss in batch 146: 0.0802155/0.23201loss in batch 147: 0.0868835/0.231033loss in batch 148: 0.158981/0.230545loss in batch 149: 0.0731506/0.229492loss in batch 150: 0.0807953/0.2285loss in batch 151: 0.110291/0.227722loss in batch 152: 0.0183411/0.226364loss in batch 153: 0.0281982/0.225067loss in batch 154: 0.0283051/0.223801loss in batch 155: 0.277023/0.224152loss in batch 156: 0.105377/0.223389loss in batch 157: 0.0830078/0.222504loss in batch 158: 0.149307/0.222031loss in batch 159: 0.0262299/0.22081loss in batch 160: 0.0118713/0.219513loss in batch 161: 0.0618591/0.218536loss in batch 162: 0.0797272/0.217697loss in batch 163: 0.161194/0.217346loss in batch 164: 0.0562592/0.21637loss in batch 165: 0.220428/0.2164loss in batch 166: 0.34111/0.217148loss in batch 167: 0.0912018/0.216385loss in batch 168: 0.478561/0.217941loss in batch 169: 0.14151/0.217499loss in batch 170: 0.148895/0.217087loss in batch 171: 0.0277252/0.215988loss in batch 172: 0.0669403/0.215118loss in batch 173: 0.593445/0.2173loss in batch 174: 0.0986786/0.216629loss in batch 175: 0.0549927/0.215714loss in batch 176: 0.839523/0.219238loss in batch 177: 0.345428/0.21994loss in batch 178: 0.264877/0.2202loss in batch 179: 0.207092/0.220123loss in batch 180: 0.0348206/0.219101loss in batch 181: 0.183197/0.218903loss in batch 182: 0.120911/0.218369loss in batch 183: 0.133408/0.217911loss in batch 184: 0.0468903/0.21698loss in batch 185: 0.160751/0.216675loss in batch 186: 0.126373/0.216202loss in batch 187: 0.0305328/0.21521loss in batch 188: 0.0552368/0.214355loss in batch 189: 0.198044/0.214279loss in batch 190: 0.305634/0.214752loss in batch 191: 0.369949/0.215561loss in batch 192: 0.0128479/0.214508loss in batch 193: 0.0362854/0.213593loss in batch 194: 0.329468/0.214172loss in batch 195: 0.0155182/0.213165loss in batch 196: 0.0653229/0.212418loss in batch 197: 0.196014/0.212326loss in batch 198: 0.0540771/0.211548loss in batch 199: 0.0618896/0.2108loss in batch 200: 0.0270386/0.209885loss in batch 201: 0.349167/0.210571loss in batch 202: 0.0493317/0.209778loss in batch 203: 0.0381165/0.208939loss in batch 204: 0.132751/0.208557loss in batch 205: 0.0660553/0.20787loss in batch 206: 0.117905/0.207443loss in batch 207: 0.37088/0.208221loss in batch 208: 0.262054/0.208481loss in batch 209: 0.252167/0.208679loss in batch 210: 0.182602/0.208557loss in batch 211: 0.0853882/0.207993loss in batch 212: 0.0448151/0.207214
done with epoch 7
train_acc: 0.938967 (400/426)
test loss: 0.044815
acc: 0.937061 (134/143)
loss in batch 0: 0.0996857/0.0996857loss in batch 1: 0.355988/0.227829loss in batch 2: 0.595413/0.350357loss in batch 3: 0.134354/0.296356loss in batch 4: 0.0246582/0.24202loss in batch 5: 0.773621/0.330612loss in batch 6: 0.0329742/0.288101loss in batch 7: 0.347885/0.295563loss in batch 8: 0.0826111/0.271912loss in batch 9: 0.143188/0.259048loss in batch 10: 0.120621/0.246445loss in batch 11: 0.0332031/0.228683loss in batch 12: 0.060257/0.215729loss in batch 13: 0.115356/0.208557loss in batch 14: 0.0600891/0.198654loss in batch 15: 1.05623/0.252258loss in batch 16: 0.321777/0.256348loss in batch 17: 0.081192/0.246613loss in batch 18: 0.0875549/0.238235loss in batch 19: 0.050293/0.228851loss in batch 20: 0.286758/0.231613loss in batch 21: 0.158875/0.228302loss in batch 22: 0.0334778/0.219833loss in batch 23: 0.123093/0.21579loss in batch 24: 0.0429993/0.208878loss in batch 25: 0.16095/0.207031loss in batch 26: 0.072113/0.202042loss in batch 27: 0.0869293/0.197937loss in batch 28: 0.21077/0.19838loss in batch 29: 0.0412903/0.193146loss in batch 30: 0.0865326/0.189697loss in batch 31: 0.0494995/0.185318loss in batch 32: 0.169266/0.184845loss in batch 33: 0.0375977/0.180496loss in batch 34: 0.104095/0.178329loss in batch 35: 0.0687714/0.175278loss in batch 36: 0.210907/0.176239loss in batch 37: 0.234497/0.177765loss in batch 38: 0.159943/0.177322loss in batch 39: 0.237503/0.178818loss in batch 40: 0.0203552/0.174957loss in batch 41: 0.216217/0.175934loss in batch 42: 0.0409088/0.172806loss in batch 43: 0.283646/0.175323loss in batch 44: 0.272385/0.177475loss in batch 45: 0.150955/0.176895loss in batch 46: 0.146759/0.176254loss in batch 47: 0.289368/0.178619loss in batch 48: 0.0600128/0.176193loss in batch 49: 0.01297/0.172928loss in batch 50: 0.0717163/0.170944loss in batch 51: 0.220779/0.171906loss in batch 52: 0.0909424/0.17038loss in batch 53: 0.00656128/0.167343loss in batch 54: 0.633438/0.175812loss in batch 55: 0.0332489/0.173279loss in batch 56: 0.255981/0.174728loss in batch 57: 0.0755157/0.173004loss in batch 58: 0.0907135/0.171616loss in batch 59: 0.103302/0.170486loss in batch 60: 0.0752563/0.168915loss in batch 61: 0.0697021/0.167313loss in batch 62: 0.0553894/0.165543loss in batch 63: 0.0532684/0.163773loss in batch 64: 0.189468/0.164185loss in batch 65: 0.0934906/0.163101loss in batch 66: 0.0875397/0.161987loss in batch 67: 0.336029/0.164551loss in batch 68: 0.0593109/0.16301loss in batch 69: 0.0488739/0.161392loss in batch 70: 0.0388794/0.159653loss in batch 71: 0.216873/0.160461loss in batch 72: 0.0801544/0.159363loss in batch 73: 0.0229645/0.157501loss in batch 74: 0.0257111/0.155746loss in batch 75: 0.0361633/0.15419loss in batch 76: 0.344955/0.156647loss in batch 77: 0.624832/0.162659loss in batch 78: 1.35191/0.177719loss in batch 79: 0.0355225/0.175934loss in batch 80: 0.0596924/0.1745loss in batch 81: 0.0107727/0.172501loss in batch 82: 0.402252/0.175278loss in batch 83: 0.103027/0.174408loss in batch 84: 0.0743408/0.173233loss in batch 85: 0.245499/0.174088loss in batch 86: 0.0110474/0.172195loss in batch 87: 0.0487213/0.170807loss in batch 88: 0.403229/0.173416loss in batch 89: 0.229965/0.174042loss in batch 90: 0.101776/0.173248loss in batch 91: 0.309601/0.174728loss in batch 92: 0.16156/0.174576loss in batch 93: 0.330627/0.176239loss in batch 94: 0.084549/0.175278loss in batch 95: 0.286087/0.176422loss in batch 96: 0.405334/0.178802loss in batch 97: 0.381897/0.180862loss in batch 98: 0.157242/0.180618loss in batch 99: 0.155533/0.180374loss in batch 100: 0.0993195/0.179581loss in batch 101: 0.185608/0.179626loss in batch 102: 0.111588/0.17897loss in batch 103: 0.0482941/0.177719loss in batch 104: 0.440186/0.180222loss in batch 105: 0.017334/0.17868loss in batch 106: 0.0831299/0.17778loss in batch 107: 0.0760956/0.176849loss in batch 108: 0.171402/0.176804loss in batch 109: 0.109741/0.176178loss in batch 110: 0.0490875/0.175034loss in batch 111: 0.410385/0.177139loss in batch 112: 0.0538788/0.176041loss in batch 113: 0.10321/0.175415loss in batch 114: 0.0626678/0.174438loss in batch 115: 0.532379/0.177521loss in batch 116: 0.207336/0.17778loss in batch 117: 0.184448/0.177826loss in batch 118: 0.0495758/0.176743loss in batch 119: 0.231339/0.1772loss in batch 120: 0.367599/0.178772loss in batch 121: 0.131546/0.178391loss in batch 122: 0.118149/0.177902loss in batch 123: 0.0672302/0.177002loss in batch 124: 0.0998077/0.176392loss in batch 125: 0.195175/0.176544loss in batch 126: 0.129547/0.176178loss in batch 127: 0.122238/0.175735loss in batch 128: 0.0718689/0.174942loss in batch 129: 0.0673828/0.174118loss in batch 130: 0.26152/0.174774loss in batch 131: 0.143555/0.174545loss in batch 132: 0.222656/0.174896loss in batch 133: 0.125595/0.174545loss in batch 134: 0.0982361/0.173981loss in batch 135: 0.0398712/0.172989loss in batch 136: 0.0516052/0.172104loss in batch 137: 0.0153809/0.170959loss in batch 138: 0.470078/0.173126loss in batch 139: 0.22699/0.173508loss in batch 140: 0.46817/0.175598loss in batch 141: 0.089325/0.174988loss in batch 142: 0.754623/0.179047loss in batch 143: 0.357544/0.180283loss in batch 144: 0.0324554/0.17926loss in batch 145: 0.00683594/0.178085loss in batch 146: 0.137482/0.177795loss in batch 147: 0.0894623/0.1772loss in batch 148: 0.191589/0.177307loss in batch 149: 0.888321/0.182053loss in batch 150: 0.110657/0.181564loss in batch 151: 0.105011/0.181061loss in batch 152: 0.398666/0.18248loss in batch 153: 0.0529633/0.181656loss in batch 154: 0.0813446/0.181loss in batch 155: 1.38463/0.188721loss in batch 156: 0.429779/0.190262loss in batch 157: 0.25946/0.190689loss in batch 158: 0.0871887/0.190033loss in batch 159: 0.916748/0.19458loss in batch 160: 0.205933/0.194656loss in batch 161: 0.521622/0.196671loss in batch 162: 0.0990143/0.196075loss in batch 163: 0.413177/0.197403loss in batch 164: 0.426575/0.198776loss in batch 165: 0.117798/0.198288loss in batch 166: 0.066803/0.19751loss in batch 167: 0.0145264/0.196411loss in batch 168: 0.0973663/0.195831loss in batch 169: 0.0634766/0.195053loss in batch 170: 0.0648804/0.19429loss in batch 171: 0.0365143/0.193375loss in batch 172: 0.10997/0.192886loss in batch 173: 0.139099/0.192596loss in batch 174: 1.09618/0.197754loss in batch 175: 0.100525/0.197205loss in batch 176: 0.120071/0.196762loss in batch 177: 0.0453644/0.195908loss in batch 178: 0.178406/0.195801loss in batch 179: 0.0483551/0.194992loss in batch 180: 0.0437927/0.194168loss in batch 181: 0.0764923/0.193512loss in batch 182: 0.141449/0.193222loss in batch 183: 0.432877/0.194519loss in batch 184: 0.341522/0.195328loss in batch 185: 0.0218048/0.194382loss in batch 186: 0.0545197/0.193649loss in batch 187: 0.0803223/0.193039loss in batch 188: 0.319138/0.193695loss in batch 189: 0.489349/0.195251loss in batch 190: 0.174011/0.195145loss in batch 191: 0.128891/0.194794loss in batch 192: 0.482681/0.196289loss in batch 193: 0.240463/0.196518loss in batch 194: 0.542892/0.198303loss in batch 195: 0.0635376/0.197617loss in batch 196: 0.217941/0.197708loss in batch 197: 0.425186/0.198868loss in batch 198: 0.257904/0.199158loss in batch 199: 0.436722/0.200348loss in batch 200: 0.0150452/0.199432loss in batch 201: 0.206207/0.199463loss in batch 202: 0.0201721/0.198578loss in batch 203: 0.453552/0.199829loss in batch 204: 0.611526/0.201828loss in batch 205: 0.042572/0.201065loss in batch 206: 0.0479889/0.200333loss in batch 207: 0.150543/0.200089loss in batch 208: 0.376358/0.200928loss in batch 209: 0.0636292/0.200272loss in batch 210: 0.117874/0.199875loss in batch 211: 0.0532837/0.199203loss in batch 212: 0.184509/0.199127
done with epoch 8
train_acc: 0.934272 (398/426)
test loss: 0.184509
acc: 0.944054 (135/143)
loss in batch 0: 0.108551/0.108551loss in batch 1: 0.150146/0.129349loss in batch 2: 0.191864/0.150192loss in batch 3: 0.103226/0.138458loss in batch 4: 0.0192261/0.114594loss in batch 5: 0.0604553/0.105576loss in batch 6: 0.298035/0.133072loss in batch 7: 0.831238/0.220352loss in batch 8: 0.130707/0.210373loss in batch 9: 0.483353/0.237671loss in batch 10: 0.122421/0.227203loss in batch 11: 0.00958252/0.209076loss in batch 12: 0.125137/0.202621loss in batch 13: 0.0614929/0.19252loss in batch 14: 0.0250702/0.181366loss in batch 15: 0.199677/0.18251loss in batch 16: 0.334702/0.191467loss in batch 17: 0.145035/0.188889loss in batch 18: 0.419159/0.201004loss in batch 19: 0.0722198/0.194565loss in batch 20: 0.141602/0.192047loss in batch 21: 0.0233459/0.184372loss in batch 22: 0.114426/0.181335loss in batch 23: 0.0155792/0.174423loss in batch 24: 0.34285/0.181168loss in batch 25: 0.0426636/0.175842loss in batch 26: 0.371155/0.18306loss in batch 27: 0.569916/0.196884loss in batch 28: 0.0716858/0.192566loss in batch 29: 0.142059/0.190887loss in batch 30: 0.181381/0.190567loss in batch 31: 0.0220337/0.185318loss in batch 32: 0.400314/0.191833loss in batch 33: 0.0240631/0.18689loss in batch 34: 0.091629/0.184174loss in batch 35: 0.0294189/0.179871loss in batch 36: 0.140411/0.178818loss in batch 37: 0.231308/0.180176loss in batch 38: 0.267136/0.182419loss in batch 39: 0.138916/0.181335loss in batch 40: 0.0765228/0.178772loss in batch 41: 0.277557/0.181122loss in batch 42: 0.475555/0.187973loss in batch 43: 0.0473938/0.184784loss in batch 44: 0.0828247/0.18251loss in batch 45: 0.533646/0.19014loss in batch 46: 0.113083/0.188507loss in batch 47: 0.0869293/0.186386loss in batch 48: 0.0596161/0.183792loss in batch 49: 0.104019/0.18222loss in batch 50: 0.0662994/0.179932loss in batch 51: 0.270874/0.181686loss in batch 52: 0.203384/0.182098loss in batch 53: 0.0599518/0.179825loss in batch 54: 0.0635376/0.177719loss in batch 55: 0.293686/0.179794loss in batch 56: 0.0535278/0.177582loss in batch 57: 0.389893/0.181229loss in batch 58: 0.4048/0.185028loss in batch 59: 0.171661/0.184799loss in batch 60: 0.325516/0.187103loss in batch 61: 0.0630188/0.185104loss in batch 62: 0.0405273/0.182816loss in batch 63: 0.0146027/0.180191loss in batch 64: 0.191589/0.180359loss in batch 65: 0.440613/0.184311loss in batch 66: 0.215958/0.184769loss in batch 67: 0.0857239/0.183319loss in batch 68: 0.113922/0.182312loss in batch 69: 0.0581512/0.180542loss in batch 70: 0.279831/0.181946loss in batch 71: 0.151047/0.181503loss in batch 72: 0.0429688/0.179611loss in batch 73: 0.0409851/0.177734loss in batch 74: 0.0620117/0.176193loss in batch 75: 0.52652/0.180801loss in batch 76: 0.254013/0.181763loss in batch 77: 0.994781/0.192169loss in batch 78: 0.0775299/0.190735loss in batch 79: 0.404572/0.19339loss in batch 80: 0.0130157/0.191162loss in batch 81: 0.216324/0.191467loss in batch 82: 0.0864563/0.190201loss in batch 83: 0.426407/0.193024loss in batch 84: 0.502075/0.196655loss in batch 85: 0.197052/0.196671loss in batch 86: 0.0457306/0.194931loss in batch 87: 0.261276/0.195694loss in batch 88: 0.142639/0.195084loss in batch 89: 0.0771027/0.193771loss in batch 90: 0.455856/0.196655loss in batch 91: 0.0543671/0.195099loss in batch 92: 0.0658417/0.19371loss in batch 93: 0.0350342/0.192032loss in batch 94: 0.0957642/0.19101loss in batch 95: 0.0304871/0.189346loss in batch 96: 0.107147/0.188492loss in batch 97: 0.210678/0.188736loss in batch 98: 0.0888214/0.187714loss in batch 99: 1.19604/0.1978loss in batch 100: 0.323944/0.199036loss in batch 101: 0.064209/0.197723loss in batch 102: 0.0219574/0.196014loss in batch 103: 0.0308838/0.194427loss in batch 104: 0.0217743/0.19278loss in batch 105: 0.306213/0.193848loss in batch 106: 0.0533447/0.192551loss in batch 107: 0.0526123/0.191254loss in batch 108: 0.0886993/0.190308loss in batch 109: 0.140671/0.18985loss in batch 110: 0.0292053/0.188416loss in batch 111: 0.0825195/0.187469loss in batch 112: 0.0370026/0.186127loss in batch 113: 0.117371/0.185532loss in batch 114: 0.217697/0.185806loss in batch 115: 0.0930328/0.185013loss in batch 116: 0.0540771/0.183884loss in batch 117: 0.0637207/0.182861loss in batch 118: 0.0375366/0.181656loss in batch 119: 0.0123901/0.180237loss in batch 120: 0.00665283/0.178802loss in batch 121: 0.749283/0.183487loss in batch 122: 0.354233/0.18486loss in batch 123: 0.330948/0.18605loss in batch 124: 0.0726013/0.18515loss in batch 125: 0.0579834/0.184128loss in batch 126: 0.059082/0.183151loss in batch 127: 0.11557/0.182617loss in batch 128: 0.272064/0.183319loss in batch 129: 0.0480804/0.182266loss in batch 130: 0.0776367/0.181473loss in batch 131: 0.160416/0.18132loss in batch 132: 0.226913/0.181656loss in batch 133: 0.36348/0.183014loss in batch 134: 0.314255/0.183975loss in batch 135: 0.0399628/0.182922loss in batch 136: 0.0963135/0.182297loss in batch 137: 0.231964/0.182663loss in batch 138: 0.0380859/0.18161loss in batch 139: 0.131622/0.181259loss in batch 140: 0.643478/0.18454loss in batch 141: 0.0369415/0.183502loss in batch 142: 0.207703/0.18367loss in batch 143: 0.0734558/0.182907loss in batch 144: 0.191849/0.182968loss in batch 145: 0.0625916/0.182129loss in batch 146: 0.0637665/0.181335loss in batch 147: 0.140289/0.181061loss in batch 148: 0.181549/0.181061loss in batch 149: 0.0582581/0.180237loss in batch 150: 0.0118256/0.179123loss in batch 151: 0.257126/0.179626loss in batch 152: 0.0141602/0.178558loss in batch 153: 0.0907593/0.177979loss in batch 154: 0.0471497/0.177139loss in batch 155: 0.0121918/0.176086loss in batch 156: 0.101974/0.175613loss in batch 157: 0.0562744/0.174866loss in batch 158: 0.547516/0.1772loss in batch 159: 0.495575/0.179184loss in batch 160: 0.065155/0.178482loss in batch 161: 0.0540619/0.177719loss in batch 162: 0.0540619/0.176956loss in batch 163: 0.302917/0.177719loss in batch 164: 0.211258/0.177933loss in batch 165: 0.0624847/0.177231loss in batch 166: 0.194031/0.177338loss in batch 167: 0.83017/0.181213loss in batch 168: 0.0724335/0.180573loss in batch 169: 0.371582/0.181702loss in batch 170: 0.964355/0.186279loss in batch 171: 0.294067/0.186905loss in batch 172: 0.0956573/0.186371loss in batch 173: 0.0467834/0.185577loss in batch 174: 0.310104/0.186279loss in batch 175: 0.352875/0.187225loss in batch 176: 0.00718689/0.186218loss in batch 177: 0.0680389/0.185547loss in batch 178: 0.0554047/0.184814loss in batch 179: 0.0615234/0.184143loss in batch 180: 0.347748/0.185043loss in batch 181: 0.169052/0.184952loss in batch 182: 0.0637817/0.184296loss in batch 183: 0.234253/0.18457loss in batch 184: 1.10954/0.18956loss in batch 185: 0.0545654/0.188828loss in batch 186: 0.268646/0.189255loss in batch 187: 0.236481/0.189514loss in batch 188: 0.140274/0.189255loss in batch 189: 0.0853271/0.188705loss in batch 190: 0.156921/0.188538loss in batch 191: 0.342743/0.189346loss in batch 192: 0.230682/0.18956loss in batch 193: 0.151321/0.189362loss in batch 194: 0.374161/0.190308loss in batch 195: 0.271698/0.190735loss in batch 196: 0.0747833/0.19014loss in batch 197: 0.0361786/0.189362loss in batch 198: 0.167068/0.18924loss in batch 199: 0.0201874/0.1884loss in batch 200: 0.0810394/0.187866loss in batch 201: 0.153976/0.187683loss in batch 202: 0.0927277/0.187225loss in batch 203: 0.3815/0.188187loss in batch 204: 0.0872955/0.187683loss in batch 205: 0.200439/0.187744loss in batch 206: 0.0602875/0.187134loss in batch 207: 0.693237/0.189575loss in batch 208: 0.0611115/0.18895loss in batch 209: 0.413193/0.190018loss in batch 210: 0.443954/0.191223loss in batch 211: 0.0253754/0.190445loss in batch 212: 0.358459/0.191238
done with epoch 9
train_acc: 0.941315 (401/426)
test loss: 0.358459
acc: 0.944054 (135/143)
loss in batch 0: 0.00987244/0.00987244loss in batch 1: 0.111877/0.0608673loss in batch 2: 0.57901/0.233582loss in batch 3: 1.40714/0.526978loss in batch 4: 0.114044/0.444382loss in batch 5: 0.0807495/0.383789loss in batch 6: 0.237274/0.362854loss in batch 7: 0.0571594/0.324646loss in batch 8: 0.0088501/0.289551loss in batch 9: 0.0762482/0.268234loss in batch 10: 0.0602264/0.249313loss in batch 11: 0.053894/0.233032loss in batch 12: 0.245621/0.234009loss in batch 13: 0.0762177/0.222717loss in batch 14: 0.064743/0.212189loss in batch 15: 0.0607758/0.202728loss in batch 16: 0.142471/0.199188loss in batch 17: 0.419098/0.211395loss in batch 18: 0.344559/0.218414loss in batch 19: 1.1288/0.263931loss in batch 20: 0.0181122/0.252228loss in batch 21: 0.182587/0.249054loss in batch 22: 0.446106/0.257629loss in batch 23: 0.458755/0.266006loss in batch 24: 0.0397644/0.256958loss in batch 25: 0.0447693/0.248795loss in batch 26: 0.380585/0.253677loss in batch 27: 0.0400391/0.246048loss in batch 28: 0.134018/0.242188loss in batch 29: 0.670792/0.25647loss in batch 30: 0.0736237/0.25058loss in batch 31: 0.678421/0.263931loss in batch 32: 0.0863953/0.25856loss in batch 33: 1.04622/0.281723loss in batch 34: 0.19455/0.279236loss in batch 35: 0.192657/0.276825loss in batch 36: 0.0222321/0.269943loss in batch 37: 0.539352/0.277054loss in batch 38: 0.130676/0.273285loss in batch 39: 0.151855/0.270248loss in batch 40: 0.156174/0.267471loss in batch 41: 0.0340118/0.261902loss in batch 42: 0.243561/0.26149loss in batch 43: 0.267853/0.261627loss in batch 44: 0.047226/0.256866loss in batch 45: 0.0772095/0.25296loss in batch 46: 0.073761/0.249146loss in batch 47: 0.0766907/0.24556loss in batch 48: 0.154861/0.243713loss in batch 49: 0.0159912/0.239151loss in batch 50: 0.245041/0.239258loss in batch 51: 0.054306/0.235718loss in batch 52: 0.594086/0.242477loss in batch 53: 0.435074/0.246033loss in batch 54: 0.0933533/0.243271loss in batch 55: 0.192764/0.242355loss in batch 56: 0.0739594/0.239395loss in batch 57: 0.0548859/0.236221loss in batch 58: 0.343185/0.238037loss in batch 59: 0.312164/0.239273loss in batch 60: 0.218918/0.238937loss in batch 61: 0.23172/0.238815loss in batch 62: 0.312775/0.23999loss in batch 63: 0.0870667/0.23761loss in batch 64: 0.0750885/0.235107loss in batch 65: 0.0279846/0.231964loss in batch 66: 0.29718/0.232941loss in batch 67: 0.039917/0.230103loss in batch 68: 0.44989/0.233292loss in batch 69: 0.0818176/0.231125loss in batch 70: 0.0993042/0.229279loss in batch 71: 0.0571899/0.226883loss in batch 72: 0.0590668/0.224579loss in batch 73: 0.0501404/0.222214loss in batch 74: 0.129868/0.220978loss in batch 75: 0.0172119/0.218307loss in batch 76: 0.0394135/0.215988loss in batch 77: 0.0767975/0.214188loss in batch 78: 0.0536652/0.212173loss in batch 79: 0.0712433/0.210403loss in batch 80: 0.230392/0.210648loss in batch 81: 0.0248413/0.208389loss in batch 82: 0.268768/0.209106loss in batch 83: 0.066925/0.207413loss in batch 84: 0.267776/0.20813loss in batch 85: 0.0450287/0.206238loss in batch 86: 0.035614/0.204269loss in batch 87: 0.0865326/0.202942loss in batch 88: 0.285202/0.203873loss in batch 89: 0.0908356/0.202606loss in batch 90: 0.983658/0.211182loss in batch 91: 0.0758209/0.209717loss in batch 92: 0.178024/0.209366loss in batch 93: 0.308823/0.210434loss in batch 94: 0.0709381/0.208969loss in batch 95: 0.0422516/0.20723loss in batch 96: 0.018692/0.205292loss in batch 97: 0.352966/0.206787loss in batch 98: 0.0248566/0.204956loss in batch 99: 0.0465698/0.203369loss in batch 100: 0.108505/0.202438loss in batch 101: 0.111298/0.201538loss in batch 102: 0.195114/0.201477loss in batch 103: 0.0671234/0.20018loss in batch 104: 0.0916901/0.199142loss in batch 105: 0.312927/0.200226loss in batch 106: 0.033371/0.198654loss in batch 107: 0.226105/0.198914loss in batch 108: 0.112503/0.19812loss in batch 109: 0.181824/0.197983loss in batch 110: 0.0313721/0.196472loss in batch 111: 0.531586/0.199463loss in batch 112: 0.0279083/0.197952loss in batch 113: 0.0161285/0.196365loss in batch 114: 0.0641937/0.195206loss in batch 115: 0.0654449/0.194092loss in batch 116: 0.072998/0.193054loss in batch 117: 0.141327/0.192612loss in batch 118: 0.0685425/0.191559loss in batch 119: 0.812927/0.196747loss in batch 120: 0.214523/0.196899loss in batch 121: 0.0735016/0.195877loss in batch 122: 0.0390167/0.194595loss in batch 123: 0.3004/0.19545loss in batch 124: 0.0229034/0.194077loss in batch 125: 0.0121613/0.192642loss in batch 126: 0.0262146/0.19133loss in batch 127: 0.138077/0.190903loss in batch 128: 0.0590057/0.189896loss in batch 129: 0.104736/0.18924loss in batch 130: 0.105896/0.188599loss in batch 131: 0.0947113/0.187881loss in batch 132: 0.0140533/0.186584loss in batch 133: 0.0487366/0.185547loss in batch 134: 0.49614/0.187851loss in batch 135: 0.670334/0.191406loss in batch 136: 0.24147/0.191772loss in batch 137: 0.0399933/0.190674loss in batch 138: 0.0377655/0.189575loss in batch 139: 0.30835/0.190414loss in batch 140: 0.263184/0.190918loss in batch 141: 0.00946045/0.189651loss in batch 142: 0.0558472/0.188721loss in batch 143: 0.0307007/0.187622loss in batch 144: 0.108673/0.187073loss in batch 145: 0.0865631/0.186386loss in batch 146: 0.0574341/0.185501loss in batch 147: 0.0424957/0.18454loss in batch 148: 0.0643616/0.183731loss in batch 149: 0.0160065/0.182617loss in batch 150: 0.163864/0.182495loss in batch 151: 0.299606/0.183258loss in batch 152: 0.0123749/0.182144loss in batch 153: 0.0636749/0.181381loss in batch 154: 0.125/0.181015loss in batch 155: 0.114655/0.180588loss in batch 156: 0.384781/0.181885loss in batch 157: 0.0326233/0.180939loss in batch 158: 0.0810547/0.180313loss in batch 159: 0.00405884/0.179214loss in batch 160: 0.88298/0.183594loss in batch 161: 0.0692596/0.182877loss in batch 162: 0.133331/0.182571loss in batch 163: 0.0847015/0.181976loss in batch 164: 0.0513763/0.181198loss in batch 165: 0.0313416/0.180283loss in batch 166: 0.0274506/0.179367loss in batch 167: 0.290359/0.180038loss in batch 168: 0.118103/0.179672loss in batch 169: 0.297928/0.180359loss in batch 170: 0.25592/0.180801loss in batch 171: 0.114731/0.18042loss in batch 172: 0.0493622/0.179657loss in batch 173: 0.576218/0.181931loss in batch 174: 0.167877/0.18187loss in batch 175: 0.02565/0.180984loss in batch 176: 0.0653687/0.180313loss in batch 177: 0.233505/0.180618loss in batch 178: 0.203003/0.18074loss in batch 179: 0.0869904/0.180222loss in batch 180: 0.111771/0.179855loss in batch 181: 0.0189209/0.178955loss in batch 182: 0.0259857/0.178131loss in batch 183: 0.35527/0.179092loss in batch 184: 0.234543/0.179382loss in batch 185: 0.0860901/0.178879loss in batch 186: 0.0812988/0.17836loss in batch 187: 0.0697937/0.17778loss in batch 188: 0.903122/0.181625loss in batch 189: 0.229446/0.18187loss in batch 190: 0.191223/0.181915loss in batch 191: 0.158859/0.181808loss in batch 192: 0.109222/0.181427loss in batch 193: 0.0678101/0.180847loss in batch 194: 0.207382/0.180984loss in batch 195: 0.0279236/0.180191loss in batch 196: 0.392746/0.181274loss in batch 197: 0.585678/0.183319loss in batch 198: 0.0949097/0.182877loss in batch 199: 0.0348663/0.182144loss in batch 200: 0.00872803/0.181274loss in batch 201: 0.636887/0.183517loss in batch 202: 0.172043/0.183472loss in batch 203: 0.0227356/0.182678loss in batch 204: 0.0553284/0.182068loss in batch 205: 0.0260773/0.181305loss in batch 206: 0.270538/0.181732loss in batch 207: 0.604279/0.183762loss in batch 208: 0.184769/0.183777loss in batch 209: 0.073349/0.183243loss in batch 210: 0.0495453/0.182617loss in batch 211: 0.535599/0.18428loss in batch 212: 0.496628/0.185745
done with epoch 10
train_acc: 0.93662 (399/426)
test loss: 0.496627
acc: 0.937061 (134/143)
loss in batch 0: 0.0471802/0.0471802loss in batch 1: 0.0969391/0.072052loss in batch 2: 0.0613098/0.0684814loss in batch 3: 0.0262451/0.0579224loss in batch 4: 0.187149/0.0837708loss in batch 5: 0.104919/0.0872803loss in batch 6: 0.524979/0.149811loss in batch 7: 0.0712433/0.139999loss in batch 8: 0.0240021/0.127106loss in batch 9: 1.08182/0.22258loss in batch 10: 0.0157623/0.203781loss in batch 11: 0.0556793/0.191437loss in batch 12: 0.148987/0.188171loss in batch 13: 0.14035/0.184753loss in batch 14: 0.0152893/0.173447loss in batch 15: 0.0647125/0.166672loss in batch 16: 0.21051/0.169235loss in batch 17: 0.0145721/0.16066loss in batch 18: 0.127029/0.158875loss in batch 19: 0.62294/0.182083loss in batch 20: 0.0734558/0.17691loss in batch 21: 0.065918/0.17186loss in batch 22: 0.209763/0.173523loss in batch 23: 0.0539703/0.168533loss in batch 24: 0.163574/0.168335loss in batch 25: 0.288818/0.172974loss in batch 26: 0.0578003/0.168701loss in batch 27: 0.714661/0.188202loss in batch 28: 0.125351/0.186035loss in batch 29: 0.310638/0.190186loss in batch 30: 0.0497284/0.185654loss in batch 31: 0.0379944/0.181046loss in batch 32: 0.0139923/0.175964loss in batch 33: 0.00639343/0.17099loss in batch 34: 0.426117/0.178284loss in batch 35: 0.0632477/0.175095loss in batch 36: 0.0217896/0.170944loss in batch 37: 0.0667572/0.168198loss in batch 38: 0.26651/0.170731loss in batch 39: 0.173294/0.170792loss in batch 40: 0.157608/0.170456loss in batch 41: 0.115784/0.169159loss in batch 42: 0.0988464/0.167526loss in batch 43: 0.0321198/0.164444loss in batch 44: 0.0922089/0.162857loss in batch 45: 0.158035/0.162735loss in batch 46: 0.274445/0.165115loss in batch 47: 0.119293/0.164169loss in batch 48: 0.331039/0.167572loss in batch 49: 0.55513/0.175308loss in batch 50: 0.119431/0.174225loss in batch 51: 0.0145874/0.171158loss in batch 52: 0.0309601/0.168518loss in batch 53: 0.113968/0.167496loss in batch 54: 0.0930328/0.166138loss in batch 55: 0.104431/0.165054loss in batch 56: 0.0297852/0.162674loss in batch 57: 0.122284/0.161972loss in batch 58: 0.335556/0.164917loss in batch 59: 0.362808/0.168213loss in batch 60: 0.325699/0.170792loss in batch 61: 0.032074/0.168549loss in batch 62: 0.160156/0.168427loss in batch 63: 0.0566101/0.166687loss in batch 64: 0.590271/0.173203loss in batch 65: 0.187515/0.173416loss in batch 66: 0.0563812/0.171661loss in batch 67: 0.221695/0.172394loss in batch 68: 0.243866/0.173447loss in batch 69: 0.0950012/0.172318loss in batch 70: 0.0938721/0.171219loss in batch 71: 0.0337524/0.169312loss in batch 72: 0.00212097/0.167007loss in batch 73: 0.15564/0.166855loss in batch 74: 0.224655/0.167618loss in batch 75: 0.127045/0.167099loss in batch 76: 0.0272675/0.165268loss in batch 77: 0.0980682/0.164429loss in batch 78: 0.149414/0.164215loss in batch 79: 0.868332/0.173035loss in batch 80: 0.0308228/0.171265loss in batch 81: 0.174149/0.17131loss in batch 82: 0.0218353/0.169495loss in batch 83: 0.541946/0.17395loss in batch 84: 0.0878448/0.172928loss in batch 85: 0.0638885/0.171661loss in batch 86: 0.247025/0.172516loss in batch 87: 0.0691223/0.171356loss in batch 88: 0.107727/0.170624loss in batch 89: 0.107239/0.169937loss in batch 90: 0.111038/0.169281loss in batch 91: 0.0265045/0.167725loss in batch 92: 0.0363159/0.166321loss in batch 93: 0.0115051/0.164673loss in batch 94: 0.416916/0.167328loss in batch 95: 0.0750275/0.166367loss in batch 96: 0.239014/0.167114loss in batch 97: 0.0738373/0.166168loss in batch 98: 0.083374/0.165329loss in batch 99: 0.314224/0.166809loss in batch 100: 0.0128479/0.165298loss in batch 101: 0.159866/0.165237loss in batch 102: 0.189316/0.165482loss in batch 103: 0.0852814/0.164703loss in batch 104: 0.0589752/0.163696loss in batch 105: 0.0543976/0.162674loss in batch 106: 0.0801697/0.16188loss in batch 107: 0.0454102/0.160812loss in batch 108: 0.0492401/0.15979loss in batch 109: 0.0824738/0.159088loss in batch 110: 0.0955811/0.158508loss in batch 111: 0.0201721/0.157288loss in batch 112: 0.156433/0.157272loss in batch 113: 0.0230255/0.156097loss in batch 114: 0.0492706/0.155167loss in batch 115: 0.0430145/0.15419loss in batch 116: 0.0943146/0.153687loss in batch 117: 0.0140076/0.152496loss in batch 118: 0.147598/0.152466loss in batch 119: 0.164673/0.152557loss in batch 120: 0.0744629/0.151917loss in batch 121: 0.55542/0.155212loss in batch 122: 0.0764618/0.154572loss in batch 123: 0.115952/0.154282loss in batch 124: 0.0575714/0.153503loss in batch 125: 0.0280609/0.152496loss in batch 126: 0.406616/0.15451loss in batch 127: 0.10202/0.154099loss in batch 128: 0.0482025/0.153275loss in batch 129: 0.0913086/0.152786loss in batch 130: 0.29158/0.15387loss in batch 131: 0.311127/0.155045loss in batch 132: 0.108704/0.154694loss in batch 133: 0.0932159/0.154251loss in batch 134: 0.504761/0.156845loss in batch 135: 0.0142822/0.155792loss in batch 136: 1.08675/0.162582loss in batch 137: 0.00709534/0.161453loss in batch 138: 0.0519562/0.160675loss in batch 139: 0.0228424/0.159698loss in batch 140: 0.00880432/0.158615loss in batch 141: 0.130478/0.158417loss in batch 142: 0.124924/0.158188loss in batch 143: 0.19046/0.158417loss in batch 144: 0.046051/0.157623loss in batch 145: 0.0594482/0.156952loss in batch 146: 0.099411/0.15657loss in batch 147: 0.736923/0.160492loss in batch 148: 0.263885/0.161179loss in batch 149: 0.113968/0.160873loss in batch 150: 0.340027/0.162048loss in batch 151: 0.0419006/0.16127loss in batch 152: 0.211487/0.161591loss in batch 153: 0.078476/0.161057loss in batch 154: 0.342758/0.162216loss in batch 155: 0.325668/0.163284loss in batch 156: 0.611847/0.166122loss in batch 157: 0.154022/0.166061loss in batch 158: 0.0179901/0.165131loss in batch 159: 0.239777/0.165588loss in batch 160: 0.494171/0.167633loss in batch 161: 0.0536957/0.166931loss in batch 162: 0.387207/0.168289loss in batch 163: 0.0950317/0.167831loss in batch 164: 0.0153351/0.166901loss in batch 165: 0.113663/0.16658loss in batch 166: 0.431427/0.168167loss in batch 167: 0.0466003/0.16745loss in batch 168: 0.368286/0.168625loss in batch 169: 0.103912/0.168243loss in batch 170: 0.235458/0.16864loss in batch 171: 0.191498/0.168793loss in batch 172: 0.0642395/0.168182loss in batch 173: 0.317825/0.169037loss in batch 174: 0.114151/0.168716loss in batch 175: 0.274521/0.169327loss in batch 176: 0.0347137/0.168564loss in batch 177: 0.338699/0.16951loss in batch 178: 0.539078/0.171585loss in batch 179: 0.321213/0.172424loss in batch 180: 0.0399628/0.171677loss in batch 181: 0.0472412/0.171005loss in batch 182: 0.233459/0.171341loss in batch 183: 0.0290222/0.170563loss in batch 184: 0.0080719/0.169693loss in batch 185: 0.341248/0.170609loss in batch 186: 0.924561/0.174652loss in batch 187: 0.00999451/0.173767loss in batch 188: 0.0614319/0.173187loss in batch 189: 0.0253448/0.172394loss in batch 190: 0.1716/0.172394loss in batch 191: 0.534988/0.174286loss in batch 192: 0.0371552/0.173569loss in batch 193: 0.205795/0.173737loss in batch 194: 0.101471/0.17337loss in batch 195: 0.0131073/0.172546loss in batch 196: 0.711029/0.175278loss in batch 197: 0.162766/0.175217loss in batch 198: 0.115524/0.174911loss in batch 199: 0.59938/0.177032loss in batch 200: 0.03685/0.176346loss in batch 201: 0.049408/0.175705loss in batch 202: 0.0401764/0.175049loss in batch 203: 0.2341/0.175339loss in batch 204: 0.295563/0.175934loss in batch 205: 1.62489/0.182953loss in batch 206: 0.13501/0.182724loss in batch 207: 0.0584717/0.182129loss in batch 208: 0.0425873/0.181473loss in batch 209: 0.111115/0.181122loss in batch 210: 0.625854/0.183228loss in batch 211: 0.105469/0.182861loss in batch 212: 0.238892/0.183121
done with epoch 11
train_acc: 0.938967 (400/426)
test loss: 0.238891
acc: 0.944054 (135/143)
loss in batch 0: 0.0666962/0.0666962loss in batch 1: 0.0131378/0.039917loss in batch 2: 0.5336/0.204468loss in batch 3: 0.027359/0.160202loss in batch 4: 0.0414581/0.136444loss in batch 5: 0.472946/0.192535loss in batch 6: 0.102005/0.179596loss in batch 7: 0.414963/0.20903loss in batch 8: 0.564499/0.24852loss in batch 9: 0.0242004/0.226074loss in batch 10: 0.0718231/0.212067loss in batch 11: 0.180771/0.209457loss in batch 12: 0.0597687/0.197952loss in batch 13: 0.223587/0.199768loss in batch 14: 0.500748/0.219833loss in batch 15: 0.0325928/0.20813loss in batch 16: 0.128448/0.203445loss in batch 17: 0.0348358/0.194077loss in batch 18: 0.243362/0.196671loss in batch 19: 0.0632477/0.190002loss in batch 20: 0.0202637/0.181915loss in batch 21: 0.340256/0.189117loss in batch 22: 0.0716858/0.184021loss in batch 23: 0.0417175/0.178085loss in batch 24: 0.0781708/0.174088loss in batch 25: 0.260986/0.177429loss in batch 26: 0.0809479/0.173859loss in batch 27: 0.370056/0.180862loss in batch 28: 0.554108/0.193741loss in batch 29: 0.05159/0.188995loss in batch 30: 0.0101471/0.183212loss in batch 31: 0.0394135/0.178741loss in batch 32: 0.496796/0.18837loss in batch 33: 0.366684/0.193604loss in batch 34: 0.0356293/0.189102loss in batch 35: 0.10051/0.186646loss in batch 36: 0.0436249/0.182785loss in batch 37: 0.0761871/0.179962loss in batch 38: 0.501923/0.188217loss in batch 39: 0.0459442/0.184662loss in batch 40: 0.608765/0.195023loss in batch 41: 0.320465/0.197998loss in batch 42: 0.0734558/0.195099loss in batch 43: 0.199066/0.19519loss in batch 44: 0.0186615/0.191269loss in batch 45: 0.0280457/0.187714loss in batch 46: 0.0151062/0.184036loss in batch 47: 0.0850067/0.181992loss in batch 48: 0.0854797/0.180008loss in batch 49: 0.0376434/0.177155loss in batch 50: 0.00302124/0.173752loss in batch 51: 0.260361/0.175415loss in batch 52: 0.275192/0.177307loss in batch 53: 0.0635071/0.175201loss in batch 54: 0.369843/0.178741loss in batch 55: 0.109253/0.17749loss in batch 56: 0.120605/0.176498loss in batch 57: 0.230469/0.177429loss in batch 58: 0.0920105/0.17598loss in batch 59: 1.28148/0.194397loss in batch 60: 0.284943/0.195877loss in batch 61: 0.0115662/0.192902loss in batch 62: 0.019928/0.19017loss in batch 63: 0.013855/0.187408loss in batch 64: 0.0292053/0.184982loss in batch 65: 0.152649/0.184494loss in batch 66: 0.0685577/0.182755loss in batch 67: 0.0700378/0.181091loss in batch 68: 0.196991/0.181335loss in batch 69: 0.274048/0.182663loss in batch 70: 0.337143/0.184845loss in batch 71: 0.424484/0.188156loss in batch 72: 0.0129089/0.18576loss in batch 73: 0.309601/0.187439loss in batch 74: 0.563766/0.192459loss in batch 75: 0.0950012/0.191177loss in batch 76: 0.0966339/0.189941loss in batch 77: 0.111343/0.188934loss in batch 78: 0.281326/0.190109loss in batch 79: 0.067215/0.188568loss in batch 80: 0.248383/0.189301loss in batch 81: 0.0283203/0.187332loss in batch 82: 0.28894/0.188568loss in batch 83: 0.0838623/0.187317loss in batch 84: 0.0791321/0.18605loss in batch 85: 0.213608/0.186371loss in batch 86: 0.425568/0.189117loss in batch 87: 0.0645905/0.187698loss in batch 88: 0.037674/0.18602loss in batch 89: 0.0193329/0.184158loss in batch 90: 0.0527344/0.182724loss in batch 91: 0.0395203/0.181168loss in batch 92: 0.0766144/0.180038loss in batch 93: 1.38347/0.192841loss in batch 94: 0.122116/0.192108loss in batch 95: 0.106598/0.191208loss in batch 96: 0.0361023/0.189621loss in batch 97: 0.345367/0.191193loss in batch 98: 0.892426/0.198273loss in batch 99: 0.366013/0.199951loss in batch 100: 0.127548/0.199249loss in batch 101: 0.0109711/0.197388loss in batch 102: 0.0711212/0.196167loss in batch 103: 0.0296326/0.194565loss in batch 104: 0.471588/0.197205loss in batch 105: 0.425919/0.199371loss in batch 106: 0.0657501/0.198105loss in batch 107: 0.0709991/0.19693loss in batch 108: 0.359634/0.198425loss in batch 109: 0.108917/0.197617loss in batch 110: 0.0423279/0.196213loss in batch 111: 0.0339508/0.194763loss in batch 112: 0.0471344/0.193466loss in batch 113: 0.0467377/0.192169loss in batch 114: 0.210556/0.192322loss in batch 115: 0.146362/0.191925loss in batch 116: 0.016571/0.190445loss in batch 117: 0.0128632/0.188934loss in batch 118: 0.423523/0.190903loss in batch 119: 0.0195007/0.189484loss in batch 120: 0.115112/0.188858loss in batch 121: 0.162567/0.188644loss in batch 122: 0.0407715/0.187454loss in batch 123: 0.108307/0.186813loss in batch 124: 0.0260773/0.185532loss in batch 125: 0.0503387/0.184448loss in batch 126: 0.232727/0.18483loss in batch 127: 0.118652/0.184311loss in batch 128: 0.0458527/0.183243loss in batch 129: 0.0766907/0.182419loss in batch 130: 0.410126/0.184158loss in batch 131: 1.4274/0.193573loss in batch 132: 0.04599/0.192459loss in batch 133: 0.0167236/0.191162loss in batch 134: 0.0660858/0.190231loss in batch 135: 0.0731506/0.189377loss in batch 136: 0.172394/0.18924loss in batch 137: 0.0242157/0.188049loss in batch 138: 0.0885925/0.187332loss in batch 139: 0.140533/0.186996loss in batch 140: 0.171555/0.18689loss in batch 141: 0.093338/0.186234loss in batch 142: 0.0349731/0.185181loss in batch 143: 0.182968/0.18515loss in batch 144: 0.101746/0.184586loss in batch 145: 0.0870056/0.183899loss in batch 146: 0.0280304/0.182846loss in batch 147: 0.151459/0.182648loss in batch 148: 0.0436554/0.181717loss in batch 149: 0.400803/0.183167loss in batch 150: 0.10228/0.182632loss in batch 151: 0.011795/0.181503loss in batch 152: 0.0365143/0.180557loss in batch 153: 0.0279694/0.179565loss in batch 154: 0.0807343/0.17894loss in batch 155: 0.666824/0.182068loss in batch 156: 0.624786/0.184875loss in batch 157: 0.0460205/0.184006loss in batch 158: 0.484436/0.185883loss in batch 159: 0.0868835/0.185272loss in batch 160: 0.0321503/0.184326loss in batch 161: 0.0740814/0.18364loss in batch 162: 0.406754/0.185013loss in batch 163: 0.107941/0.184525loss in batch 164: 0.101822/0.184036loss in batch 165: 0.200485/0.184128loss in batch 166: 0.300537/0.18483loss in batch 167: 0.0276642/0.183899loss in batch 168: 0.0209808/0.182922loss in batch 169: 0.0623474/0.182236loss in batch 170: 0.319305/0.183029loss in batch 171: 0.036911/0.182175loss in batch 172: 0.00730896/0.181168loss in batch 173: 0.0332947/0.180313loss in batch 174: 0.0338745/0.179474loss in batch 175: 0.230515/0.179764loss in batch 176: 0.194672/0.179855loss in batch 177: 0.0136871/0.178925loss in batch 178: 0.109467/0.178528loss in batch 179: 0.135391/0.178299loss in batch 180: 0.0179443/0.177399loss in batch 181: 0.0483551/0.176697loss in batch 182: 0.280579/0.177261loss in batch 183: 0.278259/0.177826loss in batch 184: 0.331436/0.178635loss in batch 185: 0.0368958/0.177872loss in batch 186: 0.22142/0.178116loss in batch 187: 0.0552216/0.17746loss in batch 188: 0.98912/0.181763loss in batch 189: 0.0372162/0.181loss in batch 190: 0.046524/0.180283loss in batch 191: 0.222855/0.180527loss in batch 192: 0.056839/0.179871loss in batch 193: 0.0762939/0.179337loss in batch 194: 0.0886841/0.178879loss in batch 195: 0.156448/0.178757loss in batch 196: 0.495865/0.180374loss in batch 197: 0.00788879/0.179489loss in batch 198: 0.0134125/0.178665loss in batch 199: 0.0382996/0.177963loss in batch 200: 0.105164/0.177597loss in batch 201: 0.1465/0.177444loss in batch 202: 0.123596/0.177185loss in batch 203: 0.0477905/0.176544loss in batch 204: 0.477631/0.178009loss in batch 205: 0.0769043/0.177521loss in batch 206: 0.209045/0.177673loss in batch 207: 0.0310211/0.176971loss in batch 208: 0.115295/0.176682loss in batch 209: 0.175888/0.176682loss in batch 210: 0.216873/0.176865loss in batch 211: 0.0509338/0.17627loss in batch 212: 0.0886383/0.175842
done with epoch 12
train_acc: 0.943662 (402/426)
test loss: 0.0886381
acc: 0.944054 (135/143)
loss in batch 0: 0.0982361/0.0982361loss in batch 1: 0.0561981/0.0772095loss in batch 2: 0.223724/0.126053loss in batch 3: 0.420013/0.199539loss in batch 4: 0.335373/0.2267loss in batch 5: 0.00402832/0.189606loss in batch 6: 0.498077/0.233658loss in batch 7: 0.0434113/0.209885loss in batch 8: 0.0558624/0.192764loss in batch 9: 0.0556946/0.179062loss in batch 10: 0.21431/0.182266loss in batch 11: 0.0374451/0.170197loss in batch 12: 0.212799/0.173477loss in batch 13: 0.0414276/0.164032loss in batch 14: 0.023819/0.154694loss in batch 15: 0.215347/0.158493loss in batch 16: 0.551697/0.18161loss in batch 17: 0.646194/0.207428loss in batch 18: 0.316833/0.213181loss in batch 19: 0.191559/0.212097loss in batch 20: 0.341431/0.218246loss in batch 21: 1.56796/0.279617loss in batch 22: 0.0253143/0.268555loss in batch 23: 0.0286865/0.25856loss in batch 24: 0.391846/0.263885loss in batch 25: 0.194/0.2612loss in batch 26: 0.00724792/0.251801loss in batch 27: 0.0518341/0.244659loss in batch 28: 0.140564/0.241074loss in batch 29: 0.0683594/0.235321loss in batch 30: 0.105759/0.231125loss in batch 31: 0.0804596/0.226425loss in batch 32: 0.240677/0.226852loss in batch 33: 0.113174/0.223511loss in batch 34: 0.0744781/0.219254loss in batch 35: 0.508713/0.227295loss in batch 36: 0.182358/0.226074loss in batch 37: 0.209946/0.225647loss in batch 38: 0.404175/0.23024loss in batch 39: 0.0825653/0.226547loss in batch 40: 0.0717468/0.222763loss in batch 41: 0.0753326/0.219269loss in batch 42: 0.0740356/0.215881loss in batch 43: 0.0235748/0.211502loss in batch 44: 0.147461/0.210083loss in batch 45: 0.305939/0.212173loss in batch 46: 0.149551/0.210831loss in batch 47: 0.0619659/0.207733loss in batch 48: 0.132309/0.206192loss in batch 49: 0.0406799/0.202881loss in batch 50: 0.114395/0.201141loss in batch 51: 0.426636/0.205475loss in batch 52: 0.0927582/0.203369loss in batch 53: 0.0489655/0.2005loss in batch 54: 0.279312/0.201935loss in batch 55: 0.0157318/0.198608loss in batch 56: 0.17247/0.198151loss in batch 57: 0.0194397/0.195068loss in batch 58: 0.0662231/0.192886loss in batch 59: 0.0227966/0.190048loss in batch 60: 0.307037/0.191956loss in batch 61: 0.0529938/0.189713loss in batch 62: 0.11647/0.188568loss in batch 63: 0.0425568/0.186279loss in batch 64: 0.260788/0.187424loss in batch 65: 0.106659/0.186203loss in batch 66: 0.0723419/0.184509loss in batch 67: 0.043869/0.182434loss in batch 68: 0.0244293/0.180145loss in batch 69: 0.323471/0.18219loss in batch 70: 0.0322723/0.180084loss in batch 71: 0.133377/0.179428loss in batch 72: 0.521362/0.184113loss in batch 73: 0.0785675/0.182678loss in batch 74: 0.0731964/0.181229loss in batch 75: 0.0269012/0.179199loss in batch 76: 0.496414/0.183319loss in batch 77: 0.0802307/0.182007loss in batch 78: 0.0612183/0.180466loss in batch 79: 0.0622253/0.179001loss in batch 80: 0.00367737/0.176819loss in batch 81: 0.391403/0.179443loss in batch 82: 0.119492/0.178726loss in batch 83: 0.0385742/0.177063loss in batch 84: 0.0321198/0.175339loss in batch 85: 0.0528107/0.173935loss in batch 86: 0.0920105/0.172974loss in batch 87: 0.446869/0.176086loss in batch 88: 0.0588837/0.174789loss in batch 89: 0.0637817/0.173553loss in batch 90: 0.364288/0.175644loss in batch 91: 0.0596466/0.174393loss in batch 92: 0.0216064/0.172729loss in batch 93: 0.0939484/0.17189loss in batch 94: 0.0686493/0.170807loss in batch 95: 0.0726166/0.169785loss in batch 96: 0.22493/0.170349loss in batch 97: 0.28154/0.171494loss in batch 98: 0.0124207/0.169876loss in batch 99: 0.107422/0.169266loss in batch 100: 0.0553131/0.168137loss in batch 101: 0.0175629/0.166656loss in batch 102: 0.0939026/0.165955loss in batch 103: 0.10936/0.165405loss in batch 104: 0.0595551/0.164398loss in batch 105: 0.180069/0.164551loss in batch 106: 0.894516/0.171371loss in batch 107: 0.0524292/0.170273loss in batch 108: 0.0406189/0.169083loss in batch 109: 0.0230103/0.16774loss in batch 110: 0.212036/0.168152loss in batch 111: 0.0164032/0.166794loss in batch 112: 0.046936/0.165726loss in batch 113: 0.0246429/0.16449loss in batch 114: 0.0313416/0.16333loss in batch 115: 0.175842/0.163437loss in batch 116: 0.431213/0.165741loss in batch 117: 0.0087738/0.164413loss in batch 118: 0.0906219/0.163788loss in batch 119: 0.273697/0.164703loss in batch 120: 0.0384521/0.163651loss in batch 121: 0.17981/0.163788loss in batch 122: 0.0518799/0.162872loss in batch 123: 1.19109/0.171173loss in batch 124: 0.76358/0.175903loss in batch 125: 0.513855/0.178589loss in batch 126: 0.205872/0.178818loss in batch 127: 0.0337982/0.177673loss in batch 128: 0.0754547/0.17688loss in batch 129: 0.408646/0.178665loss in batch 130: 0.435013/0.180618loss in batch 131: 0.0098114/0.179321loss in batch 132: 0.0201263/0.178131loss in batch 133: 0.0431213/0.177124loss in batch 134: 0.0587921/0.176254loss in batch 135: 0.468262/0.178391loss in batch 136: 0.0306854/0.177322loss in batch 137: 0.434204/0.179169loss in batch 138: 0.18811/0.179245loss in batch 139: 0.0654297/0.178436loss in batch 140: 0.137314/0.178131loss in batch 141: 0.0035553/0.17691loss in batch 142: 0.536209/0.179428loss in batch 143: 0.0512695/0.178528loss in batch 144: 0.0309906/0.177505loss in batch 145: 0.0337524/0.176529loss in batch 146: 0.0806274/0.175873loss in batch 147: 0.0219116/0.174835loss in batch 148: 0.00588989/0.173706loss in batch 149: 0.0611267/0.172943loss in batch 150: 0.0482483/0.172119loss in batch 151: 0.396805/0.173599loss in batch 152: 0.056427/0.172836loss in batch 153: 0.996994/0.178177loss in batch 154: 0.193863/0.178284loss in batch 155: 0.289673/0.179001loss in batch 156: 0.0372009/0.178101loss in batch 157: 0.2341/0.178452loss in batch 158: 0.310593/0.179291loss in batch 159: 0.440262/0.180908loss in batch 160: 0.464478/0.182678loss in batch 161: 0.12677/0.182327loss in batch 162: 0.00991821/0.181274loss in batch 163: 0.0543365/0.180496loss in batch 164: 0.455673/0.182175loss in batch 165: 0.0803986/0.181549loss in batch 166: 0.109329/0.181122loss in batch 167: 0.0140991/0.18013loss in batch 168: 0.0452576/0.179337loss in batch 169: 0.00636292/0.178314loss in batch 170: 0.0444336/0.177536loss in batch 171: 0.133743/0.177277loss in batch 172: 0.135208/0.177032loss in batch 173: 0.423431/0.178452loss in batch 174: 0.16806/0.178391loss in batch 175: 0.620285/0.180908loss in batch 176: 0.0888367/0.180374loss in batch 177: 0.324677/0.181198loss in batch 178: 0.044281/0.18042loss in batch 179: 0.00947571/0.179474loss in batch 180: 0.110947/0.179108loss in batch 181: 0.0234528/0.178238loss in batch 182: 0.0996857/0.177811loss in batch 183: 0.15947/0.177719loss in batch 184: 0.310318/0.178421loss in batch 185: 0.568024/0.180527loss in batch 186: 0.00637817/0.179581loss in batch 187: 0.00552368/0.178665loss in batch 188: 0.0279083/0.177872loss in batch 189: 0.0261078/0.177063loss in batch 190: 0.269638/0.177551loss in batch 191: 0.294006/0.178162loss in batch 192: 0.2388/0.178467loss in batch 193: 0.0578766/0.177856loss in batch 194: 0.0227509/0.177063loss in batch 195: 0.13623/0.176849loss in batch 196: 0.313278/0.177536loss in batch 197: 0.0213318/0.176758loss in batch 198: 0.0187531/0.175949loss in batch 199: 0.111801/0.175644loss in batch 200: 0.102875/0.175278loss in batch 201: 0.280365/0.175797loss in batch 202: 0.111771/0.175491loss in batch 203: 0.302521/0.176102loss in batch 204: 0.433456/0.177368loss in batch 205: 0.49794/0.178909loss in batch 206: 0.0314789/0.178207loss in batch 207: 0.0381622/0.177536loss in batch 208: 0.166397/0.177475loss in batch 209: 0.0349121/0.176804loss in batch 210: 0.0764313/0.176315loss in batch 211: 0.0426941/0.17569loss in batch 212: 0.12001/0.17543
done with epoch 13
train_acc: 0.938967 (400/426)
test loss: 0.12001
acc: 0.944054 (135/143)
loss in batch 0: 0.0363159/0.0363159loss in batch 1: 0.339569/0.187943loss in batch 2: 0.365341/0.24707loss in batch 3: 0.0753937/0.204163loss in batch 4: 0.00515747/0.164352loss in batch 5: 0.116699/0.156418loss in batch 6: 0.643845/0.226044loss in batch 7: 0.092804/0.209396loss in batch 8: 0.244751/0.213318loss in batch 9: 0.116684/0.203659loss in batch 10: 0.0649109/0.19104loss in batch 11: 0.0680237/0.180801loss in batch 12: 0.0821991/0.173218loss in batch 13: 0.0571747/0.164917loss in batch 14: 0.0283661/0.155807loss in batch 15: 0.194611/0.158249loss in batch 16: 0.0842896/0.153885loss in batch 17: 0.0547791/0.148376loss in batch 18: 0.177185/0.149902loss in batch 19: 0.0501099/0.144913loss in batch 20: 0.0471954/0.140259loss in batch 21: 0.10463/0.138641loss in batch 22: 1.47421/0.196716loss in batch 23: 0.15657/0.195038loss in batch 24: 0.0110626/0.187668loss in batch 25: 0.0966644/0.184174loss in batch 26: 0.278366/0.187653loss in batch 27: 0.0557098/0.182953loss in batch 28: 0.0895386/0.179718loss in batch 29: 0.20311/0.180511loss in batch 30: 0.0330963/0.175751loss in batch 31: 0.0761871/0.172653loss in batch 32: 0.0300293/0.16832loss in batch 33: 0.01651/0.163849loss in batch 34: 0.0356293/0.160202loss in batch 35: 0.00619507/0.155914loss in batch 36: 0.49469/0.16507loss in batch 37: 0.0737152/0.162674loss in batch 38: 0.365433/0.167877loss in batch 39: 0.112442/0.166489loss in batch 40: 0.0207825/0.162933loss in batch 41: 0.0438843/0.160095loss in batch 42: 0.313354/0.163666loss in batch 43: 0.285904/0.166428loss in batch 44: 0.0275726/0.163361loss in batch 45: 0.160446/0.163284loss in batch 46: 0.0342255/0.160538loss in batch 47: 0.0721588/0.158707loss in batch 48: 0.0237274/0.155945loss in batch 49: 0.0953217/0.154739loss in batch 50: 0.396408/0.15947loss in batch 51: 0.133224/0.158966loss in batch 52: 0.0295868/0.156525loss in batch 53: 0.0860596/0.155212loss in batch 54: 0.112274/0.154449loss in batch 55: 0.0354004/0.152313loss in batch 56: 0.0588379/0.150665loss in batch 57: 0.0869598/0.149567loss in batch 58: 0.077301/0.148346loss in batch 59: 0.0636292/0.146927loss in batch 60: 0.0224915/0.144897loss in batch 61: 0.0189667/0.142868loss in batch 62: 0.00898743/0.140732loss in batch 63: 0.321457/0.143555loss in batch 64: 0.0538025/0.142181loss in batch 65: 0.0533295/0.140839loss in batch 66: 0.0749817/0.139847loss in batch 67: 0.0516968/0.13855loss in batch 68: 0.0178986/0.13681loss in batch 69: 0.0152893/0.135071loss in batch 70: 0.342361/0.137985loss in batch 71: 0.0558929/0.136856loss in batch 72: 0.0696564/0.135925loss in batch 73: 0.25322/0.137512loss in batch 74: 0.0428162/0.136246loss in batch 75: 0.0578613/0.135223loss in batch 76: 0.0266724/0.133804loss in batch 77: 0.0218353/0.13237loss in batch 78: 0.386581/0.13559loss in batch 79: 0.0130615/0.134064loss in batch 80: 0.0600433/0.133148loss in batch 81: 0.0718689/0.132401loss in batch 82: 0.0292664/0.131149loss in batch 83: 0.00985718/0.129715loss in batch 84: 0.0445862/0.128708loss in batch 85: 0.0382538/0.12767loss in batch 86: 0.0819855/0.127136loss in batch 87: 0.40184/0.130249loss in batch 88: 0.499603/0.134399loss in batch 89: 0.0189056/0.133118loss in batch 90: 0.127289/0.133057loss in batch 91: 0.0108948/0.131729loss in batch 92: 0.0323029/0.130661loss in batch 93: 0.246109/0.131882loss in batch 94: 0.344162/0.134125loss in batch 95: 0.0482178/0.13324loss in batch 96: 0.0888367/0.132767loss in batch 97: 0.703644/0.138596loss in batch 98: 0.0778198/0.137985loss in batch 99: 0.643082/0.143036loss in batch 100: 0.0265045/0.141876loss in batch 101: 0.0317993/0.140793loss in batch 102: 0.542587/0.144699loss in batch 103: 0.193192/0.145172loss in batch 104: 0.0567322/0.144318loss in batch 105: 0.0308838/0.143265loss in batch 106: 0.363083/0.145325loss in batch 107: 0.0806427/0.144714loss in batch 108: 0.204208/0.145264loss in batch 109: 0.033371/0.144241loss in batch 110: 0.783325/0.150009loss in batch 111: 0.054306/0.149139loss in batch 112: 0.101273/0.148712loss in batch 113: 0.047699/0.147842loss in batch 114: 0.0771332/0.147217loss in batch 115: 0.491745/0.150192loss in batch 116: 0.0425873/0.149261loss in batch 117: 0.562546/0.152771loss in batch 118: 0.0696564/0.152069loss in batch 119: 0.0336151/0.151077loss in batch 120: 0.0683441/0.150406loss in batch 121: 0.0252686/0.149368loss in batch 122: 0.0490417/0.14856loss in batch 123: 0.954971/0.15506loss in batch 124: 0.0546265/0.154266loss in batch 125: 0.00512695/0.153076loss in batch 126: 0.0413818/0.152206loss in batch 127: 0.706726/0.156525loss in batch 128: 0.177719/0.156708loss in batch 129: 0.221924/0.157211loss in batch 130: 0.179352/0.157379loss in batch 131: 0.340927/0.158752loss in batch 132: 0.0445862/0.157898loss in batch 133: 0.0087738/0.156799loss in batch 134: 0.0305634/0.155838loss in batch 135: 0.0306854/0.154938loss in batch 136: 0.242966/0.155579loss in batch 137: 0.140427/0.155457loss in batch 138: 0.111206/0.155136loss in batch 139: 0.01651/0.154144loss in batch 140: 0.112671/0.153854loss in batch 141: 0.310364/0.154953loss in batch 142: 0.0868378/0.15448loss in batch 143: 0.0334167/0.153641loss in batch 144: 0.00608826/0.152618loss in batch 145: 0.0823669/0.152145loss in batch 146: 0.113297/0.151871loss in batch 147: 0.10701/0.151581loss in batch 148: 0.00302124/0.150589loss in batch 149: 0.205231/0.15094loss in batch 150: 0.0873566/0.150528loss in batch 151: 0.0125122/0.149628loss in batch 152: 0.0785522/0.149155loss in batch 153: 0.113693/0.148926loss in batch 154: 1.07147/0.154861loss in batch 155: 0.324615/0.15596loss in batch 156: 0.0252686/0.155121loss in batch 157: 0.0409546/0.154404loss in batch 158: 0.0464325/0.153717loss in batch 159: 0.327057/0.154816loss in batch 160: 0.0331421/0.154053loss in batch 161: 0.112991/0.153809loss in batch 162: 0.0839081/0.153381loss in batch 163: 0.418915/0.154984loss in batch 164: 0.437057/0.156708loss in batch 165: 0.137695/0.156586loss in batch 166: 0.336716/0.157654loss in batch 167: 0.0349274/0.156937loss in batch 168: 0.0139465/0.156097loss in batch 169: 0.143738/0.156006loss in batch 170: 0.217667/0.156387loss in batch 171: 0.068573/0.155869loss in batch 172: 0.0265503/0.155121loss in batch 173: 0.0447388/0.15448loss in batch 174: 0.00230408/0.15361loss in batch 175: 0.600021/0.156158loss in batch 176: 0.0267792/0.155426loss in batch 177: 1.36191/0.162201loss in batch 178: 0.0486145/0.161575loss in batch 179: 0.204269/0.161804loss in batch 180: 0.304459/0.162582loss in batch 181: 0.48909/0.164383loss in batch 182: 0.0321045/0.163666loss in batch 183: 0.55896/0.165802loss in batch 184: 0.25592/0.166306loss in batch 185: 0.170029/0.166321loss in batch 186: 0.159424/0.166275loss in batch 187: 0.038208/0.165604loss in batch 188: 0.0234985/0.164841loss in batch 189: 0.225067/0.165161loss in batch 190: 0.115784/0.164902loss in batch 191: 0.385208/0.166061loss in batch 192: 0.057785/0.165497loss in batch 193: 0.334076/0.166351loss in batch 194: 0.224777/0.166672loss in batch 195: 0.0323029/0.16597loss in batch 196: 0.156693/0.165924loss in batch 197: 0.340591/0.166809loss in batch 198: 0.0370026/0.166168loss in batch 199: 0.372482/0.167191loss in batch 200: 0.139542/0.167053loss in batch 201: 0.112686/0.166794loss in batch 202: 0.122025/0.16655loss in batch 203: 0.038147/0.165939loss in batch 204: 0.721878/0.168655loss in batch 205: 0.0246582/0.167953loss in batch 206: 0.0186157/0.167221loss in batch 207: 0.0365295/0.166595loss in batch 208: 0.280701/0.167145loss in batch 209: 0.431946/0.168411loss in batch 210: 0.0327911/0.167755loss in batch 211: 0.221741/0.168015loss in batch 212: 0.0531311/0.167465
done with epoch 14
train_acc: 0.946009 (403/426)
test loss: 0.053131
acc: 0.944054 (135/143)
loss in batch 0: 0.256836/0.256836loss in batch 1: 0.351303/0.304077loss in batch 2: 0.226364/0.278168loss in batch 3: 0.0542603/0.222198loss in batch 4: 0.0560608/0.188965loss in batch 5: 0.0150452/0.159988loss in batch 6: 0.218079/0.168274loss in batch 7: 0.169907/0.168472loss in batch 8: 0.0603333/0.156464loss in batch 9: 0.376038/0.178421loss in batch 10: 0.019989/0.164017loss in batch 11: 0.0166626/0.151749loss in batch 12: 0.425507/0.172806loss in batch 13: 0.0463562/0.163757loss in batch 14: 0.125046/0.161179loss in batch 15: 0.38591/0.175232loss in batch 16: 0.50975/0.194901loss in batch 17: 0.131561/0.191391loss in batch 18: 0.320984/0.198212loss in batch 19: 0.0617523/0.191391loss in batch 20: 0.064209/0.185333loss in batch 21: 0.0946655/0.181213loss in batch 22: 0.014328/0.173965loss in batch 23: 0.0144196/0.167313loss in batch 24: 1.42503/0.217606loss in batch 25: 0.0205231/0.210037loss in batch 26: 0.0774689/0.205124loss in batch 27: 1.04268/0.235046loss in batch 28: 0.0661163/0.229218loss in batch 29: 0.0223236/0.222321loss in batch 30: 0.178009/0.220886loss in batch 31: 0.0386505/0.215195loss in batch 32: 0.355515/0.219452loss in batch 33: 0.0129547/0.213379loss in batch 34: 0.0137329/0.207657loss in batch 35: 0.259689/0.209106loss in batch 36: 0.0721741/0.205414loss in batch 37: 0.25209/0.206635loss in batch 38: 0.0503845/0.202637loss in batch 39: 0.0654144/0.199203loss in batch 40: 0.128372/0.197479loss in batch 41: 0.0513916/0.194loss in batch 42: 0.422714/0.199326loss in batch 43: 0.0790405/0.196579loss in batch 44: 0.0860748/0.194122loss in batch 45: 0.10289/0.192139loss in batch 46: 0.0257874/0.188599loss in batch 47: 0.0388641/0.185486loss in batch 48: 0.497955/0.191849loss in batch 49: 0.129028/0.190613loss in batch 50: 0.013916/0.187134loss in batch 51: 0.438873/0.191986loss in batch 52: 0.0664215/0.189606loss in batch 53: 0.0947113/0.187851loss in batch 54: 0.0997925/0.186264loss in batch 55: 0.177475/0.186096loss in batch 56: 0.1138/0.18483loss in batch 57: 0.459488/0.18956loss in batch 58: 0.143234/0.188782loss in batch 59: 0.283783/0.190369loss in batch 60: 0.271332/0.191681loss in batch 61: 0.567032/0.197739loss in batch 62: 0.0718079/0.19574loss in batch 63: 0.0616302/0.193649loss in batch 64: 0.0393829/0.191269loss in batch 65: 0.317276/0.193176loss in batch 66: 0.409348/0.196411loss in batch 67: 0.14502/0.195648loss in batch 68: 0.022522/0.193146loss in batch 69: 0.0710907/0.191406loss in batch 70: 0.386581/0.194153loss in batch 71: 0.0522461/0.192184loss in batch 72: 0.0971375/0.190872loss in batch 73: 0.254959/0.191742loss in batch 74: 0.041275/0.189743loss in batch 75: 0.0911255/0.188446loss in batch 76: 0.0346375/0.186447loss in batch 77: 0.247269/0.187225loss in batch 78: 0.0556793/0.185547loss in batch 79: 0.0682678/0.184097loss in batch 80: 0.00576782/0.181885loss in batch 81: 0.0135651/0.17984loss in batch 82: 0.0273132/0.178009loss in batch 83: 0.137558/0.177521loss in batch 84: 0.485184/0.181137loss in batch 85: 0.284592/0.182343loss in batch 86: 0.0810394/0.181183loss in batch 87: 0.265213/0.182144loss in batch 88: 0.0196838/0.180313loss in batch 89: 0.0468903/0.178833loss in batch 90: 0.223679/0.179321loss in batch 91: 0.354675/0.181229loss in batch 92: 0.318787/0.182709loss in batch 93: 0.0384369/0.181168loss in batch 94: 0.028595/0.179565loss in batch 95: 0.431885/0.18219loss in batch 96: 0.28476/0.183243loss in batch 97: 0.0437164/0.181824loss in batch 98: 0.676071/0.186813loss in batch 99: 0.157654/0.186523loss in batch 100: 0.0364838/0.185028loss in batch 101: 0.11499/0.184357loss in batch 102: 0.254044/0.185028loss in batch 103: 0.0721588/0.183945loss in batch 104: 0.10498/0.183197loss in batch 105: 0.197876/0.183334loss in batch 106: 0.0168915/0.181763loss in batch 107: 0.0419464/0.180481loss in batch 108: 0.0158844/0.17897loss in batch 109: 0.200211/0.179153loss in batch 110: 0.039978/0.177902loss in batch 111: 0.375916/0.179672loss in batch 112: 0.0353241/0.178391loss in batch 113: 0.0712433/0.17746loss in batch 114: 0.0732727/0.176559loss in batch 115: 0.0857697/0.175781loss in batch 116: 0.0402222/0.174622loss in batch 117: 0.0144958/0.173248loss in batch 118: 0.355301/0.174789loss in batch 119: 0.0324554/0.173599loss in batch 120: 0.0812378/0.172836loss in batch 121: 0.0747986/0.172043loss in batch 122: 0.527649/0.174911loss in batch 123: 1.30716/0.184052loss in batch 124: 0.00587463/0.182632loss in batch 125: 0.0979156/0.181961loss in batch 126: 0.0219879/0.180695loss in batch 127: 0.177551/0.180664loss in batch 128: 0.66951/0.184464loss in batch 129: 0.0218964/0.183212loss in batch 130: 0.150574/0.182968loss in batch 131: 0.202103/0.183105loss in batch 132: 0.0548248/0.182144loss in batch 133: 0.0540924/0.181183loss in batch 134: 0.00949097/0.179916loss in batch 135: 0.193161/0.180023loss in batch 136: 0.0478668/0.179047loss in batch 137: 0.0233002/0.177917loss in batch 138: 0.464722/0.179977loss in batch 139: 0.0535889/0.179077loss in batch 140: 0.856415/0.183884loss in batch 141: 0.108643/0.183365loss in batch 142: 0.00872803/0.182144loss in batch 143: 0.436005/0.183899loss in batch 144: 0.0479279/0.182968loss in batch 145: 0.0494537/0.182037loss in batch 146: 0.0523224/0.181168loss in batch 147: 0.0743561/0.18045loss in batch 148: 0.202148/0.180588loss in batch 149: 0.0469513/0.179688loss in batch 150: 0.402008/0.181168loss in batch 151: 0.364304/0.182373loss in batch 152: 0.0425415/0.181458loss in batch 153: 0.0211182/0.180405loss in batch 154: 0.0351868/0.179489loss in batch 155: 0.00352478/0.17836loss in batch 156: 0.00483704/0.177246loss in batch 157: 0.0452881/0.176407loss in batch 158: 0.120773/0.176056loss in batch 159: 0.380661/0.177338loss in batch 160: 0.239731/0.177734loss in batch 161: 0.268494/0.178299loss in batch 162: 0.0464172/0.177475loss in batch 163: 0.0540924/0.176727loss in batch 164: 0.170776/0.176682loss in batch 165: 0.129578/0.176407loss in batch 166: 0.0231628/0.175491loss in batch 167: 0.0116882/0.174515loss in batch 168: 0.110886/0.174133loss in batch 169: 0.136734/0.17392loss in batch 170: 0.240921/0.174301loss in batch 171: 0.15918/0.174225loss in batch 172: 0.469894/0.175934loss in batch 173: 0.0327606/0.17511loss in batch 174: 0.0124817/0.174179loss in batch 175: 0.295013/0.174866loss in batch 176: 0.0207672/0.173996loss in batch 177: 0.177185/0.174011loss in batch 178: 0.0854797/0.173523loss in batch 179: 0.0440521/0.172806loss in batch 180: 0.0133057/0.171906loss in batch 181: 0.0157928/0.171066loss in batch 182: 0.0316315/0.170303loss in batch 183: 0.0718689/0.169754loss in batch 184: 0.040741/0.169067loss in batch 185: 0.0572662/0.168472loss in batch 186: 0.267212/0.169006loss in batch 187: 0.0655518/0.168442loss in batch 188: 0.041153/0.16777loss in batch 189: 0.171295/0.167786loss in batch 190: 0.0196381/0.167007loss in batch 191: 0.0279083/0.16629loss in batch 192: 1.17436/0.171509loss in batch 193: 0.12146/0.171249loss in batch 194: 0.076767/0.170776loss in batch 195: 0.0809631/0.170303loss in batch 196: 0.25885/0.170746loss in batch 197: 0.0388336/0.17009loss in batch 198: 0.0433807/0.169449loss in batch 199: 0.0466461/0.168839loss in batch 200: 0.0563507/0.168289loss in batch 201: 0.0253754/0.167572loss in batch 202: 0.586349/0.169647loss in batch 203: 0.0625153/0.169113loss in batch 204: 0.307159/0.169785loss in batch 205: 0.065033/0.169281loss in batch 206: 0.0326996/0.168625loss in batch 207: 0.0663605/0.168137loss in batch 208: 0.120056/0.167892loss in batch 209: 0.155869/0.167847loss in batch 210: 0.0824585/0.167435loss in batch 211: 0.052597/0.166901loss in batch 212: 0.0810699/0.166489
done with epoch 15
train_acc: 0.938967 (400/426)
test loss: 0.0810698
acc: 0.944054 (135/143)
loss in batch 0: 0.2108/0.2108loss in batch 1: 0.0134735/0.112137loss in batch 2: 0.0634613/0.0959167loss in batch 3: 0.0901642/0.0944672loss in batch 4: 0.155807/0.106735loss in batch 5: 0.0171661/0.0918121loss in batch 6: 0.066864/0.0882416loss in batch 7: 0.642136/0.157486loss in batch 8: 0.0652466/0.147232loss in batch 9: 0.0296021/0.135468loss in batch 10: 0.015686/0.124588loss in batch 11: 0.323105/0.141129loss in batch 12: 0.139725/0.141006loss in batch 13: 0.0279846/0.132935loss in batch 14: 0.031662/0.12619loss in batch 15: 0.315903/0.138046loss in batch 16: 0.0968628/0.13562loss in batch 17: 0.0806274/0.132568loss in batch 18: 0.0236053/0.126846loss in batch 19: 0.387497/0.139862loss in batch 20: 0.252502/0.145233loss in batch 21: 0.0373077/0.140335loss in batch 22: 0.246109/0.144928loss in batch 23: 0.445984/0.157471loss in batch 24: 0.0413208/0.152832loss in batch 25: 0.0241089/0.147873loss in batch 26: 0.0861359/0.145584loss in batch 27: 0.0399475/0.141815loss in batch 28: 0.0495148/0.138626loss in batch 29: 0.134048/0.138474loss in batch 30: 0.0271149/0.134888loss in batch 31: 0.120697/0.134445loss in batch 32: 0.354385/0.141098loss in batch 33: 0.0864868/0.139511loss in batch 34: 0.00723267/0.135727loss in batch 35: 0.21727/0.137985loss in batch 36: 0.0514069/0.135651loss in batch 37: 0.140137/0.135757loss in batch 38: 0.235046/0.138306loss in batch 39: 0.177368/0.139282loss in batch 40: 0.025116/0.13649loss in batch 41: 0.00294495/0.133316loss in batch 42: 0.0458984/0.131302loss in batch 43: 0.242462/0.13382loss in batch 44: 0.482697/0.141571loss in batch 45: 0.0701752/0.140015loss in batch 46: 0.555832/0.148865loss in batch 47: 0.0574036/0.146957loss in batch 48: 0.411819/0.152374loss in batch 49: 0.0542297/0.150391loss in batch 50: 0.00532532/0.147552loss in batch 51: 0.0546722/0.145767loss in batch 52: 0.441177/0.151337loss in batch 53: 0.0408173/0.149292loss in batch 54: 0.00463867/0.146667loss in batch 55: 0.406479/0.151306loss in batch 56: 0.00630188/0.148758loss in batch 57: 0.279327/0.151016loss in batch 58: 0.0483551/0.149277loss in batch 59: 0.426727/0.153885loss in batch 60: 0.00628662/0.151474loss in batch 61: 0.112396/0.150848loss in batch 62: 0.0377808/0.149048loss in batch 63: 0.00965881/0.146866loss in batch 64: 0.0052948/0.144699loss in batch 65: 0.0700836/0.14357loss in batch 66: 0.0169067/0.141663loss in batch 67: 0.442871/0.146118loss in batch 68: 0.0826263/0.145187loss in batch 69: 0.522049/0.150558loss in batch 70: 0.241592/0.15184loss in batch 71: 0.108017/0.15123loss in batch 72: 0.0217133/0.14946loss in batch 73: 0.0380859/0.147964loss in batch 74: 0.326309/0.150345loss in batch 75: 1.16762/0.163727loss in batch 76: 0.164932/0.163742loss in batch 77: 0.436935/0.167252loss in batch 78: 0.0712128/0.166016loss in batch 79: 0.00668335/0.164047loss in batch 80: 0.0881805/0.163101loss in batch 81: 0.124863/0.162628loss in batch 82: 0.123459/0.16217loss in batch 83: 0.0684357/0.161057loss in batch 84: 0.0362091/0.159576loss in batch 85: 0.291397/0.161118loss in batch 86: 0.201035/0.161575loss in batch 87: 0.192169/0.161911loss in batch 88: 0.0204468/0.160324loss in batch 89: 0.0780792/0.159424loss in batch 90: 0.00230408/0.1577loss in batch 91: 0.149323/0.157593loss in batch 92: 0.0209503/0.156128loss in batch 93: 0.106018/0.155594loss in batch 94: 0.119461/0.155228loss in batch 95: 0.112839/0.15477loss in batch 96: 1.37288/0.167328loss in batch 97: 0.0180969/0.165802loss in batch 98: 0.0122833/0.164261loss in batch 99: 0.0472717/0.163086loss in batch 100: 0.251068/0.163956loss in batch 101: 0.556641/0.167816loss in batch 102: 0.0223694/0.166397loss in batch 103: 0.214737/0.16687loss in batch 104: 0.0832672/0.166077loss in batch 105: 0.154251/0.165955loss in batch 106: 0.104004/0.165375loss in batch 107: 0.0691986/0.164474loss in batch 108: 0.0610352/0.163528loss in batch 109: 0.056427/0.162567loss in batch 110: 0.0983429/0.161972loss in batch 111: 0.0375824/0.160873loss in batch 112: 0.0440063/0.159836loss in batch 113: 0.288208/0.160965loss in batch 114: 0.735107/0.165955loss in batch 115: 0.0830841/0.165237loss in batch 116: 0.175232/0.165329loss in batch 117: 0.263214/0.166168loss in batch 118: 0.0239258/0.164963loss in batch 119: 1.52588e-05/0.163589loss in batch 120: 0.0111389/0.162323loss in batch 121: 0.0264587/0.161224loss in batch 122: 0.0540466/0.160339loss in batch 123: 0.018158/0.159195loss in batch 124: 0.0483398/0.15831loss in batch 125: 0.013092/0.157166loss in batch 126: 0.0189667/0.156067loss in batch 127: 0.012619/0.154953loss in batch 128: 0.249817/0.155685loss in batch 129: 0.024353/0.154663loss in batch 130: 0.0382233/0.153793loss in batch 131: 0.156876/0.153809loss in batch 132: 0.418442/0.155807loss in batch 133: 0.0780487/0.155228loss in batch 134: 0.0466919/0.154404loss in batch 135: 0.181366/0.154617loss in batch 136: 0.0214081/0.153641loss in batch 137: 0.707397/0.157654loss in batch 138: 0.0305634/0.156723loss in batch 139: 0.48674/0.159103loss in batch 140: 0.182144/0.159256loss in batch 141: 0.0991058/0.158844loss in batch 142: 0.14682/0.158752loss in batch 143: 0.197174/0.159027loss in batch 144: 0.290024/0.159927loss in batch 145: 0.241867/0.160492loss in batch 146: 0.0198364/0.159515loss in batch 147: 0.0672607/0.158905loss in batch 148: 0.0527496/0.158188loss in batch 149: 0.0982361/0.157776loss in batch 150: 0.032547/0.156952loss in batch 151: 0.0114441/0.156006loss in batch 152: 0.153519/0.155991loss in batch 153: 0.0702515/0.155426loss in batch 154: 0.107559/0.155121loss in batch 155: 0.0243225/0.154282loss in batch 156: 0.21051/0.154633loss in batch 157: 0.0405121/0.153915loss in batch 158: 0.00552368/0.152985loss in batch 159: 0.032135/0.152237loss in batch 160: 0.0142822/0.151367loss in batch 161: 0.589844/0.154083loss in batch 162: 0.0451355/0.153412loss in batch 163: 0.0284882/0.152649loss in batch 164: 0.230347/0.153122loss in batch 165: 0.00500488/0.152222loss in batch 166: 0.0103607/0.151382loss in batch 167: 0.0948334/0.151031loss in batch 168: 0.188385/0.15126loss in batch 169: 0.332062/0.152328loss in batch 170: 0.641266/0.155197loss in batch 171: 0.167297/0.155258loss in batch 172: 0.127197/0.15509loss in batch 173: 0.0519409/0.154495loss in batch 174: 0.019577/0.153732loss in batch 175: 0.0457153/0.153122loss in batch 176: 0.0719452/0.152664loss in batch 177: 0.042984/0.152039loss in batch 178: 0.0146027/0.151276loss in batch 179: 0.00942993/0.150482loss in batch 180: 0.0637054/0.150009loss in batch 181: 0.354691/0.151123loss in batch 182: 0.232712/0.151566loss in batch 183: 0.0886841/0.15123loss in batch 184: 0.909668/0.155334loss in batch 185: 0.110764/0.15509loss in batch 186: 0.858109/0.158859loss in batch 187: 0.0142517/0.158096loss in batch 188: 0.460999/0.159698loss in batch 189: 0.027298/0.158997loss in batch 190: 0.0990601/0.158676loss in batch 191: 1.09544/0.163544loss in batch 192: 0.0320587/0.162872loss in batch 193: 0.0411682/0.162247loss in batch 194: 0.447617/0.163712loss in batch 195: 0.08461/0.163315loss in batch 196: 0.185349/0.163422loss in batch 197: 0.134354/0.163269loss in batch 198: 0.460175/0.164764loss in batch 199: 0.0347443/0.164108loss in batch 200: 0.223602/0.164413loss in batch 201: 0.213745/0.164642loss in batch 202: 0.0219727/0.163956loss in batch 203: 0.11702/0.163727loss in batch 204: 0.0597992/0.163208loss in batch 205: 0.0930176/0.162888loss in batch 206: 0.0876923/0.162506loss in batch 207: 0.0167999/0.161819loss in batch 208: 0.119095/0.161606loss in batch 209: 0.314529/0.162338loss in batch 210: 0.0391846/0.161743loss in batch 211: 0.378952/0.162781loss in batch 212: 0.119614/0.162582
done with epoch 16
train_acc: 0.943662 (402/426)
test loss: 0.119613
acc: 0.944054 (135/143)
loss in batch 0: 0.352325/0.352325loss in batch 1: 0.0342255/0.193268loss in batch 2: 0.843399/0.409973loss in batch 3: 0.385605/0.4039loss in batch 4: 0.300842/0.38327loss in batch 5: 0.0970154/0.335556loss in batch 6: 0.102234/0.302246loss in batch 7: 0.038208/0.269226loss in batch 8: 0.256561/0.267822loss in batch 9: 0.109467/0.251999loss in batch 10: 0.105591/0.238678loss in batch 11: 0.136154/0.230133loss in batch 12: 0.0747681/0.218185loss in batch 13: 0.119522/0.211136loss in batch 14: 0.411636/0.224503loss in batch 15: 0.1371/0.219055loss in batch 16: 0.217758/0.218964loss in batch 17: 0.296631/0.223282loss in batch 18: 0.197037/0.221893loss in batch 19: 0.116928/0.216644loss in batch 20: 0.0137482/0.206985loss in batch 21: 0.0547028/0.200073loss in batch 22: 0.016861/0.192108loss in batch 23: 0.0691833/0.186981loss in batch 24: 0.00787354/0.17981loss in batch 25: 0.290192/0.184067loss in batch 26: 0.357544/0.190475loss in batch 27: 0.0460663/0.185333loss in batch 28: 0.104019/0.182526loss in batch 29: 0.472168/0.192184loss in batch 30: 0.105087/0.189362loss in batch 31: 0.089035/0.186234loss in batch 32: 0.213593/0.187073loss in batch 33: 0.0222015/0.182205loss in batch 34: 0.117828/0.180374loss in batch 35: 0.127563/0.178909loss in batch 36: 0.0411377/0.175186loss in batch 37: 0.0266724/0.17128loss in batch 38: 0.0731659/0.168762loss in batch 39: 0.10675/0.167221loss in batch 40: 0.0545044/0.164459loss in batch 41: 0.0309448/0.161285loss in batch 42: 0.323364/0.165054loss in batch 43: 0.102798/0.163635loss in batch 44: 0.0408173/0.160919loss in batch 45: 0.0130768/0.1577loss in batch 46: 0.0852203/0.156158loss in batch 47: 0.015152/0.153214loss in batch 48: 0.0312042/0.150711loss in batch 49: 0.164276/0.150986loss in batch 50: 0.0342102/0.148697loss in batch 51: 0.176819/0.149246loss in batch 52: 0.227173/0.150711loss in batch 53: 0.281143/0.153122loss in batch 54: 0.333267/0.156403loss in batch 55: 0.0961151/0.155319loss in batch 56: 0.00964355/0.152786loss in batch 57: 0.377319/0.156631loss in batch 58: 0.0141296/0.154236loss in batch 59: 0.0775757/0.152939loss in batch 60: 0.0605469/0.151428loss in batch 61: 0.0786438/0.150253loss in batch 62: 0.0594177/0.148819loss in batch 63: 0.0313721/0.146973loss in batch 64: 0.500107/0.152405loss in batch 65: 0.0233154/0.150467loss in batch 66: 0.0240784/0.148575loss in batch 67: 0.13475/0.148376loss in batch 68: 0.0692902/0.147232loss in batch 69: 0.380386/0.150558loss in batch 70: 0.0401459/0.149002loss in batch 71: 0.408051/0.152588loss in batch 72: 0.328827/0.155029loss in batch 73: 0.0571289/0.153687loss in batch 74: 0.131714/0.153397loss in batch 75: 0.244553/0.154602loss in batch 76: 0.349091/0.15712loss in batch 77: 0.0210114/0.155365loss in batch 78: 0.00671387/0.153503loss in batch 79: 0.0188599/0.15181loss in batch 80: 0.212997/0.152573loss in batch 81: 0.122086/0.152206loss in batch 82: 0.0279999/0.150696loss in batch 83: 0.207306/0.151367loss in batch 84: 0.216217/0.15213loss in batch 85: 0.128021/0.151871loss in batch 86: 0.00439453/0.150162loss in batch 87: 0.138016/0.150024loss in batch 88: 0.327362/0.152023loss in batch 89: 0.389297/0.154648loss in batch 90: 0.0371857/0.153351loss in batch 91: 0.0755463/0.152512loss in batch 92: 0.310333/0.154221loss in batch 93: 0.118988/0.153839loss in batch 94: 0.0665131/0.152908loss in batch 95: 0.0067749/0.151398loss in batch 96: 0.160172/0.151489loss in batch 97: 0.00730896/0.150024loss in batch 98: 0.0685577/0.149185loss in batch 99: 0.0292969/0.147995loss in batch 100: 0.347076/0.149963loss in batch 101: 0.0196686/0.148697loss in batch 102: 0.0497284/0.14772loss in batch 103: 0.141312/0.147659loss in batch 104: 0.600037/0.151962loss in batch 105: 0.528015/0.155518loss in batch 106: 0.073761/0.154755loss in batch 107: 0.0228424/0.153534loss in batch 108: 0.052597/0.152603loss in batch 109: 0.039856/0.151581loss in batch 110: 0.00759888/0.150284loss in batch 111: 0.0111847/0.149048loss in batch 112: 0.0284119/0.14798loss in batch 113: 0.308182/0.149384loss in batch 114: 0.0358582/0.148392loss in batch 115: 0.0311432/0.147385loss in batch 116: 0.0813446/0.14682loss in batch 117: 0.079361/0.146255loss in batch 118: 0.239609/0.147034loss in batch 119: 0.00718689/0.145859loss in batch 120: 0.354523/0.147598loss in batch 121: 0.0207062/0.146545loss in batch 122: 0.0667114/0.145905loss in batch 123: 0.782181/0.151047loss in batch 124: 0.146912/0.151001loss in batch 125: 0.0811005/0.150436loss in batch 126: 0.136765/0.150345loss in batch 127: 0.0748749/0.14975loss in batch 128: 0.0408478/0.148911loss in batch 129: 0.101151/0.148529loss in batch 130: 0.0504456/0.147781loss in batch 131: 0.0569305/0.147095loss in batch 132: 0.0105896/0.146088loss in batch 133: 0.492645/0.148651loss in batch 134: 0.0515442/0.147949loss in batch 135: 0.00627136/0.146912loss in batch 136: 0.107941/0.146606loss in batch 137: 1.10873/0.153595loss in batch 138: 0.426956/0.155548loss in batch 139: 0.277176/0.156433loss in batch 140: 0.342972/0.157745loss in batch 141: 0.144226/0.157654loss in batch 142: 0.0392914/0.15683loss in batch 143: 0.32782/0.15802loss in batch 144: 0.0732269/0.157425loss in batch 145: 0.071991/0.156845loss in batch 146: 0.0966034/0.156433loss in batch 147: 0.0496979/0.155716loss in batch 148: 0.259094/0.156418loss in batch 149: 0.248581/0.157013loss in batch 150: 0.0158386/0.156082loss in batch 151: 0.0691376/0.155518loss in batch 152: 0.00164795/0.15451loss in batch 153: 0.135498/0.154373loss in batch 154: 0.019455/0.153503loss in batch 155: 0.0195923/0.152649loss in batch 156: 0.242081/0.153214loss in batch 157: 0.0722656/0.15271loss in batch 158: 0.0773163/0.152237loss in batch 159: 0.00242615/0.151306loss in batch 160: 0.0205231/0.150482loss in batch 161: 0.00827026/0.149612loss in batch 162: 0.0335388/0.148895loss in batch 163: 0.0263062/0.148148loss in batch 164: 0.00332642/0.147278loss in batch 165: 0.112732/0.147064loss in batch 166: 0.0494843/0.146484loss in batch 167: 0.0736542/0.146042loss in batch 168: 0.16748/0.146164loss in batch 169: 0.0667114/0.145706loss in batch 170: 0.0749207/0.145294loss in batch 171: 0.108139/0.145081loss in batch 172: 0.0901031/0.14476loss in batch 173: 0.0343018/0.144119loss in batch 174: 0.340179/0.145233loss in batch 175: 0.052597/0.144714loss in batch 176: 0.37381/0.146011loss in batch 177: 0.542786/0.148239loss in batch 178: 0.116852/0.148056loss in batch 179: 0.0517883/0.147522loss in batch 180: 0.171097/0.147659loss in batch 181: 0.0708771/0.147232loss in batch 182: 1.41243/0.154144loss in batch 183: 0.0453796/0.153564loss in batch 184: 1.15099/0.158951loss in batch 185: 0.0293427/0.158249loss in batch 186: 0.0253601/0.157547loss in batch 187: 0.0598297/0.157028loss in batch 188: 0.220169/0.157349loss in batch 189: 0.26059/0.157913loss in batch 190: 0.0452118/0.157318loss in batch 191: 0.0208435/0.156601loss in batch 192: 0.032959/0.15596loss in batch 193: 0.0344696/0.155334loss in batch 194: 0.242264/0.155777loss in batch 195: 0.100418/0.155502loss in batch 196: 0.0010376/0.154709loss in batch 197: 0.156372/0.154724loss in batch 198: 0.0769043/0.154327loss in batch 199: 0.579071/0.156448loss in batch 200: 0.0107727/0.155731loss in batch 201: 0.506668/0.157471loss in batch 202: 0.259857/0.157974loss in batch 203: 0.0416565/0.157394loss in batch 204: 0.665558/0.159866loss in batch 205: 0.0487671/0.159332loss in batch 206: 0.0617676/0.158875loss in batch 207: 0.0475311/0.15834loss in batch 208: 0.0764618/0.157944loss in batch 209: 0.030365/0.157349loss in batch 210: 0.217163/0.157623loss in batch 211: 0.175934/0.1577loss in batch 212: 0.290024/0.158325
done with epoch 17
train_acc: 0.943662 (402/426)
test loss: 0.290023
acc: 0.944054 (135/143)
loss in batch 0: 0.0453796/0.0453796loss in batch 1: 0.408569/0.226974loss in batch 2: 0.0554657/0.1698loss in batch 3: 0.0413208/0.137695loss in batch 4: 0.00157166/0.110458loss in batch 5: 0.0544434/0.10112loss in batch 6: 0.44342/0.150024loss in batch 7: 0.375107/0.178146loss in batch 8: 0.0633392/0.165405loss in batch 9: 0.21225/0.17009loss in batch 10: 0.0178528/0.156235loss in batch 11: 0.0570831/0.147995loss in batch 12: 0.0442657/0.139999loss in batch 13: 0.394226/0.158157loss in batch 14: 0.801758/0.201065loss in batch 15: 0.394333/0.21315loss in batch 16: 0.000320435/0.200623loss in batch 17: 0.0519409/0.192368loss in batch 18: 0.0571899/0.185257loss in batch 19: 0.0433807/0.178162loss in batch 20: 0.0676575/0.172897loss in batch 21: 0.0218201/0.166031loss in batch 22: 0.0877533/0.162628loss in batch 23: 0.399933/0.172516loss in batch 24: 0.00572205/0.165848loss in batch 25: 0.0471649/0.161285loss in batch 26: 0.0288086/0.156372loss in batch 27: 0.328278/0.162521loss in batch 28: 0.0247803/0.157761loss in batch 29: 0.0362396/0.153702loss in batch 30: 0.0674286/0.150925loss in batch 31: 0.00941467/0.146515loss in batch 32: 0.083725/0.144592loss in batch 33: 0.0529175/0.141922loss in batch 34: 0.0539551/0.139389loss in batch 35: 0.0295563/0.136337loss in batch 36: 0.516525/0.146606loss in batch 37: 0.0895233/0.145111loss in batch 38: 0.0262451/0.142059loss in batch 39: 0.127563/0.141708loss in batch 40: 0.0363312/0.139145loss in batch 41: 0.0153503/0.136185loss in batch 42: 0.163727/0.136841loss in batch 43: 0.0448303/0.134735loss in batch 44: 0.0228577/0.132248loss in batch 45: 0.0461578/0.130371loss in batch 46: 0.0465393/0.128586loss in batch 47: 0.0525055/0.127014loss in batch 48: 0.133957/0.127151loss in batch 49: 0.251572/0.129639loss in batch 50: 0.505264/0.137009loss in batch 51: 0.568512/0.145294loss in batch 52: 0.0450592/0.143417loss in batch 53: 0.0660248/0.141983loss in batch 54: 0.130981/0.141769loss in batch 55: 0.0251465/0.139694loss in batch 56: 0.0540314/0.138199loss in batch 57: 0.242203/0.139999loss in batch 58: 0.0456543/0.138382loss in batch 59: 0.0351257/0.136658loss in batch 60: 0.219162/0.138016loss in batch 61: 0.0269623/0.13623loss in batch 62: 0.0279541/0.134506loss in batch 63: 0.0932159/0.133865loss in batch 64: 0.542236/0.140137loss in batch 65: 0.151138/0.14032loss in batch 66: 0.0354614/0.138748loss in batch 67: 0.0309143/0.137161loss in batch 68: 0.0129547/0.135361loss in batch 69: 0.0131989/0.133621loss in batch 70: 0.0962982/0.133087loss in batch 71: 0.0159454/0.13147loss in batch 72: 0.277603/0.133469loss in batch 73: 0.369644/0.136658loss in batch 74: 0.0212097/0.135117loss in batch 75: 0.497604/0.139877loss in batch 76: 0.0112762/0.138214loss in batch 77: 0.341125/0.140823loss in batch 78: 0.0486908/0.139648loss in batch 79: 0.794815/0.147842loss in batch 80: 0.159927/0.147995loss in batch 81: 0.0879364/0.147263loss in batch 82: 0.334579/0.149506loss in batch 83: 0.159912/0.149628loss in batch 84: 0.00921631/0.14798loss in batch 85: 0.0770416/0.147171loss in batch 86: 0.294327/0.148865loss in batch 87: 0.0150604/0.147324loss in batch 88: 0.0129395/0.145828loss in batch 89: 0.0888214/0.145187loss in batch 90: 0.037262/0.143997loss in batch 91: 0.0572968/0.143066loss in batch 92: 0.113113/0.142746loss in batch 93: 0.354614/0.145004loss in batch 94: 0.00762939/0.143555loss in batch 95: 0.398102/0.146194loss in batch 96: 0.0148315/0.144852loss in batch 97: 0.0280457/0.143646loss in batch 98: 0.644547/0.148712loss in batch 99: 0.611496/0.153351loss in batch 100: 0.327911/0.155075loss in batch 101: 0.112015/0.154648loss in batch 102: 0.0017395/0.153152loss in batch 103: 0.105988/0.15271loss in batch 104: 0.000610352/0.15126loss in batch 105: 0.0375824/0.150192loss in batch 106: 0.00502014/0.148834loss in batch 107: 0.632126/0.153305loss in batch 108: 0.074295/0.152588loss in batch 109: 0.0840759/0.151962loss in batch 110: 0.876938/0.158493loss in batch 111: 0.14682/0.158386loss in batch 112: 0.00540161/0.157028loss in batch 113: 0.00216675/0.155685loss in batch 114: 0.223984/0.156265loss in batch 115: 0.227829/0.156891loss in batch 116: 0.0291138/0.155792loss in batch 117: 0.238846/0.156494loss in batch 118: 0.0105133/0.155258loss in batch 119: 0.095459/0.15477loss in batch 120: 0.525787/0.157837loss in batch 121: 0.00854492/0.156601loss in batch 122: 0.0632324/0.155869loss in batch 123: 0.0845947/0.155289loss in batch 124: 0.0882263/0.154755loss in batch 125: 0.0319366/0.153763loss in batch 126: 0.0266266/0.152771loss in batch 127: 0.0148315/0.151688loss in batch 128: 0.0894012/0.151215loss in batch 129: 0.187195/0.151474loss in batch 130: 0.010849/0.150406loss in batch 131: 0.0505066/0.149658loss in batch 132: 0.0576172/0.148956loss in batch 133: 0.0147247/0.147949loss in batch 134: 0.0653076/0.147354loss in batch 135: 0.0547943/0.146667loss in batch 136: 0.040741/0.145889loss in batch 137: 0.0460968/0.145172loss in batch 138: 0.0164337/0.144257loss in batch 139: 0.0889435/0.143845loss in batch 140: 0.031189/0.143051loss in batch 141: 0.295135/0.144135loss in batch 142: 0.234543/0.14476loss in batch 143: 1.06502/0.151138loss in batch 144: 0.0346375/0.150345loss in batch 145: 0.0199127/0.149445loss in batch 146: 0.047821/0.148758loss in batch 147: 0.0989838/0.148422loss in batch 148: 0.0474091/0.147736loss in batch 149: 0.227982/0.14827loss in batch 150: 1.48045/0.157089loss in batch 151: 0.150894/0.157059loss in batch 152: 0.0282593/0.156219loss in batch 153: 0.241043/0.156754loss in batch 154: 0.192932/0.156998loss in batch 155: 0.0198975/0.156113loss in batch 156: 0.374619/0.157516loss in batch 157: 0.0493622/0.15683loss in batch 158: 0.062027/0.156235loss in batch 159: 0.0045929/0.155289loss in batch 160: 0.111343/0.155014loss in batch 161: 0.0717316/0.15451loss in batch 162: 0.171356/0.154602loss in batch 163: 0.232758/0.155075loss in batch 164: 0.234222/0.155563loss in batch 165: 0.0826263/0.155106loss in batch 166: 0.105469/0.154816loss in batch 167: 0.153137/0.1548loss in batch 168: 0.0458832/0.15416loss in batch 169: 0.0442963/0.153519loss in batch 170: 0.0601349/0.152985loss in batch 171: 0.0899506/0.152618loss in batch 172: 0.0141754/0.15181loss in batch 173: 0.145737/0.151779loss in batch 174: 0.320862/0.15274loss in batch 175: 0.0242462/0.152008loss in batch 176: 0.0693054/0.151535loss in batch 177: 0.410614/0.153loss in batch 178: 0.162766/0.153046loss in batch 179: 0.199387/0.153305loss in batch 180: 0.0111542/0.152527loss in batch 181: 0.0250549/0.15181loss in batch 182: 0.362091/0.152969loss in batch 183: 0.0326233/0.152313loss in batch 184: 0.0215454/0.151611loss in batch 185: 0.530075/0.153656loss in batch 186: 0.0334778/0.153loss in batch 187: 1.20502/0.1586loss in batch 188: 0.0125427/0.157822loss in batch 189: 0.0442352/0.157227loss in batch 190: 0.0665741/0.156754loss in batch 191: 0.0255737/0.156067loss in batch 192: 0.0209656/0.155365loss in batch 193: 0.262634/0.15593loss in batch 194: 0.0152283/0.155197loss in batch 195: 0.0977631/0.154907loss in batch 196: 0.0286865/0.154266loss in batch 197: 0.0500336/0.153748loss in batch 198: 0.221115/0.154083loss in batch 199: 0.45578/0.155594loss in batch 200: 0.0757599/0.155197loss in batch 201: 0.0551758/0.154709loss in batch 202: 0.260559/0.155212loss in batch 203: 0.0939484/0.154922loss in batch 204: 0.326767/0.155762loss in batch 205: 0.146729/0.155701loss in batch 206: 0.0183258/0.155045loss in batch 207: 0.059021/0.154587loss in batch 208: 0.147369/0.154541loss in batch 209: 0.128647/0.154419loss in batch 210: 0.0410156/0.153885loss in batch 211: 0.328613/0.154709loss in batch 212: 0.369934/0.155731
done with epoch 18
train_acc: 0.946009 (403/426)
test loss: 0.369933
acc: 0.944054 (135/143)
loss in batch 0: 0.00590515/0.00590515loss in batch 1: 0.136749/0.0713196loss in batch 2: 0.0513458/0.0646667loss in batch 3: 0.0432281/0.0593109loss in batch 4: 0.0412598/0.0556946loss in batch 5: 0.0969391/0.062561loss in batch 6: 0.0572815/0.0618134loss in batch 7: 0.00524902/0.0547333loss in batch 8: 0.0458527/0.0537567loss in batch 9: 0.00378418/0.0487518loss in batch 10: 0.0901337/0.0525208loss in batch 11: 0.0157318/0.0494537loss in batch 12: 0.249054/0.0648041loss in batch 13: 0.0105743/0.0609283loss in batch 14: 0.0367737/0.0593262loss in batch 15: 0.195908/0.0678558loss in batch 16: 0.0411682/0.0662842loss in batch 17: 0.0850525/0.067337loss in batch 18: 0.948135/0.113693loss in batch 19: 0.118042/0.113907loss in batch 20: 0.135574/0.114944loss in batch 21: 0.401031/0.127945loss in batch 22: 0.0567474/0.124847loss in batch 23: 0.112915/0.124359loss in batch 24: 0.31366/0.131912loss in batch 25: 0.0417633/0.128448loss in batch 26: 0.120102/0.128143loss in batch 27: 0.113602/0.127625loss in batch 28: 0.973846/0.156815loss in batch 29: 0.0456085/0.153091loss in batch 30: 0.116241/0.151901loss in batch 31: 0.0542755/0.148865loss in batch 32: 0.0722351/0.146545loss in batch 33: 0.075592/0.144455loss in batch 34: 0.0344696/0.141312loss in batch 35: 0.00691223/0.137573loss in batch 36: 0.180115/0.138718loss in batch 37: 0.0118256/0.135376loss in batch 38: 0.26059/0.138596loss in batch 39: 0.00695801/0.1353loss in batch 40: 0.0102234/0.132248loss in batch 41: 0.0212402/0.129608loss in batch 42: 0.0107727/0.126862loss in batch 43: 0.097641/0.12619loss in batch 44: 0.373672/0.131683loss in batch 45: 0.0204773/0.129257loss in batch 46: 0.0482483/0.127533loss in batch 47: 0.0427399/0.125778loss in batch 48: 0.0102997/0.123428loss in batch 49: 0.617554/0.133301loss in batch 50: 0.0288849/0.131256loss in batch 51: 0.0893555/0.130447loss in batch 52: 0.341293/0.13443loss in batch 53: 0.0210419/0.132324loss in batch 54: 0.480865/0.138657loss in batch 55: 0.22049/0.140121loss in batch 56: 0.0776367/0.139023loss in batch 57: 0.219879/0.140427loss in batch 58: 0.360764/0.144165loss in batch 59: 0.217041/0.14537loss in batch 60: 0.0439453/0.143707loss in batch 61: 0.596497/0.151001loss in batch 62: 0.041626/0.149277loss in batch 63: 0.0065155/0.147034loss in batch 64: 0.296387/0.149338loss in batch 65: 0.0680084/0.148102loss in batch 66: 0.0119171/0.146072loss in batch 67: 0.500351/0.151291loss in batch 68: 0.133408/0.151031loss in batch 69: 0.0401611/0.149445loss in batch 70: 0.0142822/0.147537loss in batch 71: 0.0217285/0.145782loss in batch 72: 0.386932/0.149094loss in batch 73: 0.0552063/0.147827loss in batch 74: 0.448593/0.15184loss in batch 75: 0.0434265/0.150421loss in batch 76: 1.12814/0.163101loss in batch 77: 0.0455933/0.161606loss in batch 78: 0.0760651/0.160522loss in batch 79: 0.442047/0.164047loss in batch 80: 0.0372162/0.162476loss in batch 81: 0.380386/0.165131loss in batch 82: 0.0994263/0.164337loss in batch 83: 0.00692749/0.162476loss in batch 84: 0.0183411/0.160767loss in batch 85: 0.0287781/0.159241loss in batch 86: 0.0536499/0.15802loss in batch 87: 0.0113983/0.156357loss in batch 88: 0.176361/0.156586loss in batch 89: 0.264709/0.157791loss in batch 90: 0.520187/0.161774loss in batch 91: 0.130997/0.161423loss in batch 92: 0.0291901/0.160004loss in batch 93: 0.00491333/0.158356loss in batch 94: 0.0222778/0.156937loss in batch 95: 0.108948/0.156433loss in batch 96: 0.0341339/0.155151loss in batch 97: 0.165237/0.155258loss in batch 98: 0.00183105/0.153717loss in batch 99: 0.00985718/0.152267loss in batch 100: 0.266754/0.153412loss in batch 101: 0.0296631/0.152206loss in batch 102: 0.0268402/0.150986loss in batch 103: 0.0607147/0.150116loss in batch 104: 0.081665/0.14946loss in batch 105: 0.0680084/0.148697loss in batch 106: 0.0252686/0.147552loss in batch 107: 0.185623/0.147888loss in batch 108: 0.00746155/0.146606loss in batch 109: 0.132904/0.146484loss in batch 110: 0.0669708/0.145767loss in batch 111: 0.227402/0.146484loss in batch 112: 0.150482/0.14653loss in batch 113: 0.012207/0.145355loss in batch 114: 0.174194/0.145599loss in batch 115: 0.048111/0.14476loss in batch 116: 0.404633/0.146973loss in batch 117: 0.266525/0.147995loss in batch 118: 0.113037/0.147705loss in batch 119: 0.0667419/0.147018loss in batch 120: 0.0309448/0.146072loss in batch 121: 0.0274506/0.145096loss in batch 122: 0.0637817/0.144424loss in batch 123: 0.072464/0.143845loss in batch 124: 0.0125732/0.142807loss in batch 125: 0.0915985/0.142395loss in batch 126: 0.460342/0.144913loss in batch 127: 0.254303/0.145767loss in batch 128: 0.0247955/0.144821loss in batch 129: 0.0156403/0.143814loss in batch 130: 0.0276794/0.142929loss in batch 131: 0.280182/0.143967loss in batch 132: 0.184616/0.144272loss in batch 133: 0.0302429/0.143417loss in batch 134: 0.283722/0.14447loss in batch 135: 0.139832/0.144424loss in batch 136: 0.0203705/0.143524loss in batch 137: 0.0213623/0.142654loss in batch 138: 0.136246/0.142609loss in batch 139: 0.0400848/0.141861loss in batch 140: 0.00793457/0.140915loss in batch 141: 0.0351105/0.140167loss in batch 142: 0.276016/0.141129loss in batch 143: 0.127167/0.141022loss in batch 144: 0.259811/0.141846loss in batch 145: 0.359253/0.143326loss in batch 146: 0.0307465/0.142563loss in batch 147: 0.0045166/0.141632loss in batch 148: 0.0229034/0.140839loss in batch 149: 0.215454/0.141327loss in batch 150: 0.198303/0.141708loss in batch 151: 0.048233/0.141098loss in batch 152: 0.0301056/0.140366loss in batch 153: 0.0296478/0.139648loss in batch 154: 0.19043/0.139984loss in batch 155: 0.377594/0.141495loss in batch 156: 0.0115204/0.140671loss in batch 157: 0.0639496/0.140182loss in batch 158: 0.00939941/0.139374loss in batch 159: 0.474503/0.141464loss in batch 160: 0.511612/0.143753loss in batch 161: 0.158157/0.14386loss in batch 162: 0.123444/0.143723loss in batch 163: 0.871887/0.148163loss in batch 164: 0.132736/0.148071loss in batch 165: 0.0490265/0.147461loss in batch 166: 0.025528/0.146759loss in batch 167: 0.00335693/0.145889loss in batch 168: 0.0222626/0.145157loss in batch 169: 0.787659/0.148941loss in batch 170: 0.102646/0.148666loss in batch 171: 0.0387878/0.148041loss in batch 172: 0.0281525/0.147339loss in batch 173: 0.0982971/0.147049loss in batch 174: 0.036377/0.146423loss in batch 175: 0.112473/0.146225loss in batch 176: 1.57599/0.154312loss in batch 177: 0.251633/0.154861loss in batch 178: 0.235352/0.155304loss in batch 179: 0.0942383/0.154953loss in batch 180: 0.198242/0.155197loss in batch 181: 0.0507507/0.154617loss in batch 182: 0.0759583/0.15419loss in batch 183: 0.0411987/0.15358loss in batch 184: 0.0170288/0.152847loss in batch 185: 0.0422058/0.152252loss in batch 186: 0.0320282/0.151611loss in batch 187: 0.0145111/0.150879loss in batch 188: 0.262222/0.151459loss in batch 189: 0.122055/0.151321loss in batch 190: 0.246765/0.15181loss in batch 191: 0.239441/0.152267loss in batch 192: 0.0536652/0.151764loss in batch 193: 0.404816/0.153061loss in batch 194: 0.00650024/0.152313loss in batch 195: 0.0351257/0.151718loss in batch 196: 0.115372/0.151535loss in batch 197: 0.0870667/0.151215loss in batch 198: 0.0919189/0.150909loss in batch 199: 0.321152/0.151764loss in batch 200: 0.015686/0.151093loss in batch 201: 0.246414/0.15155loss in batch 202: 0.0705872/0.151154loss in batch 203: 0.0146637/0.150482loss in batch 204: 0.0375977/0.149933loss in batch 205: 0.231995/0.15033loss in batch 206: 0.119949/0.150192loss in batch 207: 0.520889/0.151962loss in batch 208: 0.448746/0.153381loss in batch 209: 0.0792389/0.15303loss in batch 210: 0.150681/0.153015loss in batch 211: 0.30162/0.153717loss in batch 212: 0.668472/0.156143
done with epoch 19
train_acc: 0.943662 (402/426)
test loss: 0.668471
acc: 0.944054 (135/143)
[0.0995483, -0.0860291, -0.0904541, -0.14595, -0.0296021, -0.04599, -0.13736, -0.0204315, -0.00158691, -0.00343323, -0.395782, -0.151321, -0.00907898, -0.381439, -0.52095, 0.0442963, -0.082901, 0.00759888, 0.0285645, 0.00050354, 0.304352, 0.0446472, -0.240082, -0.0446472, 0.00750732, -0.0479279, -0.0129242, 0.0516357, -0.0650024, 0.0154419, -0.0144501, 0.0301056, -0.447205, 0.189316, -0.00212097, 0.198456, -0.13446, 0.0930786, -0.134338, 0.192093, 0.763168, -0.0101929, 0.544083, -0.00640869, -0.501022, 0.000534058, -0.00149536, -0.0851288, -0.016861, 0.0602264, 0.0247498, 0.0888214, 0.0472107, -0.0546417, -0.0775909, -0.0225525, -0.0214844, -0.0102386, -0.0604553, 0.000488281, 0.298935, 0.134109, -0.0153503, -0.0423737, 0.0123596, -0.056839, 0, 0.008255, 0.00494385, -0.0109558, -0.490463, 0.0180664, -0.0598145, 0.443817, 0.00280762, -0.0680237, -0.0109863, -0.224304, -0.0196381, -0.00134277, 0.152405, 0.000289917, 0.0177612, -0.0142517, 0.0660858, -0.0594177, -0.023468, -0.00378418, 0.0384521, 0, -0.00820923, 0.461731, -0.510757, 0.00276184, -0.00537109, -0.0563507, 0.00942993, -0.627106, -0.0440063, -0.0239105, -0.00163269, -0.0881958, -0.0978241, -0.245209, 0.0367432, -0.0157318, 0.00202942, -0.21489, 0.618744, 0.659439, -0.0615997, 0.0045166, 0.030426, -0.385498, -0.0169067, -0.045166, -0.405533, -0.0236359, -0.0613556, -0.16394, -0.0381622, -0.0369568, 0.207962, -0.143021, 0.00260925, -0.203033, -0.297287, -0.0360718, -0.0188904, -0.00521851, 0.0878906, -0.00408936, -0.0174103, -0.324036, -0.201187, -0.0136108, -0.2939, 0.471863, 6.10352e-05, -0.0116577, -0.107193, -0.0436249, 0.000457764]
Compiler: ./compile.py -R 64 breast_logistic
	821 dabits of replicated Z2^64 left
	3988 dabits of replicated Z2^64 left
2 threads spent a total of 534.601 seconds (191.786 MB, 1555063 rounds) on the online phase, 23.7639 seconds (1055.09 MB, 52492 rounds) on the preprocessing/offline phase, and 560.815 seconds idling.
Join timer: 0 559572
Finish timer: 0.0577886
Join timer: 1 547783
Finish timer: 0.0577886
Communication details (rounds in parallel threads counted double):
Passing around 191.645 MB in 1555045 rounds, taking 198.352 seconds
Receiving directly 1055.09 MB in 26246 rounds, taking 3.99638 seconds
Sending directly 1055.23 MB in 26264 rounds, taking 3.28273 seconds
CPU time = 659.965 (overall core time)
The following benchmarks are including preprocessing (offline phase).
Time = 559.63 seconds 
Data sent = 1246.88 MB in ~1607555 rounds (party 0 only)
Global data sent = 3740.34 MB (all parties)
Actual cost of program:
  Type int
     131855095           Bits
This program might benefit from some protocol options.
Consider adding the following at the beginning of your code:
	program.use_trunc_pr = True
	program.use_split(3)
This virtual machine was compiled with GCC. Recompile with 'CXX = clang++' in 'CONFIG.mine' for optimal performance.
Command line: replicated-ring-party.x --ip-file-name /HOST -p 0 -v breast_logistic
