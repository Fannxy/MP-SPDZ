Using statistical security parameter 40
Trying to run 64-bit computation
Current working directory: "/root/MP-SPDZ"
Current working directory: "/root/MP-SPDZ"
Using SGD
done with epoch 0done with epoch 1done with epoch 2done with epoch 3done with epoch 4done with epoch 5done with epoch 6done with epoch 7done with epoch 8done with epoch 9done with epoch 10done with epoch 11done with epoch 12done with epoch 13done with epoch 14done with epoch 15done with epoch 16done with epoch 17done with epoch 18done with epoch 19
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Using SGD
loss in batch 0: 0.589935/0.589935loss in batch 1: 0.464828/0.527374loss in batch 2: 0.831406/0.628723loss in batch 3: 0.772675/0.664719loss in batch 4: 0.49205/0.630188loss in batch 5: 0.581802/0.622116loss in batch 6: 0.767517/0.642883loss in batch 7: 0.793015/0.661652loss in batch 8: 0.570068/0.651489loss in batch 9: 0.715897/0.657913loss in batch 10: 0.644775/0.656723loss in batch 11: 0.636887/0.655075loss in batch 12: 0.742188/0.661774loss in batch 13: 0.603607/0.657608loss in batch 14: 0.561066/0.651184loss in batch 15: 0.8367/0.662781loss in batch 16: 0.681839/0.66391loss in batch 17: 0.532318/0.656601loss in batch 18: 0.630814/0.655228loss in batch 19: 0.45784/0.645355loss in batch 20: 0.546371/0.64064loss in batch 21: 0.769485/0.6465loss in batch 22: 0.842163/0.655014loss in batch 23: 0.871887/0.664047loss in batch 24: 0.510361/0.657898loss in batch 25: 0.513062/0.652328loss in batch 26: 0.598007/0.65033loss in batch 27: 0.531876/0.646088loss in batch 28: 0.56958/0.643448loss in batch 29: 0.534027/0.639801loss in batch 30: 0.574158/0.63768loss in batch 31: 0.629852/0.637436loss in batch 32: 0.490341/0.63298loss in batch 33: 0.664688/0.633911loss in batch 34: 0.688248/0.635468loss in batch 35: 0.600342/0.634491loss in batch 36: 0.618851/0.634079loss in batch 37: 0.506927/0.630722loss in batch 38: 0.542068/0.628448loss in batch 39: 0.494812/0.625107loss in batch 40: 0.586716/0.624161loss in batch 41: 0.579483/0.623108loss in batch 42: 0.593811/0.622421loss in batch 43: 0.8004/0.62648loss in batch 44: 0.579636/0.625427loss in batch 45: 0.626007/0.625443loss in batch 46: 0.525406/0.623306loss in batch 47: 0.604507/0.622925loss in batch 48: 0.563629/0.621704loss in batch 49: 0.47612/0.61879loss in batch 50: 0.429932/0.615097loss in batch 51: 0.753677/0.617767loss in batch 52: 0.604034/0.617493loss in batch 53: 0.479965/0.61496loss in batch 54: 0.553757/0.613846loss in batch 55: 0.514114/0.612061loss in batch 56: 0.581711/0.611526loss in batch 57: 0.675491/0.612625loss in batch 58: 0.647522/0.61322loss in batch 59: 0.55899/0.61232loss in batch 60: 0.428772/0.609299loss in batch 61: 0.714737/0.611008loss in batch 62: 0.580353/0.610535loss in batch 63: 0.569427/0.609879loss in batch 64: 0.5737/0.609329loss in batch 65: 0.493927/0.607574loss in batch 66: 0.691162/0.608826loss in batch 67: 0.483093/0.606979loss in batch 68: 0.548309/0.606125loss in batch 69: 0.751862/0.608215loss in batch 70: 0.370316/0.604858loss in batch 71: 0.348221/0.601303loss in batch 72: 0.573685/0.600922loss in batch 73: 0.688217/0.602097loss in batch 74: 0.686523/0.603226loss in batch 75: 0.73111/0.604904loss in batch 76: 0.740341/0.606659loss in batch 77: 0.489517/0.605164loss in batch 78: 0.526962/0.604172loss in batch 79: 0.515152/0.603058loss in batch 80: 0.559036/0.602509loss in batch 81: 0.406357/0.600128loss in batch 82: 0.432785/0.598099loss in batch 83: 0.670761/0.598984loss in batch 84: 0.460098/0.597336loss in batch 85: 0.598282/0.597336loss in batch 86: 0.662262/0.598099loss in batch 87: 0.46051/0.596527loss in batch 88: 0.563171/0.596161loss in batch 89: 0.653687/0.596802loss in batch 90: 0.527344/0.596039loss in batch 91: 0.38063/0.593689loss in batch 92: 0.472275/0.592392loss in batch 93: 0.497101/0.59137loss in batch 94: 0.54509/0.590881loss in batch 95: 0.55687/0.59053loss in batch 96: 0.518509/0.589783loss in batch 97: 0.360046/0.587433loss in batch 98: 0.416214/0.585724loss in batch 99: 0.348297/0.583344loss in batch 100: 0.398331/0.581512loss in batch 101: 0.54718/0.581161loss in batch 102: 0.528961/0.580658loss in batch 103: 0.537079/0.580246loss in batch 104: 0.461578/0.579117loss in batch 105: 0.514359/0.578506loss in batch 106: 0.51149/0.577881loss in batch 107: 0.413269/0.576355loss in batch 108: 0.455643/0.575256loss in batch 109: 0.450027/0.574097loss in batch 110: 0.548935/0.573868loss in batch 111: 0.414383/0.572449loss in batch 112: 0.484161/0.571671loss in batch 113: 0.453079/0.570633loss in batch 114: 0.37619/0.568939loss in batch 115: 0.442398/0.567841loss in batch 116: 0.552017/0.567719loss in batch 117: 0.370438/0.56604loss in batch 118: 0.445313/0.565033loss in batch 119: 0.662521/0.565842loss in batch 120: 0.367188/0.564194loss in batch 121: 0.272385/0.561798loss in batch 122: 0.544647/0.561661loss in batch 123: 0.542252/0.561508loss in batch 124: 0.187271/0.558517loss in batch 125: 0.328323/0.556702loss in batch 126: 0.67691/0.557632loss in batch 127: 0.209335/0.554916loss in batch 128: 0.632019/0.555511loss in batch 129: 0.123352/0.5522loss in batch 130: 0.148056/0.549118loss in batch 131: 0.94545/0.552109loss in batch 132: 0.542084/0.552032loss in batch 133: 0.159729/0.549103loss in batch 134: 1.33661/0.554947loss in batch 135: 0.170212/0.552109loss in batch 136: 1.16527/0.556595loss in batch 137: 0.649841/0.557266loss in batch 138: 0.172058/0.554489loss in batch 139: 0.724579/0.55571loss in batch 140: 0.535324/0.555557loss in batch 141: 0.462341/0.554901loss in batch 142: 0.571747/0.555023loss in batch 143: 0.229523/0.552765loss in batch 144: 0.342331/0.551315loss in batch 145: 0.621689/0.551788loss in batch 146: 0.184998/0.549286loss in batch 147: 0.605194/0.549683loss in batch 148: 0.33696/0.548233loss in batch 149: 0.393723/0.547211loss in batch 150: 0.287598/0.545502loss in batch 151: 0.323608/0.544037loss in batch 152: 0.369949/0.542892loss in batch 153: 0.489044/0.542557loss in batch 154: 0.376602/0.541473loss in batch 155: 0.623596/0.542007loss in batch 156: 0.398529/0.541092loss in batch 157: 0.345551/0.539856loss in batch 158: 0.474197/0.539444loss in batch 159: 0.504593/0.53923loss in batch 160: 0.36438/0.538147loss in batch 161: 0.7827/0.539642loss in batch 162: 0.510971/0.539459loss in batch 163: 0.37645/0.538483loss in batch 164: 0.453384/0.537964loss in batch 165: 0.486359/0.537659loss in batch 166: 0.674255/0.538467loss in batch 167: 0.470551/0.538071loss in batch 168: 0.701874/0.539032loss in batch 169: 0.373871/0.538055loss in batch 170: 0.42038/0.537369loss in batch 171: 0.400864/0.536575loss in batch 172: 0.746292/0.537781loss in batch 173: 0.452759/0.537308loss in batch 174: 0.353241/0.536255loss in batch 175: 0.389084/0.535416loss in batch 176: 0.390656/0.534607loss in batch 177: 0.658066/0.535294loss in batch 178: 0.470718/0.534943loss in batch 179: 0.293823/0.5336loss in batch 180: 0.320221/0.53241loss in batch 181: 0.324875/0.531265loss in batch 182: 0.14415/0.52916loss in batch 183: 0.402115/0.528473loss in batch 184: 0.376587/0.527649loss in batch 185: 0.440384/0.527176loss in batch 186: 0.702545/0.528107loss in batch 187: 0.292877/0.526871loss in batch 188: 0.378479/0.526077loss in batch 189: 0.462738/0.525742loss in batch 190: 0.388458/0.525024loss in batch 191: 0.189651/0.52327loss in batch 192: 0.351791/0.522385loss in batch 193: 0.253601/0.520996loss in batch 194: 0.262589/0.519669loss in batch 195: 0.338928/0.518753loss in batch 196: 0.429474/0.518295loss in batch 197: 0.433014/0.517883loss in batch 198: 0.367401/0.51712loss in batch 199: 0.36615/0.516357loss in batch 200: 0.376328/0.515671loss in batch 201: 0.313889/0.514664loss in batch 202: 0.541718/0.514786loss in batch 203: 0.356232/0.514023loss in batch 204: 0.221359/0.512589loss in batch 205: 0.358887/0.511841loss in batch 206: 0.210815/0.510391loss in batch 207: 0.509827/0.510391loss in batch 208: 0.376083/0.50975loss in batch 209: 0.392761/0.509186loss in batch 210: 0.322937/0.508316loss in batch 211: 0.459549/0.508072loss in batch 212: 0.194931/0.506607
done with epoch 0
train_acc: 0.823944 (351/426)
test loss: 0.194931
acc: 0.783215 (112/143)
loss in batch 0: 0.245377/0.245377loss in batch 1: 0.250351/0.247864loss in batch 2: 0.476273/0.32399loss in batch 3: 0.148239/0.28006loss in batch 4: 0.106506/0.245346loss in batch 5: 0.245773/0.245422loss in batch 6: 0.26712/0.24852loss in batch 7: 0.561508/0.287643loss in batch 8: 0.542862/0.315994loss in batch 9: 0.196808/0.304077loss in batch 10: 0.373413/0.310394loss in batch 11: 0.858902/0.356094loss in batch 12: 0.187576/0.343124loss in batch 13: 0.331848/0.342331loss in batch 14: 0.118179/0.327377loss in batch 15: 0.346283/0.328552loss in batch 16: 0.980469/0.366913loss in batch 17: 0.383911/0.367859loss in batch 18: 0.321899/0.365433loss in batch 19: 0.809723/0.38765loss in batch 20: 0.152023/0.376434loss in batch 21: 0.159927/0.366577loss in batch 22: 0.453583/0.370361loss in batch 23: 0.571548/0.378754loss in batch 24: 0.564728/0.386185loss in batch 25: 0.546906/0.392365loss in batch 26: 0.439423/0.394119loss in batch 27: 0.647049/0.403137loss in batch 28: 0.207855/0.396423loss in batch 29: 0.293411/0.392975loss in batch 30: 0.507126/0.396667loss in batch 31: 0.440292/0.398026loss in batch 32: 0.519852/0.401718loss in batch 33: 0.619171/0.408112loss in batch 34: 0.356781/0.406662loss in batch 35: 0.350311/0.405075loss in batch 36: 0.184448/0.399124loss in batch 37: 0.581146/0.403915loss in batch 38: 0.517075/0.406815loss in batch 39: 0.178345/0.401093loss in batch 40: 0.344193/0.399704loss in batch 41: 0.352188/0.398575loss in batch 42: 0.249649/0.395126loss in batch 43: 0.449387/0.396347loss in batch 44: 0.168762/0.391296loss in batch 45: 0.30896/0.389496loss in batch 46: 0.391312/0.389542loss in batch 47: 0.274796/0.387146loss in batch 48: 0.459564/0.388626loss in batch 49: 0.359344/0.388046loss in batch 50: 0.964523/0.399338loss in batch 51: 0.258316/0.396637loss in batch 52: 0.227142/0.393433loss in batch 53: 0.323441/0.392136loss in batch 54: 0.505386/0.394196loss in batch 55: 0.366974/0.393707loss in batch 56: 0.328278/0.392563loss in batch 57: 0.225937/0.389694loss in batch 58: 0.345886/0.388947loss in batch 59: 0.179123/0.385452loss in batch 60: 0.248184/0.383194loss in batch 61: 0.146896/0.379395loss in batch 62: 0.472336/0.380859loss in batch 63: 0.460312/0.382095loss in batch 64: 0.375427/0.382004loss in batch 65: 0.373001/0.381866loss in batch 66: 0.426102/0.382538loss in batch 67: 0.203857/0.379898loss in batch 68: 0.379471/0.379898loss in batch 69: 0.320328/0.379044loss in batch 70: 0.339127/0.378479loss in batch 71: 0.430237/0.379196loss in batch 72: 0.570862/0.381821loss in batch 73: 0.705994/0.386215loss in batch 74: 0.404907/0.386459loss in batch 75: 0.232895/0.38443loss in batch 76: 0.307281/0.383423loss in batch 77: 0.57019/0.385818loss in batch 78: 0.304352/0.384796loss in batch 79: 0.25531/0.383179loss in batch 80: 0.405106/0.383453loss in batch 81: 0.461349/0.384399loss in batch 82: 0.336975/0.383835loss in batch 83: 0.464157/0.384781loss in batch 84: 0.271912/0.383453loss in batch 85: 0.351044/0.383087loss in batch 86: 0.350159/0.382706loss in batch 87: 0.508896/0.38414loss in batch 88: 0.389832/0.384201loss in batch 89: 0.487778/0.385345loss in batch 90: 0.552414/0.387177loss in batch 91: 0.266418/0.385864loss in batch 92: 0.459946/0.386673loss in batch 93: 0.269165/0.385422loss in batch 94: 0.492523/0.386536loss in batch 95: 0.137268/0.383957loss in batch 96: 0.339539/0.383484loss in batch 97: 0.213211/0.381744loss in batch 98: 0.518402/0.383133loss in batch 99: 0.371735/0.383026loss in batch 100: 0.290314/0.382111loss in batch 101: 0.186554/0.380188loss in batch 102: 0.164413/0.378098loss in batch 103: 0.146866/0.37587loss in batch 104: 0.32608/0.375397loss in batch 105: 0.220154/0.373917loss in batch 106: 0.344864/0.373657loss in batch 107: 0.435684/0.374237loss in batch 108: 0.379776/0.374283loss in batch 109: 0.21489/0.372833loss in batch 110: 0.238251/0.371613loss in batch 111: 0.445557/0.372284loss in batch 112: 0.26178/0.371292loss in batch 113: 0.768997/0.374786loss in batch 114: 0.299683/0.374146loss in batch 115: 0.290604/0.373428loss in batch 116: 0.447006/0.374054loss in batch 117: 0.73024/0.37706loss in batch 118: 0.165176/0.375275loss in batch 119: 0.232773/0.3741loss in batch 120: 0.246048/0.373047loss in batch 121: 0.263535/0.372147loss in batch 122: 0.298752/0.371536loss in batch 123: 0.251236/0.370575loss in batch 124: 0.438828/0.371109loss in batch 125: 0.37117/0.371124loss in batch 126: 0.379105/0.37117loss in batch 127: 0.244843/0.370193loss in batch 128: 0.1987/0.368866loss in batch 129: 0.159561/0.367264loss in batch 130: 0.353821/0.367157loss in batch 131: 0.448822/0.367783loss in batch 132: 0.166718/0.366272loss in batch 133: 0.755798/0.369171loss in batch 134: 0.393158/0.369354loss in batch 135: 0.197433/0.368088loss in batch 136: 0.226852/0.36705loss in batch 137: 0.434814/0.367538loss in batch 138: 0.23436/0.366592loss in batch 139: 0.216476/0.365509loss in batch 140: 0.33046/0.365265loss in batch 141: 0.521545/0.366364loss in batch 142: 0.300858/0.365906loss in batch 143: 0.200089/0.364761loss in batch 144: 0.218018/0.363739loss in batch 145: 0.654968/0.365738loss in batch 146: 0.152771/0.364288loss in batch 147: 0.163651/0.36293loss in batch 148: 0.10379/0.361191loss in batch 149: 0.639587/0.363037loss in batch 150: 0.144623/0.361603loss in batch 151: 0.17128/0.360352loss in batch 152: 0.369553/0.360413loss in batch 153: 0.160675/0.359116loss in batch 154: 0.412018/0.359451loss in batch 155: 0.211472/0.358505loss in batch 156: 0.250778/0.357819loss in batch 157: 0.437668/0.358322loss in batch 158: 0.248032/0.357635loss in batch 159: 0.187592/0.356567loss in batch 160: 1.05495/0.360901loss in batch 161: 0.223312/0.360062loss in batch 162: 0.425201/0.360458loss in batch 163: 0.167557/0.359283loss in batch 164: 0.519379/0.360245loss in batch 165: 0.21608/0.359375loss in batch 166: 0.185745/0.358353loss in batch 167: 0.381882/0.35849loss in batch 168: 0.326736/0.358307loss in batch 169: 0.459259/0.358887loss in batch 170: 0.263657/0.358322loss in batch 171: 0.474014/0.358994loss in batch 172: 0.326294/0.35881loss in batch 173: 0.373657/0.358902loss in batch 174: 0.231018/0.35817loss in batch 175: 0.330399/0.358017loss in batch 176: 0.157593/0.356873loss in batch 177: 0.251297/0.356277loss in batch 178: 0.569397/0.357483loss in batch 179: 0.221664/0.356735loss in batch 180: 0.114075/0.355377loss in batch 181: 0.141434/0.354218loss in batch 182: 0.207428/0.353409loss in batch 183: 0.230957/0.352737loss in batch 184: 0.256104/0.352219loss in batch 185: 0.471863/0.352875loss in batch 186: 0.10054/0.351501loss in batch 187: 0.48204/0.352203loss in batch 188: 0.19191/0.351364loss in batch 189: 0.160202/0.350342loss in batch 190: 0.301056/0.350098loss in batch 191: 0.12619/0.348923loss in batch 192: 0.162399/0.347961loss in batch 193: 0.0866852/0.346619loss in batch 194: 0.867828/0.349289loss in batch 195: 0.946823/0.352325loss in batch 196: 0.2155/0.351639loss in batch 197: 0.334/0.351547loss in batch 198: 0.909363/0.354355loss in batch 199: 0.231369/0.353745loss in batch 200: 0.103821/0.352509loss in batch 201: 0.521591/0.353333loss in batch 202: 0.153809/0.352356loss in batch 203: 0.128677/0.351257loss in batch 204: 0.348862/0.351242loss in batch 205: 0.106552/0.350052loss in batch 206: 0.329102/0.349945loss in batch 207: 0.118484/0.348846loss in batch 208: 0.376099/0.348969loss in batch 209: 0.431351/0.349365loss in batch 210: 0.37381/0.349472loss in batch 211: 1.27631/0.353851loss in batch 212: 0.251785/0.353363
done with epoch 1
train_acc: 0.906103 (386/426)
test loss: 0.251785
acc: 0.930068 (133/143)
loss in batch 0: 0.262268/0.262268loss in batch 1: 0.194931/0.228592loss in batch 2: 0.210037/0.222412loss in batch 3: 0.330658/0.249481loss in batch 4: 0.233002/0.24617loss in batch 5: 0.322266/0.258865loss in batch 6: 0.839539/0.341812loss in batch 7: 0.220505/0.32666loss in batch 8: 0.127106/0.304474loss in batch 9: 0.159653/0.289993loss in batch 10: 0.21373/0.283066loss in batch 11: 0.281631/0.282944loss in batch 12: 0.694244/0.31459loss in batch 13: 0.5215/0.329361loss in batch 14: 0.641373/0.350159loss in batch 15: 0.462234/0.357178loss in batch 16: 0.360001/0.35733loss in batch 17: 0.209061/0.349106loss in batch 18: 0.152451/0.338745loss in batch 19: 0.294937/0.336548loss in batch 20: 0.223114/0.331146loss in batch 21: 0.185425/0.324524loss in batch 22: 0.252777/0.321411loss in batch 23: 0.398209/0.3246loss in batch 24: 0.413269/0.328156loss in batch 25: 0.350204/0.328995loss in batch 26: 0.202347/0.32431loss in batch 27: 0.279953/0.322723loss in batch 28: 0.418472/0.326035loss in batch 29: 0.264297/0.323975loss in batch 30: 0.15596/0.318558loss in batch 31: 0.303619/0.318085loss in batch 32: 0.408951/0.320847loss in batch 33: 0.586304/0.328644loss in batch 34: 0.149551/0.323532loss in batch 35: 0.403656/0.32576loss in batch 36: 0.17186/0.321594loss in batch 37: 0.448441/0.324936loss in batch 38: 0.41188/0.327164loss in batch 39: 0.434891/0.329849loss in batch 40: 0.204224/0.326797loss in batch 41: 0.503174/0.330994loss in batch 42: 0.43576/0.33342loss in batch 43: 0.141052/0.329071loss in batch 44: 0.458588/0.331924loss in batch 45: 0.106049/0.327011loss in batch 46: 0.399811/0.328568loss in batch 47: 0.246857/0.326874loss in batch 48: 0.277069/0.325851loss in batch 49: 0.945084/0.338242loss in batch 50: 0.477127/0.340958loss in batch 51: 0.365067/0.341415loss in batch 52: 0.28334/0.340332loss in batch 53: 0.0904694/0.335693loss in batch 54: 0.181641/0.332901loss in batch 55: 0.423782/0.334518loss in batch 56: 0.553024/0.338364loss in batch 57: 0.421921/0.339798loss in batch 58: 0.571091/0.343719loss in batch 59: 0.17926/0.340973loss in batch 60: 0.264618/0.339722loss in batch 61: 0.203186/0.337524loss in batch 62: 0.179123/0.335007loss in batch 63: 0.383652/0.33577loss in batch 64: 0.298981/0.335205loss in batch 65: 0.601929/0.339249loss in batch 66: 0.132553/0.336151loss in batch 67: 0.544556/0.339233loss in batch 68: 0.159149/0.336609loss in batch 69: 0.319366/0.336365loss in batch 70: 0.132278/0.333496loss in batch 71: 0.0692749/0.329819loss in batch 72: 0.145203/0.327301loss in batch 73: 0.134033/0.324692loss in batch 74: 0.108032/0.321808loss in batch 75: 1.3624/0.33548loss in batch 76: 0.119308/0.332672loss in batch 77: 0.198837/0.330963loss in batch 78: 0.516479/0.333313loss in batch 79: 0.277908/0.332611loss in batch 80: 0.178207/0.330704loss in batch 81: 0.0524597/0.327316loss in batch 82: 0.176056/0.3255loss in batch 83: 0.246918/0.324554loss in batch 84: 0.404587/0.3255loss in batch 85: 0.108582/0.322983loss in batch 86: 0.261673/0.322281loss in batch 87: 0.522568/0.324554loss in batch 88: 0.225067/0.323425loss in batch 89: 0.13855/0.321381loss in batch 90: 0.133057/0.319305loss in batch 91: 0.41745/0.320374loss in batch 92: 0.0942383/0.317947loss in batch 93: 0.0491333/0.315094loss in batch 94: 0.179138/0.313644loss in batch 95: 0.215714/0.312637loss in batch 96: 0.316391/0.312668loss in batch 97: 0.215836/0.311676loss in batch 98: 0.137711/0.309921loss in batch 99: 0.225479/0.309082loss in batch 100: 0.231216/0.308304loss in batch 101: 0.103043/0.306305loss in batch 102: 0.220169/0.305466loss in batch 103: 0.09935/0.303482loss in batch 104: 0.254395/0.303024loss in batch 105: 0.384735/0.303787loss in batch 106: 0.0326996/0.301254loss in batch 107: 0.253235/0.300797loss in batch 108: 0.602417/0.303574loss in batch 109: 0.383957/0.304306loss in batch 110: 0.325424/0.304489loss in batch 111: 0.214447/0.303696loss in batch 112: 0.0311432/0.301285loss in batch 113: 0.291534/0.301193loss in batch 114: 0.439392/0.302383loss in batch 115: 0.156418/0.301147loss in batch 116: 0.330124/0.301392loss in batch 117: 0.477859/0.302887loss in batch 118: 0.156601/0.301651loss in batch 119: 0.219559/0.300964loss in batch 120: 0.219345/0.300278loss in batch 121: 0.141479/0.298996loss in batch 122: 0.15712/0.297836loss in batch 123: 0.224655/0.297241loss in batch 124: 0.123672/0.295868loss in batch 125: 0.256302/0.295547loss in batch 126: 0.21051/0.294876loss in batch 127: 0.1483/0.293732loss in batch 128: 0.241623/0.29332loss in batch 129: 0.413345/0.29425loss in batch 130: 0.732224/0.297592loss in batch 131: 0.180649/0.296707loss in batch 132: 0.14653/0.295578loss in batch 133: 0.323318/0.295792loss in batch 134: 0.335709/0.296082loss in batch 135: 0.317657/0.296234loss in batch 136: 0.200089/0.295547loss in batch 137: 0.490707/0.296951loss in batch 138: 0.075592/0.295364loss in batch 139: 0.498215/0.296814loss in batch 140: 0.582275/0.298828loss in batch 141: 0.185638/0.298035loss in batch 142: 0.113342/0.296738loss in batch 143: 0.107986/0.295441loss in batch 144: 0.544571/0.29715loss in batch 145: 0.589951/0.299149loss in batch 146: 0.70166/0.30188loss in batch 147: 0.589767/0.303833loss in batch 148: 0.168701/0.302933loss in batch 149: 0.165176/0.302017loss in batch 150: 0.266281/0.301788loss in batch 151: 0.419876/0.302551loss in batch 152: 0.100311/0.301239loss in batch 153: 0.143631/0.300217loss in batch 154: 0.0978699/0.298904loss in batch 155: 0.160461/0.298019loss in batch 156: 0.0856781/0.296661loss in batch 157: 0.144211/0.2957loss in batch 158: 0.101456/0.294479loss in batch 159: 0.258743/0.29425loss in batch 160: 0.574356/0.296005loss in batch 161: 0.144562/0.295059loss in batch 162: 0.195236/0.294449loss in batch 163: 0.495758/0.295685loss in batch 164: 0.434494/0.296509loss in batch 165: 0.238968/0.296173loss in batch 166: 0.173218/0.295425loss in batch 167: 0.24086/0.295105loss in batch 168: 0.426437/0.295883loss in batch 169: 0.258224/0.295654loss in batch 170: 0.132751/0.294724loss in batch 171: 0.303146/0.294754loss in batch 172: 0.488159/0.295883loss in batch 173: 0.372986/0.296326loss in batch 174: 0.306366/0.296371loss in batch 175: 0.14473/0.295517loss in batch 176: 0.16922/0.294815loss in batch 177: 0.314163/0.294907loss in batch 178: 0.267029/0.294754loss in batch 179: 0.198639/0.29422loss in batch 180: 0.069397/0.292984loss in batch 181: 0.180695/0.292374loss in batch 182: 0.171906/0.291702loss in batch 183: 0.383621/0.292206loss in batch 184: 0.25827/0.292007loss in batch 185: 0.428146/0.292755loss in batch 186: 0.804214/0.295486loss in batch 187: 0.243439/0.295212loss in batch 188: 0.22673/0.294846loss in batch 189: 0.088974/0.293777loss in batch 190: 0.176834/0.293152loss in batch 191: 0.383774/0.293625loss in batch 192: 0.261246/0.293457loss in batch 193: 0.190582/0.292923loss in batch 194: 0.605011/0.294525loss in batch 195: 0.0792694/0.293427loss in batch 196: 0.208115/0.292999loss in batch 197: 0.418289/0.293625loss in batch 198: 0.400955/0.294159loss in batch 199: 1.06583/0.298019loss in batch 200: 0.0945587/0.297012loss in batch 201: 0.0390167/0.295731loss in batch 202: 0.202469/0.295273loss in batch 203: 0.10025/0.294312loss in batch 204: 0.135284/0.293533loss in batch 205: 0.307602/0.29361loss in batch 206: 0.232803/0.29332loss in batch 207: 0.445358/0.294052loss in batch 208: 0.236618/0.293777loss in batch 209: 0.0897522/0.292816loss in batch 210: 0.0716095/0.291763loss in batch 211: 0.191071/0.291275loss in batch 212: 0.411224/0.29184
done with epoch 2
train_acc: 0.913146 (389/426)
test loss: 0.411224
acc: 0.923075 (132/143)
loss in batch 0: 0.346588/0.346588loss in batch 1: 0.237366/0.291977loss in batch 2: 0.166321/0.250092loss in batch 3: 0.176392/0.231674loss in batch 4: 0.584427/0.302216loss in batch 5: 0.11853/0.271606loss in batch 6: 0.161102/0.255829loss in batch 7: 0.404404/0.274384loss in batch 8: 0.173615/0.263199loss in batch 9: 0.148315/0.251709loss in batch 10: 0.177292/0.244934loss in batch 11: 0.138031/0.236038loss in batch 12: 0.227921/0.235413loss in batch 13: 0.400635/0.247208loss in batch 14: 0.14473/0.240372loss in batch 15: 0.525955/0.258224loss in batch 16: 0.436584/0.268707loss in batch 17: 0.615509/0.287994loss in batch 18: 0.0628815/0.276138loss in batch 19: 0.0333405/0.263992loss in batch 20: 0.366898/0.26889loss in batch 21: 0.101639/0.261292loss in batch 22: 0.325516/0.264084loss in batch 23: 0.893646/0.290329loss in batch 24: 0.0805359/0.281921loss in batch 25: 0.651382/0.296143loss in batch 26: 0.0691376/0.287735loss in batch 27: 0.274872/0.287277loss in batch 28: 0.166595/0.283112loss in batch 29: 0.112534/0.27742loss in batch 30: 0.356308/0.279968loss in batch 31: 0.253052/0.279129loss in batch 32: 0.399094/0.282761loss in batch 33: 0.176483/0.279633loss in batch 34: 0.37117/0.282257loss in batch 35: 0.149109/0.278549loss in batch 36: 0.143921/0.274902loss in batch 37: 0.110687/0.270584loss in batch 38: 0.347412/0.272552loss in batch 39: 0.0385437/0.266708loss in batch 40: 0.543686/0.273468loss in batch 41: 0.0843658/0.268967loss in batch 42: 0.100601/0.265045loss in batch 43: 0.079834/0.260834loss in batch 44: 0.0644684/0.25647loss in batch 45: 0.384735/0.259262loss in batch 46: 0.112457/0.256149loss in batch 47: 0.319534/0.257462loss in batch 48: 0.50882/0.262589loss in batch 49: 0.115707/0.259644loss in batch 50: 0.155045/0.257599loss in batch 51: 0.143143/0.255402loss in batch 52: 0.0700073/0.251907loss in batch 53: 0.192352/0.250809loss in batch 54: 0.359024/0.252777loss in batch 55: 0.0596008/0.249313loss in batch 56: 0.259003/0.249496loss in batch 57: 0.297531/0.25032loss in batch 58: 0.189377/0.249283loss in batch 59: 0.0861969/0.246567loss in batch 60: 0.122864/0.244537loss in batch 61: 0.128128/0.242661loss in batch 62: 0.441467/0.245819loss in batch 63: 0.187729/0.244919loss in batch 64: 0.28688/0.24556loss in batch 65: 0.62178/0.251251loss in batch 66: 0.147842/0.24971loss in batch 67: 0.265518/0.249939loss in batch 68: 0.108841/0.24791loss in batch 69: 0.337524/0.249176loss in batch 70: 0.127518/0.247467loss in batch 71: 0.0457001/0.244659loss in batch 72: 0.359451/0.246246loss in batch 73: 0.11235/0.244415loss in batch 74: 0.201797/0.243866loss in batch 75: 0.371902/0.245544loss in batch 76: 0.115799/0.243851loss in batch 77: 0.0616608/0.241531loss in batch 78: 0.0848236/0.239548loss in batch 79: 0.297684/0.240265loss in batch 80: 0.0514984/0.237946loss in batch 81: 0.0789032/0.235992loss in batch 82: 0.0476837/0.233719loss in batch 83: 0.247299/0.233887loss in batch 84: 0.149963/0.232895loss in batch 85: 0.372742/0.234528loss in batch 86: 0.146881/0.233521loss in batch 87: 0.313202/0.234421loss in batch 88: 0.347443/0.235703loss in batch 89: 0.0891418/0.23407loss in batch 90: 0.221176/0.233932loss in batch 91: 0.715958/0.239166loss in batch 92: 0.238785/0.239151loss in batch 93: 0.639786/0.243423loss in batch 94: 0.0600433/0.241501loss in batch 95: 0.262543/0.241699loss in batch 96: 0.256348/0.241867loss in batch 97: 0.190964/0.241333loss in batch 98: 0.209076/0.241013loss in batch 99: 0.0954437/0.239563loss in batch 100: 0.24057/0.239563loss in batch 101: 0.157715/0.23877loss in batch 102: 0.0783081/0.237213loss in batch 103: 0.052887/0.235443loss in batch 104: 0.291428/0.235977loss in batch 105: 0.464813/0.238129loss in batch 106: 0.284958/0.238556loss in batch 107: 0.137848/0.23764loss in batch 108: 0.605789/0.241028loss in batch 109: 0.191147/0.24057loss in batch 110: 0.217697/0.240356loss in batch 111: 0.302567/0.240906loss in batch 112: 0.0676117/0.23938loss in batch 113: 0.461975/0.241333loss in batch 114: 0.376404/0.242493loss in batch 115: 0.227386/0.242371loss in batch 116: 0.0896606/0.241074loss in batch 117: 0.173035/0.240494loss in batch 118: 0.111099/0.23941loss in batch 119: 0.19754/0.239059loss in batch 120: 1.00446/0.245377loss in batch 121: 0.323578/0.246017loss in batch 122: 0.291275/0.246399loss in batch 123: 0.588913/0.249146loss in batch 124: 0.269547/0.249313loss in batch 125: 0.470947/0.251068loss in batch 126: 0.438599/0.252548loss in batch 127: 0.264648/0.25264loss in batch 128: 0.0541687/0.251099loss in batch 129: 0.380341/0.252106loss in batch 130: 0.31871/0.252609loss in batch 131: 0.317001/0.253098loss in batch 132: 0.0603943/0.251648loss in batch 133: 0.687027/0.254898loss in batch 134: 0.760605/0.258652loss in batch 135: 0.19223/0.258148loss in batch 136: 0.591858/0.26059loss in batch 137: 0.322601/0.261032loss in batch 138: 0.059433/0.259583loss in batch 139: 0.484253/0.261185loss in batch 140: 0.125778/0.260239loss in batch 141: 0.168793/0.259583loss in batch 142: 0.333191/0.260101loss in batch 143: 0.155762/0.259384loss in batch 144: 0.12529/0.258453loss in batch 145: 0.325745/0.258911loss in batch 146: 0.327072/0.259369loss in batch 147: 0.234039/0.259201loss in batch 148: 0.198776/0.258804loss in batch 149: 0.258835/0.258804loss in batch 150: 0.415314/0.259842loss in batch 151: 0.1306/0.258987loss in batch 152: 0.428406/0.260101loss in batch 153: 0.183975/0.259598loss in batch 154: 0.125702/0.258743loss in batch 155: 0.406464/0.259689loss in batch 156: 0.20723/0.259338loss in batch 157: 0.146362/0.258636loss in batch 158: 0.282898/0.258789loss in batch 159: 0.158386/0.258163loss in batch 160: 0.424606/0.259186loss in batch 161: 0.146637/0.258499loss in batch 162: 0.546982/0.260269loss in batch 163: 0.194321/0.259872loss in batch 164: 0.303833/0.260132loss in batch 165: 0.105133/0.259201loss in batch 166: 0.351135/0.25975loss in batch 167: 0.490128/0.261124loss in batch 168: 0.175507/0.26062loss in batch 169: 0.0756836/0.259521loss in batch 170: 0.2901/0.259705loss in batch 171: 0.0401306/0.258423loss in batch 172: 0.18988/0.258041loss in batch 173: 0.112381/0.257202loss in batch 174: 0.0862427/0.256226loss in batch 175: 0.121857/0.255447loss in batch 176: 0.126404/0.25473loss in batch 177: 0.185516/0.254333loss in batch 178: 0.070755/0.253326loss in batch 179: 0.474747/0.254547loss in batch 180: 0.437958/0.255569loss in batch 181: 0.162369/0.255035loss in batch 182: 0.120636/0.254318loss in batch 183: 1.00163/0.258362loss in batch 184: 0.115219/0.257599loss in batch 185: 0.236862/0.257477loss in batch 186: 0.334366/0.257889loss in batch 187: 0.172638/0.257446loss in batch 188: 0.149887/0.256866loss in batch 189: 0.0462494/0.255768loss in batch 190: 0.327103/0.256134loss in batch 191: 0.141556/0.255539loss in batch 192: 0.117432/0.254837loss in batch 193: 0.169189/0.254379loss in batch 194: 0.124405/0.253723loss in batch 195: 0.128265/0.253082loss in batch 196: 0.510406/0.254395loss in batch 197: 0.0822296/0.253525loss in batch 198: 0.417236/0.254349loss in batch 199: 0.627411/0.25621loss in batch 200: 0.373169/0.25679loss in batch 201: 0.282013/0.256912loss in batch 202: 0.351013/0.257385loss in batch 203: 0.126511/0.256729loss in batch 204: 0.258163/0.256729loss in batch 205: 0.154907/0.256241loss in batch 206: 0.148453/0.255722loss in batch 207: 0.560074/0.257187loss in batch 208: 0.231201/0.257065loss in batch 209: 0.23349/0.256943loss in batch 210: 0.420654/0.257721loss in batch 211: 0.27298/0.257797loss in batch 212: 0.191284/0.257477
done with epoch 3
train_acc: 0.93662 (399/426)
test loss: 0.191284
acc: 0.916082 (131/143)
loss in batch 0: 0.568268/0.568268loss in batch 1: 0.0395508/0.303909loss in batch 2: 0.121277/0.243027loss in batch 3: 0.0427856/0.192978loss in batch 4: 0.0972137/0.173813loss in batch 5: 0.156815/0.17099loss in batch 6: 0.176514/0.171768loss in batch 7: 0.178589/0.172623loss in batch 8: 0.117676/0.166519loss in batch 9: 0.48082/0.197952loss in batch 10: 0.508606/0.226196loss in batch 11: 0.150345/0.219879loss in batch 12: 0.207642/0.218933loss in batch 13: 0.0539093/0.207138loss in batch 14: 0.471176/0.224747loss in batch 15: 0.543503/0.244675loss in batch 16: 0.346176/0.250641loss in batch 17: 0.21817/0.24884loss in batch 18: 0.566391/0.265549loss in batch 19: 0.781509/0.291351loss in batch 20: 0.115921/0.283005loss in batch 21: 0.131714/0.276123loss in batch 22: 0.48941/0.2854loss in batch 23: 0.0359955/0.274994loss in batch 24: 0.73555/0.293411loss in batch 25: 0.0696411/0.284821loss in batch 26: 0.244583/0.283325loss in batch 27: 0.26825/0.282791loss in batch 28: 0.0811005/0.275833loss in batch 29: 0.271835/0.275696loss in batch 30: 0.103592/0.270157loss in batch 31: 0.0652771/0.263733loss in batch 32: 0.235062/0.262878loss in batch 33: 0.148682/0.259521loss in batch 34: 0.492371/0.266159loss in batch 35: 0.143616/0.262756loss in batch 36: 0.267715/0.262894loss in batch 37: 0.19133/0.261017loss in batch 38: 0.435593/0.265488loss in batch 39: 0.380264/0.268356loss in batch 40: 0.107651/0.26445loss in batch 41: 0.295242/0.265167loss in batch 42: 0.0944214/0.2612loss in batch 43: 0.0406799/0.256195loss in batch 44: 0.0917053/0.252533loss in batch 45: 0.0455627/0.248032loss in batch 46: 0.0621643/0.24408loss in batch 47: 0.282333/0.244873loss in batch 48: 0.137848/0.242706loss in batch 49: 0.393646/0.245712loss in batch 50: 0.0820313/0.242508loss in batch 51: 0.121231/0.240173loss in batch 52: 0.108688/0.237701loss in batch 53: 0.236649/0.237671loss in batch 54: 0.107178/0.235291loss in batch 55: 0.157242/0.233902loss in batch 56: 0.610641/0.240509loss in batch 57: 0.503738/0.245056loss in batch 58: 0.185898/0.244049loss in batch 59: 0.102676/0.241699loss in batch 60: 0.131348/0.239883loss in batch 61: 0.379974/0.242142loss in batch 62: 0.0501099/0.239105loss in batch 63: 0.227509/0.238907loss in batch 64: 0.240387/0.238937loss in batch 65: 0.230164/0.2388loss in batch 66: 0.0973816/0.236694loss in batch 67: 0.0704498/0.234253loss in batch 68: 0.198776/0.233734loss in batch 69: 0.071701/0.23143loss in batch 70: 0.0958862/0.229523loss in batch 71: 0.163788/0.228607loss in batch 72: 0.175201/0.227875loss in batch 73: 0.362991/0.229706loss in batch 74: 0.401154/0.231979loss in batch 75: 0.370895/0.23381loss in batch 76: 0.17392/0.233032loss in batch 77: 0.0884094/0.231186loss in batch 78: 0.303452/0.232086loss in batch 79: 0.062912/0.22998loss in batch 80: 0.278229/0.230576loss in batch 81: 0.126862/0.229309loss in batch 82: 0.860168/0.236908loss in batch 83: 0.398697/0.238831loss in batch 84: 0.0959473/0.237152loss in batch 85: 0.1465/0.236099loss in batch 86: 0.448761/0.238541loss in batch 87: 0.115326/0.237137loss in batch 88: 0.215225/0.236908loss in batch 89: 0.110504/0.235489loss in batch 90: 0.484436/0.238235loss in batch 91: 0.0858459/0.236572loss in batch 92: 0.577896/0.240234loss in batch 93: 0.0566101/0.238281loss in batch 94: 0.230438/0.238205loss in batch 95: 0.0590363/0.236343loss in batch 96: 0.0882263/0.234802loss in batch 97: 0.314056/0.235626loss in batch 98: 0.185455/0.235123loss in batch 99: 0.0801544/0.233551loss in batch 100: 0.0643768/0.231888loss in batch 101: 0.813568/0.237579loss in batch 102: 0.602081/0.241135loss in batch 103: 0.465851/0.243286loss in batch 104: 0.0632782/0.241577loss in batch 105: 0.235413/0.241516loss in batch 106: 0.105515/0.240234loss in batch 107: 0.494934/0.242599loss in batch 108: 0.0759888/0.241089loss in batch 109: 0.111053/0.239899loss in batch 110: 0.189392/0.239441loss in batch 111: 0.759308/0.24408loss in batch 112: 0.69249/0.248047loss in batch 113: 0.154709/0.247238loss in batch 114: 0.0726166/0.245712loss in batch 115: 0.121109/0.244629loss in batch 116: 0.297012/0.245087loss in batch 117: 0.103058/0.243881loss in batch 118: 0.123901/0.242874loss in batch 119: 0.27507/0.243149loss in batch 120: 0.0713043/0.241714loss in batch 121: 0.190384/0.241302loss in batch 122: 0.155533/0.240601loss in batch 123: 0.183762/0.240143loss in batch 124: 0.147858/0.23941loss in batch 125: 0.439911/0.240997loss in batch 126: 0.0744781/0.239685loss in batch 127: 0.436478/0.241226loss in batch 128: 0.205124/0.240952loss in batch 129: 0.207092/0.240692loss in batch 130: 0.15921/0.240067loss in batch 131: 0.183701/0.239624loss in batch 132: 0.566849/0.242096loss in batch 133: 0.0973358/0.241013loss in batch 134: 0.151215/0.240356loss in batch 135: 0.352295/0.24118loss in batch 136: 0.146011/0.240479loss in batch 137: 0.0981293/0.239456loss in batch 138: 0.0611725/0.238159loss in batch 139: 0.0322571/0.236694loss in batch 140: 0.202637/0.23645loss in batch 141: 0.352173/0.237274loss in batch 142: 0.167938/0.236786loss in batch 143: 0.381302/0.237793loss in batch 144: 0.193695/0.237473loss in batch 145: 0.138245/0.236801loss in batch 146: 0.41423/0.238007loss in batch 147: 0.0847015/0.236969loss in batch 148: 0.0266571/0.23555loss in batch 149: 0.0449524/0.234299loss in batch 150: 0.461914/0.235794loss in batch 151: 0.109055/0.23497loss in batch 152: 0.149963/0.234406loss in batch 153: 0.043457/0.23317loss in batch 154: 0.191498/0.232895loss in batch 155: 0.790359/0.236481loss in batch 156: 0.488892/0.238083loss in batch 157: 0.0888062/0.237137loss in batch 158: 0.252838/0.237244loss in batch 159: 0.067749/0.236176loss in batch 160: 0.28801/0.236496loss in batch 161: 0.06073/0.235413loss in batch 162: 0.186249/0.235107loss in batch 163: 0.164444/0.23468loss in batch 164: 1.07187/0.239746loss in batch 165: 0.485962/0.241241loss in batch 166: 0.203293/0.241013loss in batch 167: 0.182312/0.240662loss in batch 168: 0.268433/0.240829loss in batch 169: 0.132523/0.240189loss in batch 170: 0.0739899/0.239212loss in batch 171: 0.0829773/0.238312loss in batch 172: 0.195679/0.238068loss in batch 173: 0.111221/0.237335loss in batch 174: 0.486084/0.23877loss in batch 175: 0.332993/0.239288loss in batch 176: 0.409302/0.24025loss in batch 177: 0.00790405/0.238953loss in batch 178: 0.134735/0.238358loss in batch 179: 0.369537/0.23909loss in batch 180: 0.36528/0.239777loss in batch 181: 0.192917/0.239532loss in batch 182: 0.0386505/0.238434loss in batch 183: 0.121521/0.237808loss in batch 184: 0.047348/0.236771loss in batch 185: 0.118225/0.23613loss in batch 186: 0.265427/0.236282loss in batch 187: 0.188675/0.236023loss in batch 188: 0.086441/0.235245loss in batch 189: 0.218979/0.235153loss in batch 190: 0.497452/0.236526loss in batch 191: 0.360062/0.237167loss in batch 192: 0.213593/0.237061loss in batch 193: 0.891296/0.240433loss in batch 194: 1.08266/0.244751loss in batch 195: 0.0696564/0.243851loss in batch 196: 0.311432/0.244186loss in batch 197: 0.0811005/0.243378loss in batch 198: 0.346619/0.243896loss in batch 199: 0.0348969/0.242844loss in batch 200: 0.286255/0.243057loss in batch 201: 0.73175/0.245483loss in batch 202: 0.0527191/0.244522loss in batch 203: 0.0659485/0.243652loss in batch 204: 0.261993/0.243744loss in batch 205: 0.233368/0.243698loss in batch 206: 0.121567/0.243103loss in batch 207: 0.166946/0.242737loss in batch 208: 0.595734/0.244431loss in batch 209: 0.050766/0.2435loss in batch 210: 0.0450134/0.242569loss in batch 211: 0.0912933/0.241852loss in batch 212: 0.12117/0.241287
done with epoch 4
train_acc: 0.915493 (390/426)
test loss: 0.12117
acc: 0.930068 (133/143)
loss in batch 0: 0.0883484/0.0883484loss in batch 1: 0.102676/0.0955048loss in batch 2: 0.214828/0.135284loss in batch 3: 0.166367/0.143066loss in batch 4: 0.0225067/0.118942loss in batch 5: 0.0619049/0.109436loss in batch 6: 0.780258/0.205261loss in batch 7: 0.0795135/0.18956loss in batch 8: 0.0366211/0.172562loss in batch 9: 0.10228/0.165527loss in batch 10: 0.0686035/0.156708loss in batch 11: 0.180374/0.158691loss in batch 12: 0.184799/0.16069loss in batch 13: 0.0503235/0.152817loss in batch 14: 0.038681/0.145203loss in batch 15: 0.0948944/0.142059loss in batch 16: 0.304886/0.151642loss in batch 17: 0.388794/0.164825loss in batch 18: 0.0568695/0.159134loss in batch 19: 0.113342/0.156845loss in batch 20: 0.044632/0.151489loss in batch 21: 0.333618/0.159775loss in batch 22: 0.0448456/0.15477loss in batch 23: 1.39458/0.206436loss in batch 24: 0.182663/0.20549loss in batch 25: 0.605515/0.220886loss in batch 26: 0.132416/0.217606loss in batch 27: 1.37741/0.259018loss in batch 28: 0.0744781/0.252655loss in batch 29: 0.0525208/0.245987loss in batch 30: 0.050827/0.239685loss in batch 31: 0.394028/0.244522loss in batch 32: 0.199966/0.243164loss in batch 33: 0.055191/0.237625loss in batch 34: 0.0461426/0.232162loss in batch 35: 0.0943604/0.228348loss in batch 36: 0.0721283/0.224106loss in batch 37: 0.0959625/0.220734loss in batch 38: 0.0469513/0.216278loss in batch 39: 0.172409/0.215179loss in batch 40: 0.0456085/0.21106loss in batch 41: 0.0497284/0.207199loss in batch 42: 0.558807/0.215393loss in batch 43: 0.142502/0.21373loss in batch 44: 0.224304/0.213959loss in batch 45: 0.170349/0.213028loss in batch 46: 0.251587/0.213837loss in batch 47: 0.21344/0.213821loss in batch 48: 0.121246/0.211945loss in batch 49: 0.14325/0.210571loss in batch 50: 0.113464/0.208664loss in batch 51: 0.362396/0.211609loss in batch 52: 0.19606/0.211334loss in batch 53: 0.257675/0.212173loss in batch 54: 0.348969/0.214676loss in batch 55: 0.248367/0.215271loss in batch 56: 0.176147/0.214584loss in batch 57: 0.0216064/0.211258loss in batch 58: 0.217499/0.211365loss in batch 59: 0.463531/0.215561loss in batch 60: 0.0906677/0.213531loss in batch 61: 0.262451/0.21431loss in batch 62: 0.115555/0.212738loss in batch 63: 0.662125/0.219757loss in batch 64: 0.49913/0.22406loss in batch 65: 0.121719/0.222504loss in batch 66: 0.127625/0.221085loss in batch 67: 0.180527/0.22049loss in batch 68: 0.387924/0.222916loss in batch 69: 0.118652/0.221436loss in batch 70: 0.0824127/0.219482loss in batch 71: 0.414948/0.222198loss in batch 72: 0.0675049/0.220078loss in batch 73: 0.273834/0.220795loss in batch 74: 0.474808/0.224197loss in batch 75: 0.194717/0.223785loss in batch 76: 0.0422058/0.221436loss in batch 77: 0.108932/0.220001loss in batch 78: 0.013855/0.217392loss in batch 79: 0.10762/0.216003loss in batch 80: 0.153793/0.215256loss in batch 81: 0.312454/0.216431loss in batch 82: 0.155121/0.215698loss in batch 83: 0.046402/0.213669loss in batch 84: 0.396637/0.215836loss in batch 85: 0.0997009/0.214478loss in batch 86: 0.13118/0.213516loss in batch 87: 0.409576/0.215759loss in batch 88: 0.276001/0.216431loss in batch 89: 0.355927/0.217972loss in batch 90: 0.0848999/0.216507loss in batch 91: 0.594925/0.220627loss in batch 92: 0.0827942/0.219147loss in batch 93: 0.870102/0.226074loss in batch 94: 0.0700531/0.224426loss in batch 95: 0.322937/0.225449loss in batch 96: 0.159073/0.224777loss in batch 97: 0.707397/0.229706loss in batch 98: 0.156448/0.228958loss in batch 99: 0.199295/0.228653loss in batch 100: 0.448883/0.230835loss in batch 101: 0.208771/0.230621loss in batch 102: 0.12767/0.229614loss in batch 103: 0.144333/0.228806loss in batch 104: 0.24646/0.228973loss in batch 105: 0.489014/0.231415loss in batch 106: 0.366714/0.232681loss in batch 107: 0.0272827/0.230789loss in batch 108: 0.0495605/0.229126loss in batch 109: 0.0598297/0.2276loss in batch 110: 0.0836334/0.226288loss in batch 111: 0.139069/0.22551loss in batch 112: 0.490372/0.227859loss in batch 113: 0.185013/0.227478loss in batch 114: 0.0924988/0.226303loss in batch 115: 0.192749/0.226013loss in batch 116: 0.237106/0.22612loss in batch 117: 0.368164/0.22731loss in batch 118: 0.16954/0.226822loss in batch 119: 0.366333/0.227997loss in batch 120: 0.36528/0.229126loss in batch 121: 0.0489655/0.227646loss in batch 122: 0.198532/0.227417loss in batch 123: 0.736755/0.231522loss in batch 124: 0.289276/0.231979loss in batch 125: 0.166748/0.231476loss in batch 126: 0.314316/0.232117loss in batch 127: 0.0858917/0.230988loss in batch 128: 0.33046/0.23175loss in batch 129: 0.172836/0.231293loss in batch 130: 0.167297/0.230804loss in batch 131: 0.139236/0.230118loss in batch 132: 0.0398712/0.228683loss in batch 133: 0.0983582/0.227707loss in batch 134: 0.0753784/0.226578loss in batch 135: 0.0446472/0.225235loss in batch 136: 0.173141/0.224854loss in batch 137: 0.378342/0.225967loss in batch 138: 0.215775/0.225891loss in batch 139: 0.0552826/0.224686loss in batch 140: 0.114227/0.223892loss in batch 141: 0.334335/0.224686loss in batch 142: 0.234436/0.224731loss in batch 143: 0.195206/0.224548loss in batch 144: 0.214447/0.224472loss in batch 145: 0.0279236/0.223129loss in batch 146: 0.226318/0.223145loss in batch 147: 0.0252075/0.221817loss in batch 148: 0.150681/0.221329loss in batch 149: 0.0372467/0.220108loss in batch 150: 0.145081/0.219604loss in batch 151: 0.0874329/0.218735loss in batch 152: 0.170654/0.218414loss in batch 153: 0.228729/0.218491loss in batch 154: 0.462677/0.220062loss in batch 155: 0.203598/0.219971loss in batch 156: 0.0958862/0.219177loss in batch 157: 0.296509/0.21965loss in batch 158: 0.0682526/0.218704loss in batch 159: 0.102615/0.217987loss in batch 160: 0.114197/0.217331loss in batch 161: 0.138412/0.216843loss in batch 162: 0.368942/0.217773loss in batch 163: 1.0038/0.222565loss in batch 164: 0.543747/0.224518loss in batch 165: 0.16571/0.224167loss in batch 166: 0.403931/0.225235loss in batch 167: 0.119003/0.224609loss in batch 168: 0.078186/0.223755loss in batch 169: 0.12175/0.223145loss in batch 170: 0.452438/0.224487loss in batch 171: 0.155823/0.224091loss in batch 172: 0.0246429/0.222931loss in batch 173: 0.0690613/0.222046loss in batch 174: 0.200073/0.221924loss in batch 175: 0.156372/0.221542loss in batch 176: 0.170212/0.221252loss in batch 177: 0.397324/0.222244loss in batch 178: 0.0457611/0.221268loss in batch 179: 0.0378723/0.220245loss in batch 180: 0.14357/0.219818loss in batch 181: 0.0128632/0.218674loss in batch 182: 0.0546417/0.217789loss in batch 183: 0.218567/0.217789loss in batch 184: 0.102371/0.217163loss in batch 185: 0.0396881/0.216217loss in batch 186: 0.0307617/0.21521loss in batch 187: 0.102493/0.214615loss in batch 188: 0.820984/0.217834loss in batch 189: 0.760086/0.220688loss in batch 190: 0.0730286/0.21991loss in batch 191: 0.0884857/0.219223loss in batch 192: 0.138229/0.218811loss in batch 193: 0.40332/0.219757loss in batch 194: 0.329453/0.220322loss in batch 195: 0.148926/0.219955loss in batch 196: 0.309235/0.220413loss in batch 197: 0.0874634/0.219742loss in batch 198: 0.324036/0.220261loss in batch 199: 0.341919/0.220871loss in batch 200: 0.100998/0.220276loss in batch 201: 0.224808/0.220291loss in batch 202: 0.482605/0.221588loss in batch 203: 0.345428/0.222198loss in batch 204: 0.195358/0.222061loss in batch 205: 0.104111/0.221497loss in batch 206: 0.304153/0.221878loss in batch 207: 0.369003/0.22261loss in batch 208: 0.0786285/0.221909loss in batch 209: 0.145432/0.221558loss in batch 210: 0.351288/0.222168loss in batch 211: 0.193359/0.222015loss in batch 212: 0.368896/0.222717
done with epoch 5
train_acc: 0.934272 (398/426)
test loss: 0.368896
acc: 0.930068 (133/143)
loss in batch 0: 0.0764008/0.0764008loss in batch 1: 0.213257/0.144821loss in batch 2: 0.246017/0.178558loss in batch 3: 0.121552/0.164307loss in batch 4: 0.250565/0.181549loss in batch 5: 0.106674/0.169083loss in batch 6: 0.236557/0.178711loss in batch 7: 0.175858/0.17836loss in batch 8: 0.177979/0.178314loss in batch 9: 0.34465/0.194946loss in batch 10: 1.10416/0.277603loss in batch 11: 0.448715/0.291855loss in batch 12: 0.205948/0.285263loss in batch 13: 0.0421143/0.267899loss in batch 14: 0.20845/0.263916loss in batch 15: 0.122131/0.255066loss in batch 16: 0.17392/0.25029loss in batch 17: 0.0941467/0.241623loss in batch 18: 0.432877/0.251678loss in batch 19: 0.112457/0.24472loss in batch 20: 0.117371/0.238663loss in batch 21: 0.0961304/0.232178loss in batch 22: 0.147598/0.2285loss in batch 23: 0.24202/0.229065loss in batch 24: 0.625351/0.244919loss in batch 25: 0.0753326/0.238403loss in batch 26: 0.197388/0.236877loss in batch 27: 0.0904694/0.231644loss in batch 28: 0.0706635/0.226105loss in batch 29: 0.165359/0.224075loss in batch 30: 0.250793/0.22493loss in batch 31: 0.0653992/0.21994loss in batch 32: 0.330978/0.223312loss in batch 33: 0.0085907/0.216995loss in batch 34: 0.067627/0.212723loss in batch 35: 1.12459/0.238068loss in batch 36: 0.0221558/0.232224loss in batch 37: 0.120346/0.229279loss in batch 38: 0.0688782/0.225174loss in batch 39: 0.109909/0.22229loss in batch 40: 0.25087/0.222977loss in batch 41: 0.112503/0.220352loss in batch 42: 0.380707/0.224075loss in batch 43: 0.111023/0.221512loss in batch 44: 0.363663/0.22467loss in batch 45: 0.136215/0.222748loss in batch 46: 0.532578/0.22934loss in batch 47: 0.404724/0.233002loss in batch 48: 0.610123/0.240692loss in batch 49: 0.102844/0.237946loss in batch 50: 0.280365/0.238754loss in batch 51: 0.018692/0.234543loss in batch 52: 0.513077/0.239792loss in batch 53: 0.275406/0.240448loss in batch 54: 0.490036/0.244995loss in batch 55: 0.531998/0.250107loss in batch 56: 0.0957031/0.247406loss in batch 57: 0.226959/0.24704loss in batch 58: 0.066864/0.243988loss in batch 59: 0.0895996/0.241425loss in batch 60: 0.211624/0.240936loss in batch 61: 0.058136/0.237976loss in batch 62: 0.0606995/0.235168loss in batch 63: 0.0759125/0.232681loss in batch 64: 0.0228729/0.229462loss in batch 65: 0.151093/0.228271loss in batch 66: 0.0955658/0.226288loss in batch 67: 0.0896606/0.224289loss in batch 68: 0.28508/0.225159loss in batch 69: 0.0940552/0.223297loss in batch 70: 0.0590973/0.220978loss in batch 71: 0.0319824/0.218338loss in batch 72: 0.0605774/0.216187loss in batch 73: 0.11058/0.214767loss in batch 74: 0.162643/0.214066loss in batch 75: 0.130539/0.212967loss in batch 76: 0.270004/0.213715loss in batch 77: 0.288956/0.214661loss in batch 78: 0.0373077/0.212433loss in batch 79: 0.10289/0.211044loss in batch 80: 0.039917/0.208939loss in batch 81: 0.157211/0.208313loss in batch 82: 0.169357/0.20784loss in batch 83: 0.437256/0.210571loss in batch 84: 0.136642/0.209702loss in batch 85: 0.0463562/0.207809loss in batch 86: 0.187057/0.207565loss in batch 87: 0.0435944/0.205704loss in batch 88: 0.2276/0.205963loss in batch 89: 0.180832/0.205673loss in batch 90: 0.057663/0.204041loss in batch 91: 0.514893/0.207428loss in batch 92: 0.0245056/0.20546loss in batch 93: 0.104568/0.204391loss in batch 94: 0.322861/0.205643loss in batch 95: 0.610062/0.209839loss in batch 96: 0.0229797/0.207916loss in batch 97: 0.183807/0.207672loss in batch 98: 0.449402/0.210114loss in batch 99: 0.136093/0.209381loss in batch 100: 0.190445/0.209183loss in batch 101: 0.0607147/0.207733loss in batch 102: 0.0228424/0.205933loss in batch 103: 0.160828/0.20549loss in batch 104: 0.0211029/0.203735loss in batch 105: 0.284576/0.204514loss in batch 106: 0.21283/0.20459loss in batch 107: 0.118362/0.203781loss in batch 108: 0.316879/0.204834loss in batch 109: 0.0739746/0.203629loss in batch 110: 0.107742/0.202759loss in batch 111: 0.0715485/0.201599loss in batch 112: 0.241852/0.20195loss in batch 113: 0.412796/0.203796loss in batch 114: 0.0971222/0.202866loss in batch 115: 0.130386/0.202255loss in batch 116: 0.221161/0.202408loss in batch 117: 0.243652/0.202759loss in batch 118: 0.331284/0.203857loss in batch 119: 0.0435028/0.202499loss in batch 120: 0.0707703/0.201416loss in batch 121: 0.246613/0.201782loss in batch 122: 0.0678558/0.200699loss in batch 123: 0.0407257/0.199417loss in batch 124: 0.0793762/0.198441loss in batch 125: 0.395233/0.200012loss in batch 126: 0.109406/0.199295loss in batch 127: 1.26256/0.207611loss in batch 128: 0.420364/0.209259loss in batch 129: 0.334839/0.21022loss in batch 130: 0.290466/0.210846loss in batch 131: 0.0726929/0.209778loss in batch 132: 0.457565/0.211639loss in batch 133: 0.00325012/0.210098loss in batch 134: 0.618164/0.21312loss in batch 135: 0.0689697/0.212051loss in batch 136: 0.110413/0.211319loss in batch 137: 0.00895691/0.209854loss in batch 138: 0.664642/0.21312loss in batch 139: 0.310928/0.213806loss in batch 140: 0.284683/0.214325loss in batch 141: 0.143097/0.213821loss in batch 142: 0.111343/0.213104loss in batch 143: 0.0907745/0.21225loss in batch 144: 0.0588837/0.211197loss in batch 145: 0.0831146/0.210327loss in batch 146: 0.267548/0.210693loss in batch 147: 0.36499/0.211746loss in batch 148: 0.138931/0.211258loss in batch 149: 0.584274/0.213745loss in batch 150: 0.207657/0.213699loss in batch 151: 0.474503/0.215424loss in batch 152: 0.0949554/0.21463loss in batch 153: 0.385895/0.215759loss in batch 154: 0.0816956/0.21489loss in batch 155: 0.408325/0.21611loss in batch 156: 1.03008/0.221313loss in batch 157: 0.0616455/0.220291loss in batch 158: 0.0160065/0.219009loss in batch 159: 0.450699/0.220459loss in batch 160: 0.550858/0.222519loss in batch 161: 0.108749/0.221802loss in batch 162: 0.102798/0.221085loss in batch 163: 0.133606/0.220551loss in batch 164: 0.452148/0.221954loss in batch 165: 0.103439/0.221237loss in batch 166: 0.0847778/0.220428loss in batch 167: 0.568634/0.222504loss in batch 168: 0.171661/0.222183loss in batch 169: 0.0701599/0.221298loss in batch 170: 0.0869293/0.220505loss in batch 171: 0.0459595/0.219498loss in batch 172: 0.0622711/0.218582loss in batch 173: 0.166367/0.218277loss in batch 174: 0.215317/0.218277loss in batch 175: 0.18782/0.218094loss in batch 176: 0.356445/0.218872loss in batch 177: 0.0769348/0.218079loss in batch 178: 0.211548/0.218048loss in batch 179: 0.137756/0.21759loss in batch 180: 0.331757/0.218231loss in batch 181: 0.0238037/0.217148loss in batch 182: 0.0422974/0.216217loss in batch 183: 0.36792/0.217026loss in batch 184: 0.0623474/0.216202loss in batch 185: 0.0450897/0.215271loss in batch 186: 0.242874/0.215424loss in batch 187: 0.0888672/0.214752loss in batch 188: 0.0269165/0.213745loss in batch 189: 0.288315/0.214157loss in batch 190: 0.316681/0.214691loss in batch 191: 0.320282/0.21524loss in batch 192: 0.248734/0.215408loss in batch 193: 0.109009/0.214859loss in batch 194: 0.31868/0.215393loss in batch 195: 0.32341/0.215958loss in batch 196: 0.0317841/0.214996loss in batch 197: 0.248062/0.215179loss in batch 198: 0.273407/0.215469loss in batch 199: 0.0459747/0.214615loss in batch 200: 0.201004/0.214554loss in batch 201: 0.0796204/0.213882loss in batch 202: 0.334656/0.214478loss in batch 203: 0.353378/0.215164loss in batch 204: 0.412903/0.216125loss in batch 205: 0.365204/0.216843loss in batch 206: 0.0450745/0.216019loss in batch 207: 0.0792084/0.215347loss in batch 208: 0.106094/0.214844loss in batch 209: 0.0752258/0.214172loss in batch 210: 0.0298004/0.213303loss in batch 211: 0.0566254/0.212555loss in batch 212: 0.140457/0.212219
done with epoch 6
train_acc: 0.946009 (403/426)
test loss: 0.140457
acc: 0.937061 (134/143)
loss in batch 0: 0.165512/0.165512loss in batch 1: 0.0869446/0.126221loss in batch 2: 0.0435486/0.0986633loss in batch 3: 0.0309906/0.0817413loss in batch 4: 0.069046/0.0792084loss in batch 5: 0.0411682/0.0728607loss in batch 6: 0.064682/0.071701loss in batch 7: 0.0144958/0.0645599loss in batch 8: 0.162354/0.0754242loss in batch 9: 0.0344391/0.0713043loss in batch 10: 0.351257/0.0967712loss in batch 11: 0.120941/0.0987701loss in batch 12: 0.119598/0.100372loss in batch 13: 0.112167/0.101227loss in batch 14: 0.279358/0.113098loss in batch 15: 0.0578156/0.10965loss in batch 16: 0.0628204/0.106888loss in batch 17: 0.306061/0.11795loss in batch 18: 0.0830383/0.116119loss in batch 19: 1.0562/0.163132loss in batch 20: 0.0396271/0.157242loss in batch 21: 0.096283/0.154465loss in batch 22: 0.0269165/0.148926loss in batch 23: 0.426163/0.160477loss in batch 24: 0.434433/0.171432loss in batch 25: 0.472427/0.183014loss in batch 26: 0.133423/0.181168loss in batch 27: 0.0652771/0.177032loss in batch 28: 0.275787/0.18045loss in batch 29: 0.278152/0.183701loss in batch 30: 0.0539093/0.179504loss in batch 31: 0.0585022/0.175735loss in batch 32: 0.0809326/0.172852loss in batch 33: 0.268143/0.175659loss in batch 34: 0.12384/0.174179loss in batch 35: 0.0808868/0.171585loss in batch 36: 0.117447/0.170135loss in batch 37: 0.0620728/0.167282loss in batch 38: 0.046402/0.164185loss in batch 39: 0.489349/0.172318loss in batch 40: 0.217606/0.173416loss in batch 41: 0.176117/0.173477loss in batch 42: 0.061142/0.170868loss in batch 43: 0.0969238/0.169174loss in batch 44: 0.40213/0.174362loss in batch 45: 0.116486/0.173111loss in batch 46: 0.563583/0.181412loss in batch 47: 0.189026/0.18158loss in batch 48: 0.0449982/0.178787loss in batch 49: 0.190628/0.179016loss in batch 50: 0.0173035/0.175842loss in batch 51: 0.0868378/0.174149loss in batch 52: 0.30072/0.176529loss in batch 53: 0.0759583/0.174667loss in batch 54: 0.637482/0.18309loss in batch 55: 0.321884/0.185562loss in batch 56: 0.0847321/0.183792loss in batch 57: 0.0898438/0.182175loss in batch 58: 0.0384979/0.179733loss in batch 59: 0.0512085/0.177597loss in batch 60: 0.357529/0.180542loss in batch 61: 0.0618439/0.178619loss in batch 62: 0.0535431/0.176651loss in batch 63: 0.164703/0.176453loss in batch 64: 0.106155/0.175369loss in batch 65: 0.26384/0.176712loss in batch 66: 0.086792/0.175369loss in batch 67: 0.0441284/0.173447loss in batch 68: 0.0946655/0.172287loss in batch 69: 0.351135/0.174866loss in batch 70: 0.0269775/0.172775loss in batch 71: 0.110367/0.171906loss in batch 72: 0.312317/0.173828loss in batch 73: 0.0731354/0.17247loss in batch 74: 0.472382/0.176468loss in batch 75: 0.0397797/0.174667loss in batch 76: 0.260635/0.175797loss in batch 77: 0.0827942/0.174591loss in batch 78: 0.10405/0.173706loss in batch 79: 0.0937347/0.172699loss in batch 80: 0.0569611/0.171265loss in batch 81: 0.488205/0.17514loss in batch 82: 0.248108/0.176025loss in batch 83: 0.163437/0.175873loss in batch 84: 1.38551/0.190094loss in batch 85: 0.459961/0.193237loss in batch 86: 0.0874176/0.192017loss in batch 87: 0.0313721/0.190186loss in batch 88: 0.0793304/0.18895loss in batch 89: 0.854034/0.196335loss in batch 90: 0.366928/0.198212loss in batch 91: 0.620453/0.202789loss in batch 92: 0.188217/0.202652loss in batch 93: 0.10672/0.20163loss in batch 94: 0.357101/0.203247loss in batch 95: 0.0997314/0.202179loss in batch 96: 0.0474091/0.200577loss in batch 97: 0.0592957/0.199142loss in batch 98: 0.0640717/0.197784loss in batch 99: 0.00628662/0.195862loss in batch 100: 0.188385/0.195786loss in batch 101: 0.0731354/0.19458loss in batch 102: 0.0215454/0.192902loss in batch 103: 0.23703/0.193329loss in batch 104: 0.337326/0.194702loss in batch 105: 0.394348/0.196594loss in batch 106: 0.106049/0.19574loss in batch 107: 0.356506/0.19722loss in batch 108: 0.154984/0.196838loss in batch 109: 0.467834/0.19931loss in batch 110: 0.133972/0.198715loss in batch 111: 0.158157/0.198349loss in batch 112: 0.46106/0.200684loss in batch 113: 0.0391541/0.199265loss in batch 114: 0.118103/0.198563loss in batch 115: 0.0511322/0.197281loss in batch 116: 0.0624847/0.196136loss in batch 117: 0.0616302/0.194992loss in batch 118: 0.104019/0.194229loss in batch 119: 0.173706/0.194061loss in batch 120: 0.151443/0.19371loss in batch 121: 0.032074/0.192383loss in batch 122: 0.0674896/0.19136loss in batch 123: 0.0652161/0.190353loss in batch 124: 0.0665894/0.189362loss in batch 125: 0.0576782/0.188309loss in batch 126: 0.370209/0.189743loss in batch 127: 0.0794067/0.188889loss in batch 128: 0.561142/0.191772loss in batch 129: 0.0730133/0.190857loss in batch 130: 0.396667/0.192429loss in batch 131: 0.0681305/0.191483loss in batch 132: 0.0256653/0.190247loss in batch 133: 0.714539/0.194153loss in batch 134: 0.170959/0.193985loss in batch 135: 0.432846/0.195724loss in batch 136: 0.163712/0.195511loss in batch 137: 0.144379/0.195129loss in batch 138: 0.0368195/0.194loss in batch 139: 0.145004/0.193649loss in batch 140: 0.0749207/0.19281loss in batch 141: 0.231186/0.193069loss in batch 142: 0.284485/0.19371loss in batch 143: 0.165863/0.193512loss in batch 144: 0.154343/0.193237loss in batch 145: 0.387482/0.19458loss in batch 146: 0.420883/0.196121loss in batch 147: 0.042038/0.195068loss in batch 148: 0.128464/0.194626loss in batch 149: 0.075119/0.193832loss in batch 150: 0.0689545/0.192993loss in batch 151: 0.0844574/0.192291loss in batch 152: 0.0663757/0.191467loss in batch 153: 0.43602/0.193069loss in batch 154: 0.0777893/0.192307loss in batch 155: 0.982101/0.197372loss in batch 156: 0.100159/0.196747loss in batch 157: 0.0523682/0.195831loss in batch 158: 0.541702/0.198029loss in batch 159: 0.268799/0.198456loss in batch 160: 0.0752411/0.197693loss in batch 161: 0.198059/0.197693loss in batch 162: 0.361145/0.1987loss in batch 163: 0.101074/0.198105loss in batch 164: 0.209274/0.198166loss in batch 165: 0.0551605/0.197311loss in batch 166: 0.188446/0.19725loss in batch 167: 0.496078/0.199036loss in batch 168: 0.0435333/0.198105loss in batch 169: 0.313599/0.198792loss in batch 170: 0.197144/0.198792loss in batch 171: 0.222366/0.198929loss in batch 172: 0.165741/0.19873loss in batch 173: 0.0403137/0.197815loss in batch 174: 0.467728/0.199356loss in batch 175: 0.497009/0.20105loss in batch 176: 0.201019/0.20105loss in batch 177: 0.135422/0.200684loss in batch 178: 0.0516663/0.199844loss in batch 179: 0.0552979/0.199051loss in batch 180: 0.19313/0.19902loss in batch 181: 0.232529/0.199203loss in batch 182: 0.0348358/0.198303loss in batch 183: 0.110977/0.19783loss in batch 184: 0.0752106/0.197159loss in batch 185: 0.0510254/0.196381loss in batch 186: 0.139893/0.196075loss in batch 187: 0.809082/0.199341loss in batch 188: 0.169968/0.199173loss in batch 189: 0.0944824/0.198639loss in batch 190: 0.0471802/0.197845loss in batch 191: 0.0634918/0.197144loss in batch 192: 0.429718/0.198349loss in batch 193: 0.174149/0.198212loss in batch 194: 0.0614624/0.19751loss in batch 195: 0.267746/0.197876loss in batch 196: 0.482117/0.19931loss in batch 197: 0.0375214/0.198502loss in batch 198: 0.541458/0.200226loss in batch 199: 0.0391235/0.199417loss in batch 200: 0.0176239/0.198517loss in batch 201: 0.106476/0.198059loss in batch 202: 0.100418/0.197586loss in batch 203: 0.479797/0.198959loss in batch 204: 1.07744/0.203247loss in batch 205: 0.146683/0.202972loss in batch 206: 0.141602/0.202667loss in batch 207: 0.125183/0.202301loss in batch 208: 0.0747986/0.201691loss in batch 209: 0.110748/0.201263loss in batch 210: 0.0725708/0.200653loss in batch 211: 0.0823364/0.200104loss in batch 212: 1.00861/0.203873
done with epoch 7
train_acc: 0.92723 (395/426)
test loss: 1.0086
acc: 0.930068 (133/143)
loss in batch 0: 0.0516663/0.0516663loss in batch 1: 0.116257/0.0839539loss in batch 2: 0.0754395/0.0811157loss in batch 3: 0.210968/0.113571loss in batch 4: 0.156754/0.122208loss in batch 5: 0.133163/0.124039loss in batch 6: 0.543716/0.18399loss in batch 7: 0.0681152/0.16951loss in batch 8: 0.237946/0.177109loss in batch 9: 0.228958/0.182297loss in batch 10: 0.161423/0.180405loss in batch 11: 0.10495/0.174118loss in batch 12: 0.114777/0.169556loss in batch 13: 0.0928802/0.164063loss in batch 14: 0.0590363/0.157074loss in batch 15: 0.157928/0.157135loss in batch 16: 0.0168762/0.148865loss in batch 17: 0.109924/0.146698loss in batch 18: 0.0102386/0.139526loss in batch 19: 0.0238647/0.133743loss in batch 20: 0.958115/0.173004loss in batch 21: 0.0324554/0.166611loss in batch 22: 0.0882263/0.163193loss in batch 23: 0.285248/0.168289loss in batch 24: 0.0183258/0.162292loss in batch 25: 0.103012/0.160019loss in batch 26: 0.0852051/0.157242loss in batch 27: 0.0784912/0.154419loss in batch 28: 0.0944977/0.152359loss in batch 29: 0.31778/0.157883loss in batch 30: 0.0483093/0.154343loss in batch 31: 0.0249023/0.150299loss in batch 32: 0.349152/0.156326loss in batch 33: 0.290726/0.160278loss in batch 34: 0.265884/0.1633loss in batch 35: 0.0214691/0.159363loss in batch 36: 0.528152/0.169327loss in batch 37: 0.213211/0.170471loss in batch 38: 0.368225/0.175552loss in batch 39: 0.453552/0.182495loss in batch 40: 0.392929/0.187622loss in batch 41: 0.0708466/0.184845loss in batch 42: 0.830765/0.199875loss in batch 43: 0.0396271/0.196228loss in batch 44: 0.0689087/0.19339loss in batch 45: 0.0583801/0.19046loss in batch 46: 0.0202789/0.186829loss in batch 47: 0.115997/0.185364loss in batch 48: 0.0400238/0.182404loss in batch 49: 0.225906/0.183273loss in batch 50: 0.196457/0.183533loss in batch 51: 0.0139923/0.180267loss in batch 52: 0.107193/0.178894loss in batch 53: 0.0346527/0.176224loss in batch 54: 0.0542755/0.173996loss in batch 55: 0.0172272/0.171204loss in batch 56: 0.128464/0.170441loss in batch 57: 0.111954/0.169434loss in batch 58: 0.0525513/0.167465loss in batch 59: 0.139206/0.166992loss in batch 60: 0.076355/0.165512loss in batch 61: 0.118423/0.164734loss in batch 62: 0.277771/0.16655loss in batch 63: 0.144943/0.166214loss in batch 64: 0.472031/0.170914loss in batch 65: 0.087204/0.169647loss in batch 66: 0.0727386/0.168198loss in batch 67: 0.406052/0.171692loss in batch 68: 0.475525/0.176102loss in batch 69: 0.126205/0.175385loss in batch 70: 0.0859375/0.174133loss in batch 71: 0.0726929/0.172714loss in batch 72: 0.0608063/0.171173loss in batch 73: 0.455261/0.175018loss in batch 74: 0.176727/0.175049loss in batch 75: 0.228958/0.175751loss in batch 76: 0.292908/0.177277loss in batch 77: 0.0504761/0.175644loss in batch 78: 0.109024/0.174805loss in batch 79: 1.07361/0.186035loss in batch 80: 0.0901337/0.184845loss in batch 81: 0.377274/0.187195loss in batch 82: 0.154053/0.186798loss in batch 83: 0.0692596/0.18541loss in batch 84: 0.0411987/0.183701loss in batch 85: 0.0481873/0.182129loss in batch 86: 1.34546/0.195511loss in batch 87: 0.177032/0.195282loss in batch 88: 0.0328827/0.193466loss in batch 89: 0.0961914/0.192383loss in batch 90: 0.120895/0.191605loss in batch 91: 0.056778/0.19014loss in batch 92: 0.214691/0.190399loss in batch 93: 0.24733/0.190994loss in batch 94: 0.218765/0.191284loss in batch 95: 0.1064/0.190414loss in batch 96: 0.0857544/0.189346loss in batch 97: 0.0359955/0.187775loss in batch 98: 0.302002/0.188919loss in batch 99: 0.113235/0.188171loss in batch 100: 0.183899/0.188126loss in batch 101: 0.485825/0.19104loss in batch 102: 0.0458832/0.189621loss in batch 103: 0.5065/0.192673loss in batch 104: 0.42099/0.194855loss in batch 105: 0.068924/0.193665loss in batch 106: 0.198059/0.19371loss in batch 107: 0.0793152/0.192642loss in batch 108: 0.0474548/0.191315loss in batch 109: 0.580673/0.194855loss in batch 110: 0.286758/0.195679loss in batch 111: 0.540955/0.198761loss in batch 112: 0.0756836/0.197678loss in batch 113: 0.0854034/0.196701loss in batch 114: 0.52179/0.199524loss in batch 115: 0.0491791/0.198227loss in batch 116: 0.113266/0.197495loss in batch 117: 0.127319/0.196899loss in batch 118: 0.0762024/0.195892loss in batch 119: 0.0694122/0.194839loss in batch 120: 0.927948/0.200897loss in batch 121: 0.536133/0.203629loss in batch 122: 0.102249/0.20282loss in batch 123: 0.124893/0.202194loss in batch 124: 0.10759/0.201431loss in batch 125: 0.790649/0.2061loss in batch 126: 0.0464325/0.204849loss in batch 127: 0.0524902/0.203659loss in batch 128: 0.154037/0.203278loss in batch 129: 0.114899/0.202591loss in batch 130: 0.142212/0.202133loss in batch 131: 0.273087/0.202667loss in batch 132: 0.279129/0.203247loss in batch 133: 0.297607/0.203949loss in batch 134: 0.428101/0.205612loss in batch 135: 0.359283/0.206741loss in batch 136: 0.103271/0.205978loss in batch 137: 0.134491/0.20546loss in batch 138: 0.0287781/0.204208loss in batch 139: 0.503036/0.206329loss in batch 140: 0.00579834/0.20491loss in batch 141: 0.191498/0.204819loss in batch 142: 0.178101/0.204636loss in batch 143: 0.170197/0.204391loss in batch 144: 0.187683/0.204269loss in batch 145: 0.0291748/0.203079loss in batch 146: 0.711258/0.206528loss in batch 147: 0.317261/0.207275loss in batch 148: 0.611893/0.209991loss in batch 149: 0.188522/0.209839loss in batch 150: 0.21698/0.2099loss in batch 151: 0.0202332/0.208649loss in batch 152: 0.123734/0.208099loss in batch 153: 0.0193481/0.206879loss in batch 154: 0.293808/0.207428loss in batch 155: 0.103226/0.206757loss in batch 156: 0.106247/0.206131loss in batch 157: 0.0102081/0.20488loss in batch 158: 0.153854/0.204559loss in batch 159: 0.171524/0.204361loss in batch 160: 0.0377197/0.203323loss in batch 161: 0.0845337/0.202591loss in batch 162: 0.0428772/0.201614loss in batch 163: 0.0281982/0.200546loss in batch 164: 0.296326/0.201126loss in batch 165: 0.0566711/0.200256loss in batch 166: 0.082962/0.199554loss in batch 167: 0.215057/0.199646loss in batch 168: 0.0639343/0.198837loss in batch 169: 0.561523/0.200974loss in batch 170: 0.166748/0.20079loss in batch 171: 0.0572968/0.199951loss in batch 172: 0.779007/0.203293loss in batch 173: 0.091217/0.202652loss in batch 174: 0.209167/0.202682loss in batch 175: 0.0438232/0.201782loss in batch 176: 0.340302/0.20256loss in batch 177: 0.269394/0.202942loss in batch 178: 0.0796814/0.20224loss in batch 179: 0.0550995/0.201431loss in batch 180: 0.0603027/0.200653loss in batch 181: 0.330032/0.20137loss in batch 182: 0.154297/0.201111loss in batch 183: 0.0965881/0.200531loss in batch 184: 0.111893/0.200058loss in batch 185: 0.333069/0.200775loss in batch 186: 0.64975/0.203171loss in batch 187: 0.232742/0.203339loss in batch 188: 0.0595551/0.202576loss in batch 189: 0.0539856/0.201782loss in batch 190: 0.00888062/0.20079loss in batch 191: 0.19104/0.200729loss in batch 192: 0.460922/0.202087loss in batch 193: 0.15155/0.201828loss in batch 194: 0.208099/0.201843loss in batch 195: 0.343628/0.202576loss in batch 196: 0.0609436/0.201859loss in batch 197: 0.0259705/0.200958loss in batch 198: 0.0973053/0.200455loss in batch 199: 0.110062/0.199997loss in batch 200: 0.0160065/0.199081loss in batch 201: 0.413757/0.20015loss in batch 202: 0.0367737/0.199341loss in batch 203: 0.061554/0.198654loss in batch 204: 0.0870514/0.19812loss in batch 205: 0.0552521/0.197433loss in batch 206: 0.0757446/0.196838loss in batch 207: 0.0359039/0.19606loss in batch 208: 0.0336609/0.195282loss in batch 209: 0.253159/0.195557loss in batch 210: 0.0891266/0.195068loss in batch 211: 0.0554962/0.194397loss in batch 212: 0.437576/0.195541
done with epoch 8
train_acc: 0.93662 (399/426)
test loss: 0.437575
acc: 0.944054 (135/143)
loss in batch 0: 0.355118/0.355118loss in batch 1: 0.0991974/0.227158loss in batch 2: 0.108582/0.187622loss in batch 3: 0.11261/0.168884loss in batch 4: 0.120193/0.159134loss in batch 5: 0.0434875/0.139877loss in batch 6: 0.0375977/0.125244loss in batch 7: 0.108932/0.123215loss in batch 8: 0.0320282/0.113083loss in batch 9: 0.29686/0.13147loss in batch 10: 0.111618/0.129654loss in batch 11: 0.065979/0.124344loss in batch 12: 0.179916/0.128616loss in batch 13: 0.134033/0.129013loss in batch 14: 0.187012/0.132874loss in batch 15: 0.552277/0.159103loss in batch 16: 0.502029/0.17926loss in batch 17: 0.160965/0.178253loss in batch 18: 0.020401/0.169937loss in batch 19: 0.303558/0.17662loss in batch 20: 0.111664/0.173523loss in batch 21: 0.0833282/0.169418loss in batch 22: 0.217575/0.171524loss in batch 23: 0.155045/0.170837loss in batch 24: 0.602219/0.188095loss in batch 25: 0.0801239/0.183945loss in batch 26: 0.138916/0.182266loss in batch 27: 0.285004/0.185944loss in batch 28: 0.156143/0.184921loss in batch 29: 0.310715/0.189102loss in batch 30: 0.0745392/0.18541loss in batch 31: 0.43959/0.193344loss in batch 32: 0.074295/0.189743loss in batch 33: 0.0683289/0.186172loss in batch 34: 0.100372/0.183716loss in batch 35: 0.0327911/0.179535loss in batch 36: 0.0294189/0.175476loss in batch 37: 0.0509949/0.172195loss in batch 38: 0.085495/0.169983loss in batch 39: 0.116409/0.16864loss in batch 40: 0.270096/0.171112loss in batch 41: 0.484344/0.178574loss in batch 42: 0.0750885/0.176163loss in batch 43: 0.0392151/0.173035loss in batch 44: 0.052475/0.17038loss in batch 45: 0.326584/0.173767loss in batch 46: 0.0665588/0.171478loss in batch 47: 0.195114/0.171982loss in batch 48: 0.034729/0.169174loss in batch 49: 0.100708/0.167801loss in batch 50: 0.0609436/0.16571loss in batch 51: 0.845734/0.178787loss in batch 52: 0.0387115/0.176147loss in batch 53: 0.0435181/0.173691loss in batch 54: 0.249191/0.175049loss in batch 55: 0.566406/0.182053loss in batch 56: 0.217102/0.182663loss in batch 57: 0.207199/0.18309loss in batch 58: 0.112411/0.181885loss in batch 59: 0.14003/0.181198loss in batch 60: 0.139603/0.180511loss in batch 61: 0.587601/0.187073loss in batch 62: 0.143005/0.186371loss in batch 63: 0.356308/0.189026loss in batch 64: 0.0275116/0.186554loss in batch 65: 0.0938416/0.18515loss in batch 66: 0.00473022/0.182449loss in batch 67: 0.0470276/0.18045loss in batch 68: 0.0419312/0.178452loss in batch 69: 0.108627/0.17746loss in batch 70: 0.114807/0.176575loss in batch 71: 0.271072/0.177887loss in batch 72: 0.0487061/0.176117loss in batch 73: 0.0661163/0.174622loss in batch 74: 0.0185852/0.172546loss in batch 75: 0.238815/0.173416loss in batch 76: 0.180557/0.173523loss in batch 77: 0.242264/0.174393loss in batch 78: 0.145432/0.174026loss in batch 79: 0.087204/0.172943loss in batch 80: 0.350525/0.175125loss in batch 81: 0.327133/0.176987loss in batch 82: 0.0791168/0.175812loss in batch 83: 0.0739136/0.174591loss in batch 84: 0.161942/0.174438loss in batch 85: 0.101212/0.173599loss in batch 86: 0.0625458/0.172318loss in batch 87: 0.0552826/0.17099loss in batch 88: 0.13356/0.170563loss in batch 89: 0.102356/0.169815loss in batch 90: 0.00485229/0.167999loss in batch 91: 0.139679/0.167679loss in batch 92: 0.123184/0.167221loss in batch 93: 0.103745/0.166534loss in batch 94: 0.185059/0.166718loss in batch 95: 0.1698/0.166763loss in batch 96: 0.092514/0.166loss in batch 97: 0.0240173/0.164551loss in batch 98: 0.397507/0.166901loss in batch 99: 0.0656433/0.165894loss in batch 100: 0.0543213/0.16478loss in batch 101: 0.0417175/0.163574loss in batch 102: 0.0575714/0.162552loss in batch 103: 0.116882/0.162109loss in batch 104: 0.469543/0.165039loss in batch 105: 0.451767/0.16774loss in batch 106: 0.156738/0.167648loss in batch 107: 0.06427/0.166687loss in batch 108: 0.375153/0.168594loss in batch 109: 0.0196686/0.167236loss in batch 110: 0.0239563/0.165955loss in batch 111: 0.225601/0.166489loss in batch 112: 0.327255/0.167908loss in batch 113: 0.24118/0.168549loss in batch 114: 0.514847/0.171555loss in batch 115: 0.0686646/0.17067loss in batch 116: 0.0267487/0.169449loss in batch 117: 0.0468903/0.168411loss in batch 118: 0.0661316/0.167542loss in batch 119: 0.385101/0.169357loss in batch 120: 0.12616/0.169006loss in batch 121: 0.0235596/0.167816loss in batch 122: 0.0288391/0.166687loss in batch 123: 0.011673/0.165436loss in batch 124: 1.45076/0.175705loss in batch 125: 0.0774841/0.174927loss in batch 126: 0.0141449/0.173676loss in batch 127: 0.0971985/0.173065loss in batch 128: 0.0753632/0.172318loss in batch 129: 0.203354/0.172546loss in batch 130: 0.0614777/0.171692loss in batch 131: 0.264709/0.172409loss in batch 132: 0.0271912/0.17131loss in batch 133: 0.0431976/0.170364loss in batch 134: 0.0562286/0.16951loss in batch 135: 0.527115/0.17215loss in batch 136: 0.271286/0.172867loss in batch 137: 0.0312653/0.171829loss in batch 138: 0.133102/0.171555loss in batch 139: 0.331253/0.172699loss in batch 140: 0.0369415/0.171738loss in batch 141: 0.628265/0.174957loss in batch 142: 0.106979/0.174469loss in batch 143: 0.0377197/0.173538loss in batch 144: 0.295425/0.174377loss in batch 145: 0.0207062/0.173309loss in batch 146: 0.0666504/0.172592loss in batch 147: 0.10762/0.172165loss in batch 148: 0.240814/0.172623loss in batch 149: 0.140274/0.172394loss in batch 150: 0.0735931/0.171738loss in batch 151: 0.0788422/0.171127loss in batch 152: 1.09163/0.177139loss in batch 153: 0.0198212/0.176117loss in batch 154: 0.0113373/0.175064loss in batch 155: 0.0983429/0.174576loss in batch 156: 0.505554/0.176682loss in batch 157: 0.26976/0.177277loss in batch 158: 0.457901/0.179031loss in batch 159: 0.262558/0.17955loss in batch 160: 0.0324097/0.17865loss in batch 161: 0.233337/0.178986loss in batch 162: 0.0438995/0.178162loss in batch 163: 0.260513/0.17865loss in batch 164: 0.292694/0.179352loss in batch 165: 0.37265/0.180511loss in batch 166: 0.0792084/0.179901loss in batch 167: 0.0418243/0.179077loss in batch 168: 0.0384979/0.178253loss in batch 169: 0.328384/0.179123loss in batch 170: 0.134888/0.178879loss in batch 171: 0.193298/0.178955loss in batch 172: 0.0162354/0.178009loss in batch 173: 0.818573/0.181686loss in batch 174: 0.113052/0.181305loss in batch 175: 0.401917/0.182556loss in batch 176: 0.0328979/0.181702loss in batch 177: 0.0944519/0.181229loss in batch 178: 0.068634/0.180588loss in batch 179: 0.139435/0.180374loss in batch 180: 0.0582733/0.179703loss in batch 181: 0.0250092/0.178848loss in batch 182: 0.0919037/0.17836loss in batch 183: 0.0704041/0.17778loss in batch 184: 0.0517883/0.177094loss in batch 185: 0.0700378/0.176514loss in batch 186: 0.65596/0.179077loss in batch 187: 0.0942993/0.178635loss in batch 188: 0.252975/0.179016loss in batch 189: 0.866867/0.182648loss in batch 190: 0.0175629/0.181778loss in batch 191: 0.173111/0.181747loss in batch 192: 0.850723/0.185196loss in batch 193: 0.127991/0.184906loss in batch 194: 0.399734/0.18602loss in batch 195: 0.124405/0.185699loss in batch 196: 0.419296/0.18689loss in batch 197: 0.0543823/0.186203loss in batch 198: 0.21875/0.186371loss in batch 199: 0.23526/0.186615loss in batch 200: 0.0910492/0.186142loss in batch 201: 0.0116577/0.185272loss in batch 202: 0.136841/0.185043loss in batch 203: 0.0320129/0.184296loss in batch 204: 0.0144501/0.183472loss in batch 205: 0.0476379/0.1828loss in batch 206: 0.200058/0.182892loss in batch 207: 0.077301/0.182388loss in batch 208: 1.18614/0.18718loss in batch 209: 0.0924988/0.186737loss in batch 210: 0.312485/0.187332loss in batch 211: 0.348389/0.18808loss in batch 212: 0.129547/0.187805
done with epoch 9
train_acc: 0.943662 (402/426)
test loss: 0.129547
acc: 0.944054 (135/143)
loss in batch 0: 0.0654449/0.0654449loss in batch 1: 1.45607/0.760757loss in batch 2: 0.0787506/0.533432loss in batch 3: 0.003479/0.40094loss in batch 4: 0.180176/0.356781loss in batch 5: 0.04216/0.304337loss in batch 6: 0.0283813/0.264923loss in batch 7: 0.08638/0.242599loss in batch 8: 0.0425568/0.220383loss in batch 9: 0.111771/0.209518loss in batch 10: 0.134781/0.202728loss in batch 11: 0.0396576/0.189133loss in batch 12: 0.0779266/0.180588loss in batch 13: 0.0452728/0.170914loss in batch 14: 0.104248/0.166473loss in batch 15: 0.0619202/0.159943loss in batch 16: 0.319885/0.169342loss in batch 17: 0.160599/0.168869loss in batch 18: 0.123398/0.166473loss in batch 19: 0.32663/0.174484loss in batch 20: 0.108841/0.171356loss in batch 21: 0.112854/0.168686loss in batch 22: 0.160294/0.168335loss in batch 23: 0.116837/0.166183loss in batch 24: 0.98381/0.198883loss in batch 25: 0.166626/0.197647loss in batch 26: 0.0632935/0.192657loss in batch 27: 0.342743/0.198029loss in batch 28: 0.22052/0.198807loss in batch 29: 0.0425568/0.193588loss in batch 30: 0.203522/0.193909loss in batch 31: 0.0769958/0.190262loss in batch 32: 0.346832/0.195007loss in batch 33: 0.0912628/0.191956loss in batch 34: 0.093811/0.189148loss in batch 35: 0.10437/0.186798loss in batch 36: 0.0419312/0.182892loss in batch 37: 0.256622/0.184814loss in batch 38: 0.340332/0.188812loss in batch 39: 0.689987/0.20134loss in batch 40: 0.10556/0.199005loss in batch 41: 0.0520172/0.195496loss in batch 42: 0.0711517/0.192612loss in batch 43: 0.0348969/0.189026loss in batch 44: 0.0711975/0.186401loss in batch 45: 0.0645447/0.183762loss in batch 46: 0.0726776/0.181396loss in batch 47: 0.373276/0.185394loss in batch 48: 0.0839691/0.183319loss in batch 49: 0.12178/0.182098loss in batch 50: 0.590744/0.190109loss in batch 51: 0.040741/0.187225loss in batch 52: 0.0597076/0.18483loss in batch 53: 0.261688/0.186249loss in batch 54: 0.0604858/0.18396loss in batch 55: 0.457321/0.188843loss in batch 56: 0.105545/0.187378loss in batch 57: 0.0473633/0.184967loss in batch 58: 0.0763855/0.183121loss in batch 59: 0.189667/0.183243loss in batch 60: 0.253967/0.184402loss in batch 61: 0.0851135/0.182785loss in batch 62: 0.0402527/0.180542loss in batch 63: 0.0765228/0.178894loss in batch 64: 0.126389/0.178101loss in batch 65: 0.305405/0.180038loss in batch 66: 0.24408/0.180984loss in batch 67: 0.357101/0.183578loss in batch 68: 0.88237/0.19371loss in batch 69: 0.0380859/0.191483loss in batch 70: 0.0932465/0.190109loss in batch 71: 0.0499573/0.188156loss in batch 72: 0.29924/0.189667loss in batch 73: 0.0221252/0.187408loss in batch 74: 0.209213/0.187683loss in batch 75: 0.120712/0.186813loss in batch 76: 0.694397/0.193405loss in batch 77: 0.00343323/0.190979loss in batch 78: 0.77002/0.198303loss in batch 79: 0.0532532/0.196487loss in batch 80: 0.293259/0.197678loss in batch 81: 0.035553/0.195709loss in batch 82: 0.0600586/0.194077loss in batch 83: 0.0350647/0.192184loss in batch 84: 0.167709/0.191895loss in batch 85: 0.117188/0.191025loss in batch 86: 0.247879/0.191681loss in batch 87: 0.289658/0.19278loss in batch 88: 0.176819/0.192612loss in batch 89: 0.09552/0.191528loss in batch 90: 0.00790405/0.189514loss in batch 91: 0.0137024/0.187607loss in batch 92: 0.021286/0.185822loss in batch 93: 0.547806/0.189651loss in batch 94: 0.0745697/0.188446loss in batch 95: 0.279831/0.189407loss in batch 96: 0.184814/0.189362loss in batch 97: 0.0618134/0.188065loss in batch 98: 0.0726776/0.18689loss in batch 99: 0.0253448/0.185287loss in batch 100: 0.0790863/0.184219loss in batch 101: 0.0493164/0.182907loss in batch 102: 0.0681152/0.181793loss in batch 103: 0.0197601/0.180237loss in batch 104: 0.08461/0.179321loss in batch 105: 0.0225372/0.177841loss in batch 106: 0.543015/0.181244loss in batch 107: 0.0844269/0.180359loss in batch 108: 0.044632/0.179108loss in batch 109: 0.0646362/0.17807loss in batch 110: 0.0576172/0.176987loss in batch 111: 0.431808/0.17926loss in batch 112: 0.211487/0.17955loss in batch 113: 0.131699/0.179123loss in batch 114: 0.0294495/0.177826loss in batch 115: 0.0400085/0.176636loss in batch 116: 0.0874329/0.175873loss in batch 117: 0.511108/0.178711loss in batch 118: 0.124115/0.178253loss in batch 119: 0.0386963/0.177094loss in batch 120: 0.10585/0.176498loss in batch 121: 0.0817413/0.175735loss in batch 122: 0.0473022/0.174683loss in batch 123: 0.2509/0.175293loss in batch 124: 0.332596/0.176544loss in batch 125: 0.31871/0.177689loss in batch 126: 0.0278931/0.176514loss in batch 127: 0.465256/0.178757loss in batch 128: 0.0686798/0.177917loss in batch 129: 0.160172/0.17778loss in batch 130: 0.503601/0.180252loss in batch 131: 0.794235/0.184906loss in batch 132: 0.432846/0.186783loss in batch 133: 0.314148/0.187729loss in batch 134: 0.370972/0.189087loss in batch 135: 0.283478/0.189774loss in batch 136: 1.11824/0.196548loss in batch 137: 0.0710297/0.195633loss in batch 138: 0.126724/0.195145loss in batch 139: 0.311188/0.195969loss in batch 140: 0.110641/0.195374loss in batch 141: 0.271759/0.195908loss in batch 142: 0.0288239/0.194733loss in batch 143: 0.0324402/0.193619loss in batch 144: 0.0103455/0.192352loss in batch 145: 0.696457/0.195801loss in batch 146: 0.0182037/0.194595loss in batch 147: 0.365601/0.19574loss in batch 148: 0.0870819/0.195007loss in batch 149: 0.104324/0.194412loss in batch 150: 0.0455322/0.19342loss in batch 151: 0.0792694/0.192688loss in batch 152: 0.0214539/0.191559loss in batch 153: 0.24588/0.19191loss in batch 154: 0.00782776/0.19072loss in batch 155: 0.0117645/0.189575loss in batch 156: 0.00971985/0.188431loss in batch 157: 0.0596313/0.187607loss in batch 158: 0.113953/0.187149loss in batch 159: 0.0827942/0.186508loss in batch 160: 0.0358582/0.185577loss in batch 161: 0.149002/0.185333loss in batch 162: 0.0368042/0.184433loss in batch 163: 0.0126648/0.18338loss in batch 164: 0.6978/0.186508loss in batch 165: 0.086319/0.185883loss in batch 166: 0.346649/0.186844loss in batch 167: 0.0612946/0.186111loss in batch 168: 0.0390167/0.185242loss in batch 169: 0.112839/0.184814loss in batch 170: 0.118118/0.184433loss in batch 171: 0.0610809/0.183716loss in batch 172: 0.0438843/0.182907loss in batch 173: 0.342926/0.183823loss in batch 174: 0.509613/0.185684loss in batch 175: 0.117722/0.185303loss in batch 176: 0.0014801/0.18425loss in batch 177: 0.10347/0.183807loss in batch 178: 0.0229797/0.182907loss in batch 179: 0.037384/0.182098loss in batch 180: 0.04422/0.181335loss in batch 181: 0.474762/0.182953loss in batch 182: 0.0398102/0.182159loss in batch 183: 0.0509491/0.181458loss in batch 184: 0.0471191/0.180725loss in batch 185: 0.392868/0.18187loss in batch 186: 0.087265/0.181366loss in batch 187: 0.0607147/0.18071loss in batch 188: 0.0703888/0.180145loss in batch 189: 0.048996/0.179428loss in batch 190: 0.353531/0.180359loss in batch 191: 0.285843/0.180908loss in batch 192: 0.0374298/0.180161loss in batch 193: 0.13176/0.179901loss in batch 194: 0.0344543/0.179169loss in batch 195: 0.285202/0.179703loss in batch 196: 0.094101/0.179276loss in batch 197: 0.0448151/0.178589loss in batch 198: 0.0727386/0.17807loss in batch 199: 0.155777/0.177948loss in batch 200: 1.11197/0.182602loss in batch 201: 0.284348/0.183105loss in batch 202: 0.0767059/0.182571loss in batch 203: 0.0200806/0.181778loss in batch 204: 0.551437/0.183578loss in batch 205: 0.102982/0.183182loss in batch 206: 0.50798/0.184769loss in batch 207: 0.116058/0.184433loss in batch 208: 0.303238/0.184998loss in batch 209: 0.21875/0.185165loss in batch 210: 0.236221/0.185394loss in batch 211: 0.0396881/0.184708loss in batch 212: 0.000320435/0.183853
done with epoch 10
train_acc: 0.943662 (402/426)
test loss: 0.000320434
acc: 0.937061 (134/143)
loss in batch 0: 0.128082/0.128082loss in batch 1: 0.103256/0.115662loss in batch 2: 0.373993/0.201767loss in batch 3: 0.0767517/0.170532loss in batch 4: 0.00816345/0.138046loss in batch 5: 0.256134/0.15773loss in batch 6: 0.147354/0.15625loss in batch 7: 0.0609894/0.144333loss in batch 8: 0.0370178/0.132416loss in batch 9: 0.251038/0.144287loss in batch 10: 0.343842/0.162415loss in batch 11: 0.335373/0.176834loss in batch 12: 0.233154/0.181168loss in batch 13: 0.160767/0.179703loss in batch 14: 0.595261/0.207413loss in batch 15: 0.0838623/0.199692loss in batch 16: 0.176468/0.198318loss in batch 17: 0.207123/0.198807loss in batch 18: 0.0388489/0.190399loss in batch 19: 0.063736/0.184067loss in batch 20: 0.107681/0.18042loss in batch 21: 0.166534/0.179794loss in batch 22: 0.0900879/0.175903loss in batch 23: 1.09604/0.214218loss in batch 24: 0.110764/0.210083loss in batch 25: 0.325729/0.214539loss in batch 26: 0.0161743/0.207184loss in batch 27: 0.0485229/0.201523loss in batch 28: 0.0832367/0.197449loss in batch 29: 0.159973/0.196198loss in batch 30: 0.0102692/0.190201loss in batch 31: 0.0664825/0.18634loss in batch 32: 0.0458374/0.182083loss in batch 33: 0.0981598/0.179611loss in batch 34: 0.0714417/0.176514loss in batch 35: 0.291702/0.179718loss in batch 36: 0.0531006/0.1763loss in batch 37: 0.450424/0.183502loss in batch 38: 0.0639954/0.18045loss in batch 39: 0.0898285/0.178177loss in batch 40: 0.567169/0.187668loss in batch 41: 0.669983/0.199142loss in batch 42: 0.0163727/0.194901loss in batch 43: 0.0378876/0.19133loss in batch 44: 0.0725708/0.18869loss in batch 45: 0.0559845/0.185806loss in batch 46: 0.0852661/0.18367loss in batch 47: 0.0374756/0.180618loss in batch 48: 0.0429993/0.177811loss in batch 49: 0.0973206/0.176193loss in batch 50: 0.0883484/0.174484loss in batch 51: 0.00842285/0.171295loss in batch 52: 0.302628/0.173767loss in batch 53: 0.0375671/0.171249loss in batch 54: 0.183136/0.171463loss in batch 55: 0.0892181/0.169998loss in batch 56: 0.0890808/0.168564loss in batch 57: 0.36441/0.171951loss in batch 58: 0.208374/0.172562loss in batch 59: 0.166824/0.17247loss in batch 60: 0.0752563/0.170883loss in batch 61: 0.039093/0.168747loss in batch 62: 0.055191/0.166946loss in batch 63: 0.0473328/0.165085loss in batch 64: 0.0307617/0.163025loss in batch 65: 0.0238647/0.160904loss in batch 66: 0.0985413/0.159973loss in batch 67: 0.0480957/0.158325loss in batch 68: 0.00721741/0.156143loss in batch 69: 0.0257263/0.154266loss in batch 70: 0.0759888/0.153168loss in batch 71: 0.2146/0.154022loss in batch 72: 0.0359192/0.152405loss in batch 73: 0.134598/0.152161loss in batch 74: 0.0647888/0.151001loss in batch 75: 0.0632782/0.149857loss in batch 76: 0.0308533/0.1483loss in batch 77: 1.63962/0.167435loss in batch 78: 0.0842133/0.166367loss in batch 79: 0.134888/0.165985loss in batch 80: 0.072113/0.16481loss in batch 81: 0.215454/0.165436loss in batch 82: 0.0890503/0.16452loss in batch 83: 0.545273/0.169052loss in batch 84: 0.198227/0.169388loss in batch 85: 0.0210266/0.167664loss in batch 86: 0.180862/0.167816loss in batch 87: 0.0432739/0.166412loss in batch 88: 0.0758972/0.165375loss in batch 89: 0.155426/0.165283loss in batch 90: 0.0137482/0.16362loss in batch 91: 0.015564/0.162003loss in batch 92: 0.0278778/0.160553loss in batch 93: 0.0370483/0.159241loss in batch 94: 0.0716858/0.158325loss in batch 95: 0.372879/0.160553loss in batch 96: 0.0124664/0.159042loss in batch 97: 0.0860138/0.158279loss in batch 98: 0.332352/0.160049loss in batch 99: 0.0288086/0.158737loss in batch 100: 0.0672302/0.157822loss in batch 101: 0.101212/0.157272loss in batch 102: 0.163986/0.157333loss in batch 103: 0.117599/0.156967loss in batch 104: 0.00923157/0.155533loss in batch 105: 0.0526123/0.154572loss in batch 106: 0.0172882/0.15329loss in batch 107: 0.112885/0.152908loss in batch 108: 0.0537567/0.152008loss in batch 109: 0.0326385/0.150925loss in batch 110: 0.583389/0.154816loss in batch 111: 0.108017/0.154388loss in batch 112: 0.0432587/0.153427loss in batch 113: 0.0553741/0.152557loss in batch 114: 0.511185/0.155685loss in batch 115: 0.171768/0.155807loss in batch 116: 0.139221/0.15567loss in batch 117: 0.278564/0.156708loss in batch 118: 0.03125/0.155655loss in batch 119: 0.0410919/0.154694loss in batch 120: 0.341873/0.15625loss in batch 121: 0.237885/0.156921loss in batch 122: 0.111786/0.156555loss in batch 123: 0.0926666/0.156036loss in batch 124: 0.419296/0.158157loss in batch 125: 0.0407715/0.157227loss in batch 126: 0.341858/0.158676loss in batch 127: 0.0693817/0.157974loss in batch 128: 0.0711365/0.157303loss in batch 129: 0.286087/0.158295loss in batch 130: 0.168488/0.158371loss in batch 131: 0.12825/0.158142loss in batch 132: 0.230896/0.158691loss in batch 133: 0.0706635/0.158035loss in batch 134: 0.0231171/0.157028loss in batch 135: 0.15329/0.156998loss in batch 136: 0.154037/0.156982loss in batch 137: 0.404724/0.158783loss in batch 138: 0.0517731/0.158005loss in batch 139: 0.116913/0.157715loss in batch 140: 0.41983/0.159576loss in batch 141: 0.485672/0.161865loss in batch 142: 0.721451/0.165787loss in batch 143: 0.482147/0.167984loss in batch 144: 0.0093689/0.16687loss in batch 145: 0.149857/0.166763loss in batch 146: 0.00238037/0.165649loss in batch 147: 0.261871/0.166306loss in batch 148: 0.0825043/0.165741loss in batch 149: 0.0729828/0.165115loss in batch 150: 0.373703/0.166489loss in batch 151: 0.046051/0.16571loss in batch 152: 0.0573578/0.165009loss in batch 153: 0.00975037/0.163986loss in batch 154: 0.227661/0.164398loss in batch 155: 0.173904/0.164474loss in batch 156: 0.0965271/0.164032loss in batch 157: 0.0558624/0.163345loss in batch 158: 0.0731659/0.162781loss in batch 159: 0.211472/0.163086loss in batch 160: 0.0643616/0.16246loss in batch 161: 0.544464/0.164825loss in batch 162: 0.0986481/0.164429loss in batch 163: 0.0921936/0.163971loss in batch 164: 0.31665/0.164902loss in batch 165: 0.362457/0.166092loss in batch 166: 0.0362396/0.165314loss in batch 167: 0.180374/0.165421loss in batch 168: 0.0544739/0.164749loss in batch 169: 0.0995941/0.164368loss in batch 170: 1.43866/0.171814loss in batch 171: 0.0760345/0.171265loss in batch 172: 0.30748/0.172043loss in batch 173: 0.0637054/0.171417loss in batch 174: 0.591599/0.173828loss in batch 175: 0.109344/0.173462loss in batch 176: 0.17157/0.173447loss in batch 177: 0.0781403/0.172928loss in batch 178: 0.0358887/0.17215loss in batch 179: 0.463745/0.173767loss in batch 180: 0.0471802/0.173065loss in batch 181: 0.0236511/0.172256loss in batch 182: 0.204681/0.172424loss in batch 183: 0.0612488/0.171814loss in batch 184: 0.0444794/0.171143loss in batch 185: 0.394348/0.172333loss in batch 186: 0.339355/0.173233loss in batch 187: 0.08284/0.172745loss in batch 188: 0.0749969/0.172226loss in batch 189: 0.135895/0.172028loss in batch 190: 0.315353/0.172791loss in batch 191: 0.15036/0.172668loss in batch 192: 0.0106049/0.171829loss in batch 193: 0.0411377/0.171158loss in batch 194: 0.638779/0.173553loss in batch 195: 0.0261993/0.172806loss in batch 196: 0.197433/0.172928loss in batch 197: 0.0184631/0.17215loss in batch 198: 0.00679016/0.17131loss in batch 199: 0.0830841/0.170868loss in batch 200: 0.32074/0.171631loss in batch 201: 0.101349/0.17128loss in batch 202: 0.150635/0.171173loss in batch 203: 0.20755/0.171356loss in batch 204: 0.0745239/0.170883loss in batch 205: 0.0229492/0.170166loss in batch 206: 0.141129/0.170013loss in batch 207: 0.376343/0.171021loss in batch 208: 0.501755/0.172607loss in batch 209: 1.19823/0.177475loss in batch 210: 0.0225372/0.176743loss in batch 211: 0.0245972/0.176025loss in batch 212: 0.901703/0.179428
done with epoch 11
train_acc: 0.943662 (402/426)
test loss: 0.901701
acc: 0.937061 (134/143)
loss in batch 0: 0.110077/0.110077loss in batch 1: 0.0463562/0.0782166loss in batch 2: 0.142242/0.0995636loss in batch 3: 0.10788/0.101639loss in batch 4: 0.0713959/0.0955963loss in batch 5: 0.106689/0.0974274loss in batch 6: 0.319427/0.12915loss in batch 7: 1.45168/0.294464loss in batch 8: 0.135025/0.276749loss in batch 9: 0.0262909/0.251709loss in batch 10: 0.0428314/0.232712loss in batch 11: 0.266479/0.235535loss in batch 12: 1.14697/0.305649loss in batch 13: 0.11528/0.292053loss in batch 14: 0.325989/0.294296loss in batch 15: 0.438934/0.303345loss in batch 16: 0.0482788/0.288345loss in batch 17: 0.0804749/0.276794loss in batch 18: 0.264694/0.276154loss in batch 19: 0.0764923/0.266174loss in batch 20: 0.0514679/0.255951loss in batch 21: 0.0210724/0.24527loss in batch 22: 0.0812988/0.238144loss in batch 23: 0.0982971/0.232315loss in batch 24: 0.0354614/0.224442loss in batch 25: 0.00973511/0.216171loss in batch 26: 0.340469/0.220795loss in batch 27: 0.332657/0.224792loss in batch 28: 0.065979/0.219299loss in batch 29: 0.324677/0.222824loss in batch 30: 0.074173/0.218018loss in batch 31: 0.0329285/0.21225loss in batch 32: 0.0353088/0.206879loss in batch 33: 0.0675812/0.202789loss in batch 34: 0.0101929/0.197281loss in batch 35: 0.403259/0.203003loss in batch 36: 0.555725/0.21254loss in batch 37: 0.0206146/0.207489loss in batch 38: 0.264923/0.208954loss in batch 39: 0.0282745/0.204437loss in batch 40: 0.0262146/0.200104loss in batch 41: 0.0197296/0.195801loss in batch 42: 0.13765/0.194443loss in batch 43: 0.0535278/0.191238loss in batch 44: 0.137268/0.190048loss in batch 45: 0.11705/0.188461loss in batch 46: 0.0329285/0.18515loss in batch 47: 0.0670929/0.182693loss in batch 48: 0.122086/0.181442loss in batch 49: 0.0224762/0.178268loss in batch 50: 0.0179749/0.175125loss in batch 51: 0.192139/0.175461loss in batch 52: 0.240814/0.176697loss in batch 53: 0.0168915/0.173737loss in batch 54: 0.0475922/0.171432loss in batch 55: 0.0239105/0.168808loss in batch 56: 0.551376/0.175522loss in batch 57: 0.0664978/0.17363loss in batch 58: 0.544907/0.179916loss in batch 59: 0.0330811/0.177475loss in batch 60: 0.181534/0.177551loss in batch 61: 0.106461/0.176392loss in batch 62: 0.0158844/0.173859loss in batch 63: 0.113815/0.172913loss in batch 64: 0.0961609/0.171738loss in batch 65: 0.0818787/0.17038loss in batch 66: 0.438461/0.174377loss in batch 67: 0.266525/0.17572loss in batch 68: 0.400375/0.178986loss in batch 69: 0.0450592/0.177078loss in batch 70: 0.519348/0.1819loss in batch 71: 0.122971/0.181076loss in batch 72: 0.0847168/0.179749loss in batch 73: 0.246719/0.180649loss in batch 74: 0.296906/0.182205loss in batch 75: 0.253098/0.183136loss in batch 76: 0.0150909/0.180969loss in batch 77: 0.0428619/0.179184loss in batch 78: 0.0322418/0.177322loss in batch 79: 0.147736/0.176956loss in batch 80: 0.0404816/0.175262loss in batch 81: 0.0718689/0.174011loss in batch 82: 0.11145/0.173248loss in batch 83: 0.225113/0.173874loss in batch 84: 0.0367889/0.172256loss in batch 85: 0.0465393/0.170807loss in batch 86: 0.0180664/0.169037loss in batch 87: 0.299347/0.170532loss in batch 88: 0.260147/0.171524loss in batch 89: 0.0344696/0.170013loss in batch 90: 0.0569458/0.168777loss in batch 91: 0.0843658/0.167847loss in batch 92: 0.42363/0.170609loss in batch 93: 0.508041/0.174194loss in batch 94: 0.0454407/0.172836loss in batch 95: 0.0242462/0.171295loss in batch 96: 0.0451813/0.169983loss in batch 97: 0.108917/0.169357loss in batch 98: 0.120987/0.168869loss in batch 99: 0.0971222/0.168152loss in batch 100: 0.0941925/0.167435loss in batch 101: 0.0657806/0.166428loss in batch 102: 0.0856628/0.165649loss in batch 103: 0.0215912/0.164261loss in batch 104: 0.373474/0.16626loss in batch 105: 0.0228271/0.164902loss in batch 106: 0.106735/0.164368loss in batch 107: 0.0384827/0.163177loss in batch 108: 0.0196228/0.161865loss in batch 109: 0.0313416/0.16069loss in batch 110: 0.0615845/0.15979loss in batch 111: 0.11348/0.159393loss in batch 112: 0.0966644/0.158813loss in batch 113: 0.020752/0.157623loss in batch 114: 0.013916/0.156372loss in batch 115: 0.0433502/0.15538loss in batch 116: 0.270584/0.156387loss in batch 117: 0.0324097/0.155319loss in batch 118: 0.0384674/0.154343loss in batch 119: 1.19124/0.162994loss in batch 120: 0.0932617/0.162399loss in batch 121: 0.628036/0.166229loss in batch 122: 0.101028/0.165695loss in batch 123: 0.147812/0.165558loss in batch 124: 0.152145/0.165436loss in batch 125: 0.273239/0.166306loss in batch 126: 0.44371/0.168488loss in batch 127: 0.0600281/0.167633loss in batch 128: 0.0561066/0.166779loss in batch 129: 0.0128784/0.165588loss in batch 130: 0.0357056/0.164597loss in batch 131: 0.141327/0.164429loss in batch 132: 0.060379/0.163635loss in batch 133: 0.661209/0.167358loss in batch 134: 0.0559235/0.166519loss in batch 135: 0.250198/0.167145loss in batch 136: 0.133026/0.166885loss in batch 137: 0.389801/0.168503loss in batch 138: 0.160477/0.168442loss in batch 139: 0.0166626/0.167374loss in batch 140: 0.461914/0.169449loss in batch 141: 0.224274/0.169846loss in batch 142: 0.041214/0.16893loss in batch 143: 0.0658264/0.168228loss in batch 144: 0.374542/0.169647loss in batch 145: 0.237/0.170105loss in batch 146: 0.288803/0.170914loss in batch 147: 0.135849/0.17067loss in batch 148: 0.0397797/0.1698loss in batch 149: 0.0114288/0.168747loss in batch 150: 0.424652/0.170441loss in batch 151: 0.322922/0.171432loss in batch 152: 0.0558014/0.170685loss in batch 153: 0.347763/0.171844loss in batch 154: 0.0934601/0.171341loss in batch 155: 0.0450134/0.170517loss in batch 156: 0.0409393/0.169693loss in batch 157: 0.105316/0.169281loss in batch 158: 0.16568/0.169266loss in batch 159: 0.23201/0.169647loss in batch 160: 0.237854/0.17009loss in batch 161: 0.699234/0.17334loss in batch 162: 0.0582733/0.172638loss in batch 163: 0.0580597/0.171936loss in batch 164: 0.508224/0.173981loss in batch 165: 0.0593414/0.173294loss in batch 166: 0.0347748/0.172455loss in batch 167: 0.0816193/0.171921loss in batch 168: 0.0852509/0.171417loss in batch 169: 0.297882/0.17215loss in batch 170: 0.0119019/0.171204loss in batch 171: 0.289017/0.171906loss in batch 172: 0.0684509/0.171295loss in batch 173: 0.0770416/0.170761loss in batch 174: 0.154373/0.17067loss in batch 175: 0.448685/0.172241loss in batch 176: 0.0381317/0.171494loss in batch 177: 0.41246/0.172852loss in batch 178: 0.564117/0.175034loss in batch 179: 0.133896/0.174805loss in batch 180: 0.100845/0.174377loss in batch 181: 0.0464783/0.173691loss in batch 182: 0.162811/0.17363loss in batch 183: 0.133347/0.173416loss in batch 184: 0.03125/0.172638loss in batch 185: 0.444489/0.174103loss in batch 186: 0.0661011/0.173523loss in batch 187: 0.829803/0.177017loss in batch 188: 0.179871/0.177032loss in batch 189: 0.0271912/0.176239loss in batch 190: 0.072876/0.175705loss in batch 191: 0.075882/0.175171loss in batch 192: 0.0277557/0.174423loss in batch 193: 0.301178/0.175079loss in batch 194: 0.442337/0.176437loss in batch 195: 0.000473022/0.175552loss in batch 196: 0.14093/0.175369loss in batch 197: 0.0528564/0.174744loss in batch 198: 0.211731/0.174927loss in batch 199: 0.0553589/0.174347loss in batch 200: 0.35434/0.175232loss in batch 201: 0.0647583/0.174683loss in batch 202: 0.0644531/0.174149loss in batch 203: 0.0569458/0.173569loss in batch 204: 0.199783/0.173691loss in batch 205: 0.0853729/0.173264loss in batch 206: 0.111816/0.172958loss in batch 207: 0.0829315/0.172546loss in batch 208: 0.0604401/0.171997loss in batch 209: 0.0883484/0.1716loss in batch 210: 0.0481415/0.171021loss in batch 211: 0.560638/0.172852loss in batch 212: 0.246216/0.173203
done with epoch 12
train_acc: 0.948357 (404/426)
test loss: 0.246215
acc: 0.937061 (134/143)
loss in batch 0: 0.0188446/0.0188446loss in batch 1: 0.0630798/0.0409546loss in batch 2: 0.0128479/0.0315857loss in batch 3: 0.0537415/0.0371246loss in batch 4: 0.0419159/0.0380859loss in batch 5: 0.017868/0.0347137loss in batch 6: 0.00765991/0.0308533loss in batch 7: 0.0554657/0.0339203loss in batch 8: 0.0318298/0.0336914loss in batch 9: 0.147491/0.0450745loss in batch 10: 0.49147/0.0856628loss in batch 11: 0.484207/0.118866loss in batch 12: 0.219971/0.126633loss in batch 13: 0.0213318/0.119125loss in batch 14: 0.0243988/0.112808loss in batch 15: 0.385269/0.129837loss in batch 16: 0.699677/0.163361loss in batch 17: 0.022171/0.155502loss in batch 18: 0.0566711/0.150314loss in batch 19: 0.0434113/0.144958loss in batch 20: 0.0656281/0.14119loss in batch 21: 0.169327/0.142471loss in batch 22: 0.224945/0.146042loss in batch 23: 0.0182343/0.140717loss in batch 24: 0.164246/0.141678loss in batch 25: 0.415619/0.152191loss in batch 26: 0.376144/0.160492loss in batch 27: 0.0352325/0.156021loss in batch 28: 0.0559845/0.152573loss in batch 29: 0.0582886/0.149429loss in batch 30: 0.0868835/0.147415loss in batch 31: 0.0647125/0.144836loss in batch 32: 0.0489349/0.141922loss in batch 33: 0.62616/0.156174loss in batch 34: 0.0582581/0.153366loss in batch 35: 0.0422211/0.150269loss in batch 36: 0.0216675/0.146805loss in batch 37: 0.0724792/0.144836loss in batch 38: 1.55507/0.181015loss in batch 39: 0.0314026/0.177277loss in batch 40: 0.239532/0.178787loss in batch 41: 0.424301/0.184631loss in batch 42: 0.0789795/0.182175loss in batch 43: 0.0103302/0.178284loss in batch 44: 0.111679/0.176788loss in batch 45: 0.229736/0.177948loss in batch 46: 0.0585327/0.1754loss in batch 47: 0.113693/0.174118loss in batch 48: 0.0425568/0.171432loss in batch 49: 0.08992/0.1698loss in batch 50: 0.0617065/0.167679loss in batch 51: 0.029892/0.165039loss in batch 52: 0.0364685/0.162613loss in batch 53: 0.024231/0.160034loss in batch 54: 0.0364227/0.157791loss in batch 55: 0.129715/0.157303loss in batch 56: 0.0505219/0.155426loss in batch 57: 0.0635071/0.153824loss in batch 58: 0.0720367/0.152451loss in batch 59: 0.347794/0.155701loss in batch 60: 0.0594788/0.154129loss in batch 61: 0.0975494/0.153214loss in batch 62: 0.124054/0.15274loss in batch 63: 0.0692749/0.151459loss in batch 64: 0.0320892/0.149612loss in batch 65: 0.0414581/0.147964loss in batch 66: 0.985992/0.160492loss in batch 67: 0.239456/0.161636loss in batch 68: 0.0308228/0.159744loss in batch 69: 0.360947/0.162628loss in batch 70: 0.148361/0.162415loss in batch 71: 0.43335/0.166183loss in batch 72: 0.341919/0.168579loss in batch 73: 0.0247345/0.166656loss in batch 74: 0.325546/0.168762loss in batch 75: 0.296539/0.170441loss in batch 76: 0.104782/0.169601loss in batch 77: 0.112823/0.168869loss in batch 78: 0.350082/0.171158loss in batch 79: 0.502625/0.175308loss in batch 80: 0.109604/0.174484loss in batch 81: 0.197647/0.174774loss in batch 82: 0.219116/0.175308loss in batch 83: 0.214935/0.175781loss in batch 84: 0.416016/0.178604loss in batch 85: 0.67926/0.184433loss in batch 86: 0.540024/0.188522loss in batch 87: 0.0251923/0.186661loss in batch 88: 0.0790863/0.185455loss in batch 89: 0.38472/0.187668loss in batch 90: 0.473511/0.190811loss in batch 91: 0.0935516/0.189743loss in batch 92: 0.384201/0.191833loss in batch 93: 0.100952/0.190872loss in batch 94: 0.247131/0.191452loss in batch 95: 0.217514/0.191727loss in batch 96: 0.0364532/0.19014loss in batch 97: 0.00318909/0.188232loss in batch 98: 0.186615/0.188202loss in batch 99: 0.0383148/0.186722loss in batch 100: 0.0214539/0.185074loss in batch 101: 0.166183/0.184891loss in batch 102: 0.165359/0.184692loss in batch 103: 0.0374146/0.183289loss in batch 104: 0.0167542/0.181702loss in batch 105: 0.214523/0.182007loss in batch 106: 0.013382/0.18042loss in batch 107: 0.0568848/0.179291loss in batch 108: 0.00170898/0.177658loss in batch 109: 0.0597076/0.17659loss in batch 110: 0.0551605/0.175491loss in batch 111: 0.0572052/0.174438loss in batch 112: 0.0242462/0.173111loss in batch 113: 0.139725/0.172806loss in batch 114: 0.478851/0.175476loss in batch 115: 0.0789642/0.174652loss in batch 116: 0.0261078/0.173386loss in batch 117: 0.142532/0.173111loss in batch 118: 0.123886/0.172699loss in batch 119: 0.117706/0.172241loss in batch 120: 0.0961761/0.171616loss in batch 121: 0.00805664/0.170273loss in batch 122: 9.15527e-05/0.168884loss in batch 123: 0.098175/0.16832loss in batch 124: 0.0458527/0.167343loss in batch 125: 0.0965271/0.166779loss in batch 126: 0.100616/0.16626loss in batch 127: 0.398056/0.16806loss in batch 128: 0.0151367/0.166885loss in batch 129: 0.00119019/0.165619loss in batch 130: 0.00427246/0.164383loss in batch 131: 0.258804/0.1651loss in batch 132: 0.385056/0.166748loss in batch 133: 0.0588226/0.165939loss in batch 134: 0.379761/0.167526loss in batch 135: 0.297653/0.168472loss in batch 136: 0.326523/0.169632loss in batch 137: 0.0211639/0.168564loss in batch 138: 0.0591736/0.16777loss in batch 139: 0.770508/0.172073loss in batch 140: 0.00448608/0.170883loss in batch 141: 0.0241547/0.169861loss in batch 142: 0.0318604/0.168884loss in batch 143: 0.00520325/0.167755loss in batch 144: 0.386765/0.169266loss in batch 145: 0.0619507/0.168533loss in batch 146: 0.0934753/0.168015loss in batch 147: 0.427994/0.169769loss in batch 148: 0.224121/0.170135loss in batch 149: 0.308105/0.171051loss in batch 150: 0.148926/0.170914loss in batch 151: 0.0202942/0.169922loss in batch 152: 0.0154419/0.168915loss in batch 153: 0.0628204/0.168213loss in batch 154: 0.193954/0.168381loss in batch 155: 0.274429/0.169067loss in batch 156: 0.0138092/0.168076loss in batch 157: 0.0132141/0.167099loss in batch 158: 0.0331879/0.16626loss in batch 159: 0.0916443/0.165787loss in batch 160: 0.17009/0.165802loss in batch 161: 0.207413/0.166077loss in batch 162: 0.0534363/0.16539loss in batch 163: 0.373672/0.166656loss in batch 164: 0.0359344/0.165863loss in batch 165: 0.0722656/0.165298loss in batch 166: 0.0615845/0.164673loss in batch 167: 0.0246735/0.163849loss in batch 168: 0.0632324/0.163254loss in batch 169: 0.0313568/0.162476loss in batch 170: 1.53287/0.170486loss in batch 171: 0.111282/0.170151loss in batch 172: 0.264206/0.170685loss in batch 173: 0.276245/0.171295loss in batch 174: 0.226486/0.171616loss in batch 175: 0.179108/0.171646loss in batch 176: 0.457443/0.173264loss in batch 177: 0.0513/0.172592loss in batch 178: 0.083786/0.172089loss in batch 179: 0.0256958/0.17128loss in batch 180: 0.0311279/0.170486loss in batch 181: 0.0389252/0.169769loss in batch 182: 0.124207/0.169525loss in batch 183: 0.479172/0.171204loss in batch 184: 0.0753937/0.170685loss in batch 185: 0.0783539/0.170197loss in batch 186: 0.268692/0.170731loss in batch 187: 0.0864563/0.170273loss in batch 188: 0.115143/0.169983loss in batch 189: 0.694748/0.172729loss in batch 190: 0.183746/0.172806loss in batch 191: 0.0611115/0.172211loss in batch 192: 0.234512/0.172546loss in batch 193: 0.0419617/0.17186loss in batch 194: 0.112213/0.17157loss in batch 195: 0.214752/0.171768loss in batch 196: 0.0320129/0.171082loss in batch 197: 0.130371/0.170853loss in batch 198: 0.0622253/0.170319loss in batch 199: 0.449097/0.171707loss in batch 200: 0.0549774/0.171143loss in batch 201: 0.00701904/0.170319loss in batch 202: 0.12146/0.170074loss in batch 203: 0.197998/0.170212loss in batch 204: 0.0700531/0.169724loss in batch 205: 0.107315/0.169434loss in batch 206: 0.25824/0.169846loss in batch 207: 0.182297/0.169922loss in batch 208: 0.0405731/0.169296loss in batch 209: 0.683228/0.171753loss in batch 210: 0.0143127/0.171005loss in batch 211: 0.0717163/0.170532loss in batch 212: 0.0867767/0.170135
done with epoch 13
train_acc: 0.943662 (402/426)
test loss: 0.0867766
acc: 0.937061 (134/143)
loss in batch 0: 0.114883/0.114883loss in batch 1: 0.0760498/0.095459loss in batch 2: 0.0475159/0.079483loss in batch 3: 0.03125/0.0674133loss in batch 4: 0.553055/0.164551loss in batch 5: 0.0903778/0.152191loss in batch 6: 0.59903/0.216019loss in batch 7: 0.266861/0.222382loss in batch 8: 0.0414276/0.202271loss in batch 9: 0.049469/0.186996loss in batch 10: 0.0809326/0.177353loss in batch 11: 0.0513763/0.166855loss in batch 12: 0.0489044/0.157776loss in batch 13: 0.114822/0.154709loss in batch 14: 0.544571/0.180695loss in batch 15: 0.1129/0.176468loss in batch 16: 0.0154572/0.166992loss in batch 17: 0.0765839/0.161972loss in batch 18: 0.0630798/0.156769loss in batch 19: 0.322021/0.165039loss in batch 20: 0.200485/0.166718loss in batch 21: 0.300781/0.172806loss in batch 22: 0.0358887/0.16687loss in batch 23: 0.0868835/0.163528loss in batch 24: 0.0628204/0.1595loss in batch 25: 0.052063/0.155365loss in batch 26: 0.0562134/0.151688loss in batch 27: 0.0355377/0.147537loss in batch 28: 0.0359344/0.143707loss in batch 29: 0.0863953/0.141785loss in batch 30: 0.224579/0.144455loss in batch 31: 0.0657196/0.141998loss in batch 32: 0.0211334/0.138336loss in batch 33: 0.234543/0.141174loss in batch 34: 0.597153/0.15419loss in batch 35: 0.8526/0.173599loss in batch 36: 0.309509/0.177277loss in batch 37: 0.282593/0.180038loss in batch 38: 0.0568237/0.17688loss in batch 39: 0.176178/0.176865loss in batch 40: 0.0102539/0.172791loss in batch 41: 0.00788879/0.168869loss in batch 42: 0.0177002/0.165359loss in batch 43: 0.0368652/0.16243loss in batch 44: 0.273575/0.164917loss in batch 45: 0.328018/0.168442loss in batch 46: 0.102081/0.167038loss in batch 47: 0.261948/0.169022loss in batch 48: 0.267181/0.171021loss in batch 49: 0.577423/0.179138loss in batch 50: 0.0502472/0.17662loss in batch 51: 0.00839233/0.173386loss in batch 52: 0.0883179/0.171783loss in batch 53: 0.61998/0.180084loss in batch 54: 0.0226593/0.177216loss in batch 55: 0.0762024/0.175415loss in batch 56: 0.0552826/0.173294loss in batch 57: 0.174149/0.173309loss in batch 58: 0.0186005/0.1707loss in batch 59: 0.000167847/0.167862loss in batch 60: 0.135941/0.167343loss in batch 61: 0.0331116/0.165161loss in batch 62: 0.184357/0.165482loss in batch 63: 0.00959778/0.163025loss in batch 64: 0.0662231/0.16156loss in batch 65: 0.0199432/0.159393loss in batch 66: 0.181839/0.159744loss in batch 67: 0.140884/0.159454loss in batch 68: 0.0141602/0.157364loss in batch 69: 0.0131836/0.155289loss in batch 70: 0.524063/0.160477loss in batch 71: 0.103455/0.159698loss in batch 72: 0.0149689/0.157715loss in batch 73: 0.262894/0.159134loss in batch 74: 0.0369873/0.157516loss in batch 75: 0.0325623/0.155869loss in batch 76: 0.0280304/0.15419loss in batch 77: 0.0711975/0.153137loss in batch 78: 0.0158386/0.151398loss in batch 79: 0.0148621/0.149689loss in batch 80: 0.0780945/0.148804loss in batch 81: 0.0404358/0.147491loss in batch 82: 0.182693/0.147903loss in batch 83: 0.27002/0.149353loss in batch 84: 0.085968/0.148621loss in batch 85: 0.120804/0.1483loss in batch 86: 0.0606995/0.147293loss in batch 87: 0.164917/0.147476loss in batch 88: 0.169846/0.147751loss in batch 89: 0.302841/0.14946loss in batch 90: 0.285995/0.150955loss in batch 91: 0.0203094/0.149551loss in batch 92: 0.0285339/0.148239loss in batch 93: 0.412186/0.151062loss in batch 94: 0.0527191/0.150009loss in batch 95: 0.365448/0.152267loss in batch 96: 0.0333862/0.151031loss in batch 97: 0.0670013/0.150177loss in batch 98: 0.217987/0.150864loss in batch 99: 0.38089/0.153168loss in batch 100: 0.0518494/0.152161loss in batch 101: 0.0237274/0.150909loss in batch 102: 0.0778198/0.150192loss in batch 103: 0.0852203/0.149567loss in batch 104: 0.370056/0.151657loss in batch 105: 0.0315704/0.150528loss in batch 106: 0.363068/0.152527loss in batch 107: 0.0514984/0.151581loss in batch 108: 0.0254974/0.150421loss in batch 109: 0.0254669/0.149292loss in batch 110: 0.0193939/0.148117loss in batch 111: 0.0542908/0.147278loss in batch 112: 0.0983124/0.146851loss in batch 113: 0.0312195/0.145844loss in batch 114: 0.00987244/0.144653loss in batch 115: 0.101776/0.144287loss in batch 116: 0.0215454/0.143234loss in batch 117: 0.0283051/0.142273loss in batch 118: 0.459534/0.144913loss in batch 119: 0.0987091/0.144531loss in batch 120: 0.269379/0.145569loss in batch 121: 0.0901947/0.145111loss in batch 122: 0.477417/0.147827loss in batch 123: 0.0452118/0.146988loss in batch 124: 0.0657349/0.146332loss in batch 125: 1.11513/0.154022loss in batch 126: 0.182953/0.154266loss in batch 127: 0.0290985/0.15329loss in batch 128: 0.248428/0.154022loss in batch 129: 0.406845/0.15596loss in batch 130: 0.18869/0.156219loss in batch 131: 0.0679626/0.155548loss in batch 132: 0.0306854/0.154602loss in batch 133: 0.409805/0.156525loss in batch 134: 0.34465/0.157913loss in batch 135: 0.0348663/0.156998loss in batch 136: 0.13945/0.156876loss in batch 137: 0.190491/0.157104loss in batch 138: 0.397675/0.158844loss in batch 139: 0.0521393/0.158096loss in batch 140: 0.154572/0.158066loss in batch 141: 0.331512/0.159286loss in batch 142: 0.0251923/0.158356loss in batch 143: 0.091156/0.157883loss in batch 144: 0.358643/0.159256loss in batch 145: 0.730148/0.163177loss in batch 146: 0.0899353/0.162674loss in batch 147: 0.0993347/0.162231loss in batch 148: 0.15152/0.16217loss in batch 149: 0.0250397/0.161255loss in batch 150: 0.479309/0.163361loss in batch 151: 0.0541992/0.162643loss in batch 152: 0.25145/0.163223loss in batch 153: 0.350296/0.164444loss in batch 154: 0.00233459/0.163406loss in batch 155: 0.056015/0.16272loss in batch 156: 0.0368958/0.161911loss in batch 157: 0.262863/0.162537loss in batch 158: 0.0237885/0.161682loss in batch 159: 0.236298/0.16214loss in batch 160: 0.164856/0.162155loss in batch 161: 0.0245209/0.161301loss in batch 162: 0.241821/0.161804loss in batch 163: 0.0260773/0.160965loss in batch 164: 0.0301361/0.160187loss in batch 165: 0.0483856/0.1595loss in batch 166: 0.197754/0.159729loss in batch 167: 0.0429535/0.159042loss in batch 168: 0.0187988/0.158203loss in batch 169: 0.415176/0.159729loss in batch 170: 0.0998535/0.159378loss in batch 171: 0.0441132/0.158707loss in batch 172: 0.200653/0.158951loss in batch 173: 0.676102/0.161911loss in batch 174: 0.00822449/0.161041loss in batch 175: 0.00657654/0.160156loss in batch 176: 0.0497589/0.159531loss in batch 177: 1.58081/0.167511loss in batch 178: 0.0245514/0.166733loss in batch 179: 0.122833/0.166489loss in batch 180: 0.0648193/0.165909loss in batch 181: 0.443375/0.16745loss in batch 182: 0.0469971/0.166794loss in batch 183: 0.012085/0.165939loss in batch 184: 0.0584869/0.165375loss in batch 185: 0.0841217/0.164932loss in batch 186: 0.0447083/0.164276loss in batch 187: 0.0299835/0.163559loss in batch 188: 0.00357056/0.16272loss in batch 189: 0.0562286/0.162155loss in batch 190: 0.047699/0.16156loss in batch 191: 0.17746/0.161636loss in batch 192: 0.315216/0.162445loss in batch 193: 0.0185547/0.161697loss in batch 194: 0.0964813/0.161377loss in batch 195: 0.0595093/0.160858loss in batch 196: 0.115753/0.160614loss in batch 197: 0.0434723/0.160019loss in batch 198: 0.260666/0.160522loss in batch 199: 0.103012/0.160248loss in batch 200: 1.08958/0.164871loss in batch 201: 0.396622/0.166016loss in batch 202: 0.125549/0.165817loss in batch 203: 0.0429535/0.165222loss in batch 204: 0.660187/0.167633loss in batch 205: 0.0252228/0.166946loss in batch 206: 0.08638/0.16655loss in batch 207: 0.457642/0.167953loss in batch 208: 0.0269775/0.167267loss in batch 209: 0.00790405/0.166519loss in batch 210: 0.0910797/0.166153loss in batch 211: 0.0796051/0.165756loss in batch 212: 0.194244/0.165878
done with epoch 14
train_acc: 0.941315 (401/426)
test loss: 0.194244
acc: 0.937061 (134/143)
loss in batch 0: 0.312515/0.312515loss in batch 1: 0.0217133/0.167114loss in batch 2: 0.00172424/0.111984loss in batch 3: 0.0607147/0.0991669loss in batch 4: 0.0719604/0.0937195loss in batch 5: 0.067215/0.0892944loss in batch 6: 0.0614624/0.0853271loss in batch 7: 0.0392914/0.0795746loss in batch 8: 0.0329437/0.0744019loss in batch 9: 0.0153503/0.0684814loss in batch 10: 0.0510712/0.0669098loss in batch 11: 0.0400696/0.0646667loss in batch 12: 0.0574799/0.0641174loss in batch 13: 0.224869/0.075592loss in batch 14: 0.178482/0.0824585loss in batch 15: 0.0614319/0.0811462loss in batch 16: 0.0947113/0.0819397loss in batch 17: 0.0156555/0.0782623loss in batch 18: 0.0211182/0.0752411loss in batch 19: 0.0784912/0.0754089loss in batch 20: 0.16803/0.0798187loss in batch 21: 0.0978394/0.0806427loss in batch 22: 0.150452/0.0836792loss in batch 23: 0.0803528/0.0835266loss in batch 24: 0.519073/0.100952loss in batch 25: 0.187149/0.104279loss in batch 26: 0.131393/0.105286loss in batch 27: 0.136337/0.1064loss in batch 28: 0.0672455/0.105042loss in batch 29: 0.874115/0.130676loss in batch 30: 0.059021/0.128357loss in batch 31: 0.323196/0.134445loss in batch 32: 0.293442/0.139267loss in batch 33: 0.0848846/0.137665loss in batch 34: 0.259506/0.141144loss in batch 35: 0.0075531/0.137436loss in batch 36: 0.0605927/0.135361loss in batch 37: 0.183914/0.136642loss in batch 38: 0.149933/0.136978loss in batch 39: 0.295944/0.140945loss in batch 40: 0.369141/0.14653loss in batch 41: 0.317261/0.150574loss in batch 42: 0.994675/0.170227loss in batch 43: 0.355057/0.174408loss in batch 44: 0.0277252/0.171158loss in batch 45: 0.144928/0.170578loss in batch 46: 0.0462494/0.167938loss in batch 47: 0.0245972/0.164963loss in batch 48: 0.0223999/0.162048loss in batch 49: 0.169922/0.162201loss in batch 50: 0.0172119/0.159363loss in batch 51: 0.0664368/0.157578loss in batch 52: 0.00817871/0.154755loss in batch 53: 0.563843/0.162338loss in batch 54: 0.378601/0.16626loss in batch 55: 0.0868988/0.164856loss in batch 56: 0.00038147/0.161957loss in batch 57: 0.0641174/0.160278loss in batch 58: 0.0492096/0.158386loss in batch 59: 0.260941/0.16011loss in batch 60: 0.14772/0.159912loss in batch 61: 0.0610352/0.158295loss in batch 62: 0.0471344/0.15654loss in batch 63: 0.231796/0.157715loss in batch 64: 0.386887/0.16124loss in batch 65: 0.0108795/0.158966loss in batch 66: 0.0247345/0.156967loss in batch 67: 0.136169/0.156647loss in batch 68: 0.127777/0.156235loss in batch 69: 0.0593567/0.154846loss in batch 70: 0.472595/0.159317loss in batch 71: 0.429382/0.163086loss in batch 72: 0.0932465/0.162125loss in batch 73: 0.0316925/0.160355loss in batch 74: 0.0174866/0.158463loss in batch 75: 0.0759583/0.157364loss in batch 76: 0.0660553/0.156174loss in batch 77: 0.017746/0.154404loss in batch 78: 0.122131/0.153992loss in batch 79: 0.0344391/0.152496loss in batch 80: 0.430664/0.15593loss in batch 81: 0.15744/0.15596loss in batch 82: 0.152145/0.155914loss in batch 83: 0.0157623/0.154236loss in batch 84: 0.19072/0.154663loss in batch 85: 0.17186/0.154877loss in batch 86: 0.409256/0.157791loss in batch 87: 0.148666/0.157684loss in batch 88: 0.0426788/0.156403loss in batch 89: 0.0450134/0.155151loss in batch 90: 0.00636292/0.153519loss in batch 91: 0.337631/0.155533loss in batch 92: 0.0531616/0.154434loss in batch 93: 0.0250397/0.153046loss in batch 94: 0.348221/0.155106loss in batch 95: 1.12595/0.165222loss in batch 96: 0.0331573/0.163849loss in batch 97: 0.045578/0.162659loss in batch 98: 0.0707245/0.161728loss in batch 99: 0.0502777/0.160614loss in batch 100: 0.11235/0.160126loss in batch 101: 0.0265198/0.158813loss in batch 102: 0.0503235/0.157761loss in batch 103: 0.197983/0.158157loss in batch 104: 0.026062/0.156906loss in batch 105: 0.00627136/0.155472loss in batch 106: 0.010376/0.154114loss in batch 107: 0.0150146/0.152817loss in batch 108: 0.109848/0.152435loss in batch 109: 0.237137/0.153198loss in batch 110: 0.0452118/0.152237loss in batch 111: 0.253479/0.153122loss in batch 112: 0.209534/0.153641loss in batch 113: 0.30336/0.154938loss in batch 114: 0.0606384/0.154129loss in batch 115: 0.0667725/0.153366loss in batch 116: 0.0787506/0.152725loss in batch 117: 0.5242/0.155884loss in batch 118: 0.0233917/0.15477loss in batch 119: 0.00892639/0.153549loss in batch 120: 0.00215149/0.152298loss in batch 121: 0.0460052/0.151428loss in batch 122: 0.0792999/0.150848loss in batch 123: 0.1521/0.150864loss in batch 124: 0.116913/0.150574loss in batch 125: 0.215347/0.151093loss in batch 126: 0.0367584/0.150208loss in batch 127: 0.0397949/0.149338loss in batch 128: 0.0274658/0.148392loss in batch 129: 0.0583038/0.14769loss in batch 130: 0.0185852/0.146713loss in batch 131: 0.0339813/0.145859loss in batch 132: 0.314911/0.147141loss in batch 133: 0.0325623/0.146271loss in batch 134: 0.0879974/0.145844loss in batch 135: 0.0299377/0.144989loss in batch 136: 0.362091/0.146576loss in batch 137: 0.0952759/0.14621loss in batch 138: 0.248138/0.146942loss in batch 139: 0.0475616/0.146225loss in batch 140: 0.00857544/0.145248loss in batch 141: 0.0596771/0.144653loss in batch 142: 0.0112457/0.143723loss in batch 143: 0.0935059/0.143356loss in batch 144: 0.10051/0.143066loss in batch 145: 0.0421753/0.14238loss in batch 146: 0.672394/0.145996loss in batch 147: 0.5522/0.148727loss in batch 148: 0.0249023/0.147888loss in batch 149: 0.0688171/0.147369loss in batch 150: 0.0207367/0.14653loss in batch 151: 0.439026/0.148468loss in batch 152: 0.394882/0.15007loss in batch 153: 0.0469055/0.149399loss in batch 154: 0.0561676/0.148804loss in batch 155: 0.44281/0.150681loss in batch 156: 0.167236/0.150787loss in batch 157: 0.1091/0.150528loss in batch 158: 0.209808/0.150894loss in batch 159: 0.0382233/0.150192loss in batch 160: 0.0140228/0.149338loss in batch 161: 0.195419/0.149628loss in batch 162: 0.0663452/0.149109loss in batch 163: 0.0549927/0.148544loss in batch 164: 0.0505371/0.147964loss in batch 165: 0.0521545/0.147369loss in batch 166: 0.0227509/0.146637loss in batch 167: 0.048996/0.146042loss in batch 168: 0.0952148/0.145737loss in batch 169: 0.0422211/0.145126loss in batch 170: 0.0847321/0.144775loss in batch 171: 0.238068/0.14534loss in batch 172: 0.012146/0.144547loss in batch 173: 0.265091/0.145248loss in batch 174: 0.0740356/0.144836loss in batch 175: 0.289551/0.14566loss in batch 176: 0.311768/0.146606loss in batch 177: 0.248413/0.147186loss in batch 178: 0.0750122/0.146774loss in batch 179: 0.122787/0.146637loss in batch 180: 0.0451965/0.146072loss in batch 181: 0.511978/0.148087loss in batch 182: 0.403244/0.149475loss in batch 183: 0.0279541/0.148819loss in batch 184: 0.0191498/0.148132loss in batch 185: 0.0235748/0.147446loss in batch 186: 0.0502167/0.146942loss in batch 187: 0.204422/0.147232loss in batch 188: 0.112122/0.147049loss in batch 189: 0.0448303/0.146515loss in batch 190: 0.026413/0.145874loss in batch 191: 0.0743561/0.145508loss in batch 192: 0.0132751/0.144836loss in batch 193: 0.335037/0.145813loss in batch 194: 0.202209/0.146088loss in batch 195: 0.223999/0.1465loss in batch 196: 0.226089/0.146896loss in batch 197: 0.182419/0.147079loss in batch 198: 0.610367/0.149414loss in batch 199: 0.873306/0.15303loss in batch 200: 0.33252/0.153931loss in batch 201: 0.170563/0.154007loss in batch 202: 0.0755768/0.15361loss in batch 203: 0.301849/0.154343loss in batch 204: 0.527466/0.156158loss in batch 205: 1.50468/0.16272loss in batch 206: 0.141266/0.162598loss in batch 207: 0.0753326/0.162186loss in batch 208: 0.0427856/0.161606loss in batch 209: 0.0493469/0.161087loss in batch 210: 0.0782928/0.16069loss in batch 211: 0.263824/0.161179loss in batch 212: 0.438232/0.162476
done with epoch 15
train_acc: 0.946009 (403/426)
test loss: 0.438232
acc: 0.937061 (134/143)
loss in batch 0: 0.292343/0.292343loss in batch 1: 0.00335693/0.147842loss in batch 2: 0.0685577/0.121414loss in batch 3: 0.085556/0.112442loss in batch 4: 0.372131/0.164383loss in batch 5: 0.103333/0.154221loss in batch 6: 0.033371/0.136948loss in batch 7: 0.559998/0.189835loss in batch 8: 0.197906/0.19072loss in batch 9: 0.029892/0.174637loss in batch 10: 0.40564/0.195648loss in batch 11: 0.0718384/0.185333loss in batch 12: 0.0352936/0.173798loss in batch 13: 0.0217285/0.162918loss in batch 14: 0.59375/0.19165loss in batch 15: 0.119308/0.187119loss in batch 16: 0.108673/0.18251loss in batch 17: 0.355484/0.192123loss in batch 18: 0.0978546/0.187164loss in batch 19: 0.083847/0.181992loss in batch 20: 0.0332184/0.174911loss in batch 21: 0.0429077/0.1689loss in batch 22: 0.146515/0.167938loss in batch 23: 0.121445/0.166loss in batch 24: 0.0418854/0.161041loss in batch 25: 0.117325/0.159363loss in batch 26: 0.0259247/0.154404loss in batch 27: 0.178055/0.155243loss in batch 28: 0.130234/0.154388loss in batch 29: 0.00708008/0.149475loss in batch 30: 0.222931/0.15184loss in batch 31: 0.0510101/0.148697loss in batch 32: 0.00454712/0.144318loss in batch 33: 0.0148621/0.140533loss in batch 34: 0.238632/0.143326loss in batch 35: 0.00605774/0.139511loss in batch 36: 0.0208435/0.136307loss in batch 37: 0.0257568/0.133392loss in batch 38: 0.0435791/0.131088loss in batch 39: 0.0167389/0.128235loss in batch 40: 0.178818/0.129471loss in batch 41: 0.126999/0.12941loss in batch 42: 0.0195007/0.126862loss in batch 43: 0.201157/0.128555loss in batch 44: 0.200394/0.130142loss in batch 45: 0.0262909/0.127884loss in batch 46: 0.0486908/0.12619loss in batch 47: 0.0501556/0.124619loss in batch 48: 0.0396271/0.122879loss in batch 49: 0.0463562/0.121338loss in batch 50: 0.0591125/0.120132loss in batch 51: 0.0266266/0.118332loss in batch 52: 0.296295/0.121674loss in batch 53: 0.19696/0.123077loss in batch 54: 0.117188/0.122971loss in batch 55: 0.0387115/0.121475loss in batch 56: 0.0277863/0.119827loss in batch 57: 1.56435/0.14473loss in batch 58: 0.397339/0.149017loss in batch 59: 0.0657654/0.147614loss in batch 60: 0.174728/0.148071loss in batch 61: 0.00827026/0.145813loss in batch 62: 0.0267487/0.143921loss in batch 63: 0.0197144/0.141983loss in batch 64: 0.295395/0.144333loss in batch 65: 0.0711517/0.143234loss in batch 66: 0.100006/0.142578loss in batch 67: 0.0178833/0.140747loss in batch 68: 0.0416718/0.139328loss in batch 69: 0.0361786/0.137848loss in batch 70: 0.006073/0.135986loss in batch 71: 0.0789337/0.135193loss in batch 72: 0.481384/0.139938loss in batch 73: 0.0404663/0.138596loss in batch 74: 0.0690002/0.137665loss in batch 75: 0.0420074/0.136398loss in batch 76: 0.0469971/0.135239loss in batch 77: 0.0554504/0.134216loss in batch 78: 0.0512085/0.133179loss in batch 79: 0.12912/0.133118loss in batch 80: 0.0303955/0.131851loss in batch 81: 0.0240784/0.130539loss in batch 82: 0.0849304/0.12999loss in batch 83: 0.200775/0.130829loss in batch 84: 0.0104065/0.12941loss in batch 85: 0.17984/0.130005loss in batch 86: 0.137451/0.130081loss in batch 87: 0.0621338/0.129318loss in batch 88: 0.022171/0.128113loss in batch 89: 0.123108/0.128052loss in batch 90: 0.218643/0.129059loss in batch 91: 0.15741/0.129364loss in batch 92: 0.233582/0.130478loss in batch 93: 0.312256/0.132416loss in batch 94: 0.0756683/0.131821loss in batch 95: 0.0394592/0.130844loss in batch 96: 0.104355/0.130585loss in batch 97: 0.0252686/0.129501loss in batch 98: 0.00767517/0.128265loss in batch 99: 0.167404/0.128677loss in batch 100: 0.129349/0.128677loss in batch 101: 0.0565186/0.127975loss in batch 102: 0.0348969/0.127075loss in batch 103: 0.0744171/0.126556loss in batch 104: 0.0207367/0.125549loss in batch 105: 0.0570374/0.124893loss in batch 106: 0.0242157/0.123962loss in batch 107: 0.0072937/0.122879loss in batch 108: 0.0707245/0.122406loss in batch 109: 0.000198364/0.121292loss in batch 110: 0.0474091/0.120636loss in batch 111: 0.0900726/0.120361loss in batch 112: 0.08255/0.120026loss in batch 113: 0.0146637/0.119095loss in batch 114: 0.181702/0.119644loss in batch 115: 0.838394/0.125839loss in batch 116: 0.380188/0.128006loss in batch 117: 0.0367432/0.127243loss in batch 118: 0.0705719/0.12677loss in batch 119: 0.0232086/0.1259loss in batch 120: 0.453888/0.128616loss in batch 121: 0.353485/0.130447loss in batch 122: 0.0489655/0.129791loss in batch 123: 0.44519/0.132324loss in batch 124: 0.250519/0.133286loss in batch 125: 0.840271/0.138885loss in batch 126: 0.288956/0.140076loss in batch 127: 0.0603027/0.13945loss in batch 128: 0.0708771/0.138916loss in batch 129: 0.438843/0.14122loss in batch 130: 0.256668/0.142105loss in batch 131: 0.0120239/0.141113loss in batch 132: 0.0465698/0.140411loss in batch 133: 0.141037/0.140411loss in batch 134: 0.184311/0.140747loss in batch 135: 0.690262/0.144775loss in batch 136: 0.227692/0.145386loss in batch 137: 0.0340118/0.144577loss in batch 138: 0.0380707/0.143814loss in batch 139: 0.141846/0.143799loss in batch 140: 0.0337524/0.143021loss in batch 141: 0.590912/0.146179loss in batch 142: 0.209412/0.146606loss in batch 143: 0.0203552/0.145737loss in batch 144: 0.280365/0.146667loss in batch 145: 0.303482/0.147736loss in batch 146: 0.069397/0.147202loss in batch 147: 0.0511932/0.146561loss in batch 148: 0.0254974/0.145737loss in batch 149: 0.0637207/0.145203loss in batch 150: 0.014267/0.144333loss in batch 151: 1.01828/0.150085loss in batch 152: 0.133102/0.149979loss in batch 153: 0.582916/0.152786loss in batch 154: 0.00622559/0.15184loss in batch 155: 0.403946/0.153442loss in batch 156: 0.0577393/0.152832loss in batch 157: 0.383499/0.154297loss in batch 158: 0.133698/0.154175loss in batch 159: 0.29631/0.15506loss in batch 160: 0.422195/0.156708loss in batch 161: 0.105087/0.156403loss in batch 162: 0.0220337/0.155579loss in batch 163: 0.679001/0.158768loss in batch 164: 0.0259552/0.157959loss in batch 165: 0.0290833/0.157196loss in batch 166: 0.371002/0.158463loss in batch 167: 0.489746/0.160446loss in batch 168: 0.104813/0.16011loss in batch 169: 0.0171509/0.159271loss in batch 170: 0.196533/0.159485loss in batch 171: 0.00439453/0.158585loss in batch 172: 0.056076/0.15799loss in batch 173: 0.21344/0.15831loss in batch 174: 0.427841/0.159851loss in batch 175: 0.257813/0.1604loss in batch 176: 0.0159912/0.159592loss in batch 177: 0.0661011/0.159073loss in batch 178: 0.119049/0.158829loss in batch 179: 0.192566/0.159042loss in batch 180: 0.0628204/0.158493loss in batch 181: 0.144028/0.158432loss in batch 182: 0.0256653/0.157684loss in batch 183: 0.552963/0.159836loss in batch 184: 0.397903/0.161118loss in batch 185: 0.0564117/0.160568loss in batch 186: 0.0836639/0.160156loss in batch 187: 0.0492859/0.159561loss in batch 188: 0.0574036/0.159027loss in batch 189: 0.010498/0.158249loss in batch 190: 0.0406799/0.157639loss in batch 191: 0.0348663/0.156982loss in batch 192: 0.0285797/0.156326loss in batch 193: 0.0276031/0.15567loss in batch 194: 0.0514221/0.155121loss in batch 195: 1.16016/0.160263loss in batch 196: 0.164642/0.160278loss in batch 197: 0.241531/0.160675loss in batch 198: 0.00262451/0.159882loss in batch 199: 0.0455322/0.159317loss in batch 200: 0.0680847/0.158875loss in batch 201: 0.115692/0.158646loss in batch 202: 0.211014/0.158905loss in batch 203: 0.349991/0.159836loss in batch 204: 0.0567169/0.159332loss in batch 205: 0.0865936/0.158981loss in batch 206: 0.441467/0.160355loss in batch 207: 1.06807/0.164719loss in batch 208: 0.0235443/0.164032loss in batch 209: 0.0184174/0.163361loss in batch 210: 0.0696259/0.162903loss in batch 211: 0.0245209/0.162262loss in batch 212: 0.032547/0.161636
done with epoch 16
train_acc: 0.946009 (403/426)
test loss: 0.0325469
acc: 0.951047 (136/143)
loss in batch 0: 0.0921478/0.0921478loss in batch 1: 0.185867/0.139008loss in batch 2: 0.0775757/0.11853loss in batch 3: 0.242813/0.149612loss in batch 4: 0.432999/0.206284loss in batch 5: 0.0271149/0.176422loss in batch 6: 0.0256195/0.154877loss in batch 7: 0.00274658/0.135864loss in batch 8: 1.08043/0.240814loss in batch 9: 0.0302124/0.219742loss in batch 10: 0.280609/0.225281loss in batch 11: 0.252075/0.227524loss in batch 12: 0.523071/0.250259loss in batch 13: 0.0798492/0.238068loss in batch 14: 0.272507/0.240372loss in batch 15: 0.124008/0.233109loss in batch 16: 0.0147552/0.220261loss in batch 17: 0.012558/0.20871loss in batch 18: 0.00746155/0.19812loss in batch 19: 0.0418091/0.190308loss in batch 20: 0.0156555/0.181992loss in batch 21: 0.159912/0.181loss in batch 22: 0.409088/0.190918loss in batch 23: 0.0632019/0.185593loss in batch 24: 0.076004/0.181198loss in batch 25: 0.103867/0.178238loss in batch 26: 0.0500336/0.173477loss in batch 27: 0.19458/0.17424loss in batch 28: 0.0612183/0.170334loss in batch 29: 0.104141/0.168137loss in batch 30: 0.237762/0.170364loss in batch 31: 0.00691223/0.165268loss in batch 32: 0.0275116/0.161087loss in batch 33: 0.185776/0.161819loss in batch 34: 0.0751343/0.159348loss in batch 35: 0.0294037/0.155731loss in batch 36: 0.133957/0.155136loss in batch 37: 0.0783234/0.153122loss in batch 38: 0.754272/0.168549loss in batch 39: 0.00866699/0.164551loss in batch 40: 0.241211/0.166412loss in batch 41: 0.160202/0.166275loss in batch 42: 0.0210571/0.162888loss in batch 43: 0.447968/0.169357loss in batch 44: 0.0637665/0.167023loss in batch 45: 0.00569153/0.163513loss in batch 46: 0.0211182/0.160477loss in batch 47: 0.0291748/0.157761loss in batch 48: 0.117706/0.156937loss in batch 49: 0.00894165/0.153976loss in batch 50: 0.31282/0.157089loss in batch 51: 0.00198364/0.154099loss in batch 52: 0.00970459/0.151367loss in batch 53: 0.042511/0.149353loss in batch 54: 0.0291138/0.147186loss in batch 55: 0.139954/0.147034loss in batch 56: 0.0183868/0.144791loss in batch 57: 0.053421/0.143219loss in batch 58: 0.576202/0.150558loss in batch 59: 0.0706329/0.149216loss in batch 60: 0.0740356/0.14798loss in batch 61: 0.00621033/0.145691loss in batch 62: 0.052948/0.144226loss in batch 63: 0.0162964/0.142227loss in batch 64: 0.17099/0.14267loss in batch 65: 0.446075/0.147263loss in batch 66: 0.570404/0.153595loss in batch 67: 0.407578/0.157318loss in batch 68: 0.209839/0.158081loss in batch 69: 0.0366669/0.156357loss in batch 70: 0.233093/0.157425loss in batch 71: 0.438965/0.161346loss in batch 72: 0.098175/0.160477loss in batch 73: 0.201462/0.161026loss in batch 74: 0.0290527/0.159271loss in batch 75: 0.0285187/0.157547loss in batch 76: 0.140091/0.157318loss in batch 77: 0.244888/0.158447loss in batch 78: 0.00823975/0.15654loss in batch 79: 0.0439758/0.155121loss in batch 80: 0.0685272/0.154068loss in batch 81: 0.508682/0.158386loss in batch 82: 0.283722/0.159897loss in batch 83: 0.224991/0.160675loss in batch 84: 0.0399323/0.159256loss in batch 85: 0.232605/0.16011loss in batch 86: 0.0459137/0.158798loss in batch 87: 0.122101/0.158371loss in batch 88: 0.425064/0.161377loss in batch 89: 0.340286/0.163361loss in batch 90: 0.02034/0.161789loss in batch 91: 0.0739441/0.160828loss in batch 92: 0.0713959/0.159866loss in batch 93: 0.0577698/0.158783loss in batch 94: 0.0423584/0.157562loss in batch 95: 0.104523/0.157013loss in batch 96: 0.0724487/0.156143loss in batch 97: 0.154327/0.156113loss in batch 98: 0.0193176/0.154739loss in batch 99: 0.307343/0.156265loss in batch 100: 0.0665588/0.15538loss in batch 101: 0.327713/0.157059loss in batch 102: 0.0237122/0.155762loss in batch 103: 1.41908/0.167923loss in batch 104: 0.0577698/0.16687loss in batch 105: 0.101486/0.16626loss in batch 106: 0.171112/0.166306loss in batch 107: 0.0261536/0.165009loss in batch 108: 0.442978/0.167557loss in batch 109: 0.144745/0.167343loss in batch 110: 0.105392/0.166794loss in batch 111: 0.101303/0.166199loss in batch 112: 0.948807/0.173126loss in batch 113: 0.0953979/0.17244loss in batch 114: 0.755692/0.177521loss in batch 115: 0.0750885/0.176636loss in batch 116: 0.198074/0.176819loss in batch 117: 0.157166/0.176651loss in batch 118: 0.542648/0.179718loss in batch 119: 0.251556/0.180328loss in batch 120: 0.0426483/0.179184loss in batch 121: 0.0583954/0.178192loss in batch 122: 0.227905/0.178604loss in batch 123: 0.284683/0.179443loss in batch 124: 0.018219/0.178162loss in batch 125: 0.270447/0.178894loss in batch 126: 0.0224762/0.177673loss in batch 127: 0.214706/0.177963loss in batch 128: 0.211319/0.178223loss in batch 129: 0.0103302/0.176926loss in batch 130: 0.0467834/0.175919loss in batch 131: 0.0246429/0.174774loss in batch 132: 0.0746307/0.174026loss in batch 133: 0.012146/0.172821loss in batch 134: 0.0423889/0.171844loss in batch 135: 0.278503/0.172638loss in batch 136: 0.0517731/0.171768loss in batch 137: 0.259766/0.172394loss in batch 138: 0.0528564/0.171524loss in batch 139: 0.0377502/0.170578loss in batch 140: 0.250427/0.171143loss in batch 141: 0.0665741/0.17041loss in batch 142: 0.0427094/0.16951loss in batch 143: 0.0295715/0.168549loss in batch 144: 0.140564/0.16835loss in batch 145: 0.220261/0.168716loss in batch 146: 0.0927429/0.168182loss in batch 147: 0.085968/0.167633loss in batch 148: 0.0176697/0.166626loss in batch 149: 0.558441/0.169235loss in batch 150: 0.27713/0.169952loss in batch 151: 0.0733795/0.169312loss in batch 152: 0.00914001/0.168274loss in batch 153: 0.0484161/0.16748loss in batch 154: 0.212585/0.167786loss in batch 155: 0.0584412/0.167084loss in batch 156: 0.167725/0.167084loss in batch 157: 0.0510712/0.166367loss in batch 158: 0.098587/0.165939loss in batch 159: 0.105652/0.165558loss in batch 160: 0.510651/0.167709loss in batch 161: 0.0197754/0.166794loss in batch 162: 0.0379791/0.166loss in batch 163: 0.0528717/0.165314loss in batch 164: 0.0322113/0.16449loss in batch 165: 0.308014/0.165359loss in batch 166: 0.121429/0.1651loss in batch 167: 0.0504913/0.164413loss in batch 168: 0.019577/0.163559loss in batch 169: 0.0796967/0.163071loss in batch 170: 0.0971375/0.162674loss in batch 171: 0.0572815/0.162064loss in batch 172: 0.0899963/0.161652loss in batch 173: 0.0302887/0.160889loss in batch 174: 0.110931/0.160614loss in batch 175: 0.276169/0.161255loss in batch 176: 0.177734/0.161346loss in batch 177: 0.212631/0.161636loss in batch 178: 0.0486755/0.161026loss in batch 179: 0.0254059/0.160263loss in batch 180: 0.529022/0.162292loss in batch 181: 0.0257416/0.16156loss in batch 182: 0.0661926/0.161041loss in batch 183: 0.100235/0.16069loss in batch 184: 0.0123138/0.159897loss in batch 185: 0.1306/0.159729loss in batch 186: 0.292847/0.160446loss in batch 187: 0.190781/0.160614loss in batch 188: 0.171707/0.160675loss in batch 189: 0.0209198/0.159927loss in batch 190: 0.00193787/0.159119loss in batch 191: 0.0453949/0.158508loss in batch 192: 0.0366364/0.157883loss in batch 193: 0.0273743/0.157211loss in batch 194: 0.0153503/0.156494loss in batch 195: 0.599747/0.158737loss in batch 196: 0.0733185/0.15831loss in batch 197: 0.136826/0.158203loss in batch 198: 0.0931702/0.157867loss in batch 199: 0.0248871/0.157211loss in batch 200: 0.318954/0.15802loss in batch 201: 0.0715332/0.157593loss in batch 202: 0.0674286/0.15715loss in batch 203: 0.0836792/0.156784loss in batch 204: 0.0496368/0.156265loss in batch 205: 0.163879/0.156296loss in batch 206: 0.0626984/0.155838loss in batch 207: 0.930099/0.159576loss in batch 208: 0.027771/0.158936loss in batch 209: 0.0134735/0.158249loss in batch 210: 0.069046/0.157822loss in batch 211: 0.0112457/0.15712loss in batch 212: 0.152496/0.157104
done with epoch 17
train_acc: 0.946009 (403/426)
test loss: 0.152496
acc: 0.944054 (135/143)
loss in batch 0: 0.402405/0.402405loss in batch 1: 0.0431671/0.222778loss in batch 2: 0.0350189/0.160202loss in batch 3: 0.0401306/0.130188loss in batch 4: 0.027832/0.109711loss in batch 5: 0.0397034/0.0980377loss in batch 6: 0.149673/0.105423loss in batch 7: 0.0718384/0.101227loss in batch 8: 0.0126801/0.0913849loss in batch 9: 0.0391083/0.0861664loss in batch 10: 0.0189819/0.0800476loss in batch 11: 0.0218048/0.0751953loss in batch 12: 0.0516968/0.0733795loss in batch 13: 0.0721283/0.073288loss in batch 14: 0.0867157/0.0741882loss in batch 15: 0.0643768/0.0735779loss in batch 16: 0.082489/0.0740967loss in batch 17: 0.383698/0.0912933loss in batch 18: 0.0855408/0.0909882loss in batch 19: 0.0225372/0.0875702loss in batch 20: 0.032959/0.0849762loss in batch 21: 0.222092/0.091217loss in batch 22: 0.407013/0.104935loss in batch 23: 0.29567/0.112885loss in batch 24: 0.129547/0.113556loss in batch 25: 0.049118/0.111084loss in batch 26: 0.0160828/0.107559loss in batch 27: 0.971008/0.138397loss in batch 28: 0.0536804/0.135468loss in batch 29: 0.188583/0.137238loss in batch 30: 0.208832/0.139542loss in batch 31: 0.0862732/0.137878loss in batch 32: 0.068634/0.135788loss in batch 33: 0.0803375/0.134155loss in batch 34: 0.212769/0.136398loss in batch 35: 0.0127411/0.132965loss in batch 36: 0.256393/0.136307loss in batch 37: 0.0771179/0.13475loss in batch 38: 0.0282898/0.132019loss in batch 39: 0.0185242/0.129181loss in batch 40: 1.5538/0.163925loss in batch 41: 0.0287476/0.160721loss in batch 42: 0.223587/0.16217loss in batch 43: 0.157913/0.162064loss in batch 44: 0.421555/0.167847loss in batch 45: 0.0755157/0.165833loss in batch 46: 0.0355988/0.163055loss in batch 47: 0.0168457/0.160019loss in batch 48: 0.0518951/0.157822loss in batch 49: 0.0285339/0.155228loss in batch 50: 0.0817719/0.153778loss in batch 51: 0.0339966/0.151474loss in batch 52: 0.240372/0.153152loss in batch 53: 0.0190277/0.150665loss in batch 54: 0.0612183/0.149048loss in batch 55: 0.0156097/0.146652loss in batch 56: 0.161636/0.146927loss in batch 57: 0.0493927/0.145248loss in batch 58: 0.0148621/0.143036loss in batch 59: 0.308365/0.145782loss in batch 60: 0.127258/0.145477loss in batch 61: 0.0158691/0.143387loss in batch 62: 0.0638733/0.142136loss in batch 63: 0.0529633/0.140747loss in batch 64: 0.155853/0.140976loss in batch 65: 0.0275879/0.139252loss in batch 66: 0.0247498/0.137543loss in batch 67: 0.368744/0.140945loss in batch 68: 0.0997772/0.14035loss in batch 69: 0.048645/0.139038loss in batch 70: 0.0231323/0.137405loss in batch 71: 0.0708466/0.136475loss in batch 72: 0.209641/0.137497loss in batch 73: 0.0228882/0.135925loss in batch 74: 0.208618/0.136902loss in batch 75: 0.235931/0.138199loss in batch 76: 0.0130157/0.136581loss in batch 77: 0.101364/0.136124loss in batch 78: 0.0391693/0.134903loss in batch 79: 0.789047/0.143082loss in batch 80: 0.12471/0.142853loss in batch 81: 0.352951/0.145416loss in batch 82: 0.0621643/0.144409loss in batch 83: 0.0911102/0.143768loss in batch 84: 0.00474548/0.142136loss in batch 85: 0.0615997/0.141205loss in batch 86: 0.00247192/0.139603loss in batch 87: 0.0473175/0.138565loss in batch 88: 0.0399628/0.137451loss in batch 89: 0.325882/0.139542loss in batch 90: 0.172394/0.139908loss in batch 91: 0.126999/0.139755loss in batch 92: 0.0371246/0.138672loss in batch 93: 0.653091/0.14415loss in batch 94: 0.112915/0.143814loss in batch 95: 0.0135498/0.142456loss in batch 96: 0.0119934/0.141113loss in batch 97: 0.105972/0.140747loss in batch 98: 0.253601/0.141891loss in batch 99: 0.0641632/0.141113loss in batch 100: 0.0559235/0.140259loss in batch 101: 0.333786/0.142166loss in batch 102: 0.141693/0.142166loss in batch 103: 0.0497894/0.141266loss in batch 104: 0.461243/0.144318loss in batch 105: 0.257248/0.145386loss in batch 106: 0.424088/0.147995loss in batch 107: 0.0806885/0.147369loss in batch 108: 0.0831757/0.146774loss in batch 109: 0.0110931/0.145554loss in batch 110: 0.0368042/0.144577loss in batch 111: 0.0609589/0.143814loss in batch 112: 0.0189056/0.142715loss in batch 113: 0.0298157/0.141724loss in batch 114: 0.0200043/0.140656loss in batch 115: 0.166443/0.140884loss in batch 116: 0.0414429/0.14003loss in batch 117: 0.316879/0.141541loss in batch 118: 0.0111389/0.140442loss in batch 119: 0.0318756/0.139526loss in batch 120: 0.0288849/0.138611loss in batch 121: 0.0234833/0.13768loss in batch 122: 0.0305634/0.13681loss in batch 123: 0.171005/0.137085loss in batch 124: 0.0526886/0.136414loss in batch 125: 0.641418/0.140411loss in batch 126: 0.00502014/0.139359loss in batch 127: 0.461945/0.141861loss in batch 128: 0.00746155/0.140839loss in batch 129: 0.337952/0.142334loss in batch 130: 0.0558929/0.141678loss in batch 131: 0.223572/0.142303loss in batch 132: 0.00300598/0.141251loss in batch 133: 0.0626984/0.140671loss in batch 134: 0.724258/0.144989loss in batch 135: 0.0718231/0.144455loss in batch 136: 0.0598755/0.143829loss in batch 137: 0.0705261/0.143311loss in batch 138: 0.168045/0.143494loss in batch 139: 0.0186615/0.142593loss in batch 140: 0.686157/0.146439loss in batch 141: 0.0656738/0.145874loss in batch 142: 0.00917053/0.144928loss in batch 143: 0.00782776/0.143967loss in batch 144: 0.038681/0.143234loss in batch 145: 0.0512238/0.142609loss in batch 146: 0.00128174/0.141647loss in batch 147: 0.0423431/0.140976loss in batch 148: 0.0296478/0.140228loss in batch 149: 0.0657196/0.13974loss in batch 150: 0.23381/0.14035loss in batch 151: 0.274948/0.141235loss in batch 152: 0.241791/0.141891loss in batch 153: 0.00538635/0.141022loss in batch 154: 0.336441/0.142273loss in batch 155: 0.0402374/0.141617loss in batch 156: 0.0201874/0.140839loss in batch 157: 0.032959/0.140167loss in batch 158: 0.019165/0.139404loss in batch 159: 0.0151672/0.138626loss in batch 160: 0.11441/0.138474loss in batch 161: 0.0254364/0.137787loss in batch 162: 0.199982/0.138153loss in batch 163: 0.00123596/0.137329loss in batch 164: 0.21843/0.137817loss in batch 165: 0.0282593/0.137161loss in batch 166: 0.0422821/0.136597loss in batch 167: 0.0792542/0.136246loss in batch 168: 0.339325/0.137451loss in batch 169: 0.0215302/0.13678loss in batch 170: 0.0262299/0.136124loss in batch 171: 0.236038/0.136719loss in batch 172: 0.257523/0.13739loss in batch 173: 0.00427246/0.136627loss in batch 174: 0.4189/0.138245loss in batch 175: 0.10379/0.138062loss in batch 176: 0.364365/0.139328loss in batch 177: 0.0319214/0.138718loss in batch 178: 0.0963593/0.138489loss in batch 179: 0.00411987/0.137741loss in batch 180: 0.0273438/0.137131loss in batch 181: 0.36142/0.138367loss in batch 182: 0.0352631/0.137802loss in batch 183: 0.0133209/0.137115loss in batch 184: 0.0138092/0.136459loss in batch 185: 0.0317535/0.13591loss in batch 186: 0.700516/0.138916loss in batch 187: 0.988068/0.143433loss in batch 188: 0.0661316/0.143021loss in batch 189: 0.245728/0.14357loss in batch 190: 0.0312958/0.142975loss in batch 191: 0.498169/0.144821loss in batch 192: 0.131287/0.14476loss in batch 193: 0.0618286/0.144333loss in batch 194: 0.0182343/0.143677loss in batch 195: 0.0345154/0.143127loss in batch 196: 0.0424805/0.142624loss in batch 197: 0.217865/0.14299loss in batch 198: 0.0387268/0.142471loss in batch 199: 0.0950012/0.142227loss in batch 200: 0.458847/0.143799loss in batch 201: 0.162277/0.143906loss in batch 202: 0.0826721/0.1436loss in batch 203: 0.273361/0.144226loss in batch 204: 0.287094/0.144928loss in batch 205: 0.111038/0.144775loss in batch 206: 0.824478/0.148041loss in batch 207: 0.3936/0.149231loss in batch 208: 0.422653/0.150543loss in batch 209: 0.35849/0.15152loss in batch 210: 0.0768433/0.151184loss in batch 211: 0.125092/0.151047loss in batch 212: 1.11183/0.155579
done with epoch 18
train_acc: 0.943662 (402/426)
test loss: 1.11183
acc: 0.944054 (135/143)
loss in batch 0: 0.0789032/0.0789032loss in batch 1: 0.0273895/0.0531464loss in batch 2: 0.0613861/0.0558929loss in batch 3: 0.0570221/0.0561676loss in batch 4: 0.165649/0.078064loss in batch 5: 0.0337524/0.0706787loss in batch 6: 0.0916595/0.0736847loss in batch 7: 0.279251/0.0993805loss in batch 8: 0.0269012/0.0913239loss in batch 9: 0.467819/0.128983loss in batch 10: 0.0126495/0.118393loss in batch 11: 0.0597076/0.113495loss in batch 12: 0.117477/0.113815loss in batch 13: 0.436966/0.136887loss in batch 14: 0.00538635/0.128128loss in batch 15: 0.0161285/0.121124loss in batch 16: 0.00320435/0.114197loss in batch 17: 0.279251/0.123367loss in batch 18: 0.641388/0.150635loss in batch 19: 0.065094/0.146347loss in batch 20: 0.0559235/0.142044loss in batch 21: 0.0279694/0.136856loss in batch 22: 0.225983/0.140732loss in batch 23: 0.125/0.140076loss in batch 24: 0.407089/0.150757loss in batch 25: 0.220367/0.153427loss in batch 26: 0.0143738/0.148285loss in batch 27: 0.199448/0.150101loss in batch 28: 0.447952/0.160385loss in batch 29: 0.250519/0.163391loss in batch 30: 0.0547943/0.159882loss in batch 31: 0.255569/0.162872loss in batch 32: 0.144333/0.162308loss in batch 33: 0.456146/0.170959loss in batch 34: 0.168335/0.170883loss in batch 35: 0.115891/0.169357loss in batch 36: 0.0257263/0.165466loss in batch 37: 0.210312/0.166656loss in batch 38: 0.000228882/0.162384loss in batch 39: 0.101135/0.160858loss in batch 40: 0.284821/0.163879loss in batch 41: 0.0142517/0.160324loss in batch 42: 0.490768/0.167999loss in batch 43: 0.0133514/0.164474loss in batch 44: 0.0388184/0.161697loss in batch 45: 0.0248718/0.158722loss in batch 46: 0.0598907/0.156616loss in batch 47: 0.0645752/0.154694loss in batch 48: 0.0504913/0.152573loss in batch 49: 0.495972/0.159439loss in batch 50: 0.0943298/0.158157loss in batch 51: 0.0440979/0.15596loss in batch 52: 0.426682/0.161087loss in batch 53: 0.0337982/0.158707loss in batch 54: 0.141296/0.158401loss in batch 55: 0.148392/0.158234loss in batch 56: 0.00398254/0.155518loss in batch 57: 0.0617676/0.1539loss in batch 58: 0.0223236/0.151672loss in batch 59: 0.765594/0.161911loss in batch 60: 0.001297/0.159271loss in batch 61: 0.038559/0.157318loss in batch 62: 0.150497/0.157227loss in batch 63: 0.0297699/0.155228loss in batch 64: 0.117218/0.154633loss in batch 65: 0.0336304/0.152802loss in batch 66: 0.0615387/0.151443loss in batch 67: 0.0294037/0.149658loss in batch 68: 0.061142/0.148361loss in batch 69: 0.00463867/0.146301loss in batch 70: 0.236542/0.147583loss in batch 71: 0.134857/0.1474loss in batch 72: 0.271423/0.149109loss in batch 73: 0.0981598/0.148422loss in batch 74: 1.66322/0.16861loss in batch 75: 0.272675/0.169983loss in batch 76: 0.0283966/0.168152loss in batch 77: 0.091095/0.16716loss in batch 78: 0.226456/0.167908loss in batch 79: 0.0291443/0.166183loss in batch 80: 0.403061/0.169098loss in batch 81: 0.36322/0.171463loss in batch 82: 0.131927/0.17099loss in batch 83: 0.200058/0.171341loss in batch 84: 0.25087/0.172272loss in batch 85: 0.0291901/0.170609loss in batch 86: 0.762512/0.177399loss in batch 87: 0.555573/0.181717loss in batch 88: 0.0840607/0.180618loss in batch 89: 0.0197754/0.178818loss in batch 90: 0.060379/0.177521loss in batch 91: 0.360596/0.17952loss in batch 92: 0.0643158/0.178268loss in batch 93: 0.0620117/0.177032loss in batch 94: 0.442337/0.17984loss in batch 95: 0.157242/0.179581loss in batch 96: 0.0220032/0.177963loss in batch 97: 0.00700378/0.176224loss in batch 98: 0.022583/0.174667loss in batch 99: 0.196564/0.174881loss in batch 100: 0.0893402/0.174042loss in batch 101: 0.166992/0.173981loss in batch 102: 0.337891/0.175568loss in batch 103: 0.0400696/0.174271loss in batch 104: 0.242844/0.174927loss in batch 105: 0.0583801/0.173813loss in batch 106: 0.11998/0.173309loss in batch 107: 0.0357971/0.172043loss in batch 108: 0.0605164/0.171021loss in batch 109: 0.039978/0.16983loss in batch 110: 0.0789337/0.169006loss in batch 111: 0.192688/0.16922loss in batch 112: 0.446686/0.171677loss in batch 113: 0.0326996/0.170456loss in batch 114: 0.00793457/0.169037loss in batch 115: 0.0746765/0.168228loss in batch 116: 0.180328/0.168335loss in batch 117: 0.0241089/0.167114loss in batch 118: 0.509232/0.169983loss in batch 119: 0.0666504/0.169128loss in batch 120: 0.0578918/0.168213loss in batch 121: 0.274536/0.169083loss in batch 122: 0.281036/0.169983loss in batch 123: 0.0249176/0.168823loss in batch 124: 0.0203857/0.167633loss in batch 125: 0.069397/0.166855loss in batch 126: 0.247131/0.16748loss in batch 127: 0.243057/0.16806loss in batch 128: 0.0986023/0.167542loss in batch 129: 0.0302887/0.166489loss in batch 130: 0.00897217/0.165283loss in batch 131: 0.00398254/0.164047loss in batch 132: 0.0170135/0.162949loss in batch 133: 0.00721741/0.161789loss in batch 134: 0.0782623/0.161163loss in batch 135: 0.0644531/0.160446loss in batch 136: 0.0430145/0.159592loss in batch 137: 0.0570526/0.158859loss in batch 138: 0.0458679/0.158051loss in batch 139: 0.472931/0.160294loss in batch 140: 0.00375366/0.15918loss in batch 141: 0.0617676/0.158493loss in batch 142: 0.0716553/0.157898loss in batch 143: 0.0663757/0.157257loss in batch 144: 0.0461426/0.156479loss in batch 145: 0.220047/0.156921loss in batch 146: 0.0140991/0.155945loss in batch 147: 0.0122375/0.154984loss in batch 148: 0.0544891/0.154297loss in batch 149: 0.0374756/0.153534loss in batch 150: 0.0178833/0.152618loss in batch 151: 0.0681/0.152084loss in batch 152: 0.0473175/0.151382loss in batch 153: 0.399612/0.153loss in batch 154: 0.143158/0.152924loss in batch 155: 0.0266113/0.152115loss in batch 156: 0.0200348/0.151276loss in batch 157: 0.0169067/0.150436loss in batch 158: 0.00828552/0.149536loss in batch 159: 0.282547/0.15036loss in batch 160: 0.0726318/0.149887loss in batch 161: 0.0059967/0.149002loss in batch 162: 0.0179596/0.148193loss in batch 163: 0.0489044/0.147598loss in batch 164: 0.130096/0.147491loss in batch 165: 0.0163574/0.146683loss in batch 166: 0.0984192/0.146408loss in batch 167: 0.139862/0.146362loss in batch 168: 0.00622559/0.145538loss in batch 169: 0.0510712/0.144974loss in batch 170: 0.0194092/0.144241loss in batch 171: 0.0341034/0.143616loss in batch 172: 0.027832/0.142929loss in batch 173: 0.177063/0.143127loss in batch 174: 0.0269775/0.142471loss in batch 175: 1.20995/0.148529loss in batch 176: 0.828018/0.152374loss in batch 177: 0.0141449/0.151596loss in batch 178: 0.0287781/0.150909loss in batch 179: 0.0388641/0.150284loss in batch 180: 0.0354004/0.149658loss in batch 181: 0.0265808/0.148972loss in batch 182: 0.331207/0.149963loss in batch 183: 0.0814819/0.149597loss in batch 184: 0.0310669/0.148956loss in batch 185: 0.0217133/0.14827loss in batch 186: 0.0215759/0.147598loss in batch 187: 0.0271759/0.146957loss in batch 188: 0.0272827/0.146332loss in batch 189: 1.11038/0.151398loss in batch 190: 0.194199/0.151611loss in batch 191: 0.4095/0.152969loss in batch 192: 0.0770569/0.152573loss in batch 193: 0.07547/0.152176loss in batch 194: 0.0389404/0.151596loss in batch 195: 0.0588989/0.151123loss in batch 196: 0.275955/0.151749loss in batch 197: 0.0900726/0.151443loss in batch 198: 0.00735474/0.150726loss in batch 199: 0.00941467/0.150009loss in batch 200: 0.366776/0.151093loss in batch 201: 0.394012/0.152283loss in batch 202: 0.223938/0.152649loss in batch 203: 0.543854/0.154572loss in batch 204: 0.0204773/0.153915loss in batch 205: 0.0201263/0.153259loss in batch 206: 0.362808/0.154266loss in batch 207: 0.0572052/0.153793loss in batch 208: 0.0114288/0.153122loss in batch 209: 0.0277252/0.152527loss in batch 210: 0.138077/0.152451loss in batch 211: 0.242538/0.152878loss in batch 212: 0.0365753/0.152344
done with epoch 19
train_acc: 0.946009 (403/426)
test loss: 0.0365752
acc: 0.944054 (135/143)
[0.086441, -0.103027, -0.0970612, -0.167572, -0.0302124, -0.0518494, -0.13855, -0.0234985, -0.00187683, -0.00358582, -0.398529, -0.171524, -0.00985718, -0.423523, -0.548843, 0.0417023, -0.0805969, 0.00639343, 0.0288544, 0.000396729, 0.267426, 0.0407715, -0.257919, -0.0513916, 0.0067749, -0.0555267, -0.0144348, 0.0449066, -0.0753784, 0.0123138, -0.0153351, 0.0247192, -0.47789, 0.156693, -0.00241089, 0.168655, -0.152893, 0.078186, -0.150055, 0.167984, 0.717575, -0.0102539, 0.494766, -0.0072937, -0.544312, 0.000473022, -0.00164795, -0.0969543, -0.0196533, 0.0481262, 0.0295715, 0.0886688, 0.0389557, -0.0625458, -0.08461, -0.025177, -0.026474, -0.013031, -0.0728607, 0.000427246, 0.279007, 0.118286, -0.0156555, -0.0510712, 0.0110016, -0.0669861, 0, 0.0062561, 0.00476074, -0.0124969, -0.52623, 0.0153351, -0.0684967, 0.404526, 0.00238037, -0.081604, -0.0120392, -0.242142, -0.0221252, -0.00167847, 0.130646, 0.000289917, 0.014328, -0.0147552, 0.057785, -0.0646362, -0.0255127, -0.00415039, 0.0295715, 0, -0.00813293, 0.430511, -0.563309, 0.00222778, -0.00645447, -0.0657806, 0.00778198, -0.659775, -0.0528564, -0.0262909, -0.0017395, -0.100082, -0.11058, -0.2621, 0.0309601, -0.014679, 0.00177002, -0.218552, 0.574097, 0.654694, -0.0710907, 0.00411987, 0.0258942, -0.393906, -0.0197449, -0.0553284, -0.433884, -0.0239258, -0.0721893, -0.179352, -0.0392914, -0.0413055, 0.157806, -0.156509, 0.00238037, -0.218216, -0.326462, -0.0397339, -0.0208588, -0.00611877, 0.076355, -0.004776, -0.0198059, -0.342575, -0.234009, -0.0147705, -0.332977, 0.435654, 0, -0.0120697, -0.122284, -0.0501404, 0.00038147]
Compiler: ./compile.py -R 64 breast_logistic
	899 triples of Z2^64 left
	53 dabits of Z2^64 left
	360 triples of Z2^64 left
	212 dabits of Z2^64 left
2 threads spent a total of 159.326 seconds (376.243 MB, 3110106 rounds) on the online phase, 207.45 seconds (24042.1 MB, 502175 rounds) on the preprocessing/offline phase, and 367.77 seconds idling.
Join timer: 0 367268
Finish timer: 0.00135794
Join timer: 1 362220
Finish timer: 0.00135794
Communication details (rounds in parallel threads counted double):
Exchanging one-to-one 11384.9 MB in 66717 rounds, taking 24.7627 seconds
Receiving directly 376.243 MB in 1555053 rounds, taking 130.96 seconds
Receiving one-to-one 14239.5 MB in 217729 rounds, taking 22.7189 seconds
Sending directly 376.243 MB in 1555053 rounds, taking 18.8102 seconds
Sending one-to-one 12657.3 MB in 217729 rounds, taking 2.02973 seconds
CPU time = 180.757 (overall core time)
The following benchmarks are including preprocessing (offline phase).
Time = 367.269 seconds 
Data sent = 24418.4 MB in ~3612281 rounds (party 0 only)
Global data sent = 50419 MB (all parties)
Actual cost of program:
  Type int
      22233741        Triples
     131855095           Bits
This program might benefit from some protocol options.
Consider adding the following at the beginning of your code:
	program.use_trunc_pr = True
	program.use_split(2)
This virtual machine was compiled with GCC. Recompile with 'CXX = clang++' in 'CONFIG.mine' for optimal performance.
Command line: ./semi2k-party.x -N 2 -e --ip-file-name /HOST -p 0 -v breast_logistic
