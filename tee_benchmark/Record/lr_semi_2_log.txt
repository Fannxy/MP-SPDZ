Using statistical security parameter 40
No modulus found in /Player-Data//2-Dp-128/Params-Data, generating 128-bit prime
Current working directory: "/"
Current working directory: "/"
Using SGD
done with epoch 0done with epoch 1done with epoch 2done with epoch 3done with epoch 4done with epoch 5done with epoch 6done with epoch 7done with epoch 8done with epoch 9done with epoch 10done with epoch 11done with epoch 12done with epoch 13done with epoch 14done with epoch 15done with epoch 16done with epoch 17done with epoch 18done with epoch 19
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Using SGD
loss in batch 0: 0.731369/0.731369loss in batch 1: 0.762909/0.747131loss in batch 2: 0.739197/0.744492loss in batch 3: 0.979263/0.803177loss in batch 4: 0.476669/0.737885loss in batch 5: 0.437881/0.687881loss in batch 6: 0.716705/0.691986loss in batch 7: 0.49646/0.667557loss in batch 8: 0.639709/0.664474loss in batch 9: 0.653564/0.663376loss in batch 10: 0.470413/0.645828loss in batch 11: 0.715485/0.651642loss in batch 12: 0.868332/0.668304loss in batch 13: 0.655273/0.667374loss in batch 14: 0.69191/0.669022loss in batch 15: 0.62561/0.666306loss in batch 16: 0.809692/0.674744loss in batch 17: 0.509857/0.665573loss in batch 18: 0.611069/0.662704loss in batch 19: 0.586807/0.65892loss in batch 20: 0.704666/0.661087loss in batch 21: 0.611252/0.658813loss in batch 22: 0.635056/0.657791loss in batch 23: 0.62207/0.656296loss in batch 24: 0.543884/0.651794loss in batch 25: 0.63446/0.651138loss in batch 26: 0.497208/0.645447loss in batch 27: 0.64743/0.645508loss in batch 28: 0.701294/0.64743loss in batch 29: 0.62674/0.646729loss in batch 30: 0.554733/0.643768loss in batch 31: 0.510376/0.639603loss in batch 32: 0.550644/0.636917loss in batch 33: 0.579819/0.635223loss in batch 34: 0.540543/0.632523loss in batch 35: 0.446274/0.62735loss in batch 36: 0.728546/0.630096loss in batch 37: 0.624344/0.629929loss in batch 38: 0.707214/0.631912loss in batch 39: 0.399414/0.626099loss in batch 40: 1.03096/0.635971loss in batch 41: 0.678085/0.636978loss in batch 42: 0.340515/0.630096loss in batch 43: 0.740814/0.632614loss in batch 44: 0.70488/0.634201loss in batch 45: 0.67218/0.635025loss in batch 46: 0.418365/0.630417loss in batch 47: 0.352631/0.624634loss in batch 48: 0.360092/0.619232loss in batch 49: 0.704803/0.620941loss in batch 50: 0.862503/0.625687loss in batch 51: 0.718048/0.627457loss in batch 52: 0.667282/0.62822loss in batch 53: 0.366837/0.623367loss in batch 54: 0.866409/0.627792loss in batch 55: 0.362106/0.623047loss in batch 56: 0.793701/0.626038loss in batch 57: 0.411758/0.62236loss in batch 58: 0.624634/0.622391loss in batch 59: 0.390396/0.618515loss in batch 60: 0.413147/0.615158loss in batch 61: 0.455795/0.612579loss in batch 62: 0.422073/0.609558loss in batch 63: 0.888458/0.613907loss in batch 64: 0.612106/0.613892loss in batch 65: 0.366577/0.610138loss in batch 66: 0.404068/0.607071loss in batch 67: 0.830307/0.610352loss in batch 68: 0.530029/0.609177loss in batch 69: 0.49115/0.607498loss in batch 70: 0.490891/0.605865loss in batch 71: 0.69574/0.607101loss in batch 72: 0.462509/0.605118loss in batch 73: 0.530563/0.604126loss in batch 74: 0.511612/0.60289loss in batch 75: 0.370255/0.599808loss in batch 76: 0.359375/0.596695loss in batch 77: 0.6147/0.596939loss in batch 78: 0.569366/0.596588loss in batch 79: 0.441727/0.59465loss in batch 80: 0.631104/0.595093loss in batch 81: 0.586945/0.595001loss in batch 82: 0.366196/0.592239loss in batch 83: 0.476105/0.590851loss in batch 84: 0.89621/0.594452loss in batch 85: 0.536575/0.593765loss in batch 86: 0.535645/0.593109loss in batch 87: 0.691483/0.594223loss in batch 88: 0.525742/0.59346loss in batch 89: 0.662064/0.594223loss in batch 90: 0.505829/0.593246loss in batch 91: 0.310196/0.590179loss in batch 92: 0.54155/0.589645loss in batch 93: 0.634674/0.590134loss in batch 94: 0.526611/0.589462loss in batch 95: 0.621704/0.589798loss in batch 96: 0.755356/0.591507loss in batch 97: 0.601379/0.591614loss in batch 98: 0.595932/0.591644loss in batch 99: 0.597397/0.591705loss in batch 100: 0.862427/0.594376loss in batch 101: 0.57251/0.594177loss in batch 102: 0.508286/0.593338loss in batch 103: 0.632355/0.593704loss in batch 104: 0.422623/0.592072loss in batch 105: 0.461792/0.590866loss in batch 106: 0.409149/0.589157loss in batch 107: 0.608551/0.58934loss in batch 108: 0.337051/0.587021loss in batch 109: 0.431183/0.585602loss in batch 110: 0.375839/0.58371loss in batch 111: 0.400101/0.582062loss in batch 112: 0.663101/0.582779loss in batch 113: 0.26059/0.579956loss in batch 114: 0.30542/0.577576loss in batch 115: 0.454681/0.576508loss in batch 116: 0.27832/0.573959loss in batch 117: 0.659088/0.574677loss in batch 118: 1.17314/0.579727loss in batch 119: 0.604446/0.579926loss in batch 120: 0.260361/0.577286loss in batch 121: 0.23938/0.574524loss in batch 122: 0.651337/0.575134loss in batch 123: 0.181763/0.571976loss in batch 124: 0.602814/0.57222loss in batch 125: 0.613358/0.57254loss in batch 126: 0.375549/0.570984loss in batch 127: 0.390335/0.56958loss in batch 128: 0.670822/0.570358loss in batch 129: 0.700806/0.571365loss in batch 130: 0.786743/0.573013loss in batch 131: 0.252502/0.570572loss in batch 132: 0.539398/0.570343loss in batch 133: 0.290909/0.568268loss in batch 134: 0.361435/0.566742loss in batch 135: 0.34726/0.565125loss in batch 136: 0.721237/0.566254loss in batch 137: 0.643219/0.566803loss in batch 138: 0.343918/0.565216loss in batch 139: 0.437454/0.564301loss in batch 140: 0.297684/0.562408loss in batch 141: 0.601425/0.562683loss in batch 142: 0.598068/0.562927loss in batch 143: 0.374481/0.561615loss in batch 144: 0.516937/0.56131loss in batch 145: 0.466812/0.560669loss in batch 146: 0.398407/0.55957loss in batch 147: 0.465958/0.558929loss in batch 148: 0.390289/0.5578loss in batch 149: 0.43898/0.557007loss in batch 150: 0.487/0.556534loss in batch 151: 0.424377/0.555664loss in batch 152: 0.296158/0.55397loss in batch 153: 0.52449/0.553787loss in batch 154: 0.398087/0.55278loss in batch 155: 0.674454/0.553558loss in batch 156: 0.415192/0.552673loss in batch 157: 0.525131/0.55249loss in batch 158: 0.732529/0.553635loss in batch 159: 0.390945/0.552612loss in batch 160: 0.392532/0.55162loss in batch 161: 0.45256/0.55101loss in batch 162: 0.534531/0.550919loss in batch 163: 0.618866/0.551331loss in batch 164: 0.326187/0.549957loss in batch 165: 0.294006/0.548416loss in batch 166: 0.404388/0.547546loss in batch 167: 0.324432/0.546234loss in batch 168: 0.520126/0.546066loss in batch 169: 0.40596/0.545258loss in batch 170: 0.281311/0.543701loss in batch 171: 1.02715/0.546509loss in batch 172: 0.253052/0.544815loss in batch 173: 0.432404/0.544174loss in batch 174: 0.634949/0.544693loss in batch 175: 0.582092/0.544907loss in batch 176: 0.266403/0.543335loss in batch 177: 0.721497/0.544327loss in batch 178: 0.575409/0.54451loss in batch 179: 0.248596/0.542862loss in batch 180: 0.430954/0.542252loss in batch 181: 0.626633/0.542709loss in batch 182: 0.284515/0.541306loss in batch 183: 0.393341/0.540482loss in batch 184: 0.378677/0.539627loss in batch 185: 0.472397/0.539246loss in batch 186: 0.318481/0.538071loss in batch 187: 0.307098/0.53685loss in batch 188: 0.417877/0.536209loss in batch 189: 0.220871/0.534561loss in batch 190: 0.366928/0.533676loss in batch 191: 0.514709/0.533585loss in batch 192: 0.570709/0.533768loss in batch 193: 0.164871/0.531876loss in batch 194: 0.368546/0.531021loss in batch 195: 0.33429/0.530029loss in batch 196: 0.275543/0.528748loss in batch 197: 0.29361/0.527557loss in batch 198: 0.639755/0.528107loss in batch 199: 0.295502/0.526962loss in batch 200: 0.127838/0.524963loss in batch 201: 0.416016/0.524429loss in batch 202: 0.368881/0.523666loss in batch 203: 0.302094/0.522568loss in batch 204: 0.677628/0.523331loss in batch 205: 0.386993/0.522675loss in batch 206: 0.408432/0.522125loss in batch 207: 0.45285/0.521774loss in batch 208: 0.302414/0.520737loss in batch 209: 0.495255/0.520615loss in batch 210: 0.513672/0.520569loss in batch 211: 0.42395/0.520126loss in batch 212: 0.375763/0.51944
done with epoch 0
train_acc: 0.795775 (339/426)
test loss: 0.375763
acc: 0.909091 (130/143)
loss in batch 0: 0.485413/0.485413loss in batch 1: 0.514664/0.500031loss in batch 2: 0.352905/0.450989loss in batch 3: 0.481216/0.458557loss in batch 4: 0.41095/0.44902loss in batch 5: 0.367661/0.435471loss in batch 6: 0.41803/0.432983loss in batch 7: 0.230896/0.407715loss in batch 8: 0.187881/0.383301loss in batch 9: 0.543045/0.399261loss in batch 10: 0.445648/0.403488loss in batch 11: 0.280838/0.393265loss in batch 12: 0.402924/0.393997loss in batch 13: 0.191574/0.379547loss in batch 14: 0.411179/0.381653loss in batch 15: 0.470825/0.387222loss in batch 16: 0.132034/0.372208loss in batch 17: 0.189209/0.362045loss in batch 18: 0.295029/0.358521loss in batch 19: 0.355637/0.358383loss in batch 20: 0.872345/0.382843loss in batch 21: 0.221802/0.375534loss in batch 22: 0.174606/0.366791loss in batch 23: 0.113922/0.356262loss in batch 24: 0.263672/0.352554loss in batch 25: 0.290695/0.350189loss in batch 26: 0.173737/0.343643loss in batch 27: 0.451462/0.347504loss in batch 28: 0.164078/0.341171loss in batch 29: 0.162033/0.33519loss in batch 30: 1.10393/0.360001loss in batch 31: 0.130722/0.352829loss in batch 32: 0.483994/0.356796loss in batch 33: 0.502045/0.361069loss in batch 34: 0.153397/0.355148loss in batch 35: 0.657623/0.363541loss in batch 36: 0.263016/0.360825loss in batch 37: 0.469482/0.363693loss in batch 38: 0.517456/0.36763loss in batch 39: 0.205261/0.363571loss in batch 40: 1.03154/0.379868loss in batch 41: 0.190674/0.375351loss in batch 42: 0.0991364/0.368927loss in batch 43: 0.219925/0.36554loss in batch 44: 0.575272/0.370209loss in batch 45: 0.604172/0.37529loss in batch 46: 0.301483/0.373734loss in batch 47: 0.183945/0.369766loss in batch 48: 0.817856/0.378922loss in batch 49: 0.185577/0.375046loss in batch 50: 0.472778/0.376953loss in batch 51: 0.280472/0.375107loss in batch 52: 0.499176/0.377457loss in batch 53: 0.434082/0.378494loss in batch 54: 0.271774/0.376556loss in batch 55: 0.39563/0.376892loss in batch 56: 0.580139/0.380463loss in batch 57: 0.384338/0.380539loss in batch 58: 0.30246/0.379196loss in batch 59: 0.469162/0.380707loss in batch 60: 0.347504/0.380157loss in batch 61: 0.388672/0.38031loss in batch 62: 0.361618/0.380005loss in batch 63: 0.370117/0.379837loss in batch 64: 0.0999756/0.375549loss in batch 65: 0.443604/0.376572loss in batch 66: 0.407745/0.377045loss in batch 67: 0.256439/0.375259loss in batch 68: 0.403107/0.375671loss in batch 69: 0.293793/0.374496loss in batch 70: 0.484909/0.376053loss in batch 71: 0.349701/0.375687loss in batch 72: 0.177505/0.372971loss in batch 73: 0.256989/0.371399loss in batch 74: 0.430588/0.372208loss in batch 75: 0.192169/0.369827loss in batch 76: 0.554672/0.372223loss in batch 77: 0.505569/0.373932loss in batch 78: 0.3396/0.373505loss in batch 79: 0.108139/0.370193loss in batch 80: 0.132278/0.367249loss in batch 81: 0.24649/0.365784loss in batch 82: 0.215347/0.363968loss in batch 83: 0.871735/0.37001loss in batch 84: 0.262009/0.368729loss in batch 85: 0.452377/0.369705loss in batch 86: 0.559006/0.371887loss in batch 87: 0.268768/0.370712loss in batch 88: 0.155655/0.368301loss in batch 89: 1.3913/0.379669loss in batch 90: 1.11783/0.387772loss in batch 91: 0.316452/0.386993loss in batch 92: 0.88472/0.392349loss in batch 93: 0.251053/0.390854loss in batch 94: 0.688644/0.393982loss in batch 95: 0.128983/0.39122loss in batch 96: 0.182434/0.389069loss in batch 97: 0.264816/0.387802loss in batch 98: 0.16655/0.385559loss in batch 99: 0.229187/0.384003loss in batch 100: 0.174484/0.381943loss in batch 101: 0.224564/0.380386loss in batch 102: 0.220016/0.37883loss in batch 103: 0.199814/0.377106loss in batch 104: 0.44046/0.377716loss in batch 105: 0.267944/0.376678loss in batch 106: 0.246643/0.375473loss in batch 107: 0.541794/0.376999loss in batch 108: 0.336136/0.376617loss in batch 109: 0.106277/0.374161loss in batch 110: 0.407043/0.374466loss in batch 111: 0.274841/0.373581loss in batch 112: 0.125076/0.371368loss in batch 113: 0.257156/0.370377loss in batch 114: 0.204727/0.368927loss in batch 115: 0.999527/0.374374loss in batch 116: 0.270218/0.373474loss in batch 117: 0.365067/0.373413loss in batch 118: 0.19693/0.371933loss in batch 119: 0.894562/0.376282loss in batch 120: 0.392273/0.376404loss in batch 121: 0.281418/0.375626loss in batch 122: 0.410675/0.375931loss in batch 123: 0.300369/0.375305loss in batch 124: 0.523041/0.376495loss in batch 125: 0.135895/0.374588loss in batch 126: 0.273575/0.373779loss in batch 127: 0.267426/0.372955loss in batch 128: 0.351807/0.372787loss in batch 129: 0.580261/0.37439loss in batch 130: 0.360352/0.374268loss in batch 131: 0.223679/0.373138loss in batch 132: 0.356049/0.373001loss in batch 133: 0.203445/0.37175loss in batch 134: 0.193298/0.370438loss in batch 135: 0.394974/0.37059loss in batch 136: 0.293732/0.370041loss in batch 137: 0.22934/0.369034loss in batch 138: 0.561386/0.370407loss in batch 139: 0.117172/0.368607loss in batch 140: 0.423874/0.368988loss in batch 141: 0.234726/0.368042loss in batch 142: 0.582504/0.369553loss in batch 143: 0.372437/0.369568loss in batch 144: 0.154129/0.368088loss in batch 145: 0.41684/0.368408loss in batch 146: 0.675507/0.370499loss in batch 147: 0.288284/0.369934loss in batch 148: 0.423569/0.3703loss in batch 149: 0.435699/0.370743loss in batch 150: 0.270691/0.370087loss in batch 151: 0.194244/0.368927loss in batch 152: 0.274368/0.368301loss in batch 153: 0.238449/0.367462loss in batch 154: 0.599045/0.368942loss in batch 155: 0.542725/0.370071loss in batch 156: 0.284119/0.369522loss in batch 157: 0.197784/0.368439loss in batch 158: 0.807068/0.371185loss in batch 159: 0.318283/0.370865loss in batch 160: 0.466263/0.371445loss in batch 161: 0.531769/0.372452loss in batch 162: 0.298874/0.371994loss in batch 163: 0.298996/0.371552loss in batch 164: 0.252411/0.370819loss in batch 165: 0.156937/0.369537loss in batch 166: 0.400208/0.369736loss in batch 167: 0.236862/0.368927loss in batch 168: 0.340485/0.368759loss in batch 169: 0.307434/0.368408loss in batch 170: 0.345871/0.368271loss in batch 171: 0.417252/0.368546loss in batch 172: 0.262375/0.367935loss in batch 173: 0.281296/0.367447loss in batch 174: 0.0665436/0.365723loss in batch 175: 0.449249/0.366196loss in batch 176: 0.123749/0.364838loss in batch 177: 0.49498/0.36557loss in batch 178: 0.20816/0.364685loss in batch 179: 0.237778/0.363983loss in batch 180: 0.531219/0.364899loss in batch 181: 0.528305/0.365799loss in batch 182: 0.343536/0.365677loss in batch 183: 0.448486/0.366135loss in batch 184: 0.383713/0.366226loss in batch 185: 0.276001/0.365738loss in batch 186: 0.460052/0.366226loss in batch 187: 0.523865/0.367081loss in batch 188: 0.471512/0.36763loss in batch 189: 0.288559/0.367203loss in batch 190: 0.536667/0.368103loss in batch 191: 0.343246/0.367981loss in batch 192: 0.279984/0.367523loss in batch 193: 0.134033/0.366318loss in batch 194: 0.477631/0.366882loss in batch 195: 0.268738/0.366379loss in batch 196: 0.244415/0.365753loss in batch 197: 0.582687/0.366867loss in batch 198: 0.20752/0.366058loss in batch 199: 0.254669/0.365509loss in batch 200: 0.751526/0.367432loss in batch 201: 0.187195/0.366531loss in batch 202: 0.233124/0.365875loss in batch 203: 0.319153/0.365646loss in batch 204: 0.151474/0.364594loss in batch 205: 0.283936/0.364212loss in batch 206: 0.23439/0.363586loss in batch 207: 0.615829/0.364792loss in batch 208: 0.157394/0.3638loss in batch 209: 0.192093/0.362991loss in batch 210: 0.493835/0.363602loss in batch 211: 0.176407/0.362717loss in batch 212: 0.128983/0.361618
done with epoch 1
train_acc: 0.901408 (384/426)
test loss: 0.128983
acc: 0.93007 (133/143)
loss in batch 0: 0.197403/0.197403loss in batch 1: 0.149628/0.173508loss in batch 2: 0.789734/0.378922loss in batch 3: 0.18956/0.331573loss in batch 4: 0.312149/0.327682loss in batch 5: 0.334061/0.328751loss in batch 6: 0.208023/0.311508loss in batch 7: 0.47049/0.33139loss in batch 8: 0.157104/0.312012loss in batch 9: 0.069458/0.287766loss in batch 10: 0.421616/0.299927loss in batch 11: 0.568649/0.322311loss in batch 12: 0.338333/0.323547loss in batch 13: 0.331116/0.324097loss in batch 14: 0.187683/0.315002loss in batch 15: 0.434555/0.322464loss in batch 16: 0.172058/0.313614loss in batch 17: 0.278275/0.311661loss in batch 18: 0.304626/0.311295loss in batch 19: 0.247711/0.308105loss in batch 20: 0.329086/0.309113loss in batch 21: 0.117081/0.300385loss in batch 22: 0.171021/0.294769loss in batch 23: 0.263763/0.293472loss in batch 24: 0.202835/0.289841loss in batch 25: 0.0873413/0.282059loss in batch 26: 0.388702/0.285995loss in batch 27: 0.335663/0.287781loss in batch 28: 0.338882/0.289536loss in batch 29: 0.0614319/0.281937loss in batch 30: 0.398544/0.285706loss in batch 31: 0.210266/0.28334loss in batch 32: 0.0583038/0.27652loss in batch 33: 0.458633/0.281876loss in batch 34: 0.792099/0.296463loss in batch 35: 0.71286/0.308029loss in batch 36: 0.161865/0.304077loss in batch 37: 0.617416/0.312302loss in batch 38: 0.266266/0.311142loss in batch 39: 0.142181/0.306915loss in batch 40: 0.463989/0.310745loss in batch 41: 0.109467/0.305954loss in batch 42: 0.477814/0.309937loss in batch 43: 0.472183/0.313629loss in batch 44: 0.222519/0.311615loss in batch 45: 0.236313/0.309967loss in batch 46: 0.19281/0.30748loss in batch 47: 0.391724/0.309235loss in batch 48: 0.180161/0.306595loss in batch 49: 0.228561/0.305038loss in batch 50: 0.619904/0.311203loss in batch 51: 0.258591/0.310196loss in batch 52: 0.285309/0.309738loss in batch 53: 0.452118/0.312378loss in batch 54: 0.222427/0.310745loss in batch 55: 0.091568/0.306824loss in batch 56: 0.198792/0.304916loss in batch 57: 0.180786/0.30278loss in batch 58: 0.230194/0.301559loss in batch 59: 0.168762/0.299347loss in batch 60: 0.185699/0.297485loss in batch 61: 0.133133/0.29483loss in batch 62: 0.190598/0.293182loss in batch 63: 0.183334/0.291458loss in batch 64: 0.315247/0.291824loss in batch 65: 0.203384/0.290482loss in batch 66: 0.146606/0.28833loss in batch 67: 0.517593/0.291702loss in batch 68: 0.245255/0.291031loss in batch 69: 0.290878/0.291031loss in batch 70: 0.212814/0.289932loss in batch 71: 0.448654/0.292145loss in batch 72: 0.0913391/0.289383loss in batch 73: 0.31044/0.289658loss in batch 74: 0.537949/0.292984loss in batch 75: 0.100403/0.290451loss in batch 76: 0.236084/0.289749loss in batch 77: 0.626083/0.294052loss in batch 78: 0.240952/0.293381loss in batch 79: 0.282486/0.293243loss in batch 80: 0.124146/0.291153loss in batch 81: 0.200302/0.290039loss in batch 82: 0.2854/0.289993loss in batch 83: 0.179596/0.288681loss in batch 84: 0.0712433/0.286118loss in batch 85: 0.0786896/0.283707loss in batch 86: 0.203995/0.282791loss in batch 87: 0.171844/0.281525loss in batch 88: 0.302155/0.281769loss in batch 89: 0.300278/0.281967loss in batch 90: 0.167923/0.280701loss in batch 91: 0.356232/0.28154loss in batch 92: 0.758102/0.286652loss in batch 93: 0.423981/0.288116loss in batch 94: 0.356018/0.288834loss in batch 95: 0.56189/0.291687loss in batch 96: 0.468018/0.293503loss in batch 97: 0.237656/0.292923loss in batch 98: 0.148407/0.291458loss in batch 99: 0.228638/0.290833loss in batch 100: 0.0990295/0.28894loss in batch 101: 0.257767/0.288635loss in batch 102: 0.362015/0.289352loss in batch 103: 0.275406/0.289215loss in batch 104: 0.432938/0.290588loss in batch 105: 0.121399/0.288986loss in batch 106: 0.633896/0.292206loss in batch 107: 0.210144/0.291458loss in batch 108: 0.235001/0.290939loss in batch 109: 0.544357/0.293228loss in batch 110: 0.0580444/0.291107loss in batch 111: 0.327347/0.291443loss in batch 112: 0.401764/0.292419loss in batch 113: 0.0757904/0.290512loss in batch 114: 0.251358/0.290176loss in batch 115: 0.151733/0.288971loss in batch 116: 0.284424/0.288956loss in batch 117: 0.363678/0.289566loss in batch 118: 0.354782/0.290131loss in batch 119: 0.226715/0.289597loss in batch 120: 0.328186/0.289917loss in batch 121: 0.228607/0.289413loss in batch 122: 0.810959/0.29364loss in batch 123: 0.117126/0.292236loss in batch 124: 0.348312/0.292679loss in batch 125: 0.223145/0.29213loss in batch 126: 0.524536/0.293961loss in batch 127: 0.136139/0.292709loss in batch 128: 0.18367/0.29187loss in batch 129: 0.367172/0.292465loss in batch 130: 1.50272/0.301697loss in batch 131: 0.305161/0.301712loss in batch 132: 0.0708618/0.299988loss in batch 133: 0.3806/0.300583loss in batch 134: 0.17215/0.299637loss in batch 135: 0.171371/0.298691loss in batch 136: 0.628494/0.301102loss in batch 137: 0.113495/0.299744loss in batch 138: 0.27298/0.299545loss in batch 139: 0.116959/0.298248loss in batch 140: 0.457748/0.299362loss in batch 141: 0.494995/0.300751loss in batch 142: 0.233032/0.300278loss in batch 143: 0.196747/0.299561loss in batch 144: 0.463577/0.30069loss in batch 145: 0.160034/0.299728loss in batch 146: 0.13829/0.29863loss in batch 147: 0.52829/0.300186loss in batch 148: 0.297592/0.300156loss in batch 149: 0.152008/0.299179loss in batch 150: 0.19017/0.298462loss in batch 151: 0.252533/0.298157loss in batch 152: 0.208023/0.297562loss in batch 153: 0.137817/0.296524loss in batch 154: 0.666626/0.29892loss in batch 155: 0.194794/0.298248loss in batch 156: 0.112991/0.297073loss in batch 157: 0.541061/0.298615loss in batch 158: 0.216125/0.29808loss in batch 159: 0.241241/0.297745loss in batch 160: 0.353653/0.298096loss in batch 161: 0.0805359/0.296738loss in batch 162: 0.0911713/0.295486loss in batch 163: 0.265182/0.295288loss in batch 164: 0.742142/0.298004loss in batch 165: 0.492065/0.299164loss in batch 166: 0.0948792/0.297943loss in batch 167: 0.068634/0.296585loss in batch 168: 0.289749/0.296539loss in batch 169: 0.357697/0.29689loss in batch 170: 0.292648/0.296875loss in batch 171: 0.176926/0.296173loss in batch 172: 0.390961/0.296722loss in batch 173: 0.583206/0.29837loss in batch 174: 0.161255/0.297592loss in batch 175: 0.111664/0.296539loss in batch 176: 0.430038/0.297287loss in batch 177: 0.191711/0.296692loss in batch 178: 0.0842896/0.295502loss in batch 179: 0.114487/0.29451loss in batch 180: 0.23381/0.294174loss in batch 181: 0.534378/0.295486loss in batch 182: 0.160919/0.294739loss in batch 183: 1.00459/0.298615loss in batch 184: 0.152023/0.297821loss in batch 185: 0.096344/0.296738loss in batch 186: 0.0965576/0.295654loss in batch 187: 0.47345/0.2966loss in batch 188: 0.432098/0.297318loss in batch 189: 0.291016/0.297287loss in batch 190: 0.203812/0.296814loss in batch 191: 0.511597/0.297928loss in batch 192: 0.244156/0.297638loss in batch 193: 0.735306/0.299896loss in batch 194: 0.398117/0.300415loss in batch 195: 0.195053/0.299881loss in batch 196: 0.136749/0.299026loss in batch 197: 0.0840607/0.297943loss in batch 198: 0.240067/0.297653loss in batch 199: 0.196716/0.29715loss in batch 200: 0.169083/0.296524loss in batch 201: 0.13176/0.2957loss in batch 202: 0.563202/0.297012loss in batch 203: 0.283844/0.296951loss in batch 204: 0.147095/0.296234loss in batch 205: 0.538223/0.297394loss in batch 206: 0.844009/0.300034loss in batch 207: 0.369797/0.300385loss in batch 208: 0.340973/0.300583loss in batch 209: 0.131958/0.299774loss in batch 210: 0.314163/0.299835loss in batch 211: 0.092926/0.298859loss in batch 212: 0.0439606/0.297668
done with epoch 2
train_acc: 0.91784 (391/426)
test loss: 0.0439606
acc: 0.916084 (131/143)
loss in batch 0: 0.324783/0.324783loss in batch 1: 0.0365601/0.180664loss in batch 2: 0.150772/0.1707loss in batch 3: 0.450851/0.240738loss in batch 4: 0.288391/0.250275loss in batch 5: 0.425415/0.279465loss in batch 6: 0.377579/0.293488loss in batch 7: 0.205719/0.282501loss in batch 8: 0.458572/0.302063loss in batch 9: 0.120834/0.283951loss in batch 10: 0.514236/0.304886loss in batch 11: 0.109116/0.288574loss in batch 12: 0.232758/0.284271loss in batch 13: 0.114441/0.272156loss in batch 14: 0.359009/0.277939loss in batch 15: 0.0333099/0.26265loss in batch 16: 0.155365/0.256332loss in batch 17: 0.238678/0.255356loss in batch 18: 0.475403/0.266937loss in batch 19: 0.544785/0.280823loss in batch 20: 0.138199/0.274033loss in batch 21: 0.0824127/0.26532loss in batch 22: 0.693054/0.283936loss in batch 23: 0.109024/0.276627loss in batch 24: 0.342468/0.279266loss in batch 25: 0.14476/0.274094loss in batch 26: 0.147491/0.269409loss in batch 27: 0.372177/0.273071loss in batch 28: 0.249329/0.272263loss in batch 29: 0.321365/0.273895loss in batch 30: 0.19574/0.271378loss in batch 31: 0.344879/0.273682loss in batch 32: 0.445313/0.278885loss in batch 33: 0.338181/0.280609loss in batch 34: 0.0674438/0.274521loss in batch 35: 0.152496/0.271133loss in batch 36: 0.361481/0.273575loss in batch 37: 0.341675/0.275375loss in batch 38: 0.451401/0.279877loss in batch 39: 0.160339/0.276886loss in batch 40: 0.224014/0.275604loss in batch 41: 0.154785/0.27272loss in batch 42: 0.128311/0.269363loss in batch 43: 0.368439/0.271622loss in batch 44: 0.231781/0.270737loss in batch 45: 0.0988007/0.266998loss in batch 46: 0.516922/0.272324loss in batch 47: 0.306793/0.273026loss in batch 48: 0.10643/0.269638loss in batch 49: 0.10788/0.266403loss in batch 50: 0.0875702/0.262894loss in batch 51: 0.298141/0.263565loss in batch 52: 0.219635/0.262741loss in batch 53: 0.115631/0.260025loss in batch 54: 0.173386/0.258438loss in batch 55: 0.94252/0.27066loss in batch 56: 0.203613/0.269485loss in batch 57: 0.236359/0.268921loss in batch 58: 0.484711/0.272568loss in batch 59: 0.15332/0.270569loss in batch 60: 0.487595/0.274139loss in batch 61: 0.0634613/0.270752loss in batch 62: 0.166443/0.269073loss in batch 63: 0.221359/0.268341loss in batch 64: 0.181229/0.266998loss in batch 65: 0.232025/0.266464loss in batch 66: 0.462234/0.269394loss in batch 67: 0.278397/0.269516loss in batch 68: 0.119553/0.267349loss in batch 69: 0.106323/0.265045loss in batch 70: 0.309967/0.265686loss in batch 71: 0.588043/0.270157loss in batch 72: 0.0951843/0.267761loss in batch 73: 0.111786/0.265656loss in batch 74: 0.255707/0.265518loss in batch 75: 0.207184/0.264755loss in batch 76: 0.0570068/0.262054loss in batch 77: 0.301544/0.262558loss in batch 78: 0.160263/0.261276loss in batch 79: 0.0899963/0.259125loss in batch 80: 0.47226/0.261765loss in batch 81: 0.808136/0.268417loss in batch 82: 0.175247/0.267303loss in batch 83: 0.327957/0.268021loss in batch 84: 0.257431/0.267899loss in batch 85: 0.0545654/0.265411loss in batch 86: 0.140579/0.263992loss in batch 87: 0.341187/0.264862loss in batch 88: 0.103561/0.263046loss in batch 89: 0.0875092/0.261093loss in batch 90: 0.128128/0.259628loss in batch 91: 0.707443/0.264496loss in batch 92: 0.0789337/0.262512loss in batch 93: 0.513138/0.265182loss in batch 94: 0.491959/0.267563loss in batch 95: 0.534454/0.27034loss in batch 96: 0.130173/0.26889loss in batch 97: 0.767212/0.273987loss in batch 98: 0.182327/0.273041loss in batch 99: 0.114166/0.271469loss in batch 100: 0.575485/0.274475loss in batch 101: 0.16124/0.273361loss in batch 102: 0.494797/0.275513loss in batch 103: 0.128601/0.274094loss in batch 104: 0.234283/0.273712loss in batch 105: 0.196579/0.272995loss in batch 106: 0.130325/0.271667loss in batch 107: 0.462143/0.273422loss in batch 108: 0.169357/0.272461loss in batch 109: 0.410599/0.273727loss in batch 110: 0.101074/0.272171loss in batch 111: 0.139465/0.270981loss in batch 112: 0.166916/0.270065loss in batch 113: 0.877869/0.275391loss in batch 114: 0.242004/0.275116loss in batch 115: 0.116394/0.273743loss in batch 116: 0.0870209/0.272141loss in batch 117: 0.228394/0.271759loss in batch 118: 0.263458/0.271698loss in batch 119: 0.0785065/0.270081loss in batch 120: 0.704193/0.273682loss in batch 121: 0.17952/0.272903loss in batch 122: 0.335266/0.273407loss in batch 123: 0.0999756/0.272003loss in batch 124: 0.484253/0.273712loss in batch 125: 0.238312/0.273422loss in batch 126: 0.30542/0.273682loss in batch 127: 1.06709/0.279892loss in batch 128: 0.268799/0.2798loss in batch 129: 0.032486/0.277893loss in batch 130: 0.119385/0.276688loss in batch 131: 0.146774/0.275711loss in batch 132: 0.34993/0.27626loss in batch 133: 0.382874/0.277054loss in batch 134: 0.157959/0.276169loss in batch 135: 0.370224/0.276855loss in batch 136: 0.192215/0.276245loss in batch 137: 0.358047/0.27684loss in batch 138: 0.165848/0.276031loss in batch 139: 0.0630341/0.274506loss in batch 140: 0.305862/0.27475loss in batch 141: 0.755493/0.278137loss in batch 142: 0.0543823/0.276566loss in batch 143: 0.0946503/0.275299loss in batch 144: 0.673218/0.278046loss in batch 145: 0.0784912/0.276672loss in batch 146: 0.169846/0.27594loss in batch 147: 0.437241/0.277039loss in batch 148: 0.366714/0.277649loss in batch 149: 0.832932/0.281342loss in batch 150: 0.289063/0.281403loss in batch 151: 0.853943/0.285156loss in batch 152: 0.179977/0.28447loss in batch 153: 0.132996/0.283478loss in batch 154: 1.2758/0.289886loss in batch 155: 0.253235/0.289658loss in batch 156: 0.0464935/0.288101loss in batch 157: 0.207047/0.287598loss in batch 158: 0.190414/0.286987loss in batch 159: 0.300293/0.287079loss in batch 160: 0.127747/0.286087loss in batch 161: 0.266159/0.28595loss in batch 162: 0.477783/0.28714loss in batch 163: 0.140182/0.28624loss in batch 164: 0.0606842/0.284866loss in batch 165: 0.182343/0.284241loss in batch 166: 0.208435/0.283783loss in batch 167: 0.0771332/0.282578loss in batch 168: 0.0509186/0.281189loss in batch 169: 0.294281/0.281265loss in batch 170: 0.211502/0.280869loss in batch 171: 0.122482/0.279938loss in batch 172: 0.0170135/0.278412loss in batch 173: 0.4431/0.279373loss in batch 174: 0.219711/0.279022loss in batch 175: 0.104645/0.278046loss in batch 176: 0.0671844/0.276855loss in batch 177: 0.424561/0.277679loss in batch 178: 0.2276/0.27739loss in batch 179: 0.186523/0.276886loss in batch 180: 0.0415192/0.275589loss in batch 181: 0.173935/0.275024loss in batch 182: 0.167084/0.274445loss in batch 183: 0.406525/0.275162loss in batch 184: 0.208817/0.274811loss in batch 185: 0.287064/0.274872loss in batch 186: 0.496994/0.276062loss in batch 187: 0.526871/0.27739loss in batch 188: 0.122726/0.276566loss in batch 189: 0.0926819/0.275604loss in batch 190: 0.327927/0.275879loss in batch 191: 0.47142/0.276901loss in batch 192: 0.103989/0.276001loss in batch 193: 0.0510712/0.274841loss in batch 194: 0.232513/0.274612loss in batch 195: 0.312866/0.274826loss in batch 196: 0.355087/0.275223loss in batch 197: 0.16954/0.274689loss in batch 198: 0.392334/0.275284loss in batch 199: 0.127228/0.274536loss in batch 200: 0.165375/0.274002loss in batch 201: 0.110611/0.273193loss in batch 202: 0.154663/0.272598loss in batch 203: 0.446579/0.273453loss in batch 204: 0.142624/0.272812loss in batch 205: 0.24939/0.27272loss in batch 206: 0.23465/0.272522loss in batch 207: 0.0723724/0.271561loss in batch 208: 0.309402/0.271744loss in batch 209: 0.0417786/0.270645loss in batch 210: 0.173386/0.270187loss in batch 211: 0.18428/0.269791loss in batch 212: 0.129395/0.269119
done with epoch 3
train_acc: 0.922535 (393/426)
test loss: 0.129395
acc: 0.937063 (134/143)
loss in batch 0: 0.0828247/0.0828247loss in batch 1: 0.425766/0.254303loss in batch 2: 0.0988159/0.202469loss in batch 3: 0.511047/0.279617loss in batch 4: 0.140991/0.251892loss in batch 5: 0.175583/0.239166loss in batch 6: 0.340393/0.253632loss in batch 7: 0.203125/0.247314loss in batch 8: 0.390396/0.263214loss in batch 9: 0.173935/0.254288loss in batch 10: 0.0367126/0.234512loss in batch 11: 0.22493/0.233704loss in batch 12: 0.168457/0.228699loss in batch 13: 0.0988464/0.219406loss in batch 14: 0.229843/0.220108loss in batch 15: 0.130569/0.214508loss in batch 16: 0.229721/0.215408loss in batch 17: 0.344986/0.22261loss in batch 18: 0.106476/0.216492loss in batch 19: 0.326813/0.222015loss in batch 20: 0.0695648/0.214752loss in batch 21: 0.0795593/0.208603loss in batch 22: 0.68251/0.229218loss in batch 23: 0.487961/0.23999loss in batch 24: 0.151672/0.236465loss in batch 25: 0.104919/0.2314loss in batch 26: 0.044632/0.224487loss in batch 27: 0.114502/0.220566loss in batch 28: 0.167969/0.218735loss in batch 29: 0.479446/0.227432loss in batch 30: 0.443054/0.234375loss in batch 31: 0.173676/0.232483loss in batch 32: 0.154449/0.230118loss in batch 33: 0.0757446/0.225586loss in batch 34: 0.130127/0.22287loss in batch 35: 0.619843/0.233887loss in batch 36: 0.532288/0.241943loss in batch 37: 0.449753/0.247421loss in batch 38: 0.0587006/0.242584loss in batch 39: 0.255875/0.24292loss in batch 40: 0.0735321/0.23877loss in batch 41: 0.455658/0.243942loss in batch 42: 0.046051/0.239349loss in batch 43: 0.258514/0.239777loss in batch 44: 0.130493/0.23735loss in batch 45: 0.139465/0.235229loss in batch 46: 0.44162/0.239609loss in batch 47: 0.144989/0.23764loss in batch 48: 0.135269/0.23555loss in batch 49: 0.299042/0.236832loss in batch 50: 0.0415192/0.232986loss in batch 51: 0.110489/0.230637loss in batch 52: 0.660477/0.238754loss in batch 53: 0.237091/0.238708loss in batch 54: 0.0142517/0.234634loss in batch 55: 0.212784/0.234253loss in batch 56: 0.333786/0.235992loss in batch 57: 0.163895/0.234756loss in batch 58: 0.249222/0.235001loss in batch 59: 0.24353/0.235138loss in batch 60: 0.153625/0.233795loss in batch 61: 0.121048/0.231979loss in batch 62: 0.0266266/0.228729loss in batch 63: 0.282761/0.229568loss in batch 64: 0.470444/0.233276loss in batch 65: 0.21463/0.232986loss in batch 66: 0.54808/0.237686loss in batch 67: 0.174164/0.236755loss in batch 68: 0.128159/0.235184loss in batch 69: 0.0654144/0.232758loss in batch 70: 0.730484/0.239777loss in batch 71: 0.687592/0.245987loss in batch 72: 0.410736/0.248245loss in batch 73: 0.228485/0.247971loss in batch 74: 0.0888519/0.24585loss in batch 75: 0.109741/0.244064loss in batch 76: 0.230118/0.243881loss in batch 77: 0.0390778/0.241257loss in batch 78: 0.316391/0.242218loss in batch 79: 0.43988/0.244675loss in batch 80: 0.118011/0.243118loss in batch 81: 0.570724/0.247101loss in batch 82: 0.411942/0.2491loss in batch 83: 0.161255/0.248047loss in batch 84: 0.16156/0.247025loss in batch 85: 0.455933/0.249466loss in batch 86: 0.379288/0.250946loss in batch 87: 0.226303/0.250671loss in batch 88: 0.282852/0.251038loss in batch 89: 0.281052/0.251373loss in batch 90: 0.203918/0.250854loss in batch 91: 0.281677/0.25119loss in batch 92: 0.334122/0.252075loss in batch 93: 0.0894165/0.250351loss in batch 94: 0.0973511/0.248734loss in batch 95: 0.465698/0.250992loss in batch 96: 0.406403/0.252594loss in batch 97: 0.0956116/0.250992loss in batch 98: 0.0818329/0.249283loss in batch 99: 0.286316/0.249664loss in batch 100: 0.0300903/0.247482loss in batch 101: 0.195084/0.246979loss in batch 102: 0.196518/0.246475loss in batch 103: 1.29237/0.256531loss in batch 104: 0.14711/0.255493loss in batch 105: 0.298141/0.25589loss in batch 106: 0.125961/0.254684loss in batch 107: 0.156036/0.253769loss in batch 108: 0.334808/0.254517loss in batch 109: 0.245422/0.254425loss in batch 110: 0.126266/0.253281loss in batch 111: 0.137741/0.252243loss in batch 112: 0.131088/0.25116loss in batch 113: 0.109573/0.249924loss in batch 114: 0.0265198/0.247986loss in batch 115: 0.269073/0.248169loss in batch 116: 0.118271/0.247055loss in batch 117: 0.11203/0.245911loss in batch 118: 0.0454559/0.244232loss in batch 119: 0.103531/0.243057loss in batch 120: 0.263245/0.243225loss in batch 121: 0.887344/0.248489loss in batch 122: 0.130005/0.247543loss in batch 123: 0.0540466/0.245987loss in batch 124: 0.0753326/0.244614loss in batch 125: 0.0675201/0.24321loss in batch 126: 0.0315552/0.241547loss in batch 127: 0.280823/0.241852loss in batch 128: 0.303772/0.242325loss in batch 129: 0.0425415/0.240799loss in batch 130: 0.0789642/0.239563loss in batch 131: 0.211685/0.239334loss in batch 132: 0.0716858/0.238083loss in batch 133: 0.120422/0.237198loss in batch 134: 0.0558319/0.235855loss in batch 135: 0.095108/0.234833loss in batch 136: 0.0203857/0.233261loss in batch 137: 0.177872/0.232864loss in batch 138: 0.120331/0.232056loss in batch 139: 0.194595/0.231796loss in batch 140: 0.233429/0.231796loss in batch 141: 0.389496/0.23291loss in batch 142: 0.960556/0.237991loss in batch 143: 0.260651/0.238159loss in batch 144: 0.42543/0.239441loss in batch 145: 0.417877/0.240677loss in batch 146: 0.105606/0.239746loss in batch 147: 0.388885/0.240753loss in batch 148: 0.0329895/0.239365loss in batch 149: 0.112183/0.23851loss in batch 150: 0.233231/0.238464loss in batch 151: 0.582916/0.240753loss in batch 152: 0.0921326/0.239761loss in batch 153: 0.263809/0.239929loss in batch 154: 0.0666351/0.238815loss in batch 155: 0.122681/0.238068loss in batch 156: 0.266281/0.238251loss in batch 157: 1.21071/0.2444loss in batch 158: 0.194916/0.244095loss in batch 159: 0.0267181/0.242722loss in batch 160: 0.0723267/0.241669loss in batch 161: 0.165329/0.241196loss in batch 162: 0.03862/0.23996loss in batch 163: 0.0381317/0.238724loss in batch 164: 0.270477/0.238907loss in batch 165: 0.185287/0.238586loss in batch 166: 0.483185/0.240051loss in batch 167: 0.236038/0.240036loss in batch 168: 0.0333099/0.238815loss in batch 169: 0.126968/0.238144loss in batch 170: 0.071579/0.237183loss in batch 171: 0.427658/0.238297loss in batch 172: 0.144363/0.237747loss in batch 173: 0.0680695/0.236771loss in batch 174: 0.62027/0.238968loss in batch 175: 0.16243/0.238525loss in batch 176: 0.273041/0.238724loss in batch 177: 0.172379/0.238358loss in batch 178: 0.174515/0.237991loss in batch 179: 0.135468/0.237427loss in batch 180: 0.44313/0.238556loss in batch 181: 0.234711/0.238541loss in batch 182: 0.166916/0.238144loss in batch 183: 0.42009/0.239136loss in batch 184: 0.17836/0.238815loss in batch 185: 0.12236/0.238174loss in batch 186: 0.0386963/0.237106loss in batch 187: 0.0882416/0.236313loss in batch 188: 0.451477/0.237473loss in batch 189: 0.0913544/0.236694loss in batch 190: 0.0578461/0.235748loss in batch 191: 0.187592/0.235504loss in batch 192: 0.122711/0.234924loss in batch 193: 0.148041/0.234467loss in batch 194: 0.0907288/0.233734loss in batch 195: 0.145035/0.233292loss in batch 196: 0.291321/0.233582loss in batch 197: 0.153793/0.233185loss in batch 198: 0.0993958/0.232498loss in batch 199: 1.11772/0.236923loss in batch 200: 0.105637/0.236267loss in batch 201: 0.0184326/0.235199loss in batch 202: 0.172531/0.234894loss in batch 203: 0.346649/0.235428loss in batch 204: 0.0854034/0.234711loss in batch 205: 0.561264/0.236298loss in batch 206: 0.921631/0.239609loss in batch 207: 0.108047/0.238968loss in batch 208: 0.11171/0.238358loss in batch 209: 0.383514/0.239059loss in batch 210: 0.151047/0.238632loss in batch 211: 0.388428/0.239349loss in batch 212: 0.30864/0.23967
done with epoch 4
train_acc: 0.929577 (396/426)
test loss: 0.30864
acc: 0.93007 (133/143)
loss in batch 0: 0.162811/0.162811loss in batch 1: 0.131317/0.147064loss in batch 2: 0.465088/0.253067loss in batch 3: 0.347504/0.276672loss in batch 4: 0.491486/0.319641loss in batch 5: 0.0942535/0.282074loss in batch 6: 0.215271/0.272537loss in batch 7: 0.219513/0.2659loss in batch 8: 0.199921/0.258575loss in batch 9: 0.098114/0.242523loss in batch 10: 0.226547/0.241074loss in batch 11: 0.0919342/0.228653loss in batch 12: 0.351501/0.238098loss in batch 13: 0.669113/0.26889loss in batch 14: 0.0962067/0.25737loss in batch 15: 0.229172/0.255615loss in batch 16: 0.52417/0.271408loss in batch 17: 0.213898/0.268204loss in batch 18: 0.450058/0.277771loss in batch 19: 0.396133/0.283707loss in batch 20: 0.0722504/0.273621loss in batch 21: 0.238297/0.272018loss in batch 22: 0.0959778/0.264359loss in batch 23: 0.0234985/0.254333loss in batch 24: 0.306702/0.256439loss in batch 25: 0.191589/0.253937loss in batch 26: 0.133713/0.249481loss in batch 27: 0.0985413/0.244095loss in batch 28: 0.146774/0.240738loss in batch 29: 0.185471/0.238892loss in batch 30: 0.140839/0.235718loss in batch 31: 0.0612488/0.23027loss in batch 32: 0.0209503/0.223938loss in batch 33: 0.3255/0.226929loss in batch 34: 0.252136/0.227646loss in batch 35: 0.0801392/0.223557loss in batch 36: 0.171234/0.222137loss in batch 37: 0.0249786/0.216934loss in batch 38: 0.130371/0.214722loss in batch 39: 0.350327/0.218109loss in batch 40: 0.113037/0.215546loss in batch 41: 0.126297/0.213425loss in batch 42: 0.0638733/0.209946loss in batch 43: 0.0750122/0.206879loss in batch 44: 0.717896/0.218231loss in batch 45: 0.12735/0.216263loss in batch 46: 0.327881/0.218643loss in batch 47: 0.36026/0.221588loss in batch 48: 0.060791/0.218307loss in batch 49: 0.398163/0.221909loss in batch 50: 0.0974579/0.219452loss in batch 51: 0.0626221/0.216446loss in batch 52: 0.107956/0.214401loss in batch 53: 0.122116/0.212692loss in batch 54: 0.475098/0.217468loss in batch 55: 0.2789/0.218552loss in batch 56: 0.0606842/0.21579loss in batch 57: 0.279251/0.216888loss in batch 58: 0.246155/0.217377loss in batch 59: 0.354568/0.219666loss in batch 60: 0.0959778/0.217636loss in batch 61: 0.192749/0.217224loss in batch 62: 0.171783/0.216507loss in batch 63: 0.563522/0.221939loss in batch 64: 0.0310516/0.219009loss in batch 65: 0.24173/0.219345loss in batch 66: 0.310638/0.220703loss in batch 67: 0.123749/0.219269loss in batch 68: 0.293091/0.220352loss in batch 69: 0.260895/0.220932loss in batch 70: 0.069397/0.218796loss in batch 71: 0.104523/0.217209loss in batch 72: 0.0664825/0.215149loss in batch 73: 0.384796/0.217438loss in batch 74: 0.0651398/0.215408loss in batch 75: 0.218857/0.215454loss in batch 76: 0.0480347/0.213272loss in batch 77: 0.12851/0.212189loss in batch 78: 0.0654144/0.210327loss in batch 79: 0.393494/0.212616loss in batch 80: 0.0784302/0.210953loss in batch 81: 0.537506/0.214951loss in batch 82: 0.305862/0.216049loss in batch 83: 0.135101/0.215073loss in batch 84: 0.571838/0.219269loss in batch 85: 0.564377/0.223297loss in batch 86: 0.499939/0.226471loss in batch 87: 0.107178/0.225113loss in batch 88: 0.0896301/0.223587loss in batch 89: 0.141388/0.222687loss in batch 90: 0.130356/0.221664loss in batch 91: 0.0873566/0.2202loss in batch 92: 0.562653/0.223877loss in batch 93: 0.508423/0.226913loss in batch 94: 0.0657959/0.22522loss in batch 95: 0.142151/0.22435loss in batch 96: 0.375854/0.225922loss in batch 97: 0.107254/0.224701loss in batch 98: 0.306671/0.225525loss in batch 99: 0.0340576/0.223618loss in batch 100: 0.134125/0.222733loss in batch 101: 0.0241089/0.220779loss in batch 102: 0.386215/0.222382loss in batch 103: 0.126923/0.221481loss in batch 104: 0.17981/0.221085loss in batch 105: 0.0328674/0.219299loss in batch 106: 0.550949/0.222412loss in batch 107: 0.0890808/0.221161loss in batch 108: 0.0992126/0.220047loss in batch 109: 1.23222/0.229248loss in batch 110: 0.206421/0.22905loss in batch 111: 0.0621185/0.227554loss in batch 112: 0.237122/0.227631loss in batch 113: 0.0844727/0.226379loss in batch 114: 0.172363/0.225906loss in batch 115: 0.250824/0.22612loss in batch 116: 0.960495/0.232407loss in batch 117: 0.181351/0.231979loss in batch 118: 0.244736/0.232071loss in batch 119: 0.2258/0.232025loss in batch 120: 0.0535736/0.230545loss in batch 121: 0.426987/0.232162loss in batch 122: 0.152802/0.231506loss in batch 123: 0.0898132/0.230377loss in batch 124: 0.252167/0.230545loss in batch 125: 0.479736/0.232529loss in batch 126: 0.0522614/0.23111loss in batch 127: 0.10675/0.230118loss in batch 128: 0.110611/0.229202loss in batch 129: 0.195099/0.228943loss in batch 130: 0.105164/0.227997loss in batch 131: 0.0680237/0.226791loss in batch 132: 0.173004/0.226395loss in batch 133: 0.180176/0.226028loss in batch 134: 0.0658875/0.224854loss in batch 135: 0.278641/0.22525loss in batch 136: 0.0877991/0.224243loss in batch 137: 0.100815/0.223358loss in batch 138: 0.0339355/0.221985loss in batch 139: 0.11496/0.221222loss in batch 140: 0.0992889/0.220367loss in batch 141: 0.0853119/0.219406loss in batch 142: 0.0550537/0.218246loss in batch 143: 0.153122/0.217804loss in batch 144: 0.0829163/0.216873loss in batch 145: 0.10556/0.21611loss in batch 146: 0.0873108/0.21524loss in batch 147: 0.230667/0.215347loss in batch 148: 0.123764/0.214722loss in batch 149: 0.235443/0.214859loss in batch 150: 0.226013/0.214935loss in batch 151: 0.100891/0.214188loss in batch 152: 0.0882874/0.213364loss in batch 153: 0.0980835/0.212616loss in batch 154: 0.415375/0.213928loss in batch 155: 0.0809479/0.213058loss in batch 156: 0.111099/0.212418loss in batch 157: 0.0514374/0.211395loss in batch 158: 0.156601/0.21106loss in batch 159: 0.175735/0.210831loss in batch 160: 0.103867/0.210175loss in batch 161: 0.0814056/0.209366loss in batch 162: 0.125656/0.208862loss in batch 163: 1.3676/0.215942loss in batch 164: 0.21402/0.215927loss in batch 165: 0.504517/0.217667loss in batch 166: 0.113235/0.217041loss in batch 167: 0.127625/0.216492loss in batch 168: 0.0469971/0.2155loss in batch 169: 0.0521698/0.214523loss in batch 170: 0.955719/0.218857loss in batch 171: 0.779526/0.222137loss in batch 172: 0.0701447/0.221237loss in batch 173: 0.184402/0.221039loss in batch 174: 0.452713/0.222366loss in batch 175: 0.028244/0.221252loss in batch 176: 0.0771637/0.220444loss in batch 177: 0.201279/0.220337loss in batch 178: 0.0442352/0.21936loss in batch 179: 0.238113/0.219467loss in batch 180: 0.323486/0.220032loss in batch 181: 0.0929565/0.219345loss in batch 182: 0.0335083/0.218323loss in batch 183: 0.100876/0.217682loss in batch 184: 0.409439/0.218719loss in batch 185: 0.458832/0.220001loss in batch 186: 0.396912/0.220947loss in batch 187: 0.123779/0.220444loss in batch 188: 0.43454/0.221558loss in batch 189: 0.25856/0.221771loss in batch 190: 0.0991669/0.221115loss in batch 191: 0.490036/0.222519loss in batch 192: 0.155487/0.222168loss in batch 193: 0.0959015/0.221527loss in batch 194: 0.54039/0.223175loss in batch 195: 0.111832/0.222595loss in batch 196: 0.842957/0.225739loss in batch 197: 0.0915375/0.225067loss in batch 198: 0.895493/0.228439loss in batch 199: 0.339172/0.228989loss in batch 200: 0.0275879/0.227982loss in batch 201: 0.465546/0.229156loss in batch 202: 0.088028/0.22847loss in batch 203: 0.18158/0.228241loss in batch 204: 0.0474854/0.227356loss in batch 205: 0.135162/0.226898loss in batch 206: 0.0769043/0.226181loss in batch 207: 0.0604706/0.225388loss in batch 208: 0.865997/0.228455loss in batch 209: 0.0890045/0.227798loss in batch 210: 0.05159/0.226959loss in batch 211: 0.0334473/0.226028loss in batch 212: 0.0641174/0.225281
done with epoch 5
train_acc: 0.931925 (397/426)
test loss: 0.0641174
acc: 0.93007 (133/143)
loss in batch 0: 0.0479431/0.0479431loss in batch 1: 0.979309/0.513626loss in batch 2: 0.0923462/0.373199loss in batch 3: 0.0254517/0.286255loss in batch 4: 0.043335/0.237671loss in batch 5: 0.23407/0.237076loss in batch 6: 0.129745/0.221741loss in batch 7: 0.771713/0.290482loss in batch 8: 0.141052/0.27388loss in batch 9: 0.340607/0.280563loss in batch 10: 0.210571/0.274185loss in batch 11: 0.0691833/0.257111loss in batch 12: 0.0774841/0.243301loss in batch 13: 0.0456543/0.229172loss in batch 14: 0.0345306/0.216202loss in batch 15: 0.547287/0.236893loss in batch 16: 0.0912628/0.228317loss in batch 17: 0.0122528/0.216309loss in batch 18: 0.322098/0.221893loss in batch 19: 0.135513/0.21756loss in batch 20: 0.209518/0.217178loss in batch 21: 0.150452/0.214142loss in batch 22: 0.226028/0.214676loss in batch 23: 0.124649/0.210907loss in batch 24: 0.0904846/0.2061loss in batch 25: 0.0948944/0.201828loss in batch 26: 0.264954/0.204163loss in batch 27: 0.494644/0.214523loss in batch 28: 0.0884247/0.21019loss in batch 29: 0.150299/0.208191loss in batch 30: 0.106766/0.20491loss in batch 31: 0.153778/0.203323loss in batch 32: 0.432358/0.210266loss in batch 33: 0.181198/0.209412loss in batch 34: 0.31459/0.212402loss in batch 35: 0.108673/0.209518loss in batch 36: 0.333542/0.212891loss in batch 37: 0.186646/0.212189loss in batch 38: 0.346405/0.215622loss in batch 39: 0.32756/0.21843loss in batch 40: 0.209152/0.218201loss in batch 41: 0.734116/0.230499loss in batch 42: 0.360519/0.233521loss in batch 43: 0.120743/0.230957loss in batch 44: 0.116165/0.228409loss in batch 45: 0.0573883/0.224686loss in batch 46: 0.061554/0.221207loss in batch 47: 0.119644/0.219101loss in batch 48: 0.0823059/0.216309loss in batch 49: 0.15593/0.215103loss in batch 50: 0.059906/0.212051loss in batch 51: 0.135452/0.210571loss in batch 52: 0.0667877/0.20787loss in batch 53: 0.387695/0.211197loss in batch 54: 0.197052/0.210953loss in batch 55: 0.560577/0.217178loss in batch 56: 0.08255/0.214813loss in batch 57: 0.074707/0.212402loss in batch 58: 0.385468/0.215347loss in batch 59: 0.129227/0.213898loss in batch 60: 0.131973/0.21257loss in batch 61: 0.0868835/0.210526loss in batch 62: 0.139465/0.209396loss in batch 63: 0.458847/0.213303loss in batch 64: 0.0380707/0.210602loss in batch 65: 0.123657/0.20929loss in batch 66: 0.0561676/0.207001loss in batch 67: 0.357681/0.209229loss in batch 68: 0.332748/0.211014loss in batch 69: 0.157394/0.210236loss in batch 70: 0.19725/0.210068loss in batch 71: 0.617569/0.215714loss in batch 72: 0.129166/0.214539loss in batch 73: 0.283829/0.215469loss in batch 74: 0.121735/0.214218loss in batch 75: 0.273666/0.214996loss in batch 76: 0.166946/0.214386loss in batch 77: 1.02373/0.224762loss in batch 78: 0.182175/0.224213loss in batch 79: 0.0994263/0.222672loss in batch 80: 0.529068/0.22644loss in batch 81: 0.528915/0.230133loss in batch 82: 0.361542/0.231705loss in batch 83: 0.422882/0.233994loss in batch 84: 0.426865/0.236252loss in batch 85: 0.0981445/0.23465loss in batch 86: 0.227402/0.234573loss in batch 87: 0.544128/0.238083loss in batch 88: 0.0932465/0.236465loss in batch 89: 0.39119/0.238174loss in batch 90: 0.355469/0.239471loss in batch 91: 0.16716/0.238693loss in batch 92: 0.159622/0.237823loss in batch 93: 0.127136/0.236649loss in batch 94: 0.116196/0.235382loss in batch 95: 0.135864/0.234344loss in batch 96: 0.0844727/0.232803loss in batch 97: 0.388931/0.234406loss in batch 98: 0.0779419/0.232819loss in batch 99: 0.0606537/0.231094loss in batch 100: 0.0319824/0.229126loss in batch 101: 0.102051/0.227875loss in batch 102: 0.870819/0.234131loss in batch 103: 0.114532/0.232971loss in batch 104: 0.133667/0.232025loss in batch 105: 0.386307/0.23349loss in batch 106: 0.0617065/0.231873loss in batch 107: 0.0329437/0.230026loss in batch 108: 0.192032/0.229675loss in batch 109: 0.20546/0.229462loss in batch 110: 0.0453186/0.227798loss in batch 111: 0.173889/0.227325loss in batch 112: 0.259186/0.2276loss in batch 113: 0.0280304/0.225861loss in batch 114: 0.50444/0.228271loss in batch 115: 0.160568/0.227692loss in batch 116: 0.301666/0.228333loss in batch 117: 0.211823/0.22818loss in batch 118: 0.543976/0.230835loss in batch 119: 0.0190582/0.22908loss in batch 120: 0.077301/0.227814loss in batch 121: 0.206787/0.227646loss in batch 122: 0.374908/0.228851loss in batch 123: 0.0230713/0.227188loss in batch 124: 0.122681/0.226349loss in batch 125: 0.0219116/0.224731loss in batch 126: 0.556213/0.227341loss in batch 127: 0.286026/0.227798loss in batch 128: 0.259384/0.228043loss in batch 129: 0.130066/0.227295loss in batch 130: 0.0950623/0.226273loss in batch 131: 0.105209/0.225357loss in batch 132: 0.404572/0.226715loss in batch 133: 0.171097/0.226288loss in batch 134: 0.146866/0.225708loss in batch 135: 0.0868683/0.224686loss in batch 136: 0.320801/0.225388loss in batch 137: 0.00946045/0.223831loss in batch 138: 0.348404/0.224716loss in batch 139: 0.138794/0.224106loss in batch 140: 0.0443115/0.222824loss in batch 141: 0.122116/0.222107loss in batch 142: 0.0669708/0.221024loss in batch 143: 0.384918/0.222183loss in batch 144: 0.59491/0.224747loss in batch 145: 0.138733/0.224152loss in batch 146: 0.229889/0.224197loss in batch 147: 0.0128937/0.222763loss in batch 148: 0.136658/0.222183loss in batch 149: 0.151474/0.22171loss in batch 150: 0.119919/0.221039loss in batch 151: 0.0492554/0.21991loss in batch 152: 0.105606/0.219162loss in batch 153: 0.0243683/0.217896loss in batch 154: 0.111526/0.217209loss in batch 155: 0.125046/0.216614loss in batch 156: 0.464005/0.218201loss in batch 157: 0.195297/0.218048loss in batch 158: 0.193909/0.217896loss in batch 159: 0.0929718/0.217117loss in batch 160: 0.18335/0.216904loss in batch 161: 0.07724/0.216049loss in batch 162: 0.0537872/0.215057loss in batch 163: 0.071106/0.214172loss in batch 164: 0.0271606/0.213043loss in batch 165: 0.121689/0.212494loss in batch 166: 0.322968/0.21315loss in batch 167: 0.0363312/0.212097loss in batch 168: 0.0705261/0.211258loss in batch 169: 0.134201/0.2108loss in batch 170: 0.0301208/0.209747loss in batch 171: 0.112427/0.209183loss in batch 172: 0.203003/0.209152loss in batch 173: 0.0709534/0.208359loss in batch 174: 0.0314484/0.207352loss in batch 175: 0.00354004/0.206192loss in batch 176: 0.0255432/0.205154loss in batch 177: 0.369492/0.2061loss in batch 178: 0.0448303/0.2052loss in batch 179: 0.0636292/0.204391loss in batch 180: 0.147949/0.204086loss in batch 181: 0.791504/0.207306loss in batch 182: 0.830017/0.210724loss in batch 183: 0.484528/0.212219loss in batch 184: 0.185135/0.212067loss in batch 185: 0.0395813/0.211121loss in batch 186: 0.427948/0.212296loss in batch 187: 0.109528/0.211746loss in batch 188: 0.137009/0.211349loss in batch 189: 0.324844/0.211945loss in batch 190: 0.481918/0.213364loss in batch 191: 0.196838/0.213287loss in batch 192: 0.188583/0.21315loss in batch 193: 0.0639343/0.212372loss in batch 194: 0.0588684/0.211594loss in batch 195: 0.500458/0.213058loss in batch 196: 0.059494/0.21228loss in batch 197: 0.333633/0.212906loss in batch 198: 0.099411/0.212326loss in batch 199: 1.16983/0.217102loss in batch 200: 0.183197/0.216949loss in batch 201: 0.204483/0.216888loss in batch 202: 0.190399/0.216766loss in batch 203: 0.328125/0.2173loss in batch 204: 0.0811615/0.216629loss in batch 205: 0.0961609/0.216049loss in batch 206: 0.413101/0.21701loss in batch 207: 0.115509/0.216507loss in batch 208: 0.0443268/0.215698loss in batch 209: 0.286133/0.216019loss in batch 210: 0.178253/0.215851loss in batch 211: 0.183548/0.215698loss in batch 212: 0.0920868/0.215118
done with epoch 6
train_acc: 0.92723 (395/426)
test loss: 0.0920868
acc: 0.923077 (132/143)
loss in batch 0: 0.56572/0.56572loss in batch 1: 0.136444/0.351089loss in batch 2: 0.270523/0.324219loss in batch 3: 0.0812225/0.263474loss in batch 4: 0.418594/0.294495loss in batch 5: 0.154037/0.271088loss in batch 6: 0.232529/0.265594loss in batch 7: 0.799301/0.332306loss in batch 8: 0.294174/0.328049loss in batch 9: 0.0763397/0.302887loss in batch 10: 0.0396881/0.278961loss in batch 11: 0.0507813/0.259949loss in batch 12: 0.117386/0.248978loss in batch 13: 0.255936/0.249466loss in batch 14: 0.069397/0.237473loss in batch 15: 0.0485992/0.225677loss in batch 16: 0.106857/0.218674loss in batch 17: 0.181351/0.216599loss in batch 18: 0.361588/0.224228loss in batch 19: 0.224777/0.224274loss in batch 20: 0.0351563/0.215256loss in batch 21: 0.0879974/0.209473loss in batch 22: 0.173676/0.207916loss in batch 23: 0.22673/0.208694loss in batch 24: 0.236588/0.209808loss in batch 25: 0.223419/0.210327loss in batch 26: 0.206131/0.210175loss in batch 27: 0.0820465/0.205597loss in batch 28: 0.122635/0.202744loss in batch 29: 0.131485/0.200363loss in batch 30: 0.0489349/0.19548loss in batch 31: 0.180466/0.195007loss in batch 32: 0.155807/0.193832loss in batch 33: 0.125885/0.191818loss in batch 34: 0.00395203/0.186462loss in batch 35: 0.118439/0.18457loss in batch 36: 0.309998/0.187973loss in batch 37: 0.0721893/0.184906loss in batch 38: 0.21611/0.185715loss in batch 39: 0.037674/0.182022loss in batch 40: 0.627899/0.192886loss in batch 41: 0.0358887/0.189148loss in batch 42: 0.104614/0.187195loss in batch 43: 0.242355/0.188446loss in batch 44: 0.039917/0.185135loss in batch 45: 0.173141/0.184875loss in batch 46: 0.467087/0.190872loss in batch 47: 0.112991/0.189255loss in batch 48: 0.934235/0.204468loss in batch 49: 0.325211/0.206879loss in batch 50: 0.168884/0.206131loss in batch 51: 0.28035/0.20755loss in batch 52: 0.0470886/0.204544loss in batch 53: 0.282059/0.205963loss in batch 54: 0.16655/0.205246loss in batch 55: 0.0977325/0.203323loss in batch 56: 0.375214/0.206345loss in batch 57: 0.172546/0.205765loss in batch 58: 0.133057/0.204529loss in batch 59: 0.411407/0.207977loss in batch 60: 0.0914917/0.20607loss in batch 61: 0.107529/0.204468loss in batch 62: 0.359955/0.20694loss in batch 63: 0.102493/0.205307loss in batch 64: 0.177597/0.20488loss in batch 65: 0.0545654/0.202606loss in batch 66: 0.289032/0.203903loss in batch 67: 0.0606995/0.201782loss in batch 68: 0.0320435/0.199341loss in batch 69: 0.142731/0.198532loss in batch 70: 0.102707/0.197189loss in batch 71: 0.215759/0.197433loss in batch 72: 0.218124/0.197723loss in batch 73: 0.15892/0.197189loss in batch 74: 0.0794373/0.195633loss in batch 75: 0.0740356/0.194031loss in batch 76: 0.0435181/0.192078loss in batch 77: 0.117233/0.191116loss in batch 78: 0.410583/0.193893loss in batch 79: 0.136887/0.193176loss in batch 80: 0.518631/0.197189loss in batch 81: 0.0807648/0.195786loss in batch 82: 0.0957947/0.19458loss in batch 83: 0.046051/0.192795loss in batch 84: 0.0801697/0.191467loss in batch 85: 0.177048/0.191315loss in batch 86: 0.239319/0.191864loss in batch 87: 0.190186/0.191833loss in batch 88: 0.219376/0.192154loss in batch 89: 0.153885/0.191727loss in batch 90: 0.05896/0.190262loss in batch 91: 0.181137/0.19017loss in batch 92: 0.211395/0.190399loss in batch 93: 0.199646/0.190491loss in batch 94: 0.0473785/0.18898loss in batch 95: 0.316589/0.190308loss in batch 96: 0.134201/0.189743loss in batch 97: 0.307816/0.190948loss in batch 98: 0.0765228/0.189789loss in batch 99: 0.3414/0.191299loss in batch 100: 0.454315/0.193909loss in batch 101: 0.264786/0.194595loss in batch 102: 0.402985/0.19664loss in batch 103: 0.0516968/0.195236loss in batch 104: 0.0857086/0.194183loss in batch 105: 0.402313/0.196152loss in batch 106: 0.0598145/0.19487loss in batch 107: 0.0438232/0.193481loss in batch 108: 0.209763/0.193619loss in batch 109: 0.0547333/0.192368loss in batch 110: 0.0847015/0.191406loss in batch 111: 0.0279999/0.189941loss in batch 112: 1.45424/0.201126loss in batch 113: 0.709183/0.205597loss in batch 114: 0.278732/0.206223loss in batch 115: 0.0124969/0.204544loss in batch 116: 0.381836/0.206055loss in batch 117: 0.114243/0.205276loss in batch 118: 0.0328064/0.203842loss in batch 119: 0.146454/0.203354loss in batch 120: 0.0290222/0.20192loss in batch 121: 0.0920258/0.201004loss in batch 122: 0.60376/0.204285loss in batch 123: 0.0987091/0.203445loss in batch 124: 0.13884/0.202911loss in batch 125: 0.133255/0.202362loss in batch 126: 0.0215302/0.200943loss in batch 127: 0.297485/0.201706loss in batch 128: 0.28064/0.202316loss in batch 129: 0.242722/0.202621loss in batch 130: 0.204163/0.202637loss in batch 131: 0.102951/0.201874loss in batch 132: 0.317535/0.202744loss in batch 133: 0.106552/0.202026loss in batch 134: 0.44313/0.203812loss in batch 135: 0.223419/0.203949loss in batch 136: 0.337296/0.204926loss in batch 137: 0.0340576/0.20369loss in batch 138: 0.0553589/0.202637loss in batch 139: 0.0947418/0.201859loss in batch 140: 0.0951843/0.201096loss in batch 141: 0.217682/0.201218loss in batch 142: 0.542267/0.203613loss in batch 143: 0.191269/0.203522loss in batch 144: 0.418884/0.205002loss in batch 145: 0.389374/0.206253loss in batch 146: 0.419266/0.207718loss in batch 147: 0.241684/0.207947loss in batch 148: 0.283905/0.20845loss in batch 149: 0.0930023/0.207687loss in batch 150: 0.111298/0.207047loss in batch 151: 0.0423584/0.205963loss in batch 152: 0.454483/0.207596loss in batch 153: 0.0388336/0.206497loss in batch 154: 0.0115509/0.205231loss in batch 155: 0.217407/0.205307loss in batch 156: 0.0761108/0.204498loss in batch 157: 0.184174/0.204361loss in batch 158: 0.0127869/0.203156loss in batch 159: 0.390579/0.20433loss in batch 160: 0.0655823/0.203461loss in batch 161: 0.124771/0.202988loss in batch 162: 0.0761414/0.202194loss in batch 163: 0.17627/0.202042loss in batch 164: 0.122452/0.201553loss in batch 165: 0.145157/0.201233loss in batch 166: 0.436539/0.202637loss in batch 167: 0.270172/0.203033loss in batch 168: 0.194443/0.202988loss in batch 169: 0.364166/0.203934loss in batch 170: 0.126526/0.203476loss in batch 171: 0.878586/0.207397loss in batch 172: 0.0708313/0.206604loss in batch 173: 0.356171/0.207474loss in batch 174: 0.0702209/0.206696loss in batch 175: 0.247635/0.206909loss in batch 176: 0.333847/0.207642loss in batch 177: 0.12529/0.207184loss in batch 178: 0.331833/0.20787loss in batch 179: 0.0904388/0.207214loss in batch 180: 0.290634/0.207687loss in batch 181: 0.00132751/0.206543loss in batch 182: 0.00732422/0.20546loss in batch 183: 0.232803/0.205612loss in batch 184: 0.314697/0.206192loss in batch 185: 0.0873566/0.205551loss in batch 186: 0.118896/0.205093loss in batch 187: 0.0500946/0.204269loss in batch 188: 0.046463/0.20343loss in batch 189: 0.0901642/0.202835loss in batch 190: 0.215775/0.202911loss in batch 191: 0.228226/0.203033loss in batch 192: 0.0317383/0.202148loss in batch 193: 0.048996/0.20137loss in batch 194: 0.997498/0.205444loss in batch 195: 0.23143/0.205582loss in batch 196: 0.0140533/0.204605loss in batch 197: 0.265244/0.20491loss in batch 198: 0.0441589/0.204102loss in batch 199: 0.0623779/0.2034loss in batch 200: 0.195023/0.203354loss in batch 201: 0.144089/0.203064loss in batch 202: 0.674911/0.205383loss in batch 203: 0.0679016/0.204712loss in batch 204: 0.197739/0.204681loss in batch 205: 0.055603/0.203949loss in batch 206: 0.0498505/0.203201loss in batch 207: 0.289551/0.203613loss in batch 208: 0.143204/0.203323loss in batch 209: 0.487518/0.204681loss in batch 210: 0.38121/0.205521loss in batch 211: 0.129364/0.20517loss in batch 212: 0.142227/0.20488
done with epoch 7
train_acc: 0.943662 (402/426)
test loss: 0.142227
acc: 0.93007 (133/143)
loss in batch 0: 1.33475/1.33475loss in batch 1: 0.0362091/0.685471loss in batch 2: 1.36961/0.913528loss in batch 3: 0.127899/0.717117loss in batch 4: 0.162781/0.606262loss in batch 5: 0.0928802/0.520691loss in batch 6: 0.0837555/0.458267loss in batch 7: 0.672272/0.485016loss in batch 8: 0.0306854/0.43454loss in batch 9: 0.13649/0.404724loss in batch 10: 0.0818024/0.375366loss in batch 11: 0.243454/0.36438loss in batch 12: 0.0156097/0.337555loss in batch 13: 0.228287/0.329758loss in batch 14: 0.312607/0.328598loss in batch 15: 0.165237/0.31839loss in batch 16: 0.697464/0.340683loss in batch 17: 0.0710907/0.325714loss in batch 18: 0.109192/0.314331loss in batch 19: 0.324356/0.314819loss in batch 20: 0.0286102/0.301193loss in batch 21: 0.0393982/0.289291loss in batch 22: 0.0834045/0.28035loss in batch 23: 0.0919342/0.272491loss in batch 24: 0.0666504/0.264252loss in batch 25: 0.384079/0.26886loss in batch 26: 0.104141/0.262772loss in batch 27: 0.149933/0.258728loss in batch 28: 0.188599/0.256317loss in batch 29: 0.0700073/0.250107loss in batch 30: 0.448822/0.256531loss in batch 31: 0.149582/0.253174loss in batch 32: 0.291519/0.254333loss in batch 33: 0.243027/0.254013loss in batch 34: 0.280899/0.254776loss in batch 35: 0.0952606/0.250336loss in batch 36: 0.143814/0.247452loss in batch 37: 0.0454559/0.242157loss in batch 38: 0.144409/0.239639loss in batch 39: 0.109222/0.236389loss in batch 40: 0.109329/0.233276loss in batch 41: 0.0202789/0.22821loss in batch 42: 0.0896759/0.224991loss in batch 43: 0.193069/0.224258loss in batch 44: 0.429245/0.228821loss in batch 45: 0.112701/0.226288loss in batch 46: 0.140671/0.224472loss in batch 47: 0.270386/0.225433loss in batch 48: 1.23041/0.245941loss in batch 49: 0.0979919/0.242981loss in batch 50: 0.165436/0.241455loss in batch 51: 0.0315857/0.237427loss in batch 52: 0.488556/0.242157loss in batch 53: 0.33046/0.24379loss in batch 54: 0.0208588/0.239731loss in batch 55: 0.107101/0.237381loss in batch 56: 0.112183/0.235184loss in batch 57: 0.135513/0.233459loss in batch 58: 0.108368/0.231339loss in batch 59: 0.224121/0.231216loss in batch 60: 0.20137/0.230728loss in batch 61: 0.0296936/0.227478loss in batch 62: 0.192474/0.226944loss in batch 63: 0.00456238/0.22345loss in batch 64: 0.411606/0.226364loss in batch 65: 1.06639/0.239075loss in batch 66: 0.0844269/0.236771loss in batch 67: 0.0278168/0.233688loss in batch 68: 0.452332/0.236862loss in batch 69: 0.129745/0.235336loss in batch 70: 0.061264/0.232895loss in batch 71: 0.290527/0.233688loss in batch 72: 0.251556/0.233932loss in batch 73: 0.0588226/0.231567loss in batch 74: 0.0442047/0.229065loss in batch 75: 0.220596/0.228943loss in batch 76: 0.0235901/0.226288loss in batch 77: 0.0511017/0.224045loss in batch 78: 0.242783/0.224274loss in batch 79: 0.203506/0.22403loss in batch 80: 0.0314789/0.221649loss in batch 81: 0.029953/0.219299loss in batch 82: 0.181641/0.218857loss in batch 83: 0.0798035/0.217194loss in batch 84: 0.349075/0.21875loss in batch 85: 0.063446/0.216949loss in batch 86: 0.708923/0.222595loss in batch 87: 0.109283/0.221298loss in batch 88: 0.0681305/0.219589loss in batch 89: 0.216904/0.219559loss in batch 90: 0.0986938/0.218216loss in batch 91: 0.641602/0.222824loss in batch 92: 0.360687/0.224304loss in batch 93: 0.284637/0.22496loss in batch 94: 0.0592804/0.223206loss in batch 95: 0.108475/0.222015loss in batch 96: 0.100784/0.220764loss in batch 97: 0.297592/0.221558loss in batch 98: 0.0808411/0.220123loss in batch 99: 0.0913086/0.218842loss in batch 100: 0.192398/0.218582loss in batch 101: 0.0606079/0.217026loss in batch 102: 0.253326/0.217377loss in batch 103: 0.465408/0.219772loss in batch 104: 0.56633/0.223068loss in batch 105: 0.0144348/0.2211loss in batch 106: 0.0117645/0.219131loss in batch 107: 0.062973/0.217697loss in batch 108: 0.105453/0.216675loss in batch 109: 0.107407/0.215683loss in batch 110: 0.0435791/0.214127loss in batch 111: 0.273849/0.214645loss in batch 112: 0.0525513/0.213226loss in batch 113: 0.407578/0.21492loss in batch 114: 0.100601/0.213943loss in batch 115: 0.0545044/0.212555loss in batch 116: 0.559265/0.215515loss in batch 117: 0.130981/0.214813loss in batch 118: 0.115326/0.213959loss in batch 119: 0.110397/0.213104loss in batch 120: 0.0301666/0.211594loss in batch 121: 0.0876312/0.210587loss in batch 122: 0.330429/0.211548loss in batch 123: 0.74115/0.21582loss in batch 124: 0.142715/0.21524loss in batch 125: 0.398849/0.21669loss in batch 126: 0.306595/0.217407loss in batch 127: 0.0203247/0.215866loss in batch 128: 0.112381/0.215057loss in batch 129: 0.101059/0.214172loss in batch 130: 0.123428/0.213501loss in batch 131: 0.163742/0.21312loss in batch 132: 0.565887/0.215775loss in batch 133: 0.0833588/0.214783loss in batch 134: 0.0360107/0.213455loss in batch 135: 0.401047/0.214844loss in batch 136: 0.228012/0.214935loss in batch 137: 0.146484/0.214447loss in batch 138: 0.0351715/0.213135loss in batch 139: 0.0543976/0.212006loss in batch 140: 0.283386/0.212524loss in batch 141: 0.0247803/0.211197loss in batch 142: 0.113159/0.21051loss in batch 143: 0.489456/0.212448loss in batch 144: 0.0983734/0.21167loss in batch 145: 0.129272/0.21109loss in batch 146: 0.268463/0.211487loss in batch 147: 0.0448761/0.210358loss in batch 148: 0.387222/0.211548loss in batch 149: 0.0482941/0.210464loss in batch 150: 0.0906982/0.209656loss in batch 151: 0.29213/0.210205loss in batch 152: 0.106186/0.209534loss in batch 153: 0.19725/0.209442loss in batch 154: 0.129639/0.208923loss in batch 155: 0.207596/0.208923loss in batch 156: 0.227493/0.209045loss in batch 157: 0.10173/0.208374loss in batch 158: 0.414734/0.209671loss in batch 159: 0.050827/0.208664loss in batch 160: 0.0131531/0.207458loss in batch 161: 0.526215/0.209412loss in batch 162: 0.109207/0.208801loss in batch 163: 0.0863647/0.208069loss in batch 164: 0.0665436/0.207199loss in batch 165: 0.0675049/0.20636loss in batch 166: 0.128052/0.205902loss in batch 167: 0.0454865/0.204941loss in batch 168: 0.151886/0.20462loss in batch 169: 0.188461/0.204529loss in batch 170: 0.047287/0.203613loss in batch 171: 0.0431671/0.202682loss in batch 172: 0.0616913/0.201874loss in batch 173: 0.159363/0.20163loss in batch 174: 0.0845642/0.200943loss in batch 175: 0.667068/0.203598loss in batch 176: 0.465408/0.205063loss in batch 177: 0.0988007/0.204483loss in batch 178: 0.67157/0.207092loss in batch 179: 0.0602264/0.206268loss in batch 180: 0.0505066/0.205414loss in batch 181: 0.00450134/0.2043loss in batch 182: 0.0299988/0.203354loss in batch 183: 0.0779419/0.202667loss in batch 184: 0.0197296/0.201675loss in batch 185: 0.287582/0.202148loss in batch 186: 0.267975/0.202499loss in batch 187: 0.0308533/0.201599loss in batch 188: 0.0934601/0.201004loss in batch 189: 0.0213776/0.200073loss in batch 190: 0.250763/0.200348loss in batch 191: 0.071106/0.199661loss in batch 192: 0.0179443/0.19873loss in batch 193: 0.0187378/0.197784loss in batch 194: 0.130997/0.197449loss in batch 195: 0.267853/0.1978loss in batch 196: 0.0503387/0.197067loss in batch 197: 0.380493/0.197983loss in batch 198: 0.226303/0.198135loss in batch 199: 0.932861/0.201797loss in batch 200: 0.575211/0.203659loss in batch 201: 0.0584259/0.202942loss in batch 202: 0.126511/0.20256loss in batch 203: 0.132401/0.202225loss in batch 204: 0.233841/0.202362loss in batch 205: 0.129883/0.202026loss in batch 206: 0.020462/0.201141loss in batch 207: 0.292587/0.201584loss in batch 208: 0.0436554/0.200836loss in batch 209: 0.0205994/0.199966loss in batch 210: 0.0791626/0.199402loss in batch 211: 0.0590973/0.198746loss in batch 212: 0.0463104/0.198029
done with epoch 8
train_acc: 0.93662 (399/426)
test loss: 0.0463104
acc: 0.937063 (134/143)
loss in batch 0: 0.0327454/0.0327454loss in batch 1: 0.148438/0.0905914loss in batch 2: 0.41597/0.199051loss in batch 3: 0.0597992/0.164246loss in batch 4: 0.359985/0.203384loss in batch 5: 0.247528/0.210739loss in batch 6: 0.524826/0.255615loss in batch 7: 0.288284/0.259689loss in batch 8: 0.397217/0.274979loss in batch 9: 0.0979614/0.257278loss in batch 10: 0.121155/0.244904loss in batch 11: 0.0129852/0.225571loss in batch 12: 0.224121/0.225464loss in batch 13: 0.410126/0.238647loss in batch 14: 0.463196/0.253616loss in batch 15: 0.424881/0.264328loss in batch 16: 0.0400543/0.251129loss in batch 17: 0.0463715/0.239761loss in batch 18: 0.134537/0.234222loss in batch 19: 0.402237/0.24263loss in batch 20: 0.259338/0.243423loss in batch 21: 0.15863/0.239563loss in batch 22: 0.0980988/0.233414loss in batch 23: 0.0512238/0.22583loss in batch 24: 0.0180206/0.217499loss in batch 25: 0.173874/0.21582loss in batch 26: 0.154709/0.213562loss in batch 27: 0.173355/0.212128loss in batch 28: 0.193909/0.211502loss in batch 29: 0.0553894/0.206299loss in batch 30: 0.242371/0.207458loss in batch 31: 0.0331726/0.202011loss in batch 32: 0.0753021/0.198181loss in batch 33: 0.341339/0.202393loss in batch 34: 0.0972748/0.199387loss in batch 35: 0.0713959/0.195831loss in batch 36: 0.167679/0.195068loss in batch 37: 0.0660553/0.191681loss in batch 38: 1.24142/0.218582loss in batch 39: 0.0609436/0.214645loss in batch 40: 0.0687866/0.21109loss in batch 41: 0.219269/0.211273loss in batch 42: 0.0135803/0.206696loss in batch 43: 0.0577393/0.203308loss in batch 44: 0.770233/0.215897loss in batch 45: 0.125687/0.213943loss in batch 46: 0.194855/0.213531loss in batch 47: 0.0150146/0.209396loss in batch 48: 0.0625763/0.206406loss in batch 49: 0.645264/0.215179loss in batch 50: 0.146973/0.213837loss in batch 51: 0.201584/0.213593loss in batch 52: 0.0837402/0.211151loss in batch 53: 0.107895/0.209244loss in batch 54: 0.0615845/0.206558loss in batch 55: 0.61203/0.213791loss in batch 56: 0.0911407/0.211639loss in batch 57: 0.0544891/0.208939loss in batch 58: 0.137115/0.207733loss in batch 59: 0.0225983/0.204636loss in batch 60: 0.0476074/0.202057loss in batch 61: 0.247223/0.202789loss in batch 62: 0.048996/0.200348loss in batch 63: 0.0146637/0.197449loss in batch 64: 0.0329895/0.194916loss in batch 65: 0.116806/0.193726loss in batch 66: 0.0336914/0.191345loss in batch 67: 0.0327606/0.189011loss in batch 68: 0.0522766/0.187027loss in batch 69: 0.190323/0.187073loss in batch 70: 0.071991/0.185471loss in batch 71: 0.289749/0.186905loss in batch 72: 0.493668/0.191116loss in batch 73: 0.223343/0.191544loss in batch 74: 0.0976257/0.190292loss in batch 75: 0.428513/0.193436loss in batch 76: 0.0252991/0.191238loss in batch 77: 0.0868835/0.189896loss in batch 78: 0.139389/0.18927loss in batch 79: 0.0741577/0.18782loss in batch 80: 0.0372467/0.185959loss in batch 81: 0.0446625/0.184235loss in batch 82: 0.156952/0.183914loss in batch 83: 0.0085907/0.181824loss in batch 84: 0.121872/0.181122loss in batch 85: 0.333115/0.182892loss in batch 86: 0.533127/0.18692loss in batch 87: 0.816269/0.194061loss in batch 88: 0.117081/0.193192loss in batch 89: 0.370422/0.195175loss in batch 90: 0.108505/0.194214loss in batch 91: 0.0224152/0.192352loss in batch 92: 0.249207/0.192963loss in batch 93: 0.0260315/0.191177loss in batch 94: 0.098999/0.190216loss in batch 95: 0.0937805/0.189209loss in batch 96: 0.0995941/0.188278loss in batch 97: 0.0376587/0.186737loss in batch 98: 0.0205994/0.185074loss in batch 99: 0.274048/0.185974loss in batch 100: 0.0949402/0.185059loss in batch 101: 0.0296173/0.183548loss in batch 102: 0.454819/0.186172loss in batch 103: 0.167908/0.185989loss in batch 104: 0.131989/0.185486loss in batch 105: 0.0877838/0.184555loss in batch 106: 0.0795746/0.183578loss in batch 107: 0.496323/0.186462loss in batch 108: 0.172318/0.18634loss in batch 109: 0.23407/0.186783loss in batch 110: 0.380508/0.188522loss in batch 111: 0.194199/0.188568loss in batch 112: 0.439682/0.190796loss in batch 113: 0.103745/0.190033loss in batch 114: 0.0116882/0.188477loss in batch 115: 0.136322/0.188034loss in batch 116: 0.0620575/0.186951loss in batch 117: 0.768234/0.191879loss in batch 118: 0.270813/0.192551loss in batch 119: 0.0737152/0.191559loss in batch 120: 0.238968/0.19194loss in batch 121: 0.0649719/0.190903loss in batch 122: 0.0450745/0.189713loss in batch 123: 0.118744/0.189148loss in batch 124: 0.127625/0.18866loss in batch 125: 0.0990601/0.187943loss in batch 126: 0.0562439/0.186905loss in batch 127: 0.0218658/0.185623loss in batch 128: 0.0336761/0.184448loss in batch 129: 0.154465/0.184204loss in batch 130: 0.286667/0.184998loss in batch 131: 0.0147095/0.183701loss in batch 132: 0.0764008/0.182892loss in batch 133: 0.0293274/0.181747loss in batch 134: 0.560303/0.184555loss in batch 135: 0.0600281/0.18364loss in batch 136: 0.0188293/0.182434loss in batch 137: 0.120483/0.181992loss in batch 138: 0.238342/0.182388loss in batch 139: 0.101257/0.181808loss in batch 140: 0.201431/0.181946loss in batch 141: 0.073288/0.181183loss in batch 142: 0.175995/0.181152loss in batch 143: 0.0655365/0.180344loss in batch 144: 0.11792/0.179916loss in batch 145: 0.10965/0.179428loss in batch 146: 0.17807/0.179428loss in batch 147: 0.322144/0.180389loss in batch 148: 0.0996246/0.17984loss in batch 149: 0.302917/0.180679loss in batch 150: 0.0973358/0.180115loss in batch 151: 0.355713/0.181259loss in batch 152: 0.449692/0.183029loss in batch 153: 0.394974/0.184402loss in batch 154: 0.458435/0.186157loss in batch 155: 0.068222/0.18541loss in batch 156: 0.588974/0.187988loss in batch 157: 0.134033/0.187637loss in batch 158: 0.036438/0.186691loss in batch 159: 0.341568/0.187653loss in batch 160: 0.0848083/0.187027loss in batch 161: 0.0836029/0.186371loss in batch 162: 0.0886993/0.185791loss in batch 163: 0.304749/0.186508loss in batch 164: 0.341339/0.187454loss in batch 165: 0.175827/0.187378loss in batch 166: 0.0637817/0.18663loss in batch 167: 0.0211792/0.185654loss in batch 168: 0.0114288/0.184631loss in batch 169: 0.059906/0.183884loss in batch 170: 0.0184784/0.182922loss in batch 171: 0.056076/0.18219loss in batch 172: 0.150848/0.182007loss in batch 173: 0.946884/0.186401loss in batch 174: 0.121567/0.18602loss in batch 175: 0.0678558/0.185364loss in batch 176: 0.114319/0.184967loss in batch 177: 0.0677643/0.184296loss in batch 178: 0.0296783/0.183426loss in batch 179: 0.416275/0.184723loss in batch 180: 0.223602/0.184952loss in batch 181: 0.195648/0.184998loss in batch 182: 0.386536/0.186096loss in batch 183: 0.0753784/0.185501loss in batch 184: 0.399063/0.186661loss in batch 185: 0.0521088/0.185928loss in batch 186: 0.118484/0.185562loss in batch 187: 0.204132/0.185669loss in batch 188: 0.105957/0.185242loss in batch 189: 0.192566/0.185287loss in batch 190: 0.0567169/0.184601loss in batch 191: 0.00463867/0.183685loss in batch 192: 0.574387/0.185699loss in batch 193: 0.395081/0.186768loss in batch 194: 1.23143/0.192139loss in batch 195: 0.211517/0.19223loss in batch 196: 0.415329/0.193359loss in batch 197: 0.149017/0.193146loss in batch 198: 0.309631/0.193741loss in batch 199: 0.103958/0.193268loss in batch 200: 0.0585632/0.192612loss in batch 201: 0.0538177/0.191925loss in batch 202: 0.184952/0.191895loss in batch 203: 0.250122/0.192169loss in batch 204: 0.108765/0.191772loss in batch 205: 0.0654755/0.191147loss in batch 206: 0.170654/0.191055loss in batch 207: 0.195526/0.191071loss in batch 208: 0.115707/0.19072loss in batch 209: 0.0121918/0.189865loss in batch 210: 0.0905914/0.189392loss in batch 211: 0.113861/0.189026loss in batch 212: 0.042572/0.188339
done with epoch 9
train_acc: 0.934272 (398/426)
test loss: 0.042572
acc: 0.93007 (133/143)
loss in batch 0: 0.070694/0.070694loss in batch 1: 0.00431824/0.0375061loss in batch 2: 0.0935516/0.0561829loss in batch 3: 0.312469/0.120255loss in batch 4: 0.107224/0.117645loss in batch 5: 0.882141/0.245071loss in batch 6: 0.42366/0.270584loss in batch 7: 0.242111/0.267014loss in batch 8: 0.311127/0.271927loss in batch 9: 0.412338/0.285965loss in batch 10: 0.0703125/0.266357loss in batch 11: 0.15007/0.256668loss in batch 12: 0.038559/0.239899loss in batch 13: 0.452423/0.255081loss in batch 14: 0.452164/0.268204loss in batch 15: 0.369965/0.274567loss in batch 16: 0.0263367/0.259964loss in batch 17: 0.115906/0.251968loss in batch 18: 0.0744019/0.242615loss in batch 19: 0.0484924/0.23291loss in batch 20: 0.35141/0.238556loss in batch 21: 0.303787/0.241516loss in batch 22: 0.0448456/0.232971loss in batch 23: 0.171951/0.230438loss in batch 24: 0.0437622/0.222961loss in batch 25: 0.022583/0.21524loss in batch 26: 0.192795/0.214432loss in batch 27: 0.757767/0.233826loss in batch 28: 0.0470581/0.227386loss in batch 29: 0.18364/0.225937loss in batch 30: 0.582306/0.237411loss in batch 31: 0.0393982/0.231232loss in batch 32: 0.122864/0.227951loss in batch 33: 0.0165253/0.221741loss in batch 34: 0.381668/0.226303loss in batch 35: 0.0563049/0.221588loss in batch 36: 0.0574951/0.217148loss in batch 37: 0.0339966/0.212326loss in batch 38: 0.0135345/0.20723loss in batch 39: 0.0574341/0.203476loss in batch 40: 0.026886/0.199188loss in batch 41: 0.273621/0.200943loss in batch 42: 0.445328/0.206635loss in batch 43: 0.071991/0.203568loss in batch 44: 0.0930023/0.201111loss in batch 45: 0.13681/0.199707loss in batch 46: 0.0648804/0.196854loss in batch 47: 0.0357208/0.193497loss in batch 48: 0.0314941/0.190186loss in batch 49: 0.245544/0.191299loss in batch 50: 0.105347/0.189606loss in batch 51: 0.660202/0.198654loss in batch 52: 0.293655/0.200455loss in batch 53: 0.028717/0.197281loss in batch 54: 0.0139771/0.193939loss in batch 55: 0.245743/0.194855loss in batch 56: 0.0809326/0.192856loss in batch 57: 0.0481415/0.190369loss in batch 58: 0.0504608/0.188004loss in batch 59: 0.101959/0.186569loss in batch 60: 0.118225/0.18544loss in batch 61: 0.0374756/0.183044loss in batch 62: 0.439774/0.187134loss in batch 63: 0.264587/0.188339loss in batch 64: 0.181259/0.188232loss in batch 65: 0.0870514/0.186707loss in batch 66: 0.0854797/0.185196loss in batch 67: 0.0204163/0.182755loss in batch 68: 0.0552063/0.180923loss in batch 69: 0.328781/0.183029loss in batch 70: 0.072113/0.181473loss in batch 71: 0.190002/0.18158loss in batch 72: 1.49142/0.199524loss in batch 73: 0.129333/0.198578loss in batch 74: 0.0469208/0.196564loss in batch 75: 0.0902557/0.19516loss in batch 76: 0.39563/0.197769loss in batch 77: 0.085144/0.19632loss in batch 78: 0.203735/0.196411loss in batch 79: 0.0454254/0.194519loss in batch 80: 0.133606/0.193771loss in batch 81: 0.185089/0.193665loss in batch 82: 0.541672/0.197861loss in batch 83: 0.142288/0.197189loss in batch 84: 0.317688/0.198608loss in batch 85: 0.13475/0.197876loss in batch 86: 0.365738/0.199799loss in batch 87: 0.021286/0.197784loss in batch 88: 0.0809021/0.196457loss in batch 89: 0.0444794/0.194778loss in batch 90: 0.0887299/0.193604loss in batch 91: 0.0357666/0.191895loss in batch 92: 0.153473/0.191467loss in batch 93: 0.0740967/0.190216loss in batch 94: 0.210373/0.19043loss in batch 95: 0.44397/0.193085loss in batch 96: 0.0405884/0.191513loss in batch 97: 1.54898/0.205368loss in batch 98: 0.471863/0.208054loss in batch 99: 0.10466/0.207016loss in batch 100: 0.105911/0.206024loss in batch 101: 0.13829/0.205353loss in batch 102: 0.115097/0.204483loss in batch 103: 0.0788727/0.203262loss in batch 104: 0.320099/0.204376loss in batch 105: 0.341125/0.205673loss in batch 106: 0.112869/0.204819loss in batch 107: 0.0527344/0.2034loss in batch 108: 0.0992889/0.202438loss in batch 109: 0.0901031/0.201416loss in batch 110: 0.153/0.200989loss in batch 111: 0.0644684/0.199768loss in batch 112: 0.108261/0.198959loss in batch 113: 0.604996/0.202515loss in batch 114: 0.0875854/0.201508loss in batch 115: 0.382767/0.203079loss in batch 116: 0.014679/0.201462loss in batch 117: 0.202316/0.201477loss in batch 118: 0.0452881/0.200165loss in batch 119: 0.0930481/0.199265loss in batch 120: 0.024765/0.19783loss in batch 121: 0.00834656/0.196274loss in batch 122: 0.00567627/0.194717loss in batch 123: 0.101501/0.19397loss in batch 124: 0.00442505/0.192459loss in batch 125: 0.167786/0.192261loss in batch 126: 0.453522/0.194321loss in batch 127: 0.216522/0.194504loss in batch 128: 0.0327606/0.193237loss in batch 129: 0.117172/0.192657loss in batch 130: 0.0331726/0.191437loss in batch 131: 0.120682/0.190903loss in batch 132: 0.00556946/0.189514loss in batch 133: 0.485275/0.191711loss in batch 134: 0.349762/0.192886loss in batch 135: 0.382935/0.194275loss in batch 136: 0.116287/0.193726loss in batch 137: 0.430984/0.195435loss in batch 138: 0.0890198/0.194672loss in batch 139: 0.0366058/0.193542loss in batch 140: 0.143509/0.193192loss in batch 141: 0.416565/0.194748loss in batch 142: 0.535233/0.197128loss in batch 143: 0.338272/0.19812loss in batch 144: 0.0756073/0.197281loss in batch 145: 0.0692902/0.196396loss in batch 146: 0.124222/0.195908loss in batch 147: 0.0895081/0.195175loss in batch 148: 0.449585/0.196899loss in batch 149: 0.0186005/0.195709loss in batch 150: 0.0301514/0.194611loss in batch 151: 0.187332/0.194565loss in batch 152: 0.10257/0.193954loss in batch 153: 0.109619/0.19342loss in batch 154: 0.356354/0.194458loss in batch 155: 0.412064/0.195862loss in batch 156: 0.0222473/0.194748loss in batch 157: 0.0564117/0.193878loss in batch 158: 0.583405/0.19632loss in batch 159: 0.0291595/0.195282loss in batch 160: 0.0178528/0.194183loss in batch 161: 0.021347/0.193115loss in batch 162: 0.308685/0.193817loss in batch 163: 0.0434418/0.192902loss in batch 164: 0.0780182/0.192215loss in batch 165: 0.0468903/0.19133loss in batch 166: 0.0443573/0.19046loss in batch 167: 0.0734711/0.189758loss in batch 168: 0.0657043/0.189026loss in batch 169: 0.146606/0.188766loss in batch 170: 0.0986786/0.188248loss in batch 171: 0.119949/0.187851loss in batch 172: 0.0475311/0.187042loss in batch 173: 0.0206451/0.186081loss in batch 174: 0.146835/0.185852loss in batch 175: 0.0827789/0.185272loss in batch 176: 0.00543213/0.18425loss in batch 177: 0.416183/0.185562loss in batch 178: 0.0172424/0.184616loss in batch 179: 0.0163879/0.183685loss in batch 180: 0.0316467/0.182846loss in batch 181: 0.048172/0.182114loss in batch 182: 0.106567/0.181686loss in batch 183: 0.0497894/0.180984loss in batch 184: 0.0298309/0.180161loss in batch 185: 0.0989075/0.179718loss in batch 186: 0.294601/0.180328loss in batch 187: 0.032608/0.17955loss in batch 188: 0.25087/0.179916loss in batch 189: 0.246765/0.180283loss in batch 190: 0.697189/0.182983loss in batch 191: 0.0217285/0.182144loss in batch 192: 0.0691986/0.181564loss in batch 193: 0.2565/0.181946loss in batch 194: 0.0878296/0.181473loss in batch 195: 0.500336/0.18309loss in batch 196: 0.545288/0.184921loss in batch 197: 0.0931244/0.184464loss in batch 198: 0.279205/0.184937loss in batch 199: 0.185791/0.184952loss in batch 200: 0.173508/0.184891loss in batch 201: 0.174805/0.18483loss in batch 202: 0.060257/0.184219loss in batch 203: 0.233032/0.184479loss in batch 204: 0.396027/0.185501loss in batch 205: 0.128952/0.185226loss in batch 206: 0.150986/0.185059loss in batch 207: 0.118942/0.184738loss in batch 208: 0.0321503/0.184006loss in batch 209: 0.058075/0.183411loss in batch 210: 0.0737457/0.182892loss in batch 211: 0.0465698/0.182251loss in batch 212: 0.403702/0.183289
done with epoch 10
train_acc: 0.934272 (398/426)
test loss: 0.403702
acc: 0.944056 (135/143)
loss in batch 0: 0.0294647/0.0294647loss in batch 1: 0.134964/0.0822144loss in batch 2: 0.38089/0.181763loss in batch 3: 0.0710297/0.154099loss in batch 4: 0.204788/0.16423loss in batch 5: 0.177811/0.166504loss in batch 6: 0.065033/0.151993loss in batch 7: 0.103409/0.14592loss in batch 8: 0.248947/0.157364loss in batch 9: 0.0903778/0.150665loss in batch 10: 0.298752/0.164124loss in batch 11: 0.318222/0.176971loss in batch 12: 0.183594/0.17749loss in batch 13: 0.0314941/0.167053loss in batch 14: 0.141602/0.165359loss in batch 15: 0.135651/0.163513loss in batch 16: 0.25885/0.169113loss in batch 17: 0.191589/0.170364loss in batch 18: 0.0668488/0.164917loss in batch 19: 0.244781/0.168915loss in batch 20: 0.103745/0.165802loss in batch 21: 0.0491943/0.160492loss in batch 22: 0.182739/0.161469loss in batch 23: 0.0192871/0.155533loss in batch 24: 0.0765533/0.15239loss in batch 25: 0.277863/0.157211loss in batch 26: 0.0658417/0.153824loss in batch 27: 0.0484772/0.150055loss in batch 28: 0.400391/0.158691loss in batch 29: 0.157242/0.158661loss in batch 30: 0.41037/0.166763loss in batch 31: 0.114029/0.165115loss in batch 32: 0.138428/0.164307loss in batch 33: 0.0645142/0.161377loss in batch 34: 0.0377655/0.157852loss in batch 35: 0.197083/0.158936loss in batch 36: 0.0697479/0.156525loss in batch 37: 0.060318/0.153992loss in batch 38: 0.411148/0.160599loss in batch 39: 0.0962524/0.158981loss in batch 40: 0.0980682/0.157501loss in batch 41: 0.277222/0.160355loss in batch 42: 0.204071/0.161362loss in batch 43: 0.0377808/0.158554loss in batch 44: 0.0663605/0.156509loss in batch 45: 0.397964/0.161743loss in batch 46: 0.0669556/0.159729loss in batch 47: 0.0559845/0.157578loss in batch 48: 0.0266876/0.154907loss in batch 49: 0.209991/0.156006loss in batch 50: 0.0588074/0.154099loss in batch 51: 0.444504/0.159683loss in batch 52: 0.0860443/0.158279loss in batch 53: 0.0334625/0.155975loss in batch 54: 0.145279/0.155792loss in batch 55: 0.198273/0.156555loss in batch 56: 0.00697327/0.153931loss in batch 57: 0.0830078/0.152695loss in batch 58: 0.457596/0.157867loss in batch 59: 0.139603/0.157562loss in batch 60: 0.0574493/0.155914loss in batch 61: 0.195679/0.156555loss in batch 62: 0.244156/0.157959loss in batch 63: 0.0385437/0.156082loss in batch 64: 0.0687256/0.154739loss in batch 65: 0.0609436/0.15332loss in batch 66: 0.240692/0.154633loss in batch 67: 0.0800323/0.153534loss in batch 68: 0.422516/0.157425loss in batch 69: 0.042511/0.155777loss in batch 70: 0.254639/0.157166loss in batch 71: 0.0359344/0.155487loss in batch 72: 0.0374298/0.15387loss in batch 73: 0.494583/0.158478loss in batch 74: 0.64537/0.164978loss in batch 75: 0.0682983/0.163696loss in batch 76: 0.153198/0.163559loss in batch 77: 0.0148926/0.161667loss in batch 78: 0.0242157/0.159912loss in batch 79: 0.0733948/0.158844loss in batch 80: 0.227127/0.159683loss in batch 81: 0.0121613/0.157883loss in batch 82: 0.259155/0.159103loss in batch 83: 0.347397/0.161346loss in batch 84: 0.0842285/0.160431loss in batch 85: 0.160477/0.160446loss in batch 86: 0.0752411/0.159454loss in batch 87: 0.033905/0.158035loss in batch 88: 0.776978/0.164978loss in batch 89: 0.144867/0.164764loss in batch 90: 0.592026/0.169464loss in batch 91: 0.11525/0.168854loss in batch 92: 0.227661/0.169495loss in batch 93: 0.0263214/0.167969loss in batch 94: 0.0357666/0.16658loss in batch 95: 0.0297241/0.165161loss in batch 96: 0.0618286/0.164093loss in batch 97: 0.173141/0.164185loss in batch 98: 0.0419312/0.162949loss in batch 99: 0.0539093/0.161865loss in batch 100: 0.0532837/0.160782loss in batch 101: 0.0982513/0.160172loss in batch 102: 0.0930328/0.159515loss in batch 103: 1.52588e-05/0.15799loss in batch 104: 0.0714722/0.157166loss in batch 105: 0.0633392/0.156265loss in batch 106: 0.0465393/0.155258loss in batch 107: 0.232773/0.155975loss in batch 108: 0.237076/0.156708loss in batch 109: 0.0561981/0.155792loss in batch 110: 0.0735931/0.15506loss in batch 111: 0.00617981/0.153717loss in batch 112: 0.0608063/0.152908loss in batch 113: 0.00587463/0.151611loss in batch 114: 0.122513/0.151367loss in batch 115: 0.120743/0.151108loss in batch 116: 0.078186/0.150467loss in batch 117: 0.0685883/0.14978loss in batch 118: 0.291229/0.15097loss in batch 119: 0.0512238/0.150131loss in batch 120: 0.268829/0.151123loss in batch 121: 0.0127869/0.149979loss in batch 122: 0.139481/0.149902loss in batch 123: 0.755096/0.154785loss in batch 124: 0.126984/0.154572loss in batch 125: 1.12537/0.162277loss in batch 126: 0.0147858/0.161102loss in batch 127: 0.0897064/0.160553loss in batch 128: 0.0363617/0.159592loss in batch 129: 0.0601196/0.158829loss in batch 130: 0.19487/0.159103loss in batch 131: 0.230774/0.159637loss in batch 132: 0.22319/0.16011loss in batch 133: 0.490036/0.162582loss in batch 134: 0.062088/0.161835loss in batch 135: 0.298584/0.162842loss in batch 136: 0.0282288/0.16185loss in batch 137: 0.152802/0.161789loss in batch 138: 0.0519104/0.160995loss in batch 139: 0.0169678/0.159973loss in batch 140: 0.00466919/0.158875loss in batch 141: 0.089798/0.158386loss in batch 142: 0.417618/0.160202loss in batch 143: 0.0139465/0.15918loss in batch 144: 0.188736/0.159378loss in batch 145: 0.288467/0.160263loss in batch 146: 0.14122/0.160126loss in batch 147: 0.297394/0.161057loss in batch 148: 1.30255/0.168732loss in batch 149: 0.157745/0.168655loss in batch 150: 0.395432/0.170151loss in batch 151: 0.0308685/0.169235loss in batch 152: 0.0146484/0.168228loss in batch 153: 0.188965/0.16835loss in batch 154: 0.0225525/0.167419loss in batch 155: 0.0335083/0.166565loss in batch 156: 0.0319214/0.16571loss in batch 157: 0.0604248/0.165039loss in batch 158: 0.0422516/0.164276loss in batch 159: 0.0500793/0.163559loss in batch 160: 0.866867/0.167923loss in batch 161: 0.0530548/0.167221loss in batch 162: 0.0205536/0.166321loss in batch 163: 0.066803/0.16571loss in batch 164: 0.0605621/0.16507loss in batch 165: 0.0943298/0.164642loss in batch 166: 0.0111389/0.163727loss in batch 167: 0.184357/0.163849loss in batch 168: 0.0471039/0.163162loss in batch 169: 0.153534/0.163101loss in batch 170: 0.00917053/0.162201loss in batch 171: 0.504959/0.1642loss in batch 172: 0.189728/0.164337loss in batch 173: 0.0580902/0.163727loss in batch 174: 0.0642395/0.163162loss in batch 175: 0.356644/0.164261loss in batch 176: 0.276505/0.164902loss in batch 177: 0.390564/0.166153loss in batch 178: 0.46402/0.167831loss in batch 179: 0.373871/0.168976loss in batch 180: 1.02007/0.17366loss in batch 181: 0.0185852/0.172821loss in batch 182: 0.121521/0.172546loss in batch 183: 0.0626984/0.171936loss in batch 184: 0.0178528/0.171112loss in batch 185: 0.0457306/0.170441loss in batch 186: 0.125488/0.170197loss in batch 187: 1.19684/0.175659loss in batch 188: 0.279587/0.176208loss in batch 189: 0.106537/0.175842loss in batch 190: 0.326065/0.17662loss in batch 191: 0.0123291/0.175766loss in batch 192: 0.343002/0.176636loss in batch 193: 0.0304718/0.175888loss in batch 194: 0.0244751/0.17511loss in batch 195: 0.760254/0.178101loss in batch 196: 0.541214/0.179947loss in batch 197: 0.250275/0.180283loss in batch 198: 0.430267/0.181549loss in batch 199: 0.273132/0.182007loss in batch 200: 0.151855/0.181854loss in batch 201: 0.0410461/0.181152loss in batch 202: 0.557877/0.183014loss in batch 203: 0.129852/0.182739loss in batch 204: 0.0484772/0.182098loss in batch 205: 0.0144043/0.18129loss in batch 206: 0.205261/0.181396loss in batch 207: 0.0431213/0.18074loss in batch 208: 0.314941/0.181381loss in batch 209: 0.0228577/0.180618loss in batch 210: 0.203125/0.18074loss in batch 211: 0.378738/0.181656loss in batch 212: 0.0829315/0.181198
done with epoch 11
train_acc: 0.934272 (398/426)
test loss: 0.0829315
acc: 0.937063 (134/143)
loss in batch 0: 0.147568/0.147568loss in batch 1: 0.128922/0.138245loss in batch 2: 0.107315/0.12793loss in batch 3: 0.160477/0.136078loss in batch 4: 0.527664/0.214386loss in batch 5: 0.0552216/0.187866loss in batch 6: 0.0991211/0.175186loss in batch 7: 1.54362/0.346237loss in batch 8: 0.105698/0.319504loss in batch 9: 0.0127869/0.288849loss in batch 10: 0.49675/0.307739loss in batch 11: 0.29361/0.306564loss in batch 12: 0.057312/0.287399loss in batch 13: 0.125381/0.275818loss in batch 14: 0.0166321/0.25853loss in batch 15: 0.197479/0.254715loss in batch 16: 0.102982/0.245789loss in batch 17: 0.050705/0.234955loss in batch 18: 0.260406/0.236298loss in batch 19: 0.19278/0.234131loss in batch 20: 0.0774078/0.226669loss in batch 21: 0.0532227/0.218765loss in batch 22: 0.0931396/0.213318loss in batch 23: 0.390457/0.220703loss in batch 24: 0.0266113/0.212921loss in batch 25: 0.51442/0.224533loss in batch 26: 0.11232/0.220383loss in batch 27: 0.10025/0.21608loss in batch 28: 0.13559/0.213303loss in batch 29: 0.585114/0.225708loss in batch 30: 0.0374603/0.21962loss in batch 31: 0.0191803/0.213364loss in batch 32: 0.02005/0.207504loss in batch 33: 0.0272369/0.202209loss in batch 34: 0.0132446/0.196808loss in batch 35: 0.0996246/0.194107loss in batch 36: 0.0786743/0.190994loss in batch 37: 0.0359802/0.186905loss in batch 38: 0.0910492/0.184448loss in batch 39: 0.119827/0.182831loss in batch 40: 0.0690308/0.180054loss in batch 41: 0.0979309/0.178101loss in batch 42: 0.130814/0.177002loss in batch 43: 0.185593/0.1772loss in batch 44: 0.0660248/0.174728loss in batch 45: 0.0806427/0.172684loss in batch 46: 0.0739899/0.170578loss in batch 47: 0.0252228/0.167557loss in batch 48: 0.0259857/0.164673loss in batch 49: 0.424927/0.169861loss in batch 50: 0.0601959/0.167709loss in batch 51: 0.0671234/0.165787loss in batch 52: 0.156464/0.165619loss in batch 53: 0.157898/0.165466loss in batch 54: 0.00848389/0.162613loss in batch 55: 0.513062/0.168869loss in batch 56: 0.159393/0.168701loss in batch 57: 0.261688/0.170303loss in batch 58: 0.021225/0.167786loss in batch 59: 0.029953/0.165482loss in batch 60: 0.0163727/0.16304loss in batch 61: 0.545837/0.169205loss in batch 62: 0.0161438/0.166794loss in batch 63: 0.12027/0.166061loss in batch 64: 0.109604/0.165192loss in batch 65: 0.750916/0.174057loss in batch 66: 0.116547/0.173203loss in batch 67: 0.107407/0.172241loss in batch 68: 0.211578/0.172806loss in batch 69: 0.0133209/0.170532loss in batch 70: 0.0314789/0.168579loss in batch 71: 0.291504/0.170288loss in batch 72: 0.0449829/0.168549loss in batch 73: 0.0203552/0.166565loss in batch 74: 0.306824/0.168427loss in batch 75: 0.14978/0.168182loss in batch 76: 0.286392/0.169724loss in batch 77: 0.0602875/0.16832loss in batch 78: 0.184341/0.168518loss in batch 79: 0.244537/0.169479loss in batch 80: 0.299606/0.171066loss in batch 81: 0.283752/0.17244loss in batch 82: 0.0460968/0.170914loss in batch 83: 0.127655/0.17041loss in batch 84: 0.037796/0.168854loss in batch 85: 0.348541/0.170944loss in batch 86: 0.0322723/0.169342loss in batch 87: 0.157959/0.16922loss in batch 88: 0.0332947/0.167679loss in batch 89: 0.0686493/0.166595loss in batch 90: 0.0445557/0.165253loss in batch 91: 0.436874/0.168198loss in batch 92: 0.0804138/0.167267loss in batch 93: 0.0413361/0.165924loss in batch 94: 0.0908051/0.165115loss in batch 95: 0.157486/0.165054loss in batch 96: 0.0232544/0.163589loss in batch 97: 0.554657/0.167572loss in batch 98: 0.311066/0.169022loss in batch 99: 0.216537/0.169495loss in batch 100: 0.10553/0.168869loss in batch 101: 0.390671/0.171036loss in batch 102: 0.0302582/0.169678loss in batch 103: 0.183792/0.169815loss in batch 104: 0.0388184/0.168564loss in batch 105: 0.0162201/0.16713loss in batch 106: 0.0105438/0.165665loss in batch 107: 0.0537262/0.164612loss in batch 108: 0.272873/0.165619loss in batch 109: 0.033905/0.164413loss in batch 110: 0.0661774/0.163544loss in batch 111: 0.0162201/0.162231loss in batch 112: 0.0354156/0.161102loss in batch 113: 0.0632477/0.160248loss in batch 114: 0.504791/0.163223loss in batch 115: 0.0249939/0.162048loss in batch 116: 0.042511/0.161026loss in batch 117: 0.472351/0.163666loss in batch 118: 0.0863495/0.16301loss in batch 119: 0.0257874/0.16188loss in batch 120: 0.260986/0.162689loss in batch 121: 0.375/0.164429loss in batch 122: 0.352417/0.16597loss in batch 123: 0.13504/0.16571loss in batch 124: 0.353699/0.167206loss in batch 125: 0.266479/0.167999loss in batch 126: 0.00801086/0.166748loss in batch 127: 0.477264/0.169174loss in batch 128: 0.108307/0.168701loss in batch 129: 0.0676727/0.167923loss in batch 130: 0.0350647/0.166901loss in batch 131: 0.206345/0.167206loss in batch 132: 0.0119019/0.166046loss in batch 133: 0.0558167/0.165222loss in batch 134: 0.0526428/0.164383loss in batch 135: 0.296799/0.165359loss in batch 136: 0.102966/0.164886loss in batch 137: 0.233917/0.16539loss in batch 138: 0.0098114/0.164276loss in batch 139: 0.068222/0.163589loss in batch 140: 0.125748/0.163315loss in batch 141: 0.0761261/0.162704loss in batch 142: 0.0289307/0.161774loss in batch 143: 0.0740814/0.161163loss in batch 144: 0.368195/0.162598loss in batch 145: 0.275757/0.163361loss in batch 146: 0.42749/0.165161loss in batch 147: 0.447678/0.167068loss in batch 148: 0.0274811/0.166138loss in batch 149: 0.0131073/0.1651loss in batch 150: 1.06401/0.171066loss in batch 151: 0.0634613/0.170364loss in batch 152: 1.03522/0.17601loss in batch 153: 0.0563049/0.175232loss in batch 154: 0.0813141/0.174622loss in batch 155: 0.218414/0.174911loss in batch 156: 0.0075531/0.173843loss in batch 157: 0.384811/0.175186loss in batch 158: 0.0608063/0.174454loss in batch 159: 0.252304/0.174942loss in batch 160: 0.806061/0.178879loss in batch 161: 0.0145721/0.177856loss in batch 162: 0.0276184/0.176941loss in batch 163: 0.0837097/0.176361loss in batch 164: 0.0606384/0.175659loss in batch 165: 0.182358/0.175705loss in batch 166: 0.0604401/0.175018loss in batch 167: 0.156509/0.174896loss in batch 168: 0.0734253/0.174301loss in batch 169: 0.0681305/0.173676loss in batch 170: 0.490829/0.175537loss in batch 171: 0.0417786/0.174759loss in batch 172: 0.0744476/0.174179loss in batch 173: 0.0522003/0.173462loss in batch 174: 0.0742798/0.172913loss in batch 175: 0.0651855/0.172287loss in batch 176: 0.0387268/0.171539loss in batch 177: 0.0289307/0.170731loss in batch 178: 0.0135803/0.169861loss in batch 179: 0.0052948/0.168945loss in batch 180: 0.454849/0.170517loss in batch 181: 0.123764/0.170273loss in batch 182: 0.0374908/0.16954loss in batch 183: 0.0532532/0.1689loss in batch 184: 0.0469666/0.168259loss in batch 185: 0.503677/0.170059loss in batch 186: 0.546814/0.172073loss in batch 187: 0.1772/0.172089loss in batch 188: 0.432159/0.173477loss in batch 189: 0.078598/0.172974loss in batch 190: 0.338913/0.173843loss in batch 191: 0.0708313/0.173309loss in batch 192: 0.208984/0.173492loss in batch 193: 0.204468/0.173645loss in batch 194: 0.040741/0.172974loss in batch 195: 0.0272369/0.172226loss in batch 196: 0.0889893/0.171814loss in batch 197: 0.179398/0.171844loss in batch 198: 0.102509/0.171494loss in batch 199: 1.05798/0.175919loss in batch 200: 0.0569763/0.175339loss in batch 201: 0.0159454/0.174545loss in batch 202: 0.311966/0.175217loss in batch 203: 0.0512543/0.174606loss in batch 204: 0.0671082/0.174088loss in batch 205: 0.311325/0.174759loss in batch 206: 0.143448/0.174606loss in batch 207: 0.336563/0.175385loss in batch 208: 0.411957/0.176514loss in batch 209: 0.10437/0.176178loss in batch 210: 0.0160522/0.175415loss in batch 211: 0.067215/0.174896loss in batch 212: 0.019928/0.174164
done with epoch 12
train_acc: 0.938967 (400/426)
test loss: 0.019928
acc: 0.937063 (134/143)
loss in batch 0: 0.306519/0.306519loss in batch 1: 0.118134/0.212326loss in batch 2: 0.0783081/0.167648loss in batch 3: 0.121185/0.156036loss in batch 4: 0.108932/0.146606loss in batch 5: 0.386719/0.18663loss in batch 6: 0.036087/0.165115loss in batch 7: 0.303101/0.182373loss in batch 8: 0.0711975/0.170013loss in batch 9: 0.413651/0.194382loss in batch 10: 0.127579/0.188309loss in batch 11: 0.635452/0.225571loss in batch 12: 0.0504608/0.212112loss in batch 13: 0.00366211/0.197205loss in batch 14: 0.0419922/0.186859loss in batch 15: 0.285294/0.193008loss in batch 16: 0.0941162/0.187195loss in batch 17: 0.191498/0.187439loss in batch 18: 0.255661/0.191025loss in batch 19: 0.0827789/0.185623loss in batch 20: 0.152267/0.184036loss in batch 21: 0.0696106/0.178833loss in batch 22: 0.0229187/0.172058loss in batch 23: 0.0179901/0.165634loss in batch 24: 0.0422363/0.16069loss in batch 25: 0.0142212/0.15506loss in batch 26: 0.038559/0.150742loss in batch 27: 0.00692749/0.145599loss in batch 28: 0.065155/0.142838loss in batch 29: 0.114792/0.141891loss in batch 30: 0.305939/0.147186loss in batch 31: 0.0745239/0.144928loss in batch 32: 0.29303/0.149414loss in batch 33: 0.137543/0.149063loss in batch 34: 0.0728607/0.146881loss in batch 35: 0.144501/0.146805loss in batch 36: 0.292694/0.150757loss in batch 37: 0.0626831/0.148453loss in batch 38: 0.38237/0.154434loss in batch 39: 0.2099/0.155823loss in batch 40: 0.0138397/0.152374loss in batch 41: 0.0156555/0.149109loss in batch 42: 0.0822449/0.147552loss in batch 43: 0.0211639/0.144684loss in batch 44: 0.0874634/0.143402loss in batch 45: 0.320114/0.147263loss in batch 46: 0.489288/0.154526loss in batch 47: 0.0523834/0.15239loss in batch 48: 0.539795/0.160309loss in batch 49: 0.0669098/0.158447loss in batch 50: 0.00770569/0.155487loss in batch 51: 0.295349/0.158173loss in batch 52: 0.0697784/0.156494loss in batch 53: 0.019455/0.153961loss in batch 54: 0.149094/0.153885loss in batch 55: 0.223145/0.155106loss in batch 56: 0.218521/0.156235loss in batch 57: 0.0670013/0.154678loss in batch 58: 0.00482178/0.152145loss in batch 59: 0.255035/0.153854loss in batch 60: 0.649384/0.161987loss in batch 61: 0.0913696/0.160843loss in batch 62: 0.338211/0.163666loss in batch 63: 0.696442/0.171982loss in batch 64: 0.0379791/0.169937loss in batch 65: 0.0459137/0.168045loss in batch 66: 0.423035/0.17186loss in batch 67: 0.022049/0.169647loss in batch 68: 0.019928/0.16748loss in batch 69: 0.131622/0.166977loss in batch 70: 0.24295/0.168045loss in batch 71: 0.0104828/0.165848loss in batch 72: 0.151352/0.165649loss in batch 73: 0.122528/0.16507loss in batch 74: 0.10405/0.164261loss in batch 75: 0.00640869/0.162186loss in batch 76: 0.0829773/0.161148loss in batch 77: 0.102798/0.1604loss in batch 78: 0.0374451/0.158844loss in batch 79: 0.0779419/0.157837loss in batch 80: 0.0857086/0.156937loss in batch 81: 0.013855/0.155197loss in batch 82: 0.250839/0.156357loss in batch 83: 0.0391388/0.154953loss in batch 84: 0.0631409/0.15387loss in batch 85: 0.0788727/0.153015loss in batch 86: 0.0341492/0.151642loss in batch 87: 0.079895/0.150833loss in batch 88: 0.309021/0.152603loss in batch 89: 0.233627/0.153488loss in batch 90: 0.0653992/0.152527loss in batch 91: 0.0639954/0.151566loss in batch 92: 0.105103/0.151077loss in batch 93: 0.0794678/0.150314loss in batch 94: 0.0245056/0.148987loss in batch 95: 0.127106/0.148758loss in batch 96: 0.111038/0.148361loss in batch 97: 1.16374/0.158722loss in batch 98: 0.0403595/0.157532loss in batch 99: 0.0349884/0.156311loss in batch 100: 0.0907898/0.155655loss in batch 101: 0.328293/0.157349loss in batch 102: 0.12854/0.157059loss in batch 103: 0.198898/0.157486loss in batch 104: 0.0913696/0.156845loss in batch 105: 0.11438/0.156433loss in batch 106: 0.0546417/0.155487loss in batch 107: 0.0112152/0.15416loss in batch 108: 0.020462/0.152924loss in batch 109: 0.180481/0.153183loss in batch 110: 0.246674/0.154022loss in batch 111: 0.534943/0.157425loss in batch 112: 0.0642395/0.156601loss in batch 113: 0.369308/0.158463loss in batch 114: 0.0202942/0.157257loss in batch 115: 0.0289459/0.156143loss in batch 116: 0.0615082/0.155334loss in batch 117: 0.28656/0.156464loss in batch 118: 0.0933533/0.15593loss in batch 119: 0.0339661/0.154907loss in batch 120: 0.104401/0.154495loss in batch 121: 0.0639954/0.153748loss in batch 122: 0.0332184/0.152771loss in batch 123: 0.523651/0.155762loss in batch 124: 0.0601349/0.154999loss in batch 125: 0.177444/0.155167loss in batch 126: 0.0166779/0.154099loss in batch 127: 0.110336/0.153748loss in batch 128: 0.00265503/0.152573loss in batch 129: 0.318039/0.153839loss in batch 130: 0.439011/0.156036loss in batch 131: 0.00645447/0.154892loss in batch 132: 0.0210419/0.153885loss in batch 133: 0.0225983/0.152908loss in batch 134: 0.0559235/0.152176loss in batch 135: 0.0152283/0.151184loss in batch 136: 0.492554/0.153671loss in batch 137: 0.00624084/0.152603loss in batch 138: 0.0496368/0.151855loss in batch 139: 0.327698/0.153107loss in batch 140: 1.13419/0.16008loss in batch 141: 0.345993/0.161392loss in batch 142: 0.0749969/0.160782loss in batch 143: 0.320679/0.161896loss in batch 144: 0.252274/0.162521loss in batch 145: 0.0570068/0.161789loss in batch 146: 0.468704/0.163879loss in batch 147: 0.0866547/0.163361loss in batch 148: 0.11615/0.16304loss in batch 149: 0.250534/0.16362loss in batch 150: 0.123962/0.163361loss in batch 151: 0.094986/0.162918loss in batch 152: 0.0408478/0.162109loss in batch 153: 0.357285/0.163376loss in batch 154: 0.0496826/0.162643loss in batch 155: 0.0215759/0.161743loss in batch 156: 1.12886/0.167892loss in batch 157: 0.167618/0.167908loss in batch 158: 0.0573578/0.167206loss in batch 159: 0.0384369/0.166397loss in batch 160: 0.0230713/0.165497loss in batch 161: 0.0829315/0.165009loss in batch 162: 0.0440979/0.164261loss in batch 163: 0.12088/0.163986loss in batch 164: 0.280167/0.164703loss in batch 165: 0.417633/0.166229loss in batch 166: 0.0609589/0.165588loss in batch 167: 1.536/0.173752loss in batch 168: 0.492325/0.175629loss in batch 169: 0.030304/0.174774loss in batch 170: 0.0247345/0.173889loss in batch 171: 0.130539/0.17366loss in batch 172: 0.0784454/0.173111loss in batch 173: 0.447708/0.174683loss in batch 174: 0.475021/0.176392loss in batch 175: 0.0404816/0.175613loss in batch 176: 0.0227966/0.174759loss in batch 177: 0.0535583/0.174088loss in batch 178: 0.0571442/0.173431loss in batch 179: 0.0664368/0.172836loss in batch 180: 0.247025/0.173233loss in batch 181: 0.337921/0.174149loss in batch 182: 0.276245/0.174698loss in batch 183: 0.0558777/0.174057loss in batch 184: 0.103622/0.173676loss in batch 185: 0.0258942/0.172882loss in batch 186: 0.0180206/0.172058loss in batch 187: 0.049057/0.171402loss in batch 188: 0.239777/0.171753loss in batch 189: 0.0159149/0.170944loss in batch 190: 0.176361/0.170975loss in batch 191: 0.0714569/0.170456loss in batch 192: 0.0371094/0.169754loss in batch 193: 0.0256805/0.169022loss in batch 194: 0.337799/0.169891loss in batch 195: 0.139877/0.169724loss in batch 196: 0.0885315/0.169312loss in batch 197: 0.0299683/0.16861loss in batch 198: 0.679932/0.171188loss in batch 199: 0.641968/0.173538loss in batch 200: 0.00601196/0.172714loss in batch 201: 0.269913/0.173187loss in batch 202: 0.0369415/0.172516loss in batch 203: 0.0530853/0.171936loss in batch 204: 0.779343/0.174896loss in batch 205: 0.0478973/0.174271loss in batch 206: 0.0313568/0.173584loss in batch 207: 0.127609/0.17337loss in batch 208: 0.0527496/0.172791loss in batch 209: 0.00241089/0.171982loss in batch 210: 0.0085144/0.171204loss in batch 211: 0.250305/0.17157loss in batch 212: 0.0290222/0.170914
done with epoch 13
train_acc: 0.953052 (406/426)
test loss: 0.0290222
acc: 0.944056 (135/143)
loss in batch 0: 0.375977/0.375977loss in batch 1: 0.100922/0.238449loss in batch 2: 0.0316467/0.16951loss in batch 3: 0.911804/0.355087loss in batch 4: 0.0522461/0.29451loss in batch 5: 0.0367279/0.251556loss in batch 6: 0.424423/0.27626loss in batch 7: 0.0878296/0.252701loss in batch 8: 0.306213/0.258652loss in batch 9: 0.0467834/0.237457loss in batch 10: 0.107803/0.225677loss in batch 11: 0.0192108/0.208466loss in batch 12: 0.271606/0.213333loss in batch 13: 0.151871/0.208923loss in batch 14: 0.207458/0.208832loss in batch 15: 0.0532379/0.199112loss in batch 16: 0.1082/0.193756loss in batch 17: 0.129303/0.190186loss in batch 18: 1.46869/0.257462loss in batch 19: 0.0406036/0.246628loss in batch 20: 0.574097/0.262222loss in batch 21: 0.0792084/0.253906loss in batch 22: 0.301834/0.255981loss in batch 23: 0.0341797/0.24675loss in batch 24: 0.0910645/0.240524loss in batch 25: 0.466171/0.249191loss in batch 26: 0.0714722/0.242615loss in batch 27: 0.00653076/0.234192loss in batch 28: 0.0901489/0.229218loss in batch 29: 0.145767/0.22644loss in batch 30: 0.0646362/0.221207loss in batch 31: 0.25946/0.222412loss in batch 32: 0.108871/0.218979loss in batch 33: 0.27533/0.220627loss in batch 34: 0.126129/0.217926loss in batch 35: 0.078064/0.214035loss in batch 36: 0.290756/0.21611loss in batch 37: 0.264847/0.217392loss in batch 38: 0.306335/0.219681loss in batch 39: 0.092041/0.216476loss in batch 40: 0.0318604/0.21199loss in batch 41: 0.0817261/0.208878loss in batch 42: 0.000610352/0.204041loss in batch 43: 0.0291748/0.200058loss in batch 44: 0.082016/0.197433loss in batch 45: 0.499588/0.20401loss in batch 46: 0.00358582/0.199753loss in batch 47: 0.547729/0.206985loss in batch 48: 0.0724945/0.204254loss in batch 49: 0.0286407/0.200729loss in batch 50: 0.0280914/0.197357loss in batch 51: 0.213058/0.197647loss in batch 52: 0.140976/0.196594loss in batch 53: 0.218948/0.197006loss in batch 54: 0.0962982/0.195175loss in batch 55: 0.0670776/0.192886loss in batch 56: 0.0243683/0.189926loss in batch 57: 0.0471191/0.187454loss in batch 58: 0.0101166/0.184448loss in batch 59: 0.0189819/0.181702loss in batch 60: 0.0171051/0.179001loss in batch 61: 0.0611572/0.177094loss in batch 62: 0.0354462/0.17485loss in batch 63: 0.104965/0.173752loss in batch 64: 0.015564/0.171326loss in batch 65: 0.0568085/0.169586loss in batch 66: 0.0153809/0.167297loss in batch 67: 0.0032196/0.164886loss in batch 68: 0.107819/0.164047loss in batch 69: 0.173492/0.164185loss in batch 70: 0.134995/0.163773loss in batch 71: 0.821289/0.172913loss in batch 72: 0.0402832/0.171082loss in batch 73: 0.153076/0.170853loss in batch 74: 0.0947571/0.16983loss in batch 75: 0.2957/0.171478loss in batch 76: 0.202667/0.171906loss in batch 77: 0.0165558/0.169907loss in batch 78: 0.0208282/0.168015loss in batch 79: 0.110779/0.167297loss in batch 80: 0.470673/0.171036loss in batch 81: 0.0256805/0.169266loss in batch 82: 0.0888519/0.168304loss in batch 83: 0.0473938/0.16687loss in batch 84: 0.399384/0.169601loss in batch 85: 0.0877228/0.168655loss in batch 86: 0.4272/0.171616loss in batch 87: 0.076416/0.170532loss in batch 88: 0.0434875/0.169098loss in batch 89: 0.0513306/0.167801loss in batch 90: 0.0587006/0.166611loss in batch 91: 0.0345459/0.165161loss in batch 92: 0.0129395/0.163528loss in batch 93: 0.0121155/0.161926loss in batch 94: 0.012558/0.160339loss in batch 95: 0.374176/0.162567loss in batch 96: 0.0322266/0.161224loss in batch 97: 0.0468292/0.160065loss in batch 98: 0.0545349/0.158997loss in batch 99: 0.0152893/0.157562loss in batch 100: 0.415771/0.16011loss in batch 101: 0.265335/0.161148loss in batch 102: 0.304581/0.162552loss in batch 103: 0.0174408/0.161148loss in batch 104: 0.0613708/0.160202loss in batch 105: 0.419266/0.162643loss in batch 106: 0.0636902/0.161713loss in batch 107: 0.0784149/0.160934loss in batch 108: 0.085083/0.160248loss in batch 109: 0.224167/0.160828loss in batch 110: 0.0774231/0.160065loss in batch 111: 0.360092/0.161865loss in batch 112: 0.0931396/0.161255loss in batch 113: 0.00849915/0.159912loss in batch 114: 0.0525055/0.158981loss in batch 115: 0.2883/0.160095loss in batch 116: 0.10936/0.159668loss in batch 117: 0.0437164/0.158676loss in batch 118: 0.0839691/0.158051loss in batch 119: 0.04422/0.157104loss in batch 120: 0.0529938/0.15625loss in batch 121: 0.170502/0.156357loss in batch 122: 0.0394897/0.155411loss in batch 123: 0.276474/0.156387loss in batch 124: 0.0244904/0.155334loss in batch 125: 0.0154266/0.154205loss in batch 126: 0.495651/0.156906loss in batch 127: 0.113113/0.15657loss in batch 128: 0.454163/0.158875loss in batch 129: 0.0804901/0.158279loss in batch 130: 0.0427551/0.157394loss in batch 131: 0.08638/0.156845loss in batch 132: 0.350067/0.15831loss in batch 133: 0.0905914/0.157806loss in batch 134: 0.350082/0.159225loss in batch 135: 0.0604401/0.158493loss in batch 136: 0.0462036/0.157684loss in batch 137: 0.0552826/0.156921loss in batch 138: 0.290512/0.157898loss in batch 139: 0.0279236/0.156967loss in batch 140: 0.0637665/0.156296loss in batch 141: 0.181992/0.156494loss in batch 142: 0.0657043/0.155853loss in batch 143: 0.0769958/0.155304loss in batch 144: 0.0254669/0.154419loss in batch 145: 0.0128326/0.153442loss in batch 146: 0.139465/0.153351loss in batch 147: 0.636856/0.156616loss in batch 148: 0.0141602/0.155655loss in batch 149: 0.0507507/0.154953loss in batch 150: 0.527145/0.15741loss in batch 151: 0.0310669/0.156586loss in batch 152: 0.0767822/0.156067loss in batch 153: 0.559799/0.158691loss in batch 154: 0.592255/0.161484loss in batch 155: 0.0255432/0.160614loss in batch 156: 0.014389/0.159683loss in batch 157: 0.0643158/0.159088loss in batch 158: 0.0254517/0.158234loss in batch 159: 0.0221863/0.157394loss in batch 160: 0.0989685/0.157013loss in batch 161: 0.0948639/0.156647loss in batch 162: 0.0752869/0.156143loss in batch 163: 0.00167847/0.155197loss in batch 164: 0.122955/0.155014loss in batch 165: 0.106796/0.154709loss in batch 166: 0.0120392/0.153854loss in batch 167: 0.344315/0.154984loss in batch 168: 0.0142365/0.15416loss in batch 169: 0.649475/0.157074loss in batch 170: 0.0772247/0.156616loss in batch 171: 0.0484009/0.155991loss in batch 172: 0.0489807/0.155365loss in batch 173: 0.365326/0.15657loss in batch 174: 0.0383759/0.155884loss in batch 175: 0.265839/0.156509loss in batch 176: 0.327591/0.157486loss in batch 177: 0.225906/0.157867loss in batch 178: 0.317184/0.158752loss in batch 179: 0.0486908/0.158157loss in batch 180: 0.0357513/0.157471loss in batch 181: 1.12436/0.162796loss in batch 182: 0.139481/0.162659loss in batch 183: 0.0205688/0.16188loss in batch 184: 0.00822449/0.161041loss in batch 185: 0.00892639/0.160233loss in batch 186: 0.0497742/0.159653loss in batch 187: 0.0340118/0.158981loss in batch 188: 0.236176/0.159393loss in batch 189: 0.0979919/0.159058loss in batch 190: 0.1828/0.159195loss in batch 191: 0.110977/0.158936loss in batch 192: 0.061615/0.158432loss in batch 193: 0.0534058/0.157883loss in batch 194: 0.637085/0.160355loss in batch 195: 0.444641/0.161804loss in batch 196: 0.770584/0.164886loss in batch 197: 0.282654/0.165482loss in batch 198: 0.0731506/0.165024loss in batch 199: 0.320145/0.165802loss in batch 200: 0.464554/0.167282loss in batch 201: 0.161255/0.167252loss in batch 202: 0.118622/0.167007loss in batch 203: 0.115631/0.166763loss in batch 204: 0.0970612/0.166412loss in batch 205: 0.0572815/0.165894loss in batch 206: 0.0463715/0.165298loss in batch 207: 0.416626/0.166519loss in batch 208: 0.106918/0.166245loss in batch 209: 0.0361023/0.165619loss in batch 210: 0.446899/0.166946loss in batch 211: 0.31456/0.167648loss in batch 212: 0.180267/0.167694
done with epoch 14
train_acc: 0.953052 (406/426)
test loss: 0.180267
acc: 0.937063 (134/143)
loss in batch 0: 0.0668182/0.0668182loss in batch 1: 0.554962/0.310898loss in batch 2: 0.0230103/0.21492loss in batch 3: 0.502014/0.286697loss in batch 4: 0.129715/0.255295loss in batch 5: 0.208817/0.247559loss in batch 6: 0.060379/0.22081loss in batch 7: 0.0441895/0.198746loss in batch 8: 0.175858/0.196198loss in batch 9: 0.00787354/0.177368loss in batch 10: 0.0248566/0.163498loss in batch 11: 0.0547028/0.154434loss in batch 12: 0.0248108/0.144455loss in batch 13: 0.0501862/0.137726loss in batch 14: 0.0655212/0.132919loss in batch 15: 0.0270081/0.126297loss in batch 16: 0.0205231/0.120071loss in batch 17: 0.358139/0.133301loss in batch 18: 0.340408/0.144211loss in batch 19: 0.00450134/0.137207loss in batch 20: 0.0981598/0.135345loss in batch 21: 0.113953/0.134384loss in batch 22: 0.52916/0.151535loss in batch 23: 0.14296/0.151184loss in batch 24: 0.0521698/0.147232loss in batch 25: 0.316406/0.153732loss in batch 26: 0.0328979/0.149261loss in batch 27: 0.131149/0.148605loss in batch 28: 0.0818024/0.146317loss in batch 29: 1.07021/0.177109loss in batch 30: 0.11203/0.175003loss in batch 31: 0.0453949/0.170944loss in batch 32: 0.214996/0.172287loss in batch 33: 0.0352631/0.168259loss in batch 34: 0.0782776/0.165695loss in batch 35: 0.404343/0.172318loss in batch 36: 0.0996857/0.170364loss in batch 37: 0.0239868/0.166504loss in batch 38: 0.310059/0.170181loss in batch 39: 0.0801544/0.167938loss in batch 40: 0.042984/0.164886loss in batch 41: 0.0291901/0.161667loss in batch 42: 0.0558167/0.159195loss in batch 43: 0.2789/0.161911loss in batch 44: 0.0741272/0.159973loss in batch 45: 0.201462/0.160858loss in batch 46: 0.307281/0.163986loss in batch 47: 0.224991/0.165253loss in batch 48: 0.154221/0.165024loss in batch 49: 0.0387573/0.162506loss in batch 50: 0.117569/0.161621loss in batch 51: 0.298203/0.164246loss in batch 52: 0.451447/0.169662loss in batch 53: 0.0321045/0.16713loss in batch 54: 0.0157471/0.164368loss in batch 55: 0.246307/0.165833loss in batch 56: 0.0106812/0.163101loss in batch 57: 0.0300446/0.160812loss in batch 58: 0.106766/0.159897loss in batch 59: 0.302185/0.162277loss in batch 60: 0.0983582/0.161224loss in batch 61: 0.162521/0.16124loss in batch 62: 0.262527/0.162857loss in batch 63: 0.132721/0.162369loss in batch 64: 0.0534821/0.160706loss in batch 65: 0.0661926/0.159271loss in batch 66: 0.315155/0.161606loss in batch 67: 0.0625153/0.160141loss in batch 68: 0.122543/0.159607loss in batch 69: 0.262711/0.161072loss in batch 70: 0.331421/0.163467loss in batch 71: 0.100296/0.162598loss in batch 72: 0.10759/0.161835loss in batch 73: 0.453827/0.165787loss in batch 74: 0.0367889/0.164063loss in batch 75: 0.0266876/0.162262loss in batch 76: 0.11055/0.161575loss in batch 77: 0.0354004/0.159973loss in batch 78: 0.0670013/0.158783loss in batch 79: 0.0717316/0.157715loss in batch 80: 0.131897/0.157379loss in batch 81: 0.318375/0.159348loss in batch 82: 0.313507/0.161209loss in batch 83: 0.206787/0.161758loss in batch 84: 0.0778503/0.160751loss in batch 85: 0.0392303/0.159348loss in batch 86: 0.780518/0.166489loss in batch 87: 0.0103302/0.164719loss in batch 88: 0.00276184/0.162888loss in batch 89: 0.0214081/0.161331loss in batch 90: 0.183334/0.161575loss in batch 91: 0.344208/0.163544loss in batch 92: 0.0197144/0.162003loss in batch 93: 0.069809/0.161026loss in batch 94: 0.0455322/0.159805loss in batch 95: 0.05896/0.158752loss in batch 96: 0.363846/0.160873loss in batch 97: 0.00863647/0.159302loss in batch 98: 0.324097/0.16098loss in batch 99: 0.0768127/0.160141loss in batch 100: 0.0164642/0.158722loss in batch 101: 0.141953/0.158554loss in batch 102: 0.042984/0.157425loss in batch 103: 0.499786/0.160721loss in batch 104: 0.0778961/0.159943loss in batch 105: 0.0251465/0.158661loss in batch 106: 0.178452/0.158844loss in batch 107: 0.0370636/0.157715loss in batch 108: 0.0436401/0.156662loss in batch 109: 0.7229/0.161819loss in batch 110: 1.1674/0.170883loss in batch 111: 0.0372162/0.169693loss in batch 112: 0.0587463/0.168701loss in batch 113: 0.0209656/0.167404loss in batch 114: 0.105026/0.16687loss in batch 115: 0.0656586/0.166loss in batch 116: 0.0154419/0.164703loss in batch 117: 0.0673676/0.163895loss in batch 118: 0.278137/0.164841loss in batch 119: 0.0368805/0.163773loss in batch 120: 0.102173/0.163254loss in batch 121: 0.0366364/0.162231loss in batch 122: 0.226913/0.16275loss in batch 123: 0.521942/0.165649loss in batch 124: 0.0339203/0.164597loss in batch 125: 0.0204773/0.163452loss in batch 126: 0.0316315/0.162415loss in batch 127: 0.00602722/0.161194loss in batch 128: 0.419754/0.163208loss in batch 129: 0.438507/0.165314loss in batch 130: 0.117844/0.164948loss in batch 131: 0.234589/0.165482loss in batch 132: 0.205887/0.165787loss in batch 133: 0.33168/0.167023loss in batch 134: 0.0736389/0.166321loss in batch 135: 0.0465698/0.165451loss in batch 136: 0.291107/0.166367loss in batch 137: 0.0413513/0.165451loss in batch 138: 0.0118713/0.164352loss in batch 139: 0.379333/0.165894loss in batch 140: 0.119644/0.165573loss in batch 141: 0.249512/0.166153loss in batch 142: 0.116791/0.165802loss in batch 143: 0.557907/0.168533loss in batch 144: 0.0541382/0.167755loss in batch 145: 0.0166168/0.166702loss in batch 146: 0.0401001/0.165848loss in batch 147: 0.344391/0.167053loss in batch 148: 0.0389252/0.166199loss in batch 149: 0.460083/0.168152loss in batch 150: 0.0305634/0.167236loss in batch 151: 0.0821228/0.166687loss in batch 152: 0.0969849/0.166229loss in batch 153: 0.0113068/0.165222loss in batch 154: 0.0494385/0.164474loss in batch 155: 0.181213/0.164581loss in batch 156: 0.00717163/0.163574loss in batch 157: 0.0172882/0.162643loss in batch 158: 0.80394/0.166687loss in batch 159: 0.00190735/0.165649loss in batch 160: 0.0747986/0.165085loss in batch 161: 0.134903/0.164902loss in batch 162: 0.0288544/0.164078loss in batch 163: 0.0350342/0.163284loss in batch 164: 0.0153351/0.162384loss in batch 165: 0.0446472/0.161667loss in batch 166: 0.174408/0.161758loss in batch 167: 0.0619507/0.161163loss in batch 168: 0.02565/0.160355loss in batch 169: 0.0167999/0.159515loss in batch 170: 0.257935/0.160095loss in batch 171: 0.00788879/0.15921loss in batch 172: 0.27359/0.159866loss in batch 173: 0.0118561/0.159012loss in batch 174: 0.948807/0.163528loss in batch 175: 0.00505066/0.162613loss in batch 176: 0.0342865/0.161896loss in batch 177: 0.0519867/0.161285loss in batch 178: 0.0826263/0.160858loss in batch 179: 0.473679/0.162582loss in batch 180: 0.0357971/0.16188loss in batch 181: 0.845306/0.165649loss in batch 182: 0.0349884/0.164932loss in batch 183: 1.76169/0.173615loss in batch 184: 0.0915985/0.173157loss in batch 185: 0.0660095/0.172577loss in batch 186: 0.313812/0.17334loss in batch 187: 0.113251/0.173019loss in batch 188: 0.0600586/0.172409loss in batch 189: 0.202484/0.172577loss in batch 190: 0.218918/0.172821loss in batch 191: 0.0235901/0.172043loss in batch 192: 0.0547028/0.171448loss in batch 193: 0.0302887/0.170715loss in batch 194: 0.191666/0.170822loss in batch 195: 0.372314/0.171844loss in batch 196: 0.0639191/0.171295loss in batch 197: 0.158981/0.171219loss in batch 198: 0.0985107/0.170868loss in batch 199: 0.340332/0.171707loss in batch 200: 0.0881195/0.17131loss in batch 201: 0.0361786/0.170624loss in batch 202: 0.56955/0.172592loss in batch 203: 0.112076/0.172302loss in batch 204: 0.0773163/0.171829loss in batch 205: 0.03862/0.171188loss in batch 206: 0.0998688/0.170837loss in batch 207: 0.054245/0.170288loss in batch 208: 0.00758362/0.16951loss in batch 209: 0.100464/0.169174loss in batch 210: 0.103317/0.168854loss in batch 211: 0.00421143/0.168091loss in batch 212: 0.109802/0.167816
done with epoch 15
train_acc: 0.946009 (403/426)
test loss: 0.109802
acc: 0.944056 (135/143)
loss in batch 0: 0.340286/0.340286loss in batch 1: 0.027832/0.184052loss in batch 2: 0.208755/0.192291loss in batch 3: 0.0790405/0.163986loss in batch 4: 1.43797/0.418777loss in batch 5: 0.0271759/0.3535loss in batch 6: 0.0347748/0.307983loss in batch 7: 0.0364227/0.274033loss in batch 8: 0.277039/0.274368loss in batch 9: 0.161697/0.263107loss in batch 10: 0.241287/0.261108loss in batch 11: 0.121506/0.249481loss in batch 12: 0.0776062/0.236267loss in batch 13: 0.0472565/0.222748loss in batch 14: 0.216507/0.222336loss in batch 15: 0.119644/0.215912loss in batch 16: 0.0815887/0.208023loss in batch 17: 0.555084/0.22731loss in batch 18: 0.0199585/0.216385loss in batch 19: 0.0171356/0.206421loss in batch 20: 0.0583191/0.199371loss in batch 21: 0.0981445/0.194778loss in batch 22: 0.00535583/0.186539loss in batch 23: 0.0716553/0.181747loss in batch 24: 0.0372772/0.17598loss in batch 25: 0.0437164/0.170883loss in batch 26: 0.111542/0.168686loss in batch 27: 0.0286407/0.163696loss in batch 28: 0.050705/0.15979loss in batch 29: 0.00120544/0.154495loss in batch 30: 0.0116425/0.149887loss in batch 31: 0.077774/0.147644loss in batch 32: 0.370773/0.154404loss in batch 33: 0.0931244/0.152603loss in batch 34: 0.456711/0.161301loss in batch 35: 0.319672/0.165695loss in batch 36: 0.0945129/0.163757loss in batch 37: 0.0180054/0.159927loss in batch 38: 0.0716553/0.157669loss in batch 39: 0.0243988/0.154327loss in batch 40: 0.201645/0.155487loss in batch 41: 0.029892/0.152496loss in batch 42: 0.165558/0.152802loss in batch 43: 0.343811/0.157135loss in batch 44: 0.148071/0.156952loss in batch 45: 0.0605164/0.154846loss in batch 46: 0.0230255/0.152039loss in batch 47: 0.108047/0.151123loss in batch 48: 0.0588837/0.149231loss in batch 49: 0.0457458/0.147171loss in batch 50: 0.45108/0.153122loss in batch 51: 0.0108795/0.150391loss in batch 52: 0.0386963/0.148285loss in batch 53: 0.17131/0.148712loss in batch 54: 0.432724/0.153885loss in batch 55: 0.0222473/0.15152loss in batch 56: 0.459015/0.156921loss in batch 57: 0.0205078/0.154572loss in batch 58: 0.029953/0.152451loss in batch 59: 0.0211029/0.150269loss in batch 60: 0.00434875/0.147873loss in batch 61: 0.0979767/0.147064loss in batch 62: 0.0698242/0.145844loss in batch 63: 0.0995178/0.145111loss in batch 64: 0.0728302/0.143997loss in batch 65: 0.0990753/0.143326loss in batch 66: 0.168137/0.143692loss in batch 67: 0.0346527/0.14209loss in batch 68: 0.0526428/0.140808loss in batch 69: 0.0614929/0.139664loss in batch 70: 0.0597992/0.138535loss in batch 71: 0.132355/0.138458loss in batch 72: 0.14798/0.138596loss in batch 73: 0.0324097/0.137146loss in batch 74: 0.00605774/0.135406loss in batch 75: 0.0465851/0.134232loss in batch 76: 0.0324097/0.132904loss in batch 77: 0.0740662/0.132156loss in batch 78: 0.324188/0.134583loss in batch 79: 0.0774384/0.133865loss in batch 80: 0.0992432/0.133453loss in batch 81: 0.42482/0.136993loss in batch 82: 0.21138/0.137894loss in batch 83: 0.147797/0.138016loss in batch 84: 0.0387421/0.136841loss in batch 85: 0.0206604/0.135498loss in batch 86: 0.291733/0.137283loss in batch 87: 0.145798/0.13739loss in batch 88: 0.206345/0.138153loss in batch 89: 0.0142822/0.13678loss in batch 90: 0.0352325/0.135666loss in batch 91: 0.143478/0.135742loss in batch 92: 0.017334/0.134491loss in batch 93: 0.141449/0.134552loss in batch 94: 0.0175018/0.133331loss in batch 95: 0.183594/0.13385loss in batch 96: 0.0354767/0.132828loss in batch 97: 0.347961/0.135025loss in batch 98: 0.0167542/0.133835loss in batch 99: 0.305862/0.135559loss in batch 100: 0.0157471/0.134369loss in batch 101: 0.272598/0.135727loss in batch 102: 0.00428772/0.134445loss in batch 103: 0.101303/0.134125loss in batch 104: 0.497879/0.137589loss in batch 105: 0.0292816/0.136566loss in batch 106: 0.472412/0.139709loss in batch 107: 0.0970917/0.139313loss in batch 108: 0.159988/0.139511loss in batch 109: 0.185776/0.139923loss in batch 110: 0.350815/0.14183loss in batch 111: 0.0707397/0.14119loss in batch 112: 0.762787/0.146698loss in batch 113: 0.479385/0.149612loss in batch 114: 0.0384674/0.148636loss in batch 115: 0.0366211/0.14769loss in batch 116: 0.00445557/0.146454loss in batch 117: 0.0190582/0.14537loss in batch 118: 0.123444/0.145187loss in batch 119: 0.0318604/0.144241loss in batch 120: 0.155579/0.144348loss in batch 121: 0.371109/0.146194loss in batch 122: 0.0314178/0.145264loss in batch 123: 0.578552/0.148758loss in batch 124: 0.564743/0.152084loss in batch 125: 0.0956573/0.151627loss in batch 126: 0.202484/0.152039loss in batch 127: 0.0163727/0.15097loss in batch 128: 0.0404358/0.150131loss in batch 129: 0.320572/0.151428loss in batch 130: 0.0395813/0.150574loss in batch 131: 0.359451/0.152161loss in batch 132: 0.142242/0.152084loss in batch 133: 0.0285187/0.151169loss in batch 134: 0.0313263/0.150269loss in batch 135: 0.0108032/0.149246loss in batch 136: 0.0568542/0.148575loss in batch 137: 0.0406799/0.147797loss in batch 138: 0.0463562/0.147064loss in batch 139: 0.246735/0.147766loss in batch 140: 0.103363/0.147461loss in batch 141: 0.0399017/0.146713loss in batch 142: 0.137817/0.146637loss in batch 143: 0.100433/0.146317loss in batch 144: 0.0537109/0.145676loss in batch 145: 0.415695/0.147537loss in batch 146: 1.14258/0.154312loss in batch 147: 0.0450897/0.153564loss in batch 148: 0.0232086/0.152695loss in batch 149: 0.0185852/0.151794loss in batch 150: 0.0854645/0.151352loss in batch 151: 0.0319824/0.150574loss in batch 152: 0.0278625/0.149765loss in batch 153: 0.0478516/0.149109loss in batch 154: 0.53273/0.151581loss in batch 155: 0.0643311/0.151016loss in batch 156: 0.182312/0.151215loss in batch 157: 0.230072/0.151718loss in batch 158: 0.0298767/0.150955loss in batch 159: 0.232101/0.151474loss in batch 160: 0.328049/0.152557loss in batch 161: 0.0503082/0.151917loss in batch 162: 0.358292/0.153198loss in batch 163: 0.335251/0.154297loss in batch 164: 0.0461884/0.153656loss in batch 165: 0.272446/0.154358loss in batch 166: 0.0295563/0.15361loss in batch 167: 0.174423/0.153732loss in batch 168: 0.0330811/0.153015loss in batch 169: 0.0552216/0.152451loss in batch 170: 0.0340424/0.151764loss in batch 171: 0.370773/0.15303loss in batch 172: 0.00682068/0.152191loss in batch 173: 0.432007/0.153793loss in batch 174: 0.416946/0.155289loss in batch 175: 0.109756/0.155045loss in batch 176: 0.0875092/0.154663loss in batch 177: 0.0697479/0.154175loss in batch 178: 0.0419769/0.153549loss in batch 179: 0.0206604/0.152802loss in batch 180: 0.0123901/0.152039loss in batch 181: 0.19249/0.152252loss in batch 182: 0.0897522/0.151917loss in batch 183: 0.10997/0.151688loss in batch 184: 0.42453/0.153168loss in batch 185: 0.014679/0.15242loss in batch 186: 0.0242157/0.151733loss in batch 187: 0.168793/0.151825loss in batch 188: 0.0758209/0.151413loss in batch 189: 0.114197/0.15123loss in batch 190: 0.0661621/0.150772loss in batch 191: 0.0691528/0.15036loss in batch 192: 0.489166/0.152115loss in batch 193: 0.416367/0.153473loss in batch 194: 0.0409851/0.152893loss in batch 195: 0.141296/0.152847loss in batch 196: 0.0400238/0.152267loss in batch 197: 0.0213013/0.151611loss in batch 198: 0.0515594/0.151108loss in batch 199: 1.07343/0.155716loss in batch 200: 0.873306/0.159286loss in batch 201: 0.0226288/0.158615loss in batch 202: 0.203079/0.158829loss in batch 203: 0.0262146/0.158173loss in batch 204: 0.0901031/0.157837loss in batch 205: 0.379883/0.15892loss in batch 206: 0.319977/0.159698loss in batch 207: 0.650406/0.162064loss in batch 208: 0.0261536/0.161407loss in batch 209: 0.0164185/0.160721loss in batch 210: 0.429947/0.161987loss in batch 211: 0.0560913/0.161499loss in batch 212: 0.215805/0.161743
done with epoch 16
train_acc: 0.953052 (406/426)
test loss: 0.215805
acc: 0.944056 (135/143)
loss in batch 0: 0.215866/0.215866loss in batch 1: 0.160797/0.188324loss in batch 2: 0.124435/0.167023loss in batch 3: 0.0250854/0.131546loss in batch 4: 0.285034/0.162247loss in batch 5: 0.00323486/0.135742loss in batch 6: 0.0209808/0.119339loss in batch 7: 0.0644073/0.112488loss in batch 8: 0.0727844/0.108078loss in batch 9: 0.0439606/0.101669loss in batch 10: 0.0435333/0.0963745loss in batch 11: 1.1619/0.185165loss in batch 12: 0.517624/0.210754loss in batch 13: 0.0630646/0.20018loss in batch 14: 0.0271606/0.18866loss in batch 15: 0.446579/0.204773loss in batch 16: 0.038681/0.195007loss in batch 17: 0.0307312/0.185883loss in batch 18: 0.726425/0.214325loss in batch 19: 0.0145874/0.204346loss in batch 20: 0.027771/0.195938loss in batch 21: 0.158432/0.194229loss in batch 22: 0.317139/0.199585loss in batch 23: 0.0508118/0.193375loss in batch 24: 0.35675/0.199905loss in batch 25: 0.211761/0.200363loss in batch 26: 0.0430908/0.194534loss in batch 27: 0.0917206/0.190872loss in batch 28: 0.0370178/0.185562loss in batch 29: 0.221466/0.186768loss in batch 30: 0.0347748/0.181854loss in batch 31: 0.0764618/0.178574loss in batch 32: 0.0821075/0.175629loss in batch 33: 0.0830994/0.172913loss in batch 34: 0.189911/0.173416loss in batch 35: 0.0160522/0.169037loss in batch 36: 0.105438/0.167328loss in batch 37: 0.0407715/0.163986loss in batch 38: 0.0210114/0.160324loss in batch 39: 0.047821/0.157516loss in batch 40: 0.039444/0.154633loss in batch 41: 0.0157013/0.151306loss in batch 42: 0.0210876/0.1483loss in batch 43: 1.57744/0.180771loss in batch 44: 0.216995/0.18158loss in batch 45: 0.0229034/0.178131loss in batch 46: 0.0364227/0.17511loss in batch 47: 0.0495605/0.172501loss in batch 48: 0.0740509/0.170486loss in batch 49: 0.112808/0.169327loss in batch 50: 0.549652/0.176788loss in batch 51: 0.433655/0.181732loss in batch 52: 0.323303/0.184402loss in batch 53: 0.0158386/0.181274loss in batch 54: 0.0483246/0.178864loss in batch 55: 0.499924/0.184601loss in batch 56: 0.00601196/0.181473loss in batch 57: 0.0367737/0.17897loss in batch 58: 0.00726318/0.176056loss in batch 59: 0.0121002/0.173325loss in batch 60: 0.494141/0.178589loss in batch 61: 0.0434418/0.176407loss in batch 62: 0.108154/0.175323loss in batch 63: 0.0733643/0.173721loss in batch 64: 0.383255/0.176956loss in batch 65: 0.0841064/0.175537loss in batch 66: 0.0272675/0.17334loss in batch 67: 0.0303192/0.171234loss in batch 68: 0.16507/0.171127loss in batch 69: 0.220261/0.171844loss in batch 70: 0.0502625/0.170135loss in batch 71: 0.0969238/0.169113loss in batch 72: 0.0136261/0.166977loss in batch 73: 0.00837708/0.164841loss in batch 74: 0.144333/0.164566loss in batch 75: 0.26564/0.165894loss in batch 76: 0.0797882/0.164764loss in batch 77: 0.196396/0.165192loss in batch 78: 0.775314/0.172913loss in batch 79: 0.453033/0.176407loss in batch 80: 0.0473022/0.174805loss in batch 81: 0.204132/0.175171loss in batch 82: 0.0589905/0.173782loss in batch 83: 0.054718/0.172363loss in batch 84: 0.00619507/0.170395loss in batch 85: 0.274445/0.171616loss in batch 86: 0.00938416/0.169739loss in batch 87: 0.00830078/0.167908loss in batch 88: 0.031723/0.166382loss in batch 89: 0.0247192/0.16481loss in batch 90: 0.0457458/0.163498loss in batch 91: 0.0224152/0.161957loss in batch 92: 0.239227/0.162781loss in batch 93: 0.00476074/0.161118loss in batch 94: 0.183044/0.161346loss in batch 95: 0.0277557/0.159943loss in batch 96: 0.0597229/0.15892loss in batch 97: 0.364746/0.161026loss in batch 98: 0.0953979/0.16037loss in batch 99: 0.0985718/0.159744loss in batch 100: 0.0547791/0.158707loss in batch 101: 0.00578308/0.157196loss in batch 102: 0.00108337/0.155685loss in batch 103: 0.162582/0.155746loss in batch 104: 0.0687256/0.154922loss in batch 105: 0.259735/0.155899loss in batch 106: 0.287857/0.157135loss in batch 107: 0.271729/0.158203loss in batch 108: 0.25856/0.159119loss in batch 109: 0.0714264/0.15834loss in batch 110: 0.292831/0.159531loss in batch 111: 0.113968/0.159134loss in batch 112: 0.0470428/0.158142loss in batch 113: 0.00938416/0.156845loss in batch 114: 0.0137329/0.155594loss in batch 115: 0.438858/0.158035loss in batch 116: 0.127014/0.157776loss in batch 117: 0.035141/0.156723loss in batch 118: 0.0908966/0.156174loss in batch 119: 0.119644/0.155869loss in batch 120: 0.404526/0.157928loss in batch 121: 0.120529/0.157608loss in batch 122: 0.0671082/0.156891loss in batch 123: 0.336319/0.158325loss in batch 124: 0.0444031/0.157425loss in batch 125: 0.377167/0.159164loss in batch 126: 0.138351/0.159012loss in batch 127: 0.0153961/0.157883loss in batch 128: 0.326019/0.159195loss in batch 129: 0.369217/0.160797loss in batch 130: 0.0180511/0.159714loss in batch 131: 0.107971/0.159317loss in batch 132: 0.09375/0.158829loss in batch 133: 0.114578/0.158493loss in batch 134: 0.0438385/0.157654loss in batch 135: 0.0027771/0.156509loss in batch 136: 0.0396423/0.155655loss in batch 137: 0.0566101/0.154938loss in batch 138: 0.0240326/0.153992loss in batch 139: 0.0513458/0.153259loss in batch 140: 0.270752/0.154083loss in batch 141: 0.193649/0.154358loss in batch 142: 1.07932/0.160843loss in batch 143: 0.0329742/0.159958loss in batch 144: 0.224548/0.1604loss in batch 145: 0.0102081/0.159363loss in batch 146: 0.321548/0.160461loss in batch 147: 0.0235138/0.159546loss in batch 148: 0.022522/0.15863loss in batch 149: 0.00961304/0.157623loss in batch 150: 0.194046/0.157867loss in batch 151: 0.0145111/0.156937loss in batch 152: 0.272476/0.157684loss in batch 153: 0.0210724/0.156799loss in batch 154: 0.0635834/0.156204loss in batch 155: 0.00613403/0.155228loss in batch 156: 0.382935/0.156677loss in batch 157: 0.372238/0.158051loss in batch 158: 0.0235138/0.157196loss in batch 159: 0.089386/0.156784loss in batch 160: 0.202972/0.157059loss in batch 161: 0.0663757/0.156509loss in batch 162: 0.0167847/0.155655loss in batch 163: 0.0840759/0.155212loss in batch 164: 0.243088/0.155746loss in batch 165: 0.0661774/0.155197loss in batch 166: 0.185608/0.15538loss in batch 167: 0.0183258/0.154572loss in batch 168: 0.54808/0.156906loss in batch 169: 0.102798/0.15657loss in batch 170: 0.0325775/0.155869loss in batch 171: 0.390335/0.157227loss in batch 172: 0.0780182/0.156754loss in batch 173: 0.0455627/0.156113loss in batch 174: 0.0380096/0.155441loss in batch 175: 0.0740967/0.154984loss in batch 176: 0.103226/0.154694loss in batch 177: 0.110245/0.154449loss in batch 178: 0.099823/0.154129loss in batch 179: 0.0658722/0.153641loss in batch 180: 0.868423/0.157593loss in batch 181: 0.339157/0.1586loss in batch 182: 0.132935/0.158447loss in batch 183: 0.0951691/0.158112loss in batch 184: 0.0881958/0.15773loss in batch 185: 0.0666504/0.157242loss in batch 186: 0.0812988/0.15683loss in batch 187: 0.0418701/0.156219loss in batch 188: 0.0621948/0.155731loss in batch 189: 0.258881/0.156265loss in batch 190: 0.434174/0.15773loss in batch 191: 0.533371/0.159683loss in batch 192: 0.0155029/0.158936loss in batch 193: 0.249405/0.159393loss in batch 194: 0.0754395/0.158981loss in batch 195: 0.637085/0.161423loss in batch 196: 0.0596161/0.160889loss in batch 197: 0.0170746/0.160156loss in batch 198: 0.0183868/0.159454loss in batch 199: 0.233749/0.159836loss in batch 200: 0.0732727/0.159409loss in batch 201: 0.0728302/0.158966loss in batch 202: 0.0493774/0.158432loss in batch 203: 0.0704956/0.15799loss in batch 204: 0.0186462/0.157318loss in batch 205: 0.155563/0.157303loss in batch 206: 0.29332/0.157974loss in batch 207: 0.0887146/0.157639loss in batch 208: 0.148682/0.157593loss in batch 209: 0.0250549/0.156967loss in batch 210: 0.484268/0.158508loss in batch 211: 0.0758972/0.158112loss in batch 212: 0.0433044/0.157578
done with epoch 17
train_acc: 0.953052 (406/426)
test loss: 0.0433044
acc: 0.944056 (135/143)
loss in batch 0: 0.00427246/0.00427246loss in batch 1: 0.142258/0.0732574loss in batch 2: 0.0925446/0.0796967loss in batch 3: 0.18602/0.106262loss in batch 4: 0.0346527/0.0919495loss in batch 5: 0.062027/0.0869598loss in batch 6: 0.266556/0.11261loss in batch 7: 0.107315/0.111954loss in batch 8: 0.0330811/0.103195loss in batch 9: 1.49312/0.242172loss in batch 10: 0.512329/0.266739loss in batch 11: 0.160843/0.257919loss in batch 12: 0.113937/0.246841loss in batch 13: 0.0113068/0.230011loss in batch 14: 0.0228577/0.216202loss in batch 15: 0.0131836/0.203522loss in batch 16: 0.0561218/0.194839loss in batch 17: 0.0346069/0.185944loss in batch 18: 0.0502777/0.178802loss in batch 19: 0.0336914/0.171555loss in batch 20: 0.379852/0.181473loss in batch 21: 0.111267/0.178284loss in batch 22: 0.196426/0.179077loss in batch 23: 0.334885/0.185562loss in batch 24: 0.0276642/0.179245loss in batch 25: 0.351669/0.185883loss in batch 26: 0.168793/0.185242loss in batch 27: 0.103287/0.182312loss in batch 28: 0.0735626/0.178558loss in batch 29: 0.0613556/0.174667loss in batch 30: 0.0645752/0.171097loss in batch 31: 0.0837708/0.168365loss in batch 32: 0.0106659/0.163605loss in batch 33: 0.00463867/0.15892loss in batch 34: 0.454147/0.167358loss in batch 35: 0.0422516/0.163895loss in batch 36: 0.197281/0.16478loss in batch 37: 0.0292206/0.161224loss in batch 38: 0.0339355/0.157959loss in batch 39: 0.016449/0.154404loss in batch 40: 0.409424/0.160645loss in batch 41: 0.0386963/0.157745loss in batch 42: 0.419235/0.163818loss in batch 43: 0.0332642/0.160843loss in batch 44: 0.352158/0.1651loss in batch 45: 0.222061/0.166336loss in batch 46: 0.411652/0.171555loss in batch 47: 0.0273895/0.168564loss in batch 48: 0.245316/0.17012loss in batch 49: 0.0670013/0.16806loss in batch 50: 0.248062/0.169617loss in batch 51: 0.201843/0.170258loss in batch 52: 0.0078125/0.167191loss in batch 53: 0.000930786/0.164108loss in batch 54: 0.0640106/0.162277loss in batch 55: 0.162537/0.162292loss in batch 56: 0.0252838/0.159882loss in batch 57: 0.0366211/0.157761loss in batch 58: 1.23007/0.175934loss in batch 59: 0.0680847/0.174133loss in batch 60: 0.887192/0.185822loss in batch 61: 0.0822296/0.184143loss in batch 62: 0.0982666/0.182785loss in batch 63: 0.319077/0.184921loss in batch 64: 0.0262909/0.18248loss in batch 65: 0.117096/0.181488loss in batch 66: 0.00326538/0.178833loss in batch 67: 0.0689545/0.177216loss in batch 68: 0.228256/0.177963loss in batch 69: 0.129761/0.177261loss in batch 70: 0.0143738/0.174973loss in batch 71: 0.470398/0.179077loss in batch 72: 0.446381/0.182739loss in batch 73: 0.0389252/0.180786loss in batch 74: 0.0137482/0.178574loss in batch 75: 0.0487823/0.176849loss in batch 76: 0.0583801/0.175323loss in batch 77: 0.254196/0.176331loss in batch 78: 0.0569/0.17482loss in batch 79: 0.0739899/0.173553loss in batch 80: 0.059082/0.172134loss in batch 81: 0.0294495/0.17041loss in batch 82: 0.0653076/0.169128loss in batch 83: 0.221085/0.169754loss in batch 84: 0.0767822/0.168655loss in batch 85: 0.616837/0.173874loss in batch 86: 0.28949/0.175201loss in batch 87: 0.243271/0.17598loss in batch 88: 0.140335/0.175583loss in batch 89: 0.0245361/0.173904loss in batch 90: 0.0222168/0.172241loss in batch 91: 0.533859/0.176163loss in batch 92: 0.348007/0.178009loss in batch 93: 0.0960541/0.177139loss in batch 94: 0.334381/0.178787loss in batch 95: 0.027832/0.177216loss in batch 96: 0.0463104/0.175873loss in batch 97: 0.33255/0.177475loss in batch 98: 0.40741/0.179779loss in batch 99: 0.318527/0.181183loss in batch 100: 0.416016/0.183502loss in batch 101: 0.0261536/0.181961loss in batch 102: 0.0574341/0.180756loss in batch 103: 0.101822/0.179993loss in batch 104: 0.0219116/0.178497loss in batch 105: 0.0366821/0.177155loss in batch 106: 0.0296631/0.175766loss in batch 107: 0.0891418/0.174957loss in batch 108: 0.0804291/0.174103loss in batch 109: 0.0643616/0.173111loss in batch 110: 0.00500488/0.171585loss in batch 111: 0.0242462/0.170273loss in batch 112: 0.797409/0.175827loss in batch 113: 0.0357513/0.174591loss in batch 114: 0.0436707/0.173447loss in batch 115: 0.182602/0.173523loss in batch 116: 0.037796/0.172379loss in batch 117: 0.235809/0.172913loss in batch 118: 0.00767517/0.171524loss in batch 119: 0.0970764/0.170914loss in batch 120: 0.259628/0.171646loss in batch 121: 0.0080719/0.170303loss in batch 122: 0.0211487/0.169083loss in batch 123: 0.101364/0.168549loss in batch 124: 0.0600281/0.167664loss in batch 125: 0.0702057/0.166901loss in batch 126: 0.0661621/0.166107loss in batch 127: 1.22853/0.174393loss in batch 128: 0.0288544/0.173279loss in batch 129: 0.393814/0.174973loss in batch 130: 0.571823/0.177994loss in batch 131: 0.139557/0.177704loss in batch 132: 0.047287/0.176727loss in batch 133: 0.0366211/0.17569loss in batch 134: 0.0321045/0.174622loss in batch 135: 0.179001/0.174652loss in batch 136: 0.174805/0.174652loss in batch 137: 0.0336761/0.17363loss in batch 138: 0.0845947/0.173004loss in batch 139: 0.104462/0.172501loss in batch 140: 0.0693054/0.171768loss in batch 141: 0.0177765/0.170685loss in batch 142: 0.0204926/0.169632loss in batch 143: 0.0926971/0.169113loss in batch 144: 0.262192/0.169739loss in batch 145: 0.54512/0.172318loss in batch 146: 0.0564117/0.171524loss in batch 147: 0.0238495/0.170517loss in batch 148: 0.195618/0.170685loss in batch 149: 0.0240479/0.169724loss in batch 150: 0.0594788/0.168991loss in batch 151: 0.219574/0.169327loss in batch 152: 0.0311432/0.168427loss in batch 153: 0.00419617/0.167343loss in batch 154: 0.0263519/0.166443loss in batch 155: 0.351624/0.167633loss in batch 156: 0.00416565/0.16658loss in batch 157: 0.414291/0.168152loss in batch 158: 0.00695801/0.167145loss in batch 159: 0.00756836/0.166153loss in batch 160: 0.033905/0.165314loss in batch 161: 0.51416/0.16748loss in batch 162: 0.132339/0.167267loss in batch 163: 0.0219269/0.166382loss in batch 164: 0.0480499/0.165649loss in batch 165: 0.311768/0.166534loss in batch 166: 0.0233154/0.16568loss in batch 167: 0.0385284/0.164932loss in batch 168: 0.0112915/0.164017loss in batch 169: 0.215851/0.164322loss in batch 170: 0.0015564/0.163361loss in batch 171: 0.326462/0.164322loss in batch 172: 0.292862/0.165054loss in batch 173: 0.0879669/0.164612loss in batch 174: 0.0463257/0.16394loss in batch 175: 0.0547638/0.163315loss in batch 176: 0.0136871/0.16246loss in batch 177: 0.0510254/0.16185loss in batch 178: 0.029953/0.161118loss in batch 179: 0.22966/0.161499loss in batch 180: 0.040741/0.160828loss in batch 181: 0.240738/0.16127loss in batch 182: 0.159836/0.16127loss in batch 183: 0.0614166/0.160706loss in batch 184: 0.0402069/0.160049loss in batch 185: 0.00808716/0.159241loss in batch 186: 0.00688171/0.158432loss in batch 187: 0.269043/0.159027loss in batch 188: 0.00416565/0.158188loss in batch 189: 0.137009/0.158096loss in batch 190: 0.128281/0.157944loss in batch 191: 0.431091/0.159348loss in batch 192: 0.0490723/0.158783loss in batch 193: 0.0955048/0.158463loss in batch 194: 0.0261383/0.157791loss in batch 195: 0.162354/0.157791loss in batch 196: 0.0345154/0.157181loss in batch 197: 0.0882874/0.15683loss in batch 198: 0.0268097/0.156174loss in batch 199: 0.0206757/0.155502loss in batch 200: 0.0523529/0.154984loss in batch 201: 0.10408/0.154739loss in batch 202: 0.0463562/0.15419loss in batch 203: 0.178223/0.154312loss in batch 204: 0.234497/0.154709loss in batch 205: 0.111252/0.154495loss in batch 206: 0.0188751/0.153839loss in batch 207: 0.0609589/0.153381loss in batch 208: 0.0194397/0.152756loss in batch 209: 0.692383/0.155319loss in batch 210: 0.108124/0.155106loss in batch 211: 0.433243/0.156403loss in batch 212: 0.25206/0.15686
done with epoch 18
train_acc: 0.950704 (405/426)
test loss: 0.25206
acc: 0.951049 (136/143)
loss in batch 0: 0.0144043/0.0144043loss in batch 1: 0.012619/0.013504loss in batch 2: 0.0694885/0.0321655loss in batch 3: 0.0643005/0.0402069loss in batch 4: 0.012558/0.034668loss in batch 5: 0.339615/0.085495loss in batch 6: 0.0382385/0.0787506loss in batch 7: 0.0440063/0.0744171loss in batch 8: 0.0733795/0.074295loss in batch 9: 0.00378418/0.0672302loss in batch 10: 0.0239258/0.0633087loss in batch 11: 0.0431824/0.061615loss in batch 12: 0.0211029/0.0585022loss in batch 13: 0.109177/0.0621185loss in batch 14: 0.0696411/0.0626221loss in batch 15: 0.217194/0.0722809loss in batch 16: 0.0124817/0.0687714loss in batch 17: 0.181412/0.0750275loss in batch 18: 0.0830688/0.0754395loss in batch 19: 0.055191/0.0744324loss in batch 20: 0.0172577/0.0717163loss in batch 21: 0.0196533/0.0693359loss in batch 22: 0.0751801/0.0696106loss in batch 23: 0.142929/0.0726624loss in batch 24: 0.130402/0.0749664loss in batch 25: 0.029892/0.0732269loss in batch 26: 0.00822449/0.070816loss in batch 27: 0.408173/0.0828705loss in batch 28: 0.310638/0.0907288loss in batch 29: 0.0316925/0.0887604loss in batch 30: 0.0139313/0.0863495loss in batch 31: 0.046524/0.0851135loss in batch 32: 0.0677643/0.0845795loss in batch 33: 0.721832/0.103317loss in batch 34: 0.0703888/0.102386loss in batch 35: 0.0282593/0.100327loss in batch 36: 0.0573883/0.0991669loss in batch 37: 0.175018/0.101151loss in batch 38: 0.0162811/0.0989838loss in batch 39: 0.0153503/0.0968933loss in batch 40: 0.549561/0.107925loss in batch 41: 0.11174/0.108017loss in batch 42: 0.206177/0.110291loss in batch 43: 0.0929718/0.109909loss in batch 44: 0.137497/0.110519loss in batch 45: 0.687378/0.123062loss in batch 46: 0.0769043/0.12207loss in batch 47: 0.0662384/0.120911loss in batch 48: 0.026825/0.119003loss in batch 49: 0.0149536/0.116913loss in batch 50: 0.00965881/0.114807loss in batch 51: 0.0276489/0.113144loss in batch 52: 0.0378113/0.11171loss in batch 53: 0.0653992/0.110855loss in batch 54: 0.0664978/0.110046loss in batch 55: 0.00927734/0.108261loss in batch 56: 0.0496979/0.107224loss in batch 57: 0.00169373/0.105408loss in batch 58: 0.0389099/0.104279loss in batch 59: 0.434357/0.109787loss in batch 60: 0.200333/0.111252loss in batch 61: 0.236191/0.113281loss in batch 62: 0.154663/0.113937loss in batch 63: 0.95546/0.127075loss in batch 64: 0.048172/0.12587loss in batch 65: 0.247223/0.127701loss in batch 66: 0.0899963/0.127136loss in batch 67: 0.0158691/0.125519loss in batch 68: 0.340149/0.128616loss in batch 69: 0.013382/0.126968loss in batch 70: 0.11557/0.126801loss in batch 71: 0.0222473/0.125366loss in batch 72: 0.0515747/0.124359loss in batch 73: 0.300446/0.12674loss in batch 74: 0.0565643/0.125793loss in batch 75: 0.286041/0.127899loss in batch 76: 0.0364532/0.126709loss in batch 77: 0.100662/0.126389loss in batch 78: 0.0927124/0.125961loss in batch 79: 0.0552063/0.125076loss in batch 80: 0.0811615/0.124527loss in batch 81: 0.0677948/0.12384loss in batch 82: 0.0761871/0.12326loss in batch 83: 0.0307159/0.122162loss in batch 84: 0.0651855/0.12149loss in batch 85: 0.0682373/0.120865loss in batch 86: 0.00540161/0.119537loss in batch 87: 1.42149/0.134338loss in batch 88: 0.0517578/0.133408loss in batch 89: 0.0223236/0.132172loss in batch 90: 0.00546265/0.130783loss in batch 91: 0.978607/0.139999loss in batch 92: 0.542709/0.144333loss in batch 93: 0.400146/0.147064loss in batch 94: 0.0315552/0.145844loss in batch 95: 0.00280762/0.144348loss in batch 96: 0.0169067/0.143036loss in batch 97: 0.0964203/0.142548loss in batch 98: 0.0151215/0.141281loss in batch 99: 0.0882416/0.140732loss in batch 100: 0.09021/0.140228loss in batch 101: 0.0350189/0.139206loss in batch 102: 0.012207/0.13797loss in batch 103: 0.00933838/0.136734loss in batch 104: 0.0230865/0.135651loss in batch 105: 0.197647/0.136246loss in batch 106: 0.060379/0.135529loss in batch 107: 0.00758362/0.134354loss in batch 108: 0.684692/0.139404loss in batch 109: 0.0386047/0.138474loss in batch 110: 0.198181/0.139008loss in batch 111: 0.0449066/0.138168loss in batch 112: 0.602188/0.142273loss in batch 113: 0.0681763/0.141632loss in batch 114: 0.0589142/0.140915loss in batch 115: 0.123932/0.140778loss in batch 116: 0.69899/0.145538loss in batch 117: 0.116333/0.145294loss in batch 118: 0.0486603/0.14447loss in batch 119: 0.00167847/0.14328loss in batch 120: 0.104904/0.142975loss in batch 121: 0.015625/0.141922loss in batch 122: 0.0502472/0.141174loss in batch 123: 0.0012207/0.14006loss in batch 124: 0.469955/0.142685loss in batch 125: 0.0700073/0.142105loss in batch 126: 0.148865/0.142181loss in batch 127: 0.0751953/0.141647loss in batch 128: 0.0361633/0.140839loss in batch 129: 0.230179/0.14151loss in batch 130: 0.218658/0.142105loss in batch 131: 0.450394/0.14444loss in batch 132: 0.330429/0.145844loss in batch 133: 0.00817871/0.144806loss in batch 134: 0.0255585/0.143936loss in batch 135: 0.0699921/0.143387loss in batch 136: 0.102051/0.143082loss in batch 137: 0.078598/0.142624loss in batch 138: 0.263397/0.143494loss in batch 139: 0.0264435/0.142639loss in batch 140: 0.168335/0.142838loss in batch 141: 0.026001/0.141998loss in batch 142: 0.0599976/0.141434loss in batch 143: 0.22728/0.142029loss in batch 144: 0.387054/0.143723loss in batch 145: 0.655624/0.147232loss in batch 146: 0.061142/0.146637loss in batch 147: 0.0135193/0.145737loss in batch 148: 0.0829926/0.145325loss in batch 149: 0.0363464/0.144592loss in batch 150: 0.228714/0.145142loss in batch 151: 0.16539/0.145279loss in batch 152: 0.123779/0.145157loss in batch 153: 0.190704/0.145432loss in batch 154: 0.0796814/0.145004loss in batch 155: 0.0565948/0.14444loss in batch 156: 0.177689/0.144669loss in batch 157: 0.303482/0.14566loss in batch 158: 0.133667/0.145584loss in batch 159: 0.151794/0.14563loss in batch 160: 0.0065155/0.14476loss in batch 161: 0.00535583/0.143906loss in batch 162: 0.0156403/0.143112loss in batch 163: 0.0317688/0.142441loss in batch 164: 0.00431824/0.141602loss in batch 165: 0.0302429/0.14093loss in batch 166: 0.015976/0.140182loss in batch 167: 0.235336/0.140747loss in batch 168: 0.264038/0.141479loss in batch 169: 0.0540466/0.140961loss in batch 170: 0.0252228/0.140289loss in batch 171: 0.235626/0.140854loss in batch 172: 1.15012/0.146683loss in batch 173: 0.167252/0.14679loss in batch 174: 1.06888/0.152054loss in batch 175: 0.0865021/0.151688loss in batch 176: 0.0178986/0.15094loss in batch 177: 0.128906/0.150818loss in batch 178: 0.234451/0.151276loss in batch 179: 0.204346/0.151566loss in batch 180: 0.0403748/0.150955loss in batch 181: 0.0198975/0.150238loss in batch 182: 0.203461/0.150528loss in batch 183: 0.0340424/0.149902loss in batch 184: 0.405701/0.151276loss in batch 185: 0.538803/0.153366loss in batch 186: 0.077179/0.152954loss in batch 187: 0.0293732/0.152298loss in batch 188: 0.498505/0.154129loss in batch 189: 0.324463/0.155014loss in batch 190: 0.049881/0.15448loss in batch 191: 0.220047/0.154816loss in batch 192: 0.0130615/0.154083loss in batch 193: 0.11586/0.153885loss in batch 194: 0.0272675/0.153229loss in batch 195: 0.0338745/0.152634loss in batch 196: 0.285645/0.153305loss in batch 197: 0.235626/0.153717loss in batch 198: 0.0621185/0.153259loss in batch 199: 0.360916/0.154297loss in batch 200: 0.271713/0.154892loss in batch 201: 0.0319824/0.154282loss in batch 202: 0.224701/0.154617loss in batch 203: 0.361542/0.155624loss in batch 204: 0.0785065/0.155258loss in batch 205: 0.0250397/0.154617loss in batch 206: 0.0282288/0.154007loss in batch 207: 0.0559387/0.153534loss in batch 208: 0.309509/0.154282loss in batch 209: 0.0597076/0.153824loss in batch 210: 0.090744/0.153534loss in batch 211: 0.0327148/0.152969loss in batch 212: 0.198822/0.153183
done with epoch 19
train_acc: 0.950704 (405/426)
test loss: 0.198822
acc: 0.944056 (135/143)
[0.151947, -0.0636444, -0.0678864, -0.113159, -0.0203552, -0.0321808, -0.103241, -0.0145264, -0.00117493, -0.00270081, -0.299622, -0.1147, -0.00653076, -0.320969, -0.429428, 0.0770721, -0.0528412, 0.0108032, 0.0413666, 0.000823975, 0.421799, 0.06987, -0.178375, -0.0326538, 0.0112305, -0.0356903, -0.00946045, 0.0775452, -0.0476685, 0.0196686, -0.010437, 0.0384979, -0.34903, 0.256119, -0.00146484, 0.275177, -0.100143, 0.123062, -0.0998993, 0.267654, 0.819382, -0.00735474, 0.614334, -0.00454712, -0.421585, 0.000823975, -0.00106812, -0.0670166, -0.0120697, 0.0932007, 0.0410919, 0.131607, 0.0689545, -0.0393982, -0.0561676, -0.0158844, -0.0160065, -0.0075531, -0.0463715, 0.000732422, 0.389465, 0.181442, -0.0106964, -0.0303955, 0.0178223, -0.0421906, 0, 0.0119934, 0.00708008, -0.008255, -0.39624, 0.0285339, -0.0429382, 0.527496, 0.00460815, -0.0515289, -0.00779724, -0.167145, -0.0141449, -0.0010376, 0.194046, 0.000595093, 0.0290985, -0.0102539, 0.0926056, -0.041626, -0.0169983, -0.00274658, 0.0555267, 7.62939e-05, -0.00569153, 0.563034, -0.443527, 0.00479126, -0.00405884, -0.0409546, 0.0137939, -0.531799, -0.0316772, -0.0173187, -0.00120544, -0.0644531, -0.0725708, -0.193268, 0.0546875, -0.0096283, 0.0040741, -0.156754, 0.689911, 0.674286, -0.043396, 0.00732422, 0.0471649, -0.296249, -0.0124054, -0.033432, -0.317886, -0.0158997, -0.0453186, -0.119492, -0.0271759, -0.0254059, 0.24292, -0.0985718, 0.00372314, -0.150299, -0.224411, -0.0254059, -0.0139465, -0.00386047, 0.12323, -0.00305176, -0.0126801, -0.239792, -0.142685, -0.00946045, -0.23172, 0.5625, 0.000106812, -0.00827026, -0.0791473, -0.0307312, 0.000793457]
Compiler: ./compile.py breast_logistic
	750 triples of gfp left
	804 dabits of gfp left
	920 triples of gfp left
	166 dabits of gfp left
2 threads spent a total of 887.024 seconds (785.672 MB, 2735208 rounds) on the online phase, 1673.35 seconds (75425.3 MB, 705015 rounds) on the preprocessing/offline phase, and 2568.85 seconds idling.
Join timer: 0 2.56457e+06
Finish timer: 0.143519
Join timer: 1 2.54008e+06
Finish timer: 0.143519
Communication details (rounds in parallel threads counted double):
Exchanging one-to-one 47680 MB in 69855 rounds, taking 162.519 seconds
Receiving directly 785.672 MB in 1367604 rounds, taking 225.974 seconds
Receiving one-to-one 28664.5 MB in 317580 rounds, taking 54.8424 seconds
Sending directly 785.672 MB in 1367604 rounds, taking 93.6052 seconds
Sending one-to-one 27745.2 MB in 317580 rounds, taking 17.0102 seconds
CPU time = 2498.78 (overall core time)
The following benchmarks are including preprocessing (offline phase).
Time = 2564.72 seconds 
Data sent = 76210.9 MB in ~3440223 rounds (party 0 only)
Global data sent = 153341 MB (all parties)
Actual cost of program:
  Type int
      23279330        Triples
     229817398           Bits
This program might benefit from some protocol options.
Consider adding the following at the beginning of your code:
	program.use_trunc_pr = True
	program.use_edabit(True)
This virtual machine was compiled with GCC. Recompile with 'CXX = clang++' in 'CONFIG.mine' for optimal performance.
Command line: semi-party.x -N 2 -e --ip-file-name /HOST -p 0 -v breast_logistic
