Using statistical security parameter 40
No modulus found in /Player-Data//2-Dp-128/Params-Data, generating 128-bit prime
Current working directory: "/root/MP-SPDZ"
Current working directory: "/root/MP-SPDZ"
Using SGD
done with epoch 0done with epoch 1done with epoch 2done with epoch 3done with epoch 4done with epoch 5done with epoch 6done with epoch 7done with epoch 8done with epoch 9done with epoch 10done with epoch 11done with epoch 12done with epoch 13done with epoch 14done with epoch 15done with epoch 16done with epoch 17done with epoch 18done with epoch 19
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Using SGD
loss in batch 0: 0.750732/0.750732loss in batch 1: 0.626846/0.688782loss in batch 2: 0.870224/0.749268loss in batch 3: 0.643585/0.722855loss in batch 4: 0.707642/0.719803loss in batch 5: 0.636246/0.705887loss in batch 6: 0.72963/0.709259loss in batch 7: 0.680679/0.705704loss in batch 8: 0.710617/0.706253loss in batch 9: 0.739716/0.709595loss in batch 10: 0.484344/0.689117loss in batch 11: 0.755707/0.694672loss in batch 12: 0.675171/0.693161loss in batch 13: 0.639343/0.689316loss in batch 14: 0.589874/0.682693loss in batch 15: 0.628159/0.679291loss in batch 16: 0.676514/0.679123loss in batch 17: 0.77504/0.684448loss in batch 18: 0.769669/0.688934loss in batch 19: 0.614395/0.685211loss in batch 20: 0.732117/0.687439loss in batch 21: 0.637238/0.685165loss in batch 22: 0.592606/0.681137loss in batch 23: 0.663254/0.680389loss in batch 24: 0.639069/0.678726loss in batch 25: 0.717133/0.680222loss in batch 26: 0.636276/0.678589loss in batch 27: 0.673798/0.678421loss in batch 28: 0.528625/0.673248loss in batch 29: 0.573273/0.669922loss in batch 30: 0.701385/0.670929loss in batch 31: 0.70079/0.67186loss in batch 32: 0.796478/0.675644loss in batch 33: 0.673889/0.675583loss in batch 34: 0.471268/0.669754loss in batch 35: 0.594406/0.667664loss in batch 36: 0.487747/0.662796loss in batch 37: 0.445679/0.657089loss in batch 38: 0.718567/0.658661loss in batch 39: 0.447189/0.653366loss in batch 40: 0.380676/0.646729loss in batch 41: 0.448273/0.641998loss in batch 42: 0.733292/0.644119loss in batch 43: 0.687119/0.645096loss in batch 44: 0.62735/0.644699loss in batch 45: 0.385468/0.639053loss in batch 46: 0.369019/0.633316loss in batch 47: 0.807327/0.636948loss in batch 48: 0.329498/0.630676loss in batch 49: 0.677643/0.631622loss in batch 50: 0.790146/0.63472loss in batch 51: 0.670578/0.635406loss in batch 52: 0.335495/0.629761loss in batch 53: 0.695938/0.630966loss in batch 54: 0.366257/0.62616loss in batch 55: 0.85347/0.630219loss in batch 56: 0.708862/0.631607loss in batch 57: 0.703522/0.632843loss in batch 58: 0.3078/0.627335loss in batch 59: 0.721069/0.628891loss in batch 60: 0.387787/0.624939loss in batch 61: 0.354858/0.62059loss in batch 62: 0.709412/0.621994loss in batch 63: 0.893616/0.626236loss in batch 64: 0.639252/0.626434loss in batch 65: 0.929672/0.631027loss in batch 66: 0.373672/0.627197loss in batch 67: 0.461945/0.624756loss in batch 68: 0.753479/0.626617loss in batch 69: 0.494598/0.624741loss in batch 70: 0.606964/0.624496loss in batch 71: 0.535919/0.62326loss in batch 72: 0.622803/0.623245loss in batch 73: 0.541779/0.622162loss in batch 74: 0.610794/0.622009loss in batch 75: 0.525269/0.620728loss in batch 76: 0.621429/0.620743loss in batch 77: 0.487778/0.619034loss in batch 78: 0.496323/0.617493loss in batch 79: 0.495773/0.615967loss in batch 80: 0.4776/0.614243loss in batch 81: 0.582932/0.613876loss in batch 82: 0.487976/0.61235loss in batch 83: 0.527527/0.611343loss in batch 84: 0.761093/0.613113loss in batch 85: 0.493378/0.61171loss in batch 86: 0.383881/0.609085loss in batch 87: 0.407944/0.606812loss in batch 88: 0.329178/0.603683loss in batch 89: 0.663147/0.604355loss in batch 90: 0.66394/0.605011loss in batch 91: 0.721588/0.606262loss in batch 92: 0.628632/0.606522loss in batch 93: 0.537048/0.605774loss in batch 94: 0.542252/0.605103loss in batch 95: 0.59491/0.604996loss in batch 96: 0.411011/0.602997loss in batch 97: 0.513931/0.602081loss in batch 98: 0.575012/0.601807loss in batch 99: 0.47789/0.600586loss in batch 100: 0.413467/0.598724loss in batch 101: 0.456741/0.597336loss in batch 102: 0.491791/0.596313loss in batch 103: 0.699326/0.597305loss in batch 104: 0.53241/0.596695loss in batch 105: 0.835526/0.598938loss in batch 106: 0.497086/0.597977loss in batch 107: 0.524445/0.59729loss in batch 108: 0.400726/0.59549loss in batch 109: 0.345123/0.593216loss in batch 110: 0.428009/0.591736loss in batch 111: 0.353592/0.5896loss in batch 112: 0.824554/0.591675loss in batch 113: 0.524841/0.591095loss in batch 114: 0.646851/0.591583loss in batch 115: 0.622543/0.591843loss in batch 116: 0.334976/0.589645loss in batch 117: 0.403244/0.588074loss in batch 118: 0.42659/0.586716loss in batch 119: 0.450974/0.585587loss in batch 120: 0.36821/0.583801loss in batch 121: 0.6008/0.583939loss in batch 122: 0.462921/0.582947loss in batch 123: 0.479309/0.582108loss in batch 124: 0.317795/0.580002loss in batch 125: 0.276718/0.577591loss in batch 126: 0.537415/0.577271loss in batch 127: 0.344452/0.575455loss in batch 128: 0.539536/0.575165loss in batch 129: 0.248871/0.572662loss in batch 130: 0.3181/0.570724loss in batch 131: 0.302872/0.56868loss in batch 132: 0.313492/0.566772loss in batch 133: 0.223358/0.564224loss in batch 134: 0.299713/0.562241loss in batch 135: 0.669144/0.563034loss in batch 136: 0.344604/0.561447loss in batch 137: 0.214951/0.558929loss in batch 138: 0.919128/0.561523loss in batch 139: 0.330872/0.559875loss in batch 140: 0.821533/0.561737loss in batch 141: 0.715927/0.56282loss in batch 142: 0.546539/0.562714loss in batch 143: 0.338608/0.561142loss in batch 144: 0.320923/0.559494loss in batch 145: 0.302429/0.557724loss in batch 146: 0.595581/0.557983loss in batch 147: 0.812469/0.559708loss in batch 148: 0.228271/0.557495loss in batch 149: 0.41832/0.556564loss in batch 150: 0.513672/0.556274loss in batch 151: 0.627914/0.556732loss in batch 152: 0.390198/0.555649loss in batch 153: 0.244888/0.55365loss in batch 154: 0.334061/0.552216loss in batch 155: 0.64357/0.552811loss in batch 156: 0.250168/0.550873loss in batch 157: 0.699966/0.551819loss in batch 158: 0.638657/0.552368loss in batch 159: 0.452286/0.551743loss in batch 160: 0.526917/0.55159loss in batch 161: 0.302963/0.550049loss in batch 162: 0.501266/0.549744loss in batch 163: 0.665695/0.550461loss in batch 164: 0.482834/0.550049loss in batch 165: 0.542358/0.550003loss in batch 166: 0.537598/0.549942loss in batch 167: 0.640503/0.550476loss in batch 168: 0.255783/0.548721loss in batch 169: 0.388306/0.547775loss in batch 170: 0.5905/0.548035loss in batch 171: 0.482086/0.547653loss in batch 172: 0.306107/0.546249loss in batch 173: 0.533737/0.546173loss in batch 174: 0.439362/0.545578loss in batch 175: 0.503723/0.545334loss in batch 176: 0.669937/0.546036loss in batch 177: 0.481674/0.54567loss in batch 178: 0.532684/0.545609loss in batch 179: 0.631989/0.546082loss in batch 180: 0.411423/0.545349loss in batch 181: 0.594772/0.545609loss in batch 182: 0.716339/0.546539loss in batch 183: 0.386063/0.54567loss in batch 184: 0.598053/0.545959loss in batch 185: 0.343903/0.544876loss in batch 186: 0.314865/0.54364loss in batch 187: 0.536346/0.543594loss in batch 188: 0.268387/0.542145loss in batch 189: 0.577698/0.542328loss in batch 190: 0.362579/0.541382loss in batch 191: 0.466858/0.541loss in batch 192: 0.457825/0.540573loss in batch 193: 0.354782/0.539612loss in batch 194: 0.486374/0.539337loss in batch 195: 0.319855/0.538223loss in batch 196: 0.227814/0.536652loss in batch 197: 0.390259/0.535919loss in batch 198: 0.541458/0.535934loss in batch 199: 0.30275/0.534775loss in batch 200: 0.247833/0.53334loss in batch 201: 0.247513/0.531921loss in batch 202: 0.26828/0.530624loss in batch 203: 0.817413/0.532028loss in batch 204: 0.274979/0.530777loss in batch 205: 0.437317/0.530319loss in batch 206: 0.429092/0.529831loss in batch 207: 0.385284/0.529129loss in batch 208: 0.819962/0.530533loss in batch 209: 0.46759/0.530228loss in batch 210: 0.493576/0.53006loss in batch 211: 0.536911/0.53009loss in batch 212: 0.391266/0.529434
done with epoch 0
train_acc: 0.784038 (334/426)
test loss: 0.391266
acc: 0.825175 (118/143)
loss in batch 0: 0.28949/0.28949loss in batch 1: 0.317902/0.303696loss in batch 2: 0.561874/0.389755loss in batch 3: 0.472763/0.410507loss in batch 4: 0.304794/0.389359loss in batch 5: 0.347824/0.382446loss in batch 6: 0.284683/0.368484loss in batch 7: 0.795898/0.42189loss in batch 8: 0.280487/0.406189loss in batch 9: 0.619766/0.427551loss in batch 10: 0.233826/0.409943loss in batch 11: 0.302811/0.401016loss in batch 12: 0.405991/0.401382loss in batch 13: 0.186676/0.386063loss in batch 14: 0.394913/0.386642loss in batch 15: 0.379974/0.38623loss in batch 16: 0.304718/0.381424loss in batch 17: 0.349442/0.379654loss in batch 18: 0.284821/0.374664loss in batch 19: 0.320084/0.371933loss in batch 20: 0.405212/0.37352loss in batch 21: 0.461945/0.377548loss in batch 22: 0.455902/0.380936loss in batch 23: 0.669861/0.392975loss in batch 24: 0.371933/0.392136loss in batch 25: 0.393997/0.392212loss in batch 26: 0.217163/0.385742loss in batch 27: 0.343796/0.384232loss in batch 28: 0.196274/0.377747loss in batch 29: 0.424957/0.379318loss in batch 30: 1.2231/0.40654loss in batch 31: 0.245819/0.40152loss in batch 32: 0.294937/0.398285loss in batch 33: 0.38208/0.397812loss in batch 34: 0.329285/0.395859loss in batch 35: 0.117096/0.388107loss in batch 36: 0.255432/0.384521loss in batch 37: 0.699509/0.392807loss in batch 38: 0.14772/0.386536loss in batch 39: 0.358521/0.385834loss in batch 40: 0.417557/0.386612loss in batch 41: 0.22818/0.382828loss in batch 42: 0.277039/0.380386loss in batch 43: 0.511169/0.383347loss in batch 44: 0.492783/0.385773loss in batch 45: 0.286957/0.383621loss in batch 46: 0.607147/0.388382loss in batch 47: 0.286072/0.386246loss in batch 48: 0.44046/0.38736loss in batch 49: 0.285004/0.385315loss in batch 50: 0.277649/0.383194loss in batch 51: 0.481766/0.385101loss in batch 52: 0.164261/0.380936loss in batch 53: 0.576157/0.384552loss in batch 54: 0.351608/0.383942loss in batch 55: 0.332001/0.383011loss in batch 56: 0.491455/0.384933loss in batch 57: 0.283966/0.383179loss in batch 58: 0.629349/0.38736loss in batch 59: 0.158676/0.383545loss in batch 60: 0.444595/0.384537loss in batch 61: 0.92244/0.393219loss in batch 62: 0.27269/0.391296loss in batch 63: 0.255417/0.389191loss in batch 64: 0.577637/0.392075loss in batch 65: 0.21196/0.389359loss in batch 66: 0.27626/0.387665loss in batch 67: 0.296082/0.386322loss in batch 68: 0.199265/0.383606loss in batch 69: 0.225723/0.381348loss in batch 70: 0.417984/0.381866loss in batch 71: 0.144653/0.378571loss in batch 72: 0.636887/0.382111loss in batch 73: 0.303329/0.381058loss in batch 74: 0.559891/0.383423loss in batch 75: 0.438339/0.384155loss in batch 76: 0.468719/0.385239loss in batch 77: 0.185989/0.38269loss in batch 78: 0.293671/0.381561loss in batch 79: 0.402054/0.381821loss in batch 80: 0.433777/0.382477loss in batch 81: 0.264084/0.381027loss in batch 82: 0.325027/0.380356loss in batch 83: 0.150894/0.377625loss in batch 84: 0.335068/0.377106loss in batch 85: 0.214584/0.375229loss in batch 86: 0.257217/0.373871loss in batch 87: 0.140564/0.371216loss in batch 88: 0.145523/0.368683loss in batch 89: 0.239334/0.367249loss in batch 90: 0.120804/0.364548loss in batch 91: 0.63295/0.367462loss in batch 92: 0.531204/0.369217loss in batch 93: 0.143723/0.366821loss in batch 94: 1.21376/0.375732loss in batch 95: 0.197891/0.373886loss in batch 96: 0.194031/0.372025loss in batch 97: 0.761292/0.375992loss in batch 98: 0.984512/0.382141loss in batch 99: 0.17897/0.380112loss in batch 100: 0.777344/0.384048loss in batch 101: 0.342392/0.383636loss in batch 102: 0.140839/0.381287loss in batch 103: 0.546829/0.382874loss in batch 104: 0.424393/0.38327loss in batch 105: 0.408325/0.383514loss in batch 106: 0.40419/0.383698loss in batch 107: 0.29631/0.382889loss in batch 108: 0.312668/0.382233loss in batch 109: 0.295914/0.38147loss in batch 110: 0.568924/0.383148loss in batch 111: 0.220047/0.381683loss in batch 112: 0.191116/0.380005loss in batch 113: 0.369431/0.379913loss in batch 114: 0.755905/0.383194loss in batch 115: 0.204254/0.381638loss in batch 116: 0.182709/0.379944loss in batch 117: 0.316254/0.379395loss in batch 118: 0.740784/0.382431loss in batch 119: 0.365906/0.382294loss in batch 120: 0.393677/0.382401loss in batch 121: 0.653839/0.384613loss in batch 122: 0.116028/0.382431loss in batch 123: 0.4944/0.383347loss in batch 124: 0.367416/0.383209loss in batch 125: 0.258377/0.382217loss in batch 126: 0.327469/0.38179loss in batch 127: 0.278061/0.380981loss in batch 128: 0.399353/0.381134loss in batch 129: 0.265244/0.380234loss in batch 130: 0.239166/0.37915loss in batch 131: 0.570404/0.3806loss in batch 132: 0.220383/0.379395loss in batch 133: 0.423294/0.37973loss in batch 134: 0.445206/0.380203loss in batch 135: 0.566238/0.381577loss in batch 136: 0.274445/0.380798loss in batch 137: 0.227615/0.379684loss in batch 138: 0.502151/0.380554loss in batch 139: 0.206818/0.379318loss in batch 140: 0.380432/0.379318loss in batch 141: 0.343689/0.379074loss in batch 142: 0.026474/0.376617loss in batch 143: 0.188858/0.375305loss in batch 144: 0.538544/0.376434loss in batch 145: 0.257843/0.375626loss in batch 146: 0.366165/0.375549loss in batch 147: 0.189621/0.374313loss in batch 148: 0.250595/0.373474loss in batch 149: 0.35817/0.373383loss in batch 150: 0.333923/0.373108loss in batch 151: 0.299911/0.372635loss in batch 152: 0.31871/0.372269loss in batch 153: 0.281082/0.371689loss in batch 154: 0.643341/0.373444loss in batch 155: 0.362579/0.373367loss in batch 156: 0.391144/0.373474loss in batch 157: 0.394394/0.373611loss in batch 158: 0.124023/0.37204loss in batch 159: 0.40538/0.372253loss in batch 160: 0.712906/0.374359loss in batch 161: 0.358627/0.374268loss in batch 162: 0.380432/0.374313loss in batch 163: 0.247681/0.373535loss in batch 164: 0.445419/0.373978loss in batch 165: 0.47702/0.374588loss in batch 166: 0.801559/0.377151loss in batch 167: 0.368637/0.377106loss in batch 168: 0.16568/0.375854loss in batch 169: 0.555023/0.376892loss in batch 170: 0.149078/0.375565loss in batch 171: 0.338959/0.375366loss in batch 172: 0.167053/0.374146loss in batch 173: 0.33609/0.373932loss in batch 174: 0.556671/0.374969loss in batch 175: 0.39325/0.375076loss in batch 176: 0.269608/0.374481loss in batch 177: 0.498062/0.375183loss in batch 178: 0.237305/0.374405loss in batch 179: 0.224503/0.373581loss in batch 180: 0.390457/0.373672loss in batch 181: 0.39064/0.373764loss in batch 182: 0.130554/0.372437loss in batch 183: 0.255356/0.371796loss in batch 184: 0.285172/0.371323loss in batch 185: 0.293503/0.370911loss in batch 186: 0.356277/0.370834loss in batch 187: 0.269165/0.370285loss in batch 188: 0.383469/0.370346loss in batch 189: 0.376328/0.370392loss in batch 190: 0.567642/0.371429loss in batch 191: 0.154541/0.3703loss in batch 192: 0.340195/0.370132loss in batch 193: 0.281616/0.369675loss in batch 194: 0.675308/0.371246loss in batch 195: 0.178497/0.37027loss in batch 196: 0.579758/0.371338loss in batch 197: 0.263474/0.370773loss in batch 198: 0.27005/0.370285loss in batch 199: 0.231949/0.369583loss in batch 200: 0.466888/0.370071loss in batch 201: 0.180115/0.369125loss in batch 202: 0.298874/0.36879loss in batch 203: 0.271072/0.368301loss in batch 204: 0.133087/0.367157loss in batch 205: 0.276794/0.366714loss in batch 206: 0.290588/0.366348loss in batch 207: 0.679855/0.367859loss in batch 208: 0.468536/0.368347loss in batch 209: 0.134567/0.367233loss in batch 210: 0.250732/0.366669loss in batch 211: 0.34082/0.366547loss in batch 212: 0.4095/0.36676
done with epoch 1
train_acc: 0.894366 (381/426)
test loss: 0.4095
acc: 0.909091 (130/143)
loss in batch 0: 0.194489/0.194489loss in batch 1: 0.337585/0.266037loss in batch 2: 0.472183/0.334747loss in batch 3: 0.551788/0.389023loss in batch 4: 0.331985/0.377594loss in batch 5: 0.450516/0.389771loss in batch 6: 0.163025/0.357376loss in batch 7: 0.561722/0.382904loss in batch 8: 0.199005/0.362488loss in batch 9: 0.304733/0.356705loss in batch 10: 0.247574/0.346786loss in batch 11: 0.0948639/0.32579loss in batch 12: 0.384201/0.330276loss in batch 13: 0.208771/0.321609loss in batch 14: 0.152908/0.310349loss in batch 15: 0.262482/0.307373loss in batch 16: 0.546799/0.321442loss in batch 17: 0.161758/0.312576loss in batch 18: 0.225296/0.307983loss in batch 19: 0.178116/0.301498loss in batch 20: 0.562851/0.313934loss in batch 21: 0.187622/0.308197loss in batch 22: 0.206757/0.303787loss in batch 23: 0.246628/0.301407loss in batch 24: 0.375259/0.304352loss in batch 25: 0.201767/0.300415loss in batch 26: 0.218781/0.297394loss in batch 27: 0.352005/0.299347loss in batch 28: 0.496094/0.306122loss in batch 29: 0.261063/0.304626loss in batch 30: 0.22168/0.301956loss in batch 31: 0.0717621/0.294739loss in batch 32: 0.269424/0.293976loss in batch 33: 0.254059/0.292801loss in batch 34: 0.347595/0.294373loss in batch 35: 0.101105/0.289017loss in batch 36: 0.640472/0.298508loss in batch 37: 0.0895081/0.292999loss in batch 38: 0.25827/0.292114loss in batch 39: 0.609634/0.300049loss in batch 40: 0.355667/0.301407loss in batch 41: 0.505646/0.306274loss in batch 42: 0.641739/0.314072loss in batch 43: 0.303619/0.313828loss in batch 44: 0.455048/0.316971loss in batch 45: 0.18129/0.314026loss in batch 46: 0.484314/0.317657loss in batch 47: 0.309097/0.317474loss in batch 48: 0.184875/0.314758loss in batch 49: 0.435104/0.317169loss in batch 50: 0.150482/0.313904loss in batch 51: 0.236374/0.312408loss in batch 52: 0.148331/0.309326loss in batch 53: 0.238556/0.307999loss in batch 54: 0.644547/0.314133loss in batch 55: 0.177673/0.311691loss in batch 56: 0.102371/0.308014loss in batch 57: 0.415787/0.309875loss in batch 58: 0.119812/0.306656loss in batch 59: 0.163101/0.30426loss in batch 60: 0.189209/0.302383loss in batch 61: 0.143097/0.299805loss in batch 62: 0.441376/0.302048loss in batch 63: 0.34169/0.302658loss in batch 64: 0.387741/0.303986loss in batch 65: 0.275497/0.303543loss in batch 66: 0.265182/0.302979loss in batch 67: 0.598892/0.307327loss in batch 68: 0.168839/0.305328loss in batch 69: 0.257782/0.304642loss in batch 70: 0.125931/0.302124loss in batch 71: 0.116257/0.299545loss in batch 72: 0.216293/0.298401loss in batch 73: 0.145462/0.296341loss in batch 74: 0.257294/0.295807loss in batch 75: 0.25856/0.295319loss in batch 76: 0.13884/0.293304loss in batch 77: 0.435608/0.29512loss in batch 78: 0.735886/0.300705loss in batch 79: 0.22142/0.299713loss in batch 80: 0.254776/0.299149loss in batch 81: 0.178131/0.297668loss in batch 82: 0.455872/0.299591loss in batch 83: 0.250381/0.298996loss in batch 84: 0.160873/0.297363loss in batch 85: 0.191254/0.296127loss in batch 86: 0.298004/0.296158loss in batch 87: 0.241989/0.295547loss in batch 88: 0.237091/0.294876loss in batch 89: 0.237946/0.29425loss in batch 90: 0.140747/0.292572loss in batch 91: 0.7052/0.297058loss in batch 92: 0.339615/0.297501loss in batch 93: 0.331955/0.297867loss in batch 94: 0.260757/0.297485loss in batch 95: 0.0804596/0.295227loss in batch 96: 0.472244/0.297043loss in batch 97: 0.65329/0.30069loss in batch 98: 0.274277/0.300415loss in batch 99: 0.389496/0.301315loss in batch 100: 0.161316/0.299911loss in batch 101: 0.779831/0.304626loss in batch 102: 0.0840759/0.302475loss in batch 103: 0.384949/0.303284loss in batch 104: 0.165497/0.301971loss in batch 105: 0.187668/0.300888loss in batch 106: 0.263504/0.300537loss in batch 107: 0.158844/0.299225loss in batch 108: 0.204758/0.298355loss in batch 109: 0.433609/0.299591loss in batch 110: 0.346268/0.300018loss in batch 111: 0.248718/0.299561loss in batch 112: 0.336105/0.299881loss in batch 113: 0.277664/0.299683loss in batch 114: 0.39592/0.300507loss in batch 115: 0.67894/0.303772loss in batch 116: 0.232834/0.303177loss in batch 117: 0.522141/0.305023loss in batch 118: 0.275681/0.304779loss in batch 119: 0.861618/0.309418loss in batch 120: 0.0371094/0.307175loss in batch 121: 0.322449/0.307297loss in batch 122: 0.148193/0.306015loss in batch 123: 0.194244/0.305115loss in batch 124: 0.155579/0.303909loss in batch 125: 0.433838/0.304947loss in batch 126: 0.0441895/0.302887loss in batch 127: 0.145828/0.301666loss in batch 128: 0.784378/0.305389loss in batch 129: 0.370605/0.305908loss in batch 130: 0.452148/0.307022loss in batch 131: 0.166687/0.305954loss in batch 132: 0.468155/0.307175loss in batch 133: 0.108795/0.305695loss in batch 134: 0.239716/0.305206loss in batch 135: 0.328903/0.305389loss in batch 136: 0.469772/0.30658loss in batch 137: 0.676453/0.30925loss in batch 138: 0.253586/0.308868loss in batch 139: 0.149689/0.307724loss in batch 140: 0.101105/0.306259loss in batch 141: 0.112366/0.304901loss in batch 142: 0.251144/0.30452loss in batch 143: 0.194962/0.303757loss in batch 144: 0.374268/0.304245loss in batch 145: 0.316116/0.304321loss in batch 146: 0.531799/0.305862loss in batch 147: 0.650391/0.308197loss in batch 148: 0.202637/0.30748loss in batch 149: 0.211624/0.306854loss in batch 150: 0.378891/0.307327loss in batch 151: 0.173294/0.306442loss in batch 152: 0.191116/0.305695loss in batch 153: 0.141693/0.304626loss in batch 154: 0.409775/0.305298loss in batch 155: 0.564789/0.306976loss in batch 156: 0.675812/0.309326loss in batch 157: 0.250977/0.308945loss in batch 158: 0.309296/0.30896loss in batch 159: 0.0592804/0.307388loss in batch 160: 0.318359/0.307465loss in batch 161: 0.0810547/0.306061loss in batch 162: 0.163025/0.305176loss in batch 163: 0.243027/0.304794loss in batch 164: 0.371109/0.305206loss in batch 165: 0.208237/0.304611loss in batch 166: 0.115555/0.303497loss in batch 167: 0.306137/0.303513loss in batch 168: 0.351715/0.303787loss in batch 169: 0.131287/0.30278loss in batch 170: 0.248474/0.30246loss in batch 171: 0.331711/0.302628loss in batch 172: 0.0619507/0.301239loss in batch 173: 0.656311/0.303284loss in batch 174: 0.315292/0.303345loss in batch 175: 0.207474/0.302795loss in batch 176: 0.207169/0.302261loss in batch 177: 0.162003/0.301483loss in batch 178: 0.100037/0.300354loss in batch 179: 0.202408/0.299805loss in batch 180: 0.561386/0.301239loss in batch 181: 0.104797/0.300171loss in batch 182: 0.334213/0.300369loss in batch 183: 0.146011/0.29953loss in batch 184: 0.216019/0.299072loss in batch 185: 0.0893097/0.297943loss in batch 186: 0.286362/0.297882loss in batch 187: 0.181015/0.297256loss in batch 188: 0.0550995/0.295975loss in batch 189: 1.04381/0.299911loss in batch 190: 0.549194/0.301208loss in batch 191: 0.114517/0.300232loss in batch 192: 0.170349/0.299576loss in batch 193: 0.537247/0.300781loss in batch 194: 0.365845/0.301132loss in batch 195: 0.0855408/0.300034loss in batch 196: 0.233292/0.299683loss in batch 197: 0.476471/0.300583loss in batch 198: 0.146729/0.299805loss in batch 199: 0.175278/0.299194loss in batch 200: 0.489014/0.300125loss in batch 201: 0.0486298/0.298874loss in batch 202: 0.45491/0.299652loss in batch 203: 0.370331/0.300003loss in batch 204: 0.416855/0.300568loss in batch 205: 0.299911/0.300568loss in batch 206: 0.170883/0.299942loss in batch 207: 0.137192/0.299164loss in batch 208: 0.998505/0.302505loss in batch 209: 0.092865/0.301514loss in batch 210: 0.130783/0.300705loss in batch 211: 0.375458/0.301041loss in batch 212: 0.102966/0.30011
done with epoch 2
train_acc: 0.913146 (389/426)
test loss: 0.102966
acc: 0.93007 (133/143)
loss in batch 0: 0.339706/0.339706loss in batch 1: 0.145355/0.242523loss in batch 2: 0.171616/0.218887loss in batch 3: 0.374908/0.257889loss in batch 4: 0.663544/0.33902loss in batch 5: 0.346252/0.340225loss in batch 6: 0.410187/0.350235loss in batch 7: 0.155182/0.325851loss in batch 8: 0.139465/0.30513loss in batch 9: 0.330276/0.307648loss in batch 10: 0.25293/0.302673loss in batch 11: 0.627136/0.329712loss in batch 12: 0.456329/0.339447loss in batch 13: 0.219009/0.330856loss in batch 14: 0.240753/0.324844loss in batch 15: 0.203552/0.317261loss in batch 16: 0.180344/0.309204loss in batch 17: 0.461929/0.317688loss in batch 18: 0.811401/0.343689loss in batch 19: 0.176361/0.335312loss in batch 20: 0.283569/0.33284loss in batch 21: 0.394989/0.335678loss in batch 22: 0.170212/0.328476loss in batch 23: 0.38913/0.330994loss in batch 24: 0.2742/0.328735loss in batch 25: 0.147186/0.321747loss in batch 26: 0.154388/0.315552loss in batch 27: 0.448715/0.320313loss in batch 28: 0.361938/0.321747loss in batch 29: 0.353577/0.3228loss in batch 30: 0.343994/0.323486loss in batch 31: 0.352325/0.324387loss in batch 32: 0.277344/0.322968loss in batch 33: 0.750259/0.335526loss in batch 34: 0.167221/0.330719loss in batch 35: 0.159866/0.325974loss in batch 36: 0.173386/0.321854loss in batch 37: 0.628311/0.329926loss in batch 38: 0.45932/0.333237loss in batch 39: 0.547882/0.338593loss in batch 40: 0.190292/0.334976loss in batch 41: 0.229294/0.332458loss in batch 42: 0.0995789/0.327042loss in batch 43: 0.159531/0.323242loss in batch 44: 0.18486/0.32016loss in batch 45: 0.145447/0.316376loss in batch 46: 0.638/0.323212loss in batch 47: 0.434647/0.325531loss in batch 48: 0.29628/0.324936loss in batch 49: 0.0590973/0.319611loss in batch 50: 0.548187/0.324097loss in batch 51: 0.228699/0.322266loss in batch 52: 0.134354/0.318726loss in batch 53: 0.0362854/0.313492loss in batch 54: 0.0821075/0.309296loss in batch 55: 0.441864/0.311661loss in batch 56: 0.113846/0.308182loss in batch 57: 0.246323/0.307129loss in batch 58: 0.313904/0.307236loss in batch 59: 0.154114/0.304688loss in batch 60: 0.45575/0.307159loss in batch 61: 0.263077/0.306458loss in batch 62: 0.074234/0.302765loss in batch 63: 0.341385/0.30336loss in batch 64: 0.367752/0.304352loss in batch 65: 0.19577/0.302704loss in batch 66: 0.248337/0.301895loss in batch 67: 0.251816/0.301163loss in batch 68: 0.0657349/0.297745loss in batch 69: 0.632874/0.302536loss in batch 70: 0.646133/0.307373loss in batch 71: 0.167023/0.30542loss in batch 72: 0.504776/0.308151loss in batch 73: 0.170105/0.30629loss in batch 74: 0.297363/0.306183loss in batch 75: 0.141953/0.304001loss in batch 76: 0.124008/0.301682loss in batch 77: 0.299942/0.301651loss in batch 78: 0.257584/0.301102loss in batch 79: 0.254044/0.300507loss in batch 80: 0.549561/0.303574loss in batch 81: 0.197556/0.302277loss in batch 82: 0.408752/0.303574loss in batch 83: 0.193649/0.302261loss in batch 84: 0.334351/0.302628loss in batch 85: 0.256088/0.302094loss in batch 86: 0.180618/0.300705loss in batch 87: 0.152466/0.299011loss in batch 88: 0.0787048/0.296539loss in batch 89: 0.135284/0.294754loss in batch 90: 0.704819/0.299255loss in batch 91: 0.0980835/0.297073loss in batch 92: 0.165253/0.295654loss in batch 93: 0.127563/0.293854loss in batch 94: 0.374725/0.294724loss in batch 95: 0.0832367/0.292511loss in batch 96: 1.05669/0.300385loss in batch 97: 0.153503/0.298889loss in batch 98: 0.301559/0.29892loss in batch 99: 0.292923/0.298859loss in batch 100: 0.305222/0.29892loss in batch 101: 0.131088/0.297272loss in batch 102: 0.250748/0.296814loss in batch 103: 0.529007/0.299057loss in batch 104: 0.175568/0.297882loss in batch 105: 0.0943298/0.295959loss in batch 106: 0.13324/0.294434loss in batch 107: 0.0467682/0.292145loss in batch 108: 0.121948/0.290588loss in batch 109: 0.183701/0.289612loss in batch 110: 0.0942841/0.287857loss in batch 111: 0.0736084/0.28595loss in batch 112: 0.139679/0.284653loss in batch 113: 0.218903/0.284058loss in batch 114: 0.524429/0.286163loss in batch 115: 0.255875/0.285904loss in batch 116: 0.61853/0.288742loss in batch 117: 0.119629/0.287308loss in batch 118: 0.10556/0.285782loss in batch 119: 0.203094/0.285095loss in batch 120: 0.39888/0.286026loss in batch 121: 0.545761/0.288162loss in batch 122: 0.0861816/0.28653loss in batch 123: 0.172119/0.285599loss in batch 124: 0.164078/0.284622loss in batch 125: 0.143402/0.283508loss in batch 126: 0.0802002/0.281906loss in batch 127: 0.217987/0.281403loss in batch 128: 0.287003/0.281448loss in batch 129: 0.0536499/0.279694loss in batch 130: 0.322632/0.280014loss in batch 131: 0.0684357/0.278427loss in batch 132: 0.324173/0.278763loss in batch 133: 0.173248/0.277985loss in batch 134: 0.126663/0.276855loss in batch 135: 0.113983/0.27565loss in batch 136: 0.184113/0.274994loss in batch 137: 0.264969/0.274918loss in batch 138: 0.141434/0.273956loss in batch 139: 0.204422/0.273453loss in batch 140: 0.810822/0.277267loss in batch 141: 0.511383/0.278931loss in batch 142: 0.0957031/0.277649loss in batch 143: 0.577713/0.279709loss in batch 144: 0.11702/0.278595loss in batch 145: 0.0907135/0.277298loss in batch 146: 0.121765/0.276245loss in batch 147: 0.0427551/0.274673loss in batch 148: 0.15686/0.27388loss in batch 149: 0.168182/0.273178loss in batch 150: 0.150467/0.272369loss in batch 151: 0.18573/0.271805loss in batch 152: 0.30275/0.272003loss in batch 153: 0.397736/0.272812loss in batch 154: 0.263992/0.272751loss in batch 155: 0.110092/0.271713loss in batch 156: 0.092453/0.270569loss in batch 157: 0.548462/0.272339loss in batch 158: 0.17981/0.271759loss in batch 159: 0.836624/0.275284loss in batch 160: 0.31105/0.275497loss in batch 161: 0.183472/0.274933loss in batch 162: 0.393143/0.275665loss in batch 163: 0.377441/0.276291loss in batch 164: 0.683548/0.278763loss in batch 165: 0.263641/0.278671loss in batch 166: 0.103714/0.277618loss in batch 167: 0.0945587/0.27652loss in batch 168: 0.306442/0.276703loss in batch 169: 0.141129/0.275909loss in batch 170: 0.315628/0.276138loss in batch 171: 0.178268/0.275558loss in batch 172: 0.492416/0.27681loss in batch 173: 0.102249/0.275818loss in batch 174: 0.395126/0.276505loss in batch 175: 0.168686/0.275894loss in batch 176: 0.378815/0.276474loss in batch 177: 0.217819/0.276138loss in batch 178: 0.106796/0.275192loss in batch 179: 0.0729218/0.274063loss in batch 180: 0.11264/0.273178loss in batch 181: 0.244858/0.27301loss in batch 182: 0.0631104/0.271881loss in batch 183: 0.170731/0.271332loss in batch 184: 0.147812/0.270645loss in batch 185: 0.313049/0.270874loss in batch 186: 0.0523834/0.269714loss in batch 187: 0.295547/0.269852loss in batch 188: 1.00711/0.273758loss in batch 189: 0.210739/0.273422loss in batch 190: 0.231995/0.273209loss in batch 191: 0.103622/0.272308loss in batch 192: 0.136871/0.271622loss in batch 193: 0.186462/0.271179loss in batch 194: 0.328476/0.271469loss in batch 195: 0.45842/0.27243loss in batch 196: 0.167786/0.271896loss in batch 197: 0.0680084/0.270874loss in batch 198: 0.244339/0.270737loss in batch 199: 0.0791779/0.269775loss in batch 200: 0.124329/0.269058loss in batch 201: 0.19136/0.268677loss in batch 202: 0.262756/0.268646loss in batch 203: 0.21431/0.268372loss in batch 204: 0.108154/0.267593loss in batch 205: 0.290604/0.2677loss in batch 206: 0.112717/0.266953loss in batch 207: 0.267746/0.266953loss in batch 208: 0.125916/0.266281loss in batch 209: 0.0911407/0.265442loss in batch 210: 0.125137/0.264786loss in batch 211: 0.173782/0.264343loss in batch 212: 0.131805/0.263733
done with epoch 3
train_acc: 0.92723 (395/426)
test loss: 0.131805
acc: 0.909091 (130/143)
loss in batch 0: 0.226242/0.226242loss in batch 1: 0.564499/0.39537loss in batch 2: 0.114807/0.301849loss in batch 3: 0.379547/0.321274loss in batch 4: 0.199707/0.296951loss in batch 5: 0.0881958/0.262161loss in batch 6: 0.153305/0.246613loss in batch 7: 0.862091/0.323563loss in batch 8: 0.0645447/0.294769loss in batch 9: 0.0313721/0.268433loss in batch 10: 0.188919/0.2612loss in batch 11: 0.136917/0.250839loss in batch 12: 0.185059/0.245789loss in batch 13: 0.119675/0.236771loss in batch 14: 0.278763/0.239578loss in batch 15: 0.271652/0.241577loss in batch 16: 0.19075/0.238586loss in batch 17: 0.111679/0.231552loss in batch 18: 0.140686/0.226761loss in batch 19: 0.199188/0.225388loss in batch 20: 0.914322/0.258179loss in batch 21: 0.0585632/0.249115loss in batch 22: 0.0617065/0.240967loss in batch 23: 0.222198/0.240189loss in batch 24: 0.178986/0.237747loss in batch 25: 0.153061/0.234482loss in batch 26: 0.332199/0.238098loss in batch 27: 0.986374/0.264816loss in batch 28: 0.42276/0.270279loss in batch 29: 0.345306/0.272766loss in batch 30: 0.238937/0.271683loss in batch 31: 0.208542/0.269714loss in batch 32: 0.0545197/0.263184loss in batch 33: 0.0990143/0.258362loss in batch 34: 0.254761/0.258255loss in batch 35: 0.166183/0.255692loss in batch 36: 0.0573578/0.250336loss in batch 37: 0.643509/0.260681loss in batch 38: 0.286163/0.261337loss in batch 39: 0.304596/0.262421loss in batch 40: 0.105759/0.258606loss in batch 41: 0.218445/0.257645loss in batch 42: 0.210892/0.256546loss in batch 43: 0.163361/0.25444loss in batch 44: 0.140991/0.251907loss in batch 45: 0.302719/0.253021loss in batch 46: 0.39183/0.255981loss in batch 47: 0.0737305/0.252167loss in batch 48: 0.430466/0.255814loss in batch 49: 0.284119/0.256378loss in batch 50: 0.237106/0.255997loss in batch 51: 0.109909/0.253189loss in batch 52: 0.136002/0.250977loss in batch 53: 0.118301/0.24852loss in batch 54: 0.0700378/0.245285loss in batch 55: 0.313477/0.24649loss in batch 56: 0.0447998/0.24295loss in batch 57: 0.24411/0.242981loss in batch 58: 0.237686/0.242889loss in batch 59: 0.123489/0.240906loss in batch 60: 0.174484/0.239807loss in batch 61: 0.167374/0.238632loss in batch 62: 0.261993/0.239014loss in batch 63: 0.139526/0.237457loss in batch 64: 0.292953/0.238312loss in batch 65: 0.291916/0.23912loss in batch 66: 0.0970612/0.237loss in batch 67: 0.161758/0.235886loss in batch 68: 0.0869904/0.233734loss in batch 69: 0.239822/0.233826loss in batch 70: 0.546555/0.238235loss in batch 71: 0.20578/0.237778loss in batch 72: 0.423004/0.240311loss in batch 73: 0.088623/0.238266loss in batch 74: 0.120468/0.236694loss in batch 75: 0.460434/0.239639loss in batch 76: 0.370316/0.241333loss in batch 77: 0.133194/0.23996loss in batch 78: 0.224609/0.239761loss in batch 79: 0.340683/0.241013loss in batch 80: 0.162537/0.240051loss in batch 81: 0.0552521/0.237793loss in batch 82: 0.288223/0.238403loss in batch 83: 0.430237/0.240692loss in batch 84: 0.135468/0.239441loss in batch 85: 0.805222/0.246033loss in batch 86: 0.0825348/0.244141loss in batch 87: 0.107727/0.242599loss in batch 88: 0.430481/0.244705loss in batch 89: 0.519547/0.247757loss in batch 90: 0.269684/0.248001loss in batch 91: 0.146652/0.246902loss in batch 92: 0.0707397/0.24501loss in batch 93: 0.223846/0.244781loss in batch 94: 0.0765686/0.243011loss in batch 95: 0.143753/0.241974loss in batch 96: 0.627777/0.245956loss in batch 97: 0.0187225/0.243637loss in batch 98: 0.328094/0.244492loss in batch 99: 0.0905609/0.24295loss in batch 100: 0.187531/0.242401loss in batch 101: 0.0652771/0.240677loss in batch 102: 0.17836/0.240067loss in batch 103: 0.0891571/0.238617loss in batch 104: 0.431992/0.240448loss in batch 105: 0.0585175/0.238739loss in batch 106: 0.161667/0.238007loss in batch 107: 0.259552/0.23822loss in batch 108: 0.115555/0.237076loss in batch 109: 0.0906219/0.235748loss in batch 110: 0.119156/0.234711loss in batch 111: 0.917297/0.240799loss in batch 112: 0.0604095/0.239197loss in batch 113: 0.431595/0.240891loss in batch 114: 0.200851/0.24054loss in batch 115: 0.119064/0.239502loss in batch 116: 0.69899/0.243423loss in batch 117: 0.0925903/0.242142loss in batch 118: 0.270828/0.242386loss in batch 119: 0.168335/0.241776loss in batch 120: 0.6931/0.245499loss in batch 121: 0.191101/0.245041loss in batch 122: 0.117615/0.244019loss in batch 123: 0.112427/0.242966loss in batch 124: 0.0283051/0.241241loss in batch 125: 0.313812/0.241821loss in batch 126: 0.680008/0.24527loss in batch 127: 0.289215/0.245605loss in batch 128: 0.103363/0.244507loss in batch 129: 0.367859/0.245453loss in batch 130: 0.107925/0.244415loss in batch 131: 0.371689/0.245377loss in batch 132: 0.152832/0.244659loss in batch 133: 0.41954/0.245972loss in batch 134: 0.180328/0.245499loss in batch 135: 0.408966/0.246704loss in batch 136: 0.558746/0.248962loss in batch 137: 0.968201/0.254181loss in batch 138: 0.47612/0.255783loss in batch 139: 0.717545/0.259079loss in batch 140: 0.202972/0.258682loss in batch 141: 0.423538/0.259842loss in batch 142: 0.20195/0.259445loss in batch 143: 0.0474243/0.257965loss in batch 144: 0.0795135/0.256729loss in batch 145: 0.0389862/0.255249loss in batch 146: 0.327362/0.255737loss in batch 147: 0.462784/0.257141loss in batch 148: 0.13298/0.256302loss in batch 149: 0.184006/0.255814loss in batch 150: 0.16156/0.255203loss in batch 151: 0.356735/0.255859loss in batch 152: 0.142853/0.255112loss in batch 153: 0.346039/0.255707loss in batch 154: 0.322601/0.256134loss in batch 155: 0.244507/0.256073loss in batch 156: 0.263519/0.256119loss in batch 157: 0.36998/0.256836loss in batch 158: 0.297379/0.257095loss in batch 159: 0.420395/0.258118loss in batch 160: 0.0515442/0.256821loss in batch 161: 0.161469/0.256256loss in batch 162: 0.095932/0.255264loss in batch 163: 0.410324/0.25621loss in batch 164: 0.063736/0.255035loss in batch 165: 0.121536/0.254227loss in batch 166: 0.0728455/0.253143loss in batch 167: 0.240616/0.253067loss in batch 168: 0.0928192/0.252121loss in batch 169: 0.117676/0.251343loss in batch 170: 0.109741/0.250504loss in batch 171: 0.302307/0.250809loss in batch 172: 0.155197/0.250244loss in batch 173: 0.0632019/0.249176loss in batch 174: 0.247101/0.249161loss in batch 175: 0.106628/0.248352loss in batch 176: 0.254105/0.248383loss in batch 177: 0.526352/0.249954loss in batch 178: 0.0707703/0.248947loss in batch 179: 0.557541/0.250671loss in batch 180: 0.411728/0.251556loss in batch 181: 0.116608/0.250809loss in batch 182: 0.119202/0.250107loss in batch 183: 0.104843/0.249298loss in batch 184: 0.0455017/0.248199loss in batch 185: 0.990341/0.252197loss in batch 186: 0.0534668/0.251129loss in batch 187: 0.0848694/0.250244loss in batch 188: 0.207336/0.250015loss in batch 189: 0.0516357/0.248978loss in batch 190: 0.0657959/0.248016loss in batch 191: 0.161041/0.247559loss in batch 192: 0.065033/0.246613loss in batch 193: 0.162201/0.246185loss in batch 194: 0.161713/0.245743loss in batch 195: 0.0271454/0.244644loss in batch 196: 0.181473/0.244308loss in batch 197: 0.286118/0.244522loss in batch 198: 0.155441/0.24408loss in batch 199: 0.0687256/0.243195loss in batch 200: 0.31665/0.243561loss in batch 201: 0.113846/0.24292loss in batch 202: 0.184601/0.24263loss in batch 203: 0.310608/0.242981loss in batch 204: 0.101593/0.242279loss in batch 205: 0.222061/0.242188loss in batch 206: 0.405441/0.242966loss in batch 207: 0.0422821/0.242004loss in batch 208: 0.499115/0.24324loss in batch 209: 0.342728/0.243713loss in batch 210: 0.722427/0.245972loss in batch 211: 0.0995331/0.245285loss in batch 212: 0.0947418/0.244568
done with epoch 4
train_acc: 0.924883 (394/426)
test loss: 0.0947418
acc: 0.93007 (133/143)
loss in batch 0: 0.117523/0.117523loss in batch 1: 0.092804/0.105164loss in batch 2: 0.208115/0.139481loss in batch 3: 0.441742/0.215042loss in batch 4: 0.126328/0.197296loss in batch 5: 0.486145/0.245438loss in batch 6: 0.225067/0.242523loss in batch 7: 0.230743/0.241058loss in batch 8: 0.173096/0.233505loss in batch 9: 0.0602722/0.216187loss in batch 10: 0.167725/0.211777loss in batch 11: 0.0431519/0.197723loss in batch 12: 0.269943/0.203293loss in batch 13: 0.0635834/0.193298loss in batch 14: 0.297272/0.200226loss in batch 15: 0.432678/0.214752loss in batch 16: 0.0680695/0.206131loss in batch 17: 0.501282/0.222534loss in batch 18: 0.0606537/0.214005loss in batch 19: 0.103928/0.208496loss in batch 20: 0.160309/0.206207loss in batch 21: 0.768234/0.231766loss in batch 22: 0.0505676/0.223877loss in batch 23: 0.0452576/0.216431loss in batch 24: 0.0912781/0.211426loss in batch 25: 0.0712738/0.206039loss in batch 26: 0.134415/0.203384loss in batch 27: 0.305893/0.207047loss in batch 28: 0.206009/0.207016loss in batch 29: 0.0563049/0.201981loss in batch 30: 0.117554/0.199265loss in batch 31: 0.0608215/0.194931loss in batch 32: 0.0370483/0.190155loss in batch 33: 0.801361/0.20813loss in batch 34: 0.162415/0.206818loss in batch 35: 0.0458527/0.202347loss in batch 36: 0.0947266/0.199448loss in batch 37: 0.117584/0.197296loss in batch 38: 0.153732/0.196167loss in batch 39: 0.344574/0.199875loss in batch 40: 0.212646/0.200195loss in batch 41: 0.304382/0.202667loss in batch 42: 0.0656281/0.199493loss in batch 43: 0.318237/0.202179loss in batch 44: 0.30101/0.204376loss in batch 45: 0.543869/0.211761loss in batch 46: 0.232651/0.212204loss in batch 47: 0.340576/0.214874loss in batch 48: 0.0597687/0.211716loss in batch 49: 0.274261/0.212967loss in batch 50: 0.31572/0.214981loss in batch 51: 0.0354156/0.211517loss in batch 52: 0.214478/0.211578loss in batch 53: 0.202896/0.211426loss in batch 54: 0.0842438/0.209106loss in batch 55: 0.0338593/0.205978loss in batch 56: 0.155884/0.205109loss in batch 57: 0.154358/0.204224loss in batch 58: 0.681168/0.212311loss in batch 59: 0.0669556/0.209885loss in batch 60: 0.546371/0.215408loss in batch 61: 0.652802/0.222458loss in batch 62: 0.8004/0.231644loss in batch 63: 0.0471191/0.22876loss in batch 64: 0.108994/0.226913loss in batch 65: 0.149597/0.225723loss in batch 66: 0.244919/0.226013loss in batch 67: 0.364151/0.228058loss in batch 68: 0.169968/0.227203loss in batch 69: 0.466034/0.230621loss in batch 70: 0.541351/0.235001loss in batch 71: 1.26338/0.249283loss in batch 72: 0.225006/0.248962loss in batch 73: 0.0427399/0.246155loss in batch 74: 0.170242/0.245148loss in batch 75: 0.0632782/0.242767loss in batch 76: 0.114441/0.241089loss in batch 77: 0.0892029/0.239151loss in batch 78: 0.0608521/0.236893loss in batch 79: 0.313782/0.237854loss in batch 80: 0.152924/0.236801loss in batch 81: 0.0215454/0.234177loss in batch 82: 0.145126/0.233093loss in batch 83: 0.0507355/0.230942loss in batch 84: 0.257385/0.231247loss in batch 85: 0.223282/0.231155loss in batch 86: 0.439804/0.233551loss in batch 87: 0.0903625/0.231918loss in batch 88: 0.107758/0.230515loss in batch 89: 0.238144/0.230621loss in batch 90: 0.483292/0.233398loss in batch 91: 0.338867/0.234543loss in batch 92: 0.361832/0.235901loss in batch 93: 0.416901/0.237823loss in batch 94: 0.420761/0.239746loss in batch 95: 0.0713348/0.237991loss in batch 96: 0.0565186/0.23613loss in batch 97: 0.0919952/0.234665loss in batch 98: 0.143097/0.233734loss in batch 99: 0.17514/0.233139loss in batch 100: 0.143799/0.232254loss in batch 101: 0.0639648/0.230606loss in batch 102: 0.0933685/0.229279loss in batch 103: 0.30452/0.230011loss in batch 104: 0.52774/0.232849loss in batch 105: 0.15007/0.232056loss in batch 106: 1.08749/0.240051loss in batch 107: 0.155289/0.239273loss in batch 108: 0.237869/0.239243loss in batch 109: 0.196854/0.238876loss in batch 110: 0.291214/0.239349loss in batch 111: 0.542175/0.24205loss in batch 112: 0.247925/0.242096loss in batch 113: 0.436081/0.24379loss in batch 114: 0.0589447/0.242188loss in batch 115: 0.205765/0.241882loss in batch 116: 0.145813/0.241058loss in batch 117: 0.0876923/0.239761loss in batch 118: 0.0992889/0.238571loss in batch 119: 0.251495/0.238693loss in batch 120: 0.221695/0.238541loss in batch 121: 0.114716/0.237518loss in batch 122: 0.119873/0.236572loss in batch 123: 0.47374/0.23848loss in batch 124: 0.151611/0.237793loss in batch 125: 0.0120544/0.236008loss in batch 126: 0.349091/0.236893loss in batch 127: 0.134583/0.236084loss in batch 128: 0.0715637/0.234818loss in batch 129: 0.03125/0.233246loss in batch 130: 0.728943/0.237045loss in batch 131: 0.0578003/0.235672loss in batch 132: 0.163895/0.235123loss in batch 133: 0.117691/0.234253loss in batch 134: 0.368484/0.235245loss in batch 135: 0.236969/0.23526loss in batch 136: 0.679214/0.23851loss in batch 137: 0.203873/0.238251loss in batch 138: 0.105164/0.237305loss in batch 139: 0.177826/0.236877loss in batch 140: 0.0564423/0.23558loss in batch 141: 0.0498199/0.234283loss in batch 142: 0.508575/0.236206loss in batch 143: 0.109116/0.235321loss in batch 144: 0.207703/0.235123loss in batch 145: 0.370377/0.236053loss in batch 146: 0.105392/0.235153loss in batch 147: 0.15979/0.23465loss in batch 148: 0.0223083/0.233231loss in batch 149: 0.0396576/0.231934loss in batch 150: 0.0187988/0.23053loss in batch 151: 0.162292/0.230087loss in batch 152: 0.0307465/0.228775loss in batch 153: 0.345184/0.229538loss in batch 154: 0.0208282/0.228195loss in batch 155: 0.223328/0.228165loss in batch 156: 0.139267/0.227585loss in batch 157: 0.0776062/0.226639loss in batch 158: 0.132645/0.226044loss in batch 159: 0.0954895/0.225235loss in batch 160: 0.0957031/0.224426loss in batch 161: 0.107971/0.223709loss in batch 162: 0.080368/0.222824loss in batch 163: 0.30014/0.223297loss in batch 164: 0.104691/0.22258loss in batch 165: 0.0871735/0.221771loss in batch 166: 0.0765533/0.220901loss in batch 167: 0.231186/0.220963loss in batch 168: 0.0861511/0.220154loss in batch 169: 0.340363/0.220871loss in batch 170: 0.0488739/0.219864loss in batch 171: 0.302795/0.220352loss in batch 172: 0.469437/0.221771loss in batch 173: 0.124466/0.221237loss in batch 174: 0.16925/0.220932loss in batch 175: 0.0554199/0.219986loss in batch 176: 0.0886078/0.219238loss in batch 177: 0.559509/0.221161loss in batch 178: 0.120911/0.220596loss in batch 179: 0.0823364/0.219833loss in batch 180: 0.180023/0.219604loss in batch 181: 0.35347/0.220352loss in batch 182: 0.508835/0.221924loss in batch 183: 0.09552/0.221237loss in batch 184: 0.404053/0.222214loss in batch 185: 0.0934753/0.221527loss in batch 186: 0.586609/0.22348loss in batch 187: 0.112671/0.2229loss in batch 188: 0.251495/0.223053loss in batch 189: 0.365005/0.223785loss in batch 190: 0.0174713/0.222717loss in batch 191: 0.162323/0.222397loss in batch 192: 0.263214/0.22261loss in batch 193: 0.10022/0.221985loss in batch 194: 0.124222/0.221481loss in batch 195: 0.170898/0.221222loss in batch 196: 0.282425/0.221527loss in batch 197: 0.0177917/0.220505loss in batch 198: 0.268265/0.220734loss in batch 199: 0.427704/0.221771loss in batch 200: 0.0945282/0.221146loss in batch 201: 1.14784/0.225739loss in batch 202: 0.101364/0.225113loss in batch 203: 0.296844/0.225479loss in batch 204: 0.0184631/0.224472loss in batch 205: 0.285217/0.224747loss in batch 206: 0.356339/0.225388loss in batch 207: 0.240967/0.225464loss in batch 208: 0.0941772/0.224838loss in batch 209: 0.0785828/0.224152loss in batch 210: 0.474197/0.225327loss in batch 211: 0.12088/0.224823loss in batch 212: 0.450287/0.225891
done with epoch 5
train_acc: 0.929577 (396/426)
test loss: 0.450287
acc: 0.93007 (133/143)
loss in batch 0: 0.296997/0.296997loss in batch 1: 0.0558472/0.176422loss in batch 2: 0.0139618/0.122269loss in batch 3: 0.0869293/0.113434loss in batch 4: 0.36911/0.164566loss in batch 5: 0.077179/0.150009loss in batch 6: 0.21489/0.159271loss in batch 7: 0.363739/0.18483loss in batch 8: 0.0443726/0.16922loss in batch 9: 0.827362/0.235031loss in batch 10: 0.284286/0.239517loss in batch 11: 0.0962677/0.227585loss in batch 12: 0.165131/0.222778loss in batch 13: 0.0821381/0.212723loss in batch 14: 0.0775299/0.20372loss in batch 15: 0.451385/0.219208loss in batch 16: 0.112442/0.212906loss in batch 17: 0.264145/0.215759loss in batch 18: 0.0224152/0.205582loss in batch 19: 0.0590363/0.198257loss in batch 20: 0.240982/0.200287loss in batch 21: 0.0584717/0.193848loss in batch 22: 0.112549/0.190323loss in batch 23: 0.158722/0.188995loss in batch 24: 0.32843/0.194565loss in batch 25: 0.22728/0.195831loss in batch 26: 0.221085/0.196762loss in batch 27: 0.0919495/0.193024loss in batch 28: 0.129913/0.190857loss in batch 29: 0.388214/0.197418loss in batch 30: 0.605927/0.210602loss in batch 31: 0.0929413/0.206924loss in batch 32: 0.172943/0.205887loss in batch 33: 0.120453/0.203384loss in batch 34: 0.0491333/0.198975loss in batch 35: 0.246033/0.200287loss in batch 36: 0.0158844/0.195297loss in batch 37: 0.28215/0.197586loss in batch 38: 0.0317535/0.193329loss in batch 39: 0.202209/0.193558loss in batch 40: 0.115601/0.19165loss in batch 41: 0.513809/0.199326loss in batch 42: 0.363068/0.20314loss in batch 43: 0.461655/0.209loss in batch 44: 0.141724/0.207504loss in batch 45: 0.0720978/0.204575loss in batch 46: 0.23819/0.205292loss in batch 47: 0.0573883/0.202194loss in batch 48: 0.0644226/0.199402loss in batch 49: 0.0789032/0.196976loss in batch 50: 0.21167/0.197266loss in batch 51: 0.138092/0.196121loss in batch 52: 0.133316/0.194946loss in batch 53: 0.115311/0.193481loss in batch 54: 0.142487/0.192551loss in batch 55: 0.14978/0.191772loss in batch 56: 0.642639/0.199692loss in batch 57: 0.168396/0.199158loss in batch 58: 0.17984/0.198822loss in batch 59: 0.242691/0.199554loss in batch 60: 0.24501/0.200287loss in batch 61: 0.353134/0.202759loss in batch 62: 0.126297/0.201553loss in batch 63: 0.370514/0.204193loss in batch 64: 0.0435181/0.201721loss in batch 65: 0.13559/0.200714loss in batch 66: 0.134171/0.199722loss in batch 67: 0.109085/0.19838loss in batch 68: 0.0431519/0.196152loss in batch 69: 0.276962/0.197296loss in batch 70: 0.0144348/0.194733loss in batch 71: 0.418808/0.19783loss in batch 72: 0.0778046/0.196198loss in batch 73: 0.191956/0.196136loss in batch 74: 0.884048/0.205307loss in batch 75: 0.108261/0.204025loss in batch 76: 0.358826/0.206039loss in batch 77: 0.144943/0.205246loss in batch 78: 0.0740356/0.203583loss in batch 79: 0.112457/0.202454loss in batch 80: 0.0819244/0.200958loss in batch 81: 0.0651703/0.19931loss in batch 82: 0.408981/0.201843loss in batch 83: 0.251282/0.202423loss in batch 84: 0.0303802/0.200394loss in batch 85: 0.715149/0.20639loss in batch 86: 0.0437012/0.204514loss in batch 87: 0.271149/0.205276loss in batch 88: 0.613159/0.209854loss in batch 89: 0.113251/0.208771loss in batch 90: 0.0586395/0.207123loss in batch 91: 0.059494/0.205536loss in batch 92: 0.125671/0.204666loss in batch 93: 0.00680542/0.202576loss in batch 94: 0.0937195/0.201416loss in batch 95: 0.0204163/0.199539loss in batch 96: 0.618256/0.203842loss in batch 97: 0.211609/0.203934loss in batch 98: 0.403824/0.205948loss in batch 99: 0.279541/0.20668loss in batch 100: 0.571457/0.210281loss in batch 101: 0.365768/0.211823loss in batch 102: 0.0660095/0.210403loss in batch 103: 0.0216675/0.208588loss in batch 104: 1.03246/0.216431loss in batch 105: 0.030777/0.214691loss in batch 106: 0.152695/0.214096loss in batch 107: 0.302246/0.21492loss in batch 108: 0.0386047/0.213303loss in batch 109: 0.432098/0.215286loss in batch 110: 0.0714417/0.214005loss in batch 111: 0.0497742/0.212524loss in batch 112: 0.126495/0.211761loss in batch 113: 0.381378/0.213257loss in batch 114: 0.240753/0.213501loss in batch 115: 0.309875/0.214325loss in batch 116: 0.0649872/0.213043loss in batch 117: 0.212372/0.213043loss in batch 118: 0.0776215/0.211899loss in batch 119: 0.3797/0.213303loss in batch 120: 0.17131/0.212952loss in batch 121: 0.0985718/0.212021loss in batch 122: 0.072937/0.210892loss in batch 123: 0.2948/0.211578loss in batch 124: 0.062149/0.210373loss in batch 125: 0.251862/0.210693loss in batch 126: 0.014801/0.209167loss in batch 127: 0.0965576/0.208282loss in batch 128: 0.20932/0.208282loss in batch 129: 0.462158/0.210236loss in batch 130: 0.0896301/0.20932loss in batch 131: 0.160843/0.208939loss in batch 132: 0.342285/0.209961loss in batch 133: 0.208786/0.209946loss in batch 134: 0.171051/0.209656loss in batch 135: 0.0902405/0.208771loss in batch 136: 0.429123/0.210388loss in batch 137: 0.139267/0.209869loss in batch 138: 0.211639/0.209885loss in batch 139: 0.301575/0.210541loss in batch 140: 0.12587/0.20993loss in batch 141: 0.498093/0.211975loss in batch 142: 0.314926/0.212692loss in batch 143: 0.0601349/0.211624loss in batch 144: 0.158768/0.211273loss in batch 145: 0.106827/0.210556loss in batch 146: 0.478638/0.212372loss in batch 147: 0.159729/0.212006loss in batch 148: 0.195328/0.211899loss in batch 149: 0.180206/0.2117loss in batch 150: 0.346939/0.212585loss in batch 151: 0.214874/0.212601loss in batch 152: 0.217346/0.212631loss in batch 153: 0.400162/0.213852loss in batch 154: 0.155594/0.213486loss in batch 155: 0.116348/0.212845loss in batch 156: 0.304169/0.213425loss in batch 157: 0.0457458/0.212372loss in batch 158: 0.178207/0.212158loss in batch 159: 0.108902/0.211502loss in batch 160: 0.159332/0.211197loss in batch 161: 0.040451/0.210129loss in batch 162: 0.144669/0.209732loss in batch 163: 0.0909119/0.209015loss in batch 164: 0.245804/0.209229loss in batch 165: 0.914627/0.213486loss in batch 166: 0.0478668/0.212479loss in batch 167: 0.102341/0.211823loss in batch 168: 0.127167/0.211334loss in batch 169: 0.574478/0.21347loss in batch 170: 0.0402222/0.212448loss in batch 171: 0.0737305/0.211655loss in batch 172: 0.378433/0.212616loss in batch 173: 0.0218201/0.211517loss in batch 174: 1.35175/0.218033loss in batch 175: 0.0634003/0.217163loss in batch 176: 0.0819702/0.216385loss in batch 177: 0.0289612/0.215347loss in batch 178: 0.0497742/0.214417loss in batch 179: 0.159958/0.214111loss in batch 180: 0.127884/0.213638loss in batch 181: 0.345367/0.214355loss in batch 182: 0.249588/0.214554loss in batch 183: 0.133484/0.214111loss in batch 184: 0.640579/0.216415loss in batch 185: 0.214432/0.2164loss in batch 186: 0.286972/0.216782loss in batch 187: 0.0249634/0.215759loss in batch 188: 0.086441/0.215073loss in batch 189: 0.0175018/0.21405loss in batch 190: 0.829483/0.217255loss in batch 191: 0.28566/0.217621loss in batch 192: 0.17157/0.217377loss in batch 193: 0.42627/0.218445loss in batch 194: 0.0270386/0.217468loss in batch 195: 0.248795/0.217636loss in batch 196: 0.130615/0.217194loss in batch 197: 0.36145/0.217926loss in batch 198: 0.765503/0.220673loss in batch 199: 0.108398/0.220108loss in batch 200: 0.0584717/0.219299loss in batch 201: 0.0642548/0.218536loss in batch 202: 0.122711/0.218063loss in batch 203: 0.413788/0.219025loss in batch 204: 0.18689/0.218872loss in batch 205: 0.270432/0.219116loss in batch 206: 0.0527191/0.218307loss in batch 207: 0.262009/0.218521loss in batch 208: 0.0497437/0.217712loss in batch 209: 0.15242/0.217392loss in batch 210: 0.11557/0.216919loss in batch 211: 0.0729065/0.216248loss in batch 212: 0.328842/0.216766
done with epoch 6
train_acc: 0.93662 (399/426)
test loss: 0.328842
acc: 0.937063 (134/143)
loss in batch 0: 0.174515/0.174515loss in batch 1: 0.130325/0.15242loss in batch 2: 0.0639954/0.12294loss in batch 3: 0.130829/0.124908loss in batch 4: 0.384659/0.176865loss in batch 5: 0.20462/0.181488loss in batch 6: 0.394775/0.21196loss in batch 7: 0.153214/0.20462loss in batch 8: 0.90274/0.282181loss in batch 9: 0.576767/0.311646loss in batch 10: 0.429413/0.322357loss in batch 11: 0.0210266/0.297241loss in batch 12: 0.0539703/0.278534loss in batch 13: 0.143829/0.268906loss in batch 14: 0.376602/0.276077loss in batch 15: 0.204987/0.271637loss in batch 16: 0.064743/0.25946loss in batch 17: 0.0686493/0.248871loss in batch 18: 0.218063/0.247238loss in batch 19: 0.358444/0.252808loss in batch 20: 0.121078/0.246536loss in batch 21: 0.108261/0.24025loss in batch 22: 0.0274506/0.231003loss in batch 23: 0.115585/0.226196loss in batch 24: 0.786224/0.248596loss in batch 25: 0.0799255/0.242111loss in batch 26: 0.121399/0.23764loss in batch 27: 0.151154/0.234543loss in batch 28: 0.303146/0.236908loss in batch 29: 0.0604401/0.231033loss in batch 30: 0.026535/0.224426loss in batch 31: 0.355667/0.228531loss in batch 32: 0.502563/0.236832loss in batch 33: 0.226761/0.236542loss in batch 34: 0.0909119/0.232376loss in batch 35: 0.0707397/0.22789loss in batch 36: 0.0687714/0.223587loss in batch 37: 0.34433/0.226761loss in batch 38: 0.15596/0.22496loss in batch 39: 0.34906/0.228058loss in batch 40: 0.0308533/0.223236loss in batch 41: 0.0997772/0.220306loss in batch 42: 0.301971/0.222198loss in batch 43: 0.126114/0.220016loss in batch 44: 0.107117/0.217499loss in batch 45: 0.0690155/0.214279loss in batch 46: 0.374863/0.217697loss in batch 47: 0.0230255/0.213638loss in batch 48: 0.0661011/0.210632loss in batch 49: 0.0714874/0.20784loss in batch 50: 0.105576/0.205841loss in batch 51: 0.145523/0.204681loss in batch 52: 0.747894/0.214935loss in batch 53: 0.341339/0.21727loss in batch 54: 0.0639343/0.214493loss in batch 55: 0.0386658/0.211334loss in batch 56: 0.250336/0.212021loss in batch 57: 0.101379/0.210114loss in batch 58: 0.0361786/0.207184loss in batch 59: 0.0221252/0.204086loss in batch 60: 0.229477/0.204498loss in batch 61: 0.0829468/0.202545loss in batch 62: 0.387711/0.205475loss in batch 63: 0.788513/0.2146loss in batch 64: 0.0737457/0.212418loss in batch 65: 0.0700531/0.210266loss in batch 66: 0.926849/0.220963loss in batch 67: 0.0256653/0.218094loss in batch 68: 0.0613556/0.21582loss in batch 69: 0.0453186/0.213379loss in batch 70: 0.0879517/0.211624loss in batch 71: 0.0424652/0.209259loss in batch 72: 0.28743/0.210342loss in batch 73: 0.607391/0.215714loss in batch 74: 0.531021/0.21991loss in batch 75: 0.257019/0.220398loss in batch 76: 0.197189/0.220093loss in batch 77: 0.252869/0.22052loss in batch 78: 0.112427/0.219147loss in batch 79: 0.0681305/0.217255loss in batch 80: 0.173615/0.216736loss in batch 81: 0.117599/0.215515loss in batch 82: 0.039505/0.213379loss in batch 83: 0.0850067/0.211853loss in batch 84: 0.0774384/0.210281loss in batch 85: 0.0748901/0.20871loss in batch 86: 0.509491/0.212158loss in batch 87: 0.00889587/0.209854loss in batch 88: 0.0614319/0.208191loss in batch 89: 0.105316/0.207047loss in batch 90: 0.113876/0.206024loss in batch 91: 0.0156555/0.203964loss in batch 92: 0.0671539/0.202484loss in batch 93: 0.0862579/0.201248loss in batch 94: 0.132202/0.200516loss in batch 95: 0.200058/0.200516loss in batch 96: 0.136032/0.199844loss in batch 97: 0.124146/0.199066loss in batch 98: 0.503403/0.202164loss in batch 99: 0.221161/0.202332loss in batch 100: 0.20311/0.202347loss in batch 101: 0.462585/0.20491loss in batch 102: 0.069809/0.203598loss in batch 103: 0.0602722/0.202209loss in batch 104: 0.0689087/0.200943loss in batch 105: 0.408737/0.202911loss in batch 106: 0.158005/0.202484loss in batch 107: 1.4886/0.214386loss in batch 108: 0.232666/0.214554loss in batch 109: 0.0774994/0.213303loss in batch 110: 0.178238/0.212997loss in batch 111: 0.00759888/0.211151loss in batch 112: 0.0528107/0.209763loss in batch 113: 0.0446167/0.208313loss in batch 114: 0.215103/0.208374loss in batch 115: 0.049118/0.207001loss in batch 116: 0.11174/0.206177loss in batch 117: 0.106766/0.205338loss in batch 118: 0.0720673/0.204224loss in batch 119: 0.283478/0.20488loss in batch 120: 0.092926/0.203949loss in batch 121: 0.58429/0.207077loss in batch 122: 0.395554/0.208603loss in batch 123: 0.154785/0.20816loss in batch 124: 0.280914/0.208755loss in batch 125: 0.171783/0.20845loss in batch 126: 0.106094/0.207657loss in batch 127: 0.149887/0.207199loss in batch 128: 0.061554/0.20607loss in batch 129: 0.154938/0.205673loss in batch 130: 0.0396576/0.204407loss in batch 131: 0.157608/0.204056loss in batch 132: 0.0182953/0.202652loss in batch 133: 0.123352/0.202072loss in batch 134: 0.0618134/0.201035loss in batch 135: 0.0693054/0.200058loss in batch 136: 0.100449/0.199341loss in batch 137: 0.381332/0.200653loss in batch 138: 0.235367/0.200897loss in batch 139: 0.0322723/0.199692loss in batch 140: 0.626907/0.202728loss in batch 141: 0.218613/0.202835loss in batch 142: 0.0263824/0.201599loss in batch 143: 0.0766144/0.200729loss in batch 144: 0.0801697/0.199905loss in batch 145: 0.108414/0.19928loss in batch 146: 0.147385/0.198929loss in batch 147: 0.278564/0.199463loss in batch 148: 0.0203247/0.198273loss in batch 149: 0.0335846/0.197174loss in batch 150: 0.00709534/0.195908loss in batch 151: 0.453033/0.197601loss in batch 152: 0.216507/0.197723loss in batch 153: 0.240555/0.197998loss in batch 154: 0.205673/0.198044loss in batch 155: 0.0559235/0.197144loss in batch 156: 0.525528/0.199219loss in batch 157: 0.0549011/0.198318loss in batch 158: 0.112671/0.197784loss in batch 159: 0.441528/0.199295loss in batch 160: 0.101257/0.1987loss in batch 161: 0.048584/0.197754loss in batch 162: 0.0660248/0.19696loss in batch 163: 0.0623779/0.196136loss in batch 164: 0.0770721/0.195419loss in batch 165: 0.0792389/0.194717loss in batch 166: 0.0514069/0.193863loss in batch 167: 0.119492/0.19342loss in batch 168: 0.301849/0.194061loss in batch 169: 0.164673/0.193893loss in batch 170: 0.020752/0.192871loss in batch 171: 0.043457/0.192001loss in batch 172: 0.0399475/0.191116loss in batch 173: 0.186523/0.191086loss in batch 174: 0.1548/0.190887loss in batch 175: 0.0394287/0.190033loss in batch 176: 0.244568/0.190338loss in batch 177: 0.112381/0.189896loss in batch 178: 0.354813/0.190811loss in batch 179: 0.534409/0.192734loss in batch 180: 0.0353241/0.191864loss in batch 181: 0.634079/0.19429loss in batch 182: 0.601685/0.196518loss in batch 183: 1.2003/0.201965loss in batch 184: 0.0773315/0.201294loss in batch 185: 0.314133/0.201904loss in batch 186: 0.0609283/0.201141loss in batch 187: 0.189377/0.20108loss in batch 188: 0.426865/0.202286loss in batch 189: 1.10907/0.207047loss in batch 190: 0.22879/0.207169loss in batch 191: 0.0826263/0.206512loss in batch 192: 0.0581207/0.20575loss in batch 193: 0.0161743/0.204773loss in batch 194: 0.0897064/0.204178loss in batch 195: 0.247986/0.204407loss in batch 196: 0.132797/0.204041loss in batch 197: 0.124603/0.203644loss in batch 198: 0.290802/0.204071loss in batch 199: 0.357361/0.204849loss in batch 200: 0.336884/0.205505loss in batch 201: 0.108459/0.205017loss in batch 202: 0.160965/0.204803loss in batch 203: 0.275116/0.205154loss in batch 204: 0.0874939/0.204575loss in batch 205: 0.0193481/0.20369loss in batch 206: 0.361603/0.204437loss in batch 207: 0.0406189/0.203644loss in batch 208: 0.535904/0.205231loss in batch 209: 0.268845/0.205536loss in batch 210: 0.0757599/0.204941loss in batch 211: 0.0923157/0.204407loss in batch 212: 0.405502/0.205353
done with epoch 7
train_acc: 0.931925 (397/426)
test loss: 0.405502
acc: 0.916084 (131/143)
loss in batch 0: 0.219421/0.219421loss in batch 1: 0.354523/0.286972loss in batch 2: 0.886475/0.486801loss in batch 3: 0.999664/0.615021loss in batch 4: 0.166336/0.525284loss in batch 5: 0.072403/0.449814loss in batch 6: 0.195114/0.413422loss in batch 7: 0.0589142/0.36911loss in batch 8: 0.211868/0.351624loss in batch 9: 0.0466003/0.321136loss in batch 10: 0.0981903/0.300873loss in batch 11: 0.0799866/0.282455loss in batch 12: 0.0157471/0.261948loss in batch 13: 0.052124/0.246948loss in batch 14: 0.220352/0.245178loss in batch 15: 0.00758362/0.230331loss in batch 16: 0.280823/0.233292loss in batch 17: 0.0532684/0.223312loss in batch 18: 0.175171/0.220764loss in batch 19: 0.0792999/0.213684loss in batch 20: 0.427643/0.223892loss in batch 21: 0.158279/0.220886loss in batch 22: 0.103241/0.21579loss in batch 23: 0.0541077/0.209045loss in batch 24: 0.0369873/0.202164loss in batch 25: 0.20636/0.202316loss in batch 26: 0.0260162/0.195786loss in batch 27: 1.42653/0.239746loss in batch 28: 0.104919/0.235107loss in batch 29: 0.502625/0.244019loss in batch 30: 0.137024/0.240555loss in batch 31: 0.0567017/0.234833loss in batch 32: 0.560516/0.244705loss in batch 33: 0.123352/0.241119loss in batch 34: 0.123703/0.237778loss in batch 35: 0.111862/0.234268loss in batch 36: 0.729095/0.247635loss in batch 37: 0.347931/0.25029loss in batch 38: 0.161423/0.248001loss in batch 39: 0.224503/0.247421loss in batch 40: 0.745544/0.259567loss in batch 41: 0.105469/0.25589loss in batch 42: 0.0356903/0.250778loss in batch 43: 0.0503998/0.246231loss in batch 44: 0.134918/0.243744loss in batch 45: 0.163086/0.242004loss in batch 46: 0.414276/0.245651loss in batch 47: 0.19722/0.244659loss in batch 48: 0.361862/0.247055loss in batch 49: 0.30072/0.248123loss in batch 50: 0.0842285/0.244904loss in batch 51: 0.0196228/0.24057loss in batch 52: 0.33374/0.24234loss in batch 53: 0.153702/0.240692loss in batch 54: 0.697418/0.248993loss in batch 55: 0.141449/0.24707loss in batch 56: 0.0239868/0.243149loss in batch 57: 0.261612/0.243469loss in batch 58: 0.0509644/0.240219loss in batch 59: 0.258743/0.240524loss in batch 60: 0.153931/0.239105loss in batch 61: 0.318375/0.240372loss in batch 62: 0.160522/0.23912loss in batch 63: 0.161377/0.2379loss in batch 64: 0.0401154/0.234863loss in batch 65: 0.743729/0.242569loss in batch 66: 0.297943/0.243393loss in batch 67: 0.162613/0.242218loss in batch 68: 0.0578003/0.239532loss in batch 69: 0.0922241/0.237427loss in batch 70: 0.0101318/0.234238loss in batch 71: 0.0776062/0.232056loss in batch 72: 0.205872/0.231689loss in batch 73: 0.159332/0.230728loss in batch 74: 0.0505524/0.228317loss in batch 75: 0.0678253/0.226196loss in batch 76: 0.133286/0.224991loss in batch 77: 0.0745239/0.223068loss in batch 78: 0.0398865/0.220749loss in batch 79: 0.0669098/0.218826loss in batch 80: 0.0339661/0.216553loss in batch 81: 0.300995/0.217575loss in batch 82: 0.400604/0.219788loss in batch 83: 0.47995/0.222885loss in batch 84: 0.0265045/0.220566loss in batch 85: 0.0611267/0.218704loss in batch 86: 0.104401/0.217392loss in batch 87: 0.0502167/0.2155loss in batch 88: 0.0986481/0.214188loss in batch 89: 0.0368958/0.212204loss in batch 90: 0.0365143/0.210281loss in batch 91: 0.118744/0.20929loss in batch 92: 0.303009/0.210281loss in batch 93: 0.0537262/0.208633loss in batch 94: 0.073822/0.207214loss in batch 95: 0.111038/0.206207loss in batch 96: 0.336105/0.20755loss in batch 97: 0.24939/0.207977loss in batch 98: 0.110016/0.206985loss in batch 99: 0.374191/0.208664loss in batch 100: 0.0924835/0.207504loss in batch 101: 0.554367/0.210907loss in batch 102: 1.04036/0.218964loss in batch 103: 0.0873566/0.217682loss in batch 104: 0.0360107/0.215958loss in batch 105: 0.357803/0.2173loss in batch 106: 0.528397/0.2202loss in batch 107: 0.0646362/0.218765loss in batch 108: 0.074707/0.217453loss in batch 109: 0.0820923/0.216217loss in batch 110: 1.42252/0.227081loss in batch 111: 0.313309/0.227859loss in batch 112: 0.0992889/0.226715loss in batch 113: 0.00926208/0.224808loss in batch 114: 0.022171/0.223038loss in batch 115: 0.0247955/0.221344loss in batch 116: 0.119675/0.220474loss in batch 117: 0.120697/0.21962loss in batch 118: 0.0906067/0.218536loss in batch 119: 0.192215/0.218307loss in batch 120: 0.140121/0.217667loss in batch 121: 0.217148/0.217667loss in batch 122: 0.121841/0.216888loss in batch 123: 0.101273/0.215958loss in batch 124: 0.0675507/0.214783loss in batch 125: 0.0743103/0.213654loss in batch 126: 0.0636444/0.212479loss in batch 127: 0.0976715/0.211578loss in batch 128: 0.168823/0.211243loss in batch 129: 0.0555878/0.210037loss in batch 130: 0.0711823/0.209loss in batch 131: 0.339172/0.209976loss in batch 132: 0.0941467/0.209106loss in batch 133: 0.0883636/0.208206loss in batch 134: 0.0640564/0.207138loss in batch 135: 0.0241089/0.205795loss in batch 136: 0.18985/0.205673loss in batch 137: 0.0685577/0.204681loss in batch 138: 0.366135/0.205841loss in batch 139: 0.0250854/0.204544loss in batch 140: 0.693192/0.208008loss in batch 141: 0.280106/0.208511loss in batch 142: 0.147751/0.208099loss in batch 143: 0.479019/0.209976loss in batch 144: 0.258621/0.210327loss in batch 145: 0.450821/0.21196loss in batch 146: 0.146698/0.211533loss in batch 147: 0.0325012/0.210312loss in batch 148: 0.0691223/0.209366loss in batch 149: 0.362595/0.210388loss in batch 150: 0.402466/0.211655loss in batch 151: 0.284821/0.212143loss in batch 152: 0.109909/0.211472loss in batch 153: 0.436661/0.212921loss in batch 154: 0.0909882/0.212143loss in batch 155: 0.047287/0.211075loss in batch 156: 0.162506/0.21077loss in batch 157: 0.20285/0.210724loss in batch 158: 0.148819/0.210342loss in batch 159: 0.0722351/0.209473loss in batch 160: 0.0829315/0.208694loss in batch 161: 0.109421/0.208069loss in batch 162: 0.0936584/0.207367loss in batch 163: 0.00999451/0.206177loss in batch 164: 0.177902/0.205994loss in batch 165: 0.10614/0.205399loss in batch 166: 0.0692139/0.20459loss in batch 167: 0.390442/0.205688loss in batch 168: 0.188141/0.205582loss in batch 169: 0.146683/0.205246loss in batch 170: 0.502991/0.206985loss in batch 171: 0.10701/0.206406loss in batch 172: 0.00114441/0.205215loss in batch 173: 0.320847/0.205872loss in batch 174: 0.470016/0.207382loss in batch 175: 0.100266/0.206772loss in batch 176: 0.343979/0.207565loss in batch 177: 0.0483856/0.206665loss in batch 178: 0.296875/0.207169loss in batch 179: 0.0147552/0.206085loss in batch 180: 0.0690613/0.205338loss in batch 181: 0.128662/0.20491loss in batch 182: 0.206451/0.204926loss in batch 183: 0.171509/0.204742loss in batch 184: 0.162109/0.204498loss in batch 185: 0.273071/0.20488loss in batch 186: 0.135834/0.204514loss in batch 187: 0.1035/0.203979loss in batch 188: 0.24144/0.204178loss in batch 189: 0.134476/0.203812loss in batch 190: 0.471313/0.205215loss in batch 191: 0.381409/0.206116loss in batch 192: 0.324432/0.206726loss in batch 193: 0.0605011/0.205978loss in batch 194: 0.242325/0.206161loss in batch 195: 0.0995941/0.205627loss in batch 196: 0.45314/0.206879loss in batch 197: 0.670319/0.209229loss in batch 198: 0.0843964/0.208603loss in batch 199: 0.0147552/0.207626loss in batch 200: 0.00183105/0.206604loss in batch 201: 0.17691/0.206451loss in batch 202: 0.0713806/0.205795loss in batch 203: 0.0782318/0.20517loss in batch 204: 0.124283/0.204773loss in batch 205: 0.0979309/0.204254loss in batch 206: 0.079422/0.203644loss in batch 207: 0.114578/0.203217loss in batch 208: 0.189117/0.203156loss in batch 209: 0.26561/0.203445loss in batch 210: 0.255997/0.203705loss in batch 211: 0.0460968/0.202957loss in batch 212: 0.538986/0.204544
done with epoch 8
train_acc: 0.934272 (398/426)
test loss: 0.538986
acc: 0.93007 (133/143)
loss in batch 0: 0.063736/0.063736loss in batch 1: 0.385406/0.224564loss in batch 2: 0.137436/0.195526loss in batch 3: 0.668091/0.31366loss in batch 4: 0.112122/0.273361loss in batch 5: 0.097168/0.243988loss in batch 6: 0.0401917/0.214874loss in batch 7: 0.256104/0.220032loss in batch 8: 0.0491028/0.201035loss in batch 9: 1.00702/0.281647loss in batch 10: 0.843079/0.332672loss in batch 11: 0.0145874/0.306168loss in batch 12: 0.125366/0.292267loss in batch 13: 0.185883/0.284668loss in batch 14: 0.0661469/0.270096loss in batch 15: 0.28273/0.270874loss in batch 16: 0.0835114/0.259857loss in batch 17: 0.12648/0.252457loss in batch 18: 0.136932/0.246368loss in batch 19: 0.704163/0.269257loss in batch 20: 0.259079/0.268768loss in batch 21: 0.0839996/0.260376loss in batch 22: 0.167984/0.256363loss in batch 23: 0.0630341/0.248306loss in batch 24: 0.126007/0.243423loss in batch 25: 0.376129/0.24852loss in batch 26: 0.0235291/0.240189loss in batch 27: 0.127258/0.23616loss in batch 28: 0.239807/0.236282loss in batch 29: 0.442551/0.243149loss in batch 30: 0.356171/0.246796loss in batch 31: 0.0785675/0.241547loss in batch 32: 0.145126/0.238617loss in batch 33: 0.0422516/0.232849loss in batch 34: 0.0343781/0.227188loss in batch 35: 0.074707/0.222946loss in batch 36: 0.03125/0.217758loss in batch 37: 0.103439/0.214752loss in batch 38: 0.404358/0.21962loss in batch 39: 0.143066/0.217697loss in batch 40: 0.0897827/0.214584loss in batch 41: 0.473297/0.220749loss in batch 42: 0.184723/0.21991loss in batch 43: 0.207703/0.21962loss in batch 44: 0.391937/0.223465loss in batch 45: 0.750626/0.234924loss in batch 46: 0.085144/0.23172loss in batch 47: 0.0888214/0.22876loss in batch 48: 0.0787659/0.225693loss in batch 49: 0.102066/0.223221loss in batch 50: 0.0639496/0.220093loss in batch 51: 0.0277252/0.216385loss in batch 52: 0.0648193/0.213531loss in batch 53: 0.237289/0.213974loss in batch 54: 0.119644/0.212265loss in batch 55: 0.0692444/0.209702loss in batch 56: 0.414063/0.213287loss in batch 57: 0.313278/0.215027loss in batch 58: 0.0685883/0.21254loss in batch 59: 0.036972/0.209595loss in batch 60: 0.0364075/0.206772loss in batch 61: 0.00787354/0.203552loss in batch 62: 0.294617/0.205002loss in batch 63: 0.130798/0.203842loss in batch 64: 0.0443878/0.201385loss in batch 65: 0.0538025/0.199158loss in batch 66: 0.114609/0.197891loss in batch 67: 0.546448/0.203033loss in batch 68: 0.124908/0.201889loss in batch 69: 0.0523224/0.199753loss in batch 70: 0.0574799/0.197754loss in batch 71: 1.08644/0.210083loss in batch 72: 0.44281/0.213287loss in batch 73: 0.130096/0.212158loss in batch 74: 0.562683/0.216827loss in batch 75: 0.0593719/0.214752loss in batch 76: 0.0588684/0.212738loss in batch 77: 0.0564575/0.210724loss in batch 78: 0.320724/0.212128loss in batch 79: 0.0500488/0.210083loss in batch 80: 0.110626/0.208862loss in batch 81: 0.142426/0.208054loss in batch 82: 0.0574951/0.206238loss in batch 83: 0.135025/0.205383loss in batch 84: 0.0178528/0.203186loss in batch 85: 0.483032/0.206451loss in batch 86: 0.0591583/0.204742loss in batch 87: 0.102905/0.203598loss in batch 88: 0.037796/0.201721loss in batch 89: 0.0246887/0.199753loss in batch 90: 0.0180969/0.197769loss in batch 91: 0.234955/0.198166loss in batch 92: 0.834457/0.205017loss in batch 93: 0.0438538/0.203308loss in batch 94: 0.123947/0.202454loss in batch 95: 1.692/0.217972loss in batch 96: 0.156494/0.217346loss in batch 97: 0.152771/0.21669loss in batch 98: 0.131927/0.215836loss in batch 99: 0.100662/0.214676loss in batch 100: 0.144699/0.213989loss in batch 101: 0.0292969/0.212173loss in batch 102: 0.0601959/0.210693loss in batch 103: 0.117828/0.209793loss in batch 104: 0.0751801/0.208511loss in batch 105: 0.0279083/0.206818loss in batch 106: 0.0459442/0.205322loss in batch 107: 0.258408/0.205811loss in batch 108: 0.129166/0.205109loss in batch 109: 0.292175/0.205902loss in batch 110: 0.044342/0.204437loss in batch 111: 0.0725861/0.203262loss in batch 112: 0.431976/0.205276loss in batch 113: 0.330917/0.20639loss in batch 114: 0.266327/0.206909loss in batch 115: 0.0768433/0.205795loss in batch 116: 0.0685425/0.204605loss in batch 117: 0.0262146/0.203094loss in batch 118: 0.088028/0.202133loss in batch 119: 0.0696716/0.201035loss in batch 120: 0.263443/0.201553loss in batch 121: 0.046875/0.200272loss in batch 122: 0.046402/0.199036loss in batch 123: 0.0492249/0.19783loss in batch 124: 0.459366/0.199905loss in batch 125: 0.0704803/0.198883loss in batch 126: 0.0400238/0.197632loss in batch 127: 0.282211/0.198288loss in batch 128: 0.215424/0.198425loss in batch 129: 0.0719299/0.197449loss in batch 130: 0.0587769/0.196396loss in batch 131: 0.0559845/0.195328loss in batch 132: 0.106171/0.194656loss in batch 133: 0.0441742/0.193542loss in batch 134: 0.542191/0.196121loss in batch 135: 0.018158/0.194809loss in batch 136: 0.311859/0.195679loss in batch 137: 0.350815/0.196793loss in batch 138: 0.030426/0.195587loss in batch 139: 0.0581207/0.194611loss in batch 140: 0.175415/0.194489loss in batch 141: 0.0252228/0.193283loss in batch 142: 0.0775604/0.192474loss in batch 143: 0.183853/0.192413loss in batch 144: 0.0956726/0.191742loss in batch 145: 0.212173/0.191895loss in batch 146: 0.201569/0.191956loss in batch 147: 0.0321808/0.190872loss in batch 148: 0.111191/0.190338loss in batch 149: 0.0741577/0.189575loss in batch 150: 0.459137/0.191345loss in batch 151: 0.124222/0.190918loss in batch 152: 0.0112762/0.189743loss in batch 153: 0.304855/0.190475loss in batch 154: 0.0980988/0.189896loss in batch 155: 0.161652/0.189713loss in batch 156: 0.308548/0.19046loss in batch 157: 0.0890045/0.189819loss in batch 158: 0.040741/0.188873loss in batch 159: 1.15247/0.194916loss in batch 160: 0.198807/0.194931loss in batch 161: 0.144012/0.194611loss in batch 162: 0.703369/0.197739loss in batch 163: 0.108063/0.197189loss in batch 164: 0.49353/0.19899loss in batch 165: 0.0152283/0.197876loss in batch 166: 0.158249/0.197632loss in batch 167: 0.0979004/0.197052loss in batch 168: 0.0217896/0.196014loss in batch 169: 0.0528259/0.195175loss in batch 170: 0.115891/0.194702loss in batch 171: 0.132736/0.194351loss in batch 172: 0.0595856/0.193558loss in batch 173: 0.273346/0.194031loss in batch 174: 0.0267639/0.193069loss in batch 175: 0.0466309/0.19223loss in batch 176: 0.0794067/0.191605loss in batch 177: 0.168854/0.191467loss in batch 178: 0.0832672/0.190857loss in batch 179: 0.0467682/0.190063loss in batch 180: 0.290253/0.190628loss in batch 181: 0.0427246/0.189804loss in batch 182: 0.0627289/0.189117loss in batch 183: 0.583023/0.191254loss in batch 184: 0.0610504/0.190552loss in batch 185: 0.330719/0.191299loss in batch 186: 0.0170593/0.190369loss in batch 187: 0.014801/0.189438loss in batch 188: 0.124542/0.189102loss in batch 189: 0.107376/0.18866loss in batch 190: 0.34642/0.189484loss in batch 191: 0.430603/0.19075loss in batch 192: 0.40419/0.191849loss in batch 193: 0.00460815/0.190887loss in batch 194: 0.0433197/0.190125loss in batch 195: 0.0124359/0.189224loss in batch 196: 0.322189/0.189896loss in batch 197: 0.253571/0.190216loss in batch 198: 0.160172/0.190063loss in batch 199: 0.44899/0.19136loss in batch 200: 0.257904/0.191696loss in batch 201: 0.473328/0.193085loss in batch 202: 0.0292053/0.192291loss in batch 203: 0.442429/0.193512loss in batch 204: 0.0927887/0.193008loss in batch 205: 0.0534668/0.192337loss in batch 206: 0.393219/0.193314loss in batch 207: 0.0484009/0.192612loss in batch 208: 0.0296783/0.191833loss in batch 209: 0.0671539/0.191238loss in batch 210: 0.0707092/0.190674loss in batch 211: 0.161697/0.190521loss in batch 212: 0.106903/0.19014
done with epoch 9
train_acc: 0.93662 (399/426)
test loss: 0.106903
acc: 0.93007 (133/143)
loss in batch 0: 0.176697/0.176697loss in batch 1: 0.0256195/0.101151loss in batch 2: 0.119705/0.107346loss in batch 3: 0.20546/0.131882loss in batch 4: 0.0895081/0.123398loss in batch 5: 0.142746/0.126633loss in batch 6: 0.0439148/0.114807loss in batch 7: 0.109634/0.114166loss in batch 8: 0.0715027/0.109421loss in batch 9: 0.42453/0.14093loss in batch 10: 0.272171/0.152863loss in batch 11: 0.0524139/0.144501loss in batch 12: 0.0535126/0.137482loss in batch 13: 0.605408/0.170914loss in batch 14: 0.143921/0.169113loss in batch 15: 0.388672/0.182846loss in batch 16: 0.0680847/0.176086loss in batch 17: 1.1474/0.230057loss in batch 18: 0.117279/0.224106loss in batch 19: 0.0504913/0.215424loss in batch 20: 0.343063/0.221512loss in batch 21: 0.00521851/0.21167loss in batch 22: 1.25655/0.257111loss in batch 23: 0.107849/0.250885loss in batch 24: 0.26239/0.251358loss in batch 25: 0.0317078/0.242905loss in batch 26: 0.17366/0.240341loss in batch 27: 0.13031/0.236404loss in batch 28: 0.0700684/0.230682loss in batch 29: 0.0486298/0.224609loss in batch 30: 0.085144/0.220093loss in batch 31: 0.261948/0.221405loss in batch 32: 0.128967/0.218613loss in batch 33: 0.0241241/0.212891loss in batch 34: 0.490738/0.22084loss in batch 35: 0.154312/0.218994loss in batch 36: 0.219864/0.219009loss in batch 37: 0.0724335/0.215149loss in batch 38: 0.982529/0.234833loss in batch 39: 0.0427246/0.230026loss in batch 40: 0.215195/0.22966loss in batch 41: 0.0189056/0.224655loss in batch 42: 0.0933075/0.221588loss in batch 43: 0.0668488/0.218063loss in batch 44: 0.0446472/0.214218loss in batch 45: 0.0698242/0.21109loss in batch 46: 0.17923/0.210403loss in batch 47: 0.313522/0.21254loss in batch 48: 0.0136108/0.208496loss in batch 49: 0.119812/0.206711loss in batch 50: 0.101837/0.204651loss in batch 51: 0.0572815/0.201828loss in batch 52: 0.558044/0.208542loss in batch 53: 0.0223236/0.205093loss in batch 54: 0.215546/0.205276loss in batch 55: 0.059433/0.202682loss in batch 56: 0.0599976/0.20018loss in batch 57: 0.090683/0.198303loss in batch 58: 0.0575714/0.195908loss in batch 59: 0.0886841/0.194122loss in batch 60: 0.297363/0.195801loss in batch 61: 0.396378/0.199036loss in batch 62: 0.0912323/0.197327loss in batch 63: 0.389252/0.200333loss in batch 64: 0.078476/0.198456loss in batch 65: 0.0680847/0.196487loss in batch 66: 0.0195313/0.193848loss in batch 67: 0.0472107/0.191681loss in batch 68: 0.343033/0.193878loss in batch 69: 0.442886/0.197433loss in batch 70: 0.442261/0.200897loss in batch 71: 0.460739/0.204498loss in batch 72: 0.0686798/0.202637loss in batch 73: 0.0273895/0.200256loss in batch 74: 0.358353/0.202362loss in batch 75: 0.269897/0.203262loss in batch 76: 0.0885468/0.201782loss in batch 77: 0.0633545/0.199997loss in batch 78: 0.270386/0.200882loss in batch 79: 0.0899048/0.199493loss in batch 80: 0.158752/0.19899loss in batch 81: 0.335678/0.200668loss in batch 82: 0.0341644/0.198669loss in batch 83: 0.293549/0.199783loss in batch 84: 0.205551/0.19986loss in batch 85: 0.0718384/0.198364loss in batch 86: 0.113327/0.197388loss in batch 87: 0.217957/0.197632loss in batch 88: 0.014679/0.195572loss in batch 89: 0.16153/0.19519loss in batch 90: 0.0749664/0.193863loss in batch 91: 0.100311/0.192856loss in batch 92: 0.035553/0.191162loss in batch 93: 0.26796/0.191971loss in batch 94: 0.37767/0.193939loss in batch 95: 0.337601/0.195419loss in batch 96: 0.858185/0.202255loss in batch 97: 0.173706/0.201981loss in batch 98: 0.207382/0.202026loss in batch 99: 0.10054/0.201004loss in batch 100: 0.0558777/0.199585loss in batch 101: 0.106232/0.198654loss in batch 102: 0.198608/0.198669loss in batch 103: 0.0186462/0.19693loss in batch 104: 0.116455/0.196152loss in batch 105: 0.126221/0.195496loss in batch 106: 0.311142/0.196579loss in batch 107: 1.00949/0.204117loss in batch 108: 0.023468/0.202438loss in batch 109: 0.0967255/0.201492loss in batch 110: 0.0108643/0.199768loss in batch 111: 0.108322/0.198959loss in batch 112: 0.0849915/0.197952loss in batch 113: 0.0903931/0.197006loss in batch 114: 0.188614/0.19693loss in batch 115: 0.092453/0.19603loss in batch 116: 0.000823975/0.194366loss in batch 117: 0.0120697/0.192825loss in batch 118: 0.385498/0.194443loss in batch 119: 0.0573273/0.193298loss in batch 120: 0.0622559/0.1922loss in batch 121: 0.768478/0.19693loss in batch 122: 0.0886078/0.19606loss in batch 123: 0.105042/0.195313loss in batch 124: 0.161148/0.195053loss in batch 125: 0.0532684/0.193924loss in batch 126: 0.043396/0.192734loss in batch 127: 0.196274/0.192764loss in batch 128: 0.553162/0.195557loss in batch 129: 0.230469/0.195831loss in batch 130: 0.292068/0.196564loss in batch 131: 0.0299683/0.195297loss in batch 132: 0.0561066/0.194244loss in batch 133: 0.0944824/0.193512loss in batch 134: 0.463211/0.195511loss in batch 135: 0.0402985/0.194366loss in batch 136: 0.0641327/0.19342loss in batch 137: 0.140289/0.193024loss in batch 138: 0.100006/0.192368loss in batch 139: 0.0298157/0.191193loss in batch 140: 0.728638/0.195007loss in batch 141: 0.056427/0.194031loss in batch 142: 0.258881/0.194489loss in batch 143: 0.120941/0.19397loss in batch 144: 0.0194244/0.192764loss in batch 145: 0.17395/0.192642loss in batch 146: 0.446045/0.194366loss in batch 147: 0.153992/0.194092loss in batch 148: 0.329895/0.195007loss in batch 149: 1.18091/0.201569loss in batch 150: 0.699646/0.204865loss in batch 151: 0.187729/0.204773loss in batch 152: 0.0945129/0.204056loss in batch 153: 0.14711/0.203659loss in batch 154: 0.0413055/0.202621loss in batch 155: 0.0115662/0.201401loss in batch 156: 0.0153198/0.200211loss in batch 157: 0.0109711/0.19902loss in batch 158: 0.11467/0.198502loss in batch 159: 0.0302734/0.197433loss in batch 160: 0.0414124/0.196472loss in batch 161: 0.0376129/0.19548loss in batch 162: 0.0578003/0.194641loss in batch 163: 0.0480652/0.193741loss in batch 164: 0.202209/0.193802loss in batch 165: 0.00497437/0.192657loss in batch 166: 0.0195465/0.191635loss in batch 167: 0.0953827/0.191055loss in batch 168: 0.0804596/0.190399loss in batch 169: 0.265244/0.190826loss in batch 170: 0.184555/0.190796loss in batch 171: 0.0485077/0.189972loss in batch 172: 0.0432587/0.189133loss in batch 173: 0.270721/0.18959loss in batch 174: 0.138046/0.189301loss in batch 175: 0.17218/0.189209loss in batch 176: 0.0123444/0.188217loss in batch 177: 0.0578156/0.187469loss in batch 178: 0.164063/0.187347loss in batch 179: 0.0167694/0.186401loss in batch 180: 0.158066/0.186249loss in batch 181: 0.322403/0.186981loss in batch 182: 0.00765991/0.186005loss in batch 183: 0.0869598/0.185471loss in batch 184: 0.0515594/0.184738loss in batch 185: 0.0200348/0.183868loss in batch 186: 0.035614/0.183075loss in batch 187: 0.0323639/0.182251loss in batch 188: 0.261322/0.182678loss in batch 189: 0.0794678/0.182144loss in batch 190: 0.0798645/0.181595loss in batch 191: 0.0179749/0.180756loss in batch 192: 0.222641/0.180969loss in batch 193: 0.0839081/0.180466loss in batch 194: 0.0536804/0.179825loss in batch 195: 0.203156/0.179932loss in batch 196: 0.056427/0.179321loss in batch 197: 0.0826416/0.178818loss in batch 198: 0.0129852/0.177994loss in batch 199: 0.0427399/0.177307loss in batch 200: 0.319275/0.178024loss in batch 201: 0.677246/0.180496loss in batch 202: 0.0645142/0.179932loss in batch 203: 0.524765/0.181595loss in batch 204: 0.0475006/0.180954loss in batch 205: 0.38208/0.181931loss in batch 206: 0.00476074/0.181061loss in batch 207: 0.656067/0.183365loss in batch 208: 0.0205078/0.182587loss in batch 209: 0.528824/0.184235loss in batch 210: 0.405838/0.185272loss in batch 211: 0.170776/0.185211loss in batch 212: 0.210205/0.185333
done with epoch 10
train_acc: 0.93662 (399/426)
test loss: 0.210205
acc: 0.937063 (134/143)
loss in batch 0: 0.142303/0.142303loss in batch 1: 0.406738/0.274521loss in batch 2: 0.118866/0.222626loss in batch 3: 0.0746002/0.185638loss in batch 4: 0.016571/0.15181loss in batch 5: 0.604736/0.22731loss in batch 6: 0.0853271/0.207016loss in batch 7: 0.0112305/0.182541loss in batch 8: 0.365829/0.202911loss in batch 9: 0.0926361/0.191879loss in batch 10: 0.0106049/0.175415loss in batch 11: 0.0111237/0.161713loss in batch 12: 0.262451/0.169464loss in batch 13: 0.0188446/0.158707loss in batch 14: 0.186279/0.160538loss in batch 15: 0.0397034/0.152985loss in batch 16: 0.0504913/0.146957loss in batch 17: 0.0539703/0.141785loss in batch 18: 0.197617/0.14473loss in batch 19: 0.0237122/0.138672loss in batch 20: 0.406876/0.151443loss in batch 21: 0.190552/0.153229loss in batch 22: 0.0852966/0.150269loss in batch 23: 0.218307/0.153107loss in batch 24: 0.263947/0.157547loss in batch 25: 0.185776/0.15863loss in batch 26: 0.284271/0.163284loss in batch 27: 0.0746307/0.160126loss in batch 28: 0.0532074/0.156433loss in batch 29: 0.0782623/0.153824loss in batch 30: 0.042923/0.150238loss in batch 31: 0.112381/0.149063loss in batch 32: 0.118423/0.148148loss in batch 33: 0.00796509/0.144012loss in batch 34: 0.0511627/0.141357loss in batch 35: 0.258804/0.144623loss in batch 36: 0.122299/0.144012loss in batch 37: 0.0451813/0.141418loss in batch 38: 0.176849/0.142319loss in batch 39: 0.127274/0.141953loss in batch 40: 0.017334/0.138901loss in batch 41: 0.375885/0.144547loss in batch 42: 0.0473175/0.142303loss in batch 43: 0.0612183/0.140457loss in batch 44: 0.0672302/0.138824loss in batch 45: 0.0425873/0.136719loss in batch 46: 0.0632172/0.135162loss in batch 47: 0.368286/0.140015loss in batch 48: 0.510681/0.147583loss in batch 49: 0.0574493/0.145782loss in batch 50: 0.131134/0.145493loss in batch 51: 0.149597/0.145569loss in batch 52: 0.0986633/0.144684loss in batch 53: 0.595505/0.15303loss in batch 54: 0.0176239/0.150589loss in batch 55: 0.214752/0.151718loss in batch 56: 0.552505/0.158752loss in batch 57: 0.376221/0.162506loss in batch 58: 0.0636139/0.160828loss in batch 59: 0.276047/0.16275loss in batch 60: 1.08122/0.177811loss in batch 61: 0.380722/0.181076loss in batch 62: 0.0508423/0.179016loss in batch 63: 0.103195/0.177826loss in batch 64: 0.270691/0.17926loss in batch 65: 0.110947/0.178207loss in batch 66: 0.148834/0.17778loss in batch 67: 0.037796/0.17572loss in batch 68: 0.0154419/0.173401loss in batch 69: 0.101532/0.172379loss in batch 70: 1.40561/0.189743loss in batch 71: 0.0777435/0.188187loss in batch 72: 0.0282745/0.185989loss in batch 73: 0.151169/0.185532loss in batch 74: 0.554352/0.190445loss in batch 75: 0.026947/0.188293loss in batch 76: 0.053009/0.186539loss in batch 77: 0.0322876/0.184555loss in batch 78: 0.526062/0.188873loss in batch 79: 0.366867/0.191101loss in batch 80: 0.288818/0.192307loss in batch 81: 0.0279083/0.190308loss in batch 82: 0.0818634/0.188995loss in batch 83: 0.113739/0.18811loss in batch 84: 0.0325317/0.186264loss in batch 85: 0.588013/0.190948loss in batch 86: 0.10376/0.189941loss in batch 87: 0.696823/0.195709loss in batch 88: 0.450409/0.198563loss in batch 89: 0.0485992/0.196899loss in batch 90: 0.347855/0.198563loss in batch 91: 0.207275/0.198654loss in batch 92: 0.0256958/0.196793loss in batch 93: 0.0864868/0.195633loss in batch 94: 0.136948/0.195007loss in batch 95: 0.370178/0.196838loss in batch 96: 0.13652/0.196213loss in batch 97: 0.387985/0.198151loss in batch 98: 0.110458/0.197281loss in batch 99: 0.0540771/0.195847loss in batch 100: 1.07675/0.204559loss in batch 101: 0.135117/0.203888loss in batch 102: 0.0240021/0.202148loss in batch 103: 0.0324554/0.2005loss in batch 104: 0.0761108/0.199326loss in batch 105: 0.362946/0.200867loss in batch 106: 0.312241/0.201904loss in batch 107: 0.148254/0.201416loss in batch 108: 0.260834/0.20195loss in batch 109: 0.00289917/0.20015loss in batch 110: 0.0893097/0.199142loss in batch 111: 0.0591278/0.197891loss in batch 112: 0.0625/0.196686loss in batch 113: 0.352783/0.198074loss in batch 114: 0.0610962/0.196884loss in batch 115: 0.0517578/0.195633loss in batch 116: 0.382111/0.19722loss in batch 117: 0.140259/0.196732loss in batch 118: 0.239899/0.197098loss in batch 119: 0.340378/0.198288loss in batch 120: 0.1371/0.197784loss in batch 121: 0.107147/0.197037loss in batch 122: 0.146484/0.19664loss in batch 123: 0.0524902/0.195465loss in batch 124: 0.246338/0.195877loss in batch 125: 0.0468292/0.194687loss in batch 126: 0.0140686/0.193283loss in batch 127: 0.131821/0.192795loss in batch 128: 0.0460205/0.191666loss in batch 129: 0.192871/0.191666loss in batch 130: 0.280197/0.192337loss in batch 131: 0.139252/0.19194loss in batch 132: 0.0120392/0.190582loss in batch 133: 0.155594/0.190323loss in batch 134: 0.128357/0.189865loss in batch 135: 0.101471/0.189224loss in batch 136: 0.204803/0.189316loss in batch 137: 0.407333/0.190918loss in batch 138: 0.00675964/0.18959loss in batch 139: 0.0378571/0.188507loss in batch 140: 0.0244141/0.187332loss in batch 141: 0.0537872/0.186401loss in batch 142: 0.095047/0.18576loss in batch 143: 0.115753/0.185272loss in batch 144: 0.219543/0.185501loss in batch 145: 0.589432/0.188278loss in batch 146: 0.0374756/0.187256loss in batch 147: 0.0164032/0.186096loss in batch 148: 0.0357971/0.185089loss in batch 149: 0.596832/0.18782loss in batch 150: 0.0129395/0.186661loss in batch 151: 0.0271301/0.185608loss in batch 152: 0.036026/0.184647loss in batch 153: 0.0357513/0.183685loss in batch 154: 0.0431061/0.18277loss in batch 155: 0.0348206/0.181824loss in batch 156: 0.0418549/0.180923loss in batch 157: 0.163513/0.180817loss in batch 158: 0.115387/0.180405loss in batch 159: 0.0927887/0.179855loss in batch 160: 0.213501/0.180069loss in batch 161: 0.189499/0.18013loss in batch 162: 0.289185/0.180801loss in batch 163: 0.0227356/0.17984loss in batch 164: 0.196381/0.179947loss in batch 165: 0.0209198/0.178986loss in batch 166: 0.277695/0.179565loss in batch 167: 0.065033/0.178894loss in batch 168: 0.0330963/0.178024loss in batch 169: 0.0186005/0.177078loss in batch 170: 0.495941/0.178955loss in batch 171: 0.0646515/0.178284loss in batch 172: 0.0131836/0.177322loss in batch 173: 0.0858612/0.176804loss in batch 174: 0.830154/0.180542loss in batch 175: 0.0670166/0.179901loss in batch 176: 0.0281219/0.179031loss in batch 177: 0.230133/0.179321loss in batch 178: 0.497009/0.181091loss in batch 179: 0.267822/0.18158loss in batch 180: 0.978851/0.185989loss in batch 181: 0.271255/0.186447loss in batch 182: 0.417419/0.187714loss in batch 183: 0.296402/0.188309loss in batch 184: 0.0110931/0.187347loss in batch 185: 0.0863342/0.186813loss in batch 186: 0.0201874/0.185913loss in batch 187: 0.0229797/0.185059loss in batch 188: 0.184372/0.185043loss in batch 189: 0.0772095/0.184464loss in batch 190: 0.234604/0.184738loss in batch 191: 0.110275/0.184357loss in batch 192: 0.506546/0.18602loss in batch 193: 0.071991/0.185425loss in batch 194: 0.197662/0.185501loss in batch 195: 0.198654/0.185562loss in batch 196: 0.266068/0.185959loss in batch 197: 0.381973/0.186966loss in batch 198: 0.103836/0.186539loss in batch 199: 0.0611267/0.185913loss in batch 200: 0.0153198/0.185074loss in batch 201: 0.0815582/0.184555loss in batch 202: 0.0387421/0.183838loss in batch 203: 0.0979309/0.183426loss in batch 204: 0.0735779/0.182877loss in batch 205: 0.106415/0.18251loss in batch 206: 0.604401/0.184555loss in batch 207: 0.0356293/0.183838loss in batch 208: 0.0317078/0.183105loss in batch 209: 0.321121/0.183762loss in batch 210: 0.00927734/0.182938loss in batch 211: 0.0836639/0.182465loss in batch 212: 0.0149078/0.181671
done with epoch 11
train_acc: 0.943662 (402/426)
test loss: 0.0149078
acc: 0.937063 (134/143)
loss in batch 0: 0.0480957/0.0480957loss in batch 1: 0.0449677/0.046524loss in batch 2: 0.0838928/0.0589905loss in batch 3: 0.430664/0.151917loss in batch 4: 0.0781403/0.137146loss in batch 5: 0.0499878/0.12262loss in batch 6: 0.105545/0.120178loss in batch 7: 0.426376/0.158447loss in batch 8: 0.0697784/0.148605loss in batch 9: 0.408508/0.174591loss in batch 10: 0.0437622/0.162689loss in batch 11: 0.0818787/0.155975loss in batch 12: 0.289841/0.16626loss in batch 13: 0.236099/0.171249loss in batch 14: 0.0578003/0.163681loss in batch 15: 0.131775/0.161697loss in batch 16: 0.254974/0.167175loss in batch 17: 0.407089/0.180511loss in batch 18: 0.327286/0.188232loss in batch 19: 0.073761/0.18251loss in batch 20: 0.0105896/0.174332loss in batch 21: 0.0779114/0.169937loss in batch 22: 0.0407257/0.164322loss in batch 23: 0.150299/0.163742loss in batch 24: 0.261948/0.167664loss in batch 25: 0.450592/0.178558loss in batch 26: 0.245239/0.181015loss in batch 27: 0.0978088/0.178055loss in batch 28: 0.139587/0.176712loss in batch 29: 0.27623/0.180038loss in batch 30: 0.0483856/0.175781loss in batch 31: 0.0792084/0.172775loss in batch 32: 0.106277/0.170761loss in batch 33: 0.281815/0.174026loss in batch 34: 0.0804291/0.171356loss in batch 35: 0.0681763/0.168488loss in batch 36: 0.029892/0.164734loss in batch 37: 0.0313263/0.161224loss in batch 38: 0.173767/0.16156loss in batch 39: 0.449387/0.168747loss in batch 40: 0.0904846/0.16684loss in batch 41: 0.217468/0.168045loss in batch 42: 0.0619507/0.165573loss in batch 43: 0.0456085/0.162842loss in batch 44: 0.0717621/0.160828loss in batch 45: 0.330185/0.164505loss in batch 46: 0.0122833/0.16127loss in batch 47: 0.11087/0.160217loss in batch 48: 0.480087/0.166748loss in batch 49: 0.122482/0.165863loss in batch 50: 0.337601/0.16922loss in batch 51: 0.0250244/0.166458loss in batch 52: 0.113022/0.165451loss in batch 53: 0.457092/0.170853loss in batch 54: 0.0851135/0.169281loss in batch 55: 0.191833/0.169693loss in batch 56: 0.0797577/0.168106loss in batch 57: 0.0939331/0.16684loss in batch 58: 0.0408173/0.164703loss in batch 59: 0.0199738/0.162292loss in batch 60: 0.0697784/0.160782loss in batch 61: 0.0236359/0.158554loss in batch 62: 0.229233/0.159683loss in batch 63: 0.0113373/0.157364loss in batch 64: 0.041153/0.155563loss in batch 65: 0.360199/0.158676loss in batch 66: 0.0401001/0.156906loss in batch 67: 0.377747/0.160141loss in batch 68: 0.667297/0.167496loss in batch 69: 0.230148/0.168396loss in batch 70: 0.0304718/0.166443loss in batch 71: 0.335846/0.168808loss in batch 72: 0.00469971/0.16655loss in batch 73: 0.00952148/0.164444loss in batch 74: 0.0489197/0.162903loss in batch 75: 0.192749/0.163284loss in batch 76: 0.096344/0.162415loss in batch 77: 0.0403595/0.160858loss in batch 78: 0.264221/0.162155loss in batch 79: 0.0298004/0.160522loss in batch 80: 0.019104/0.158768loss in batch 81: 0.0786438/0.157791loss in batch 82: 0.0233002/0.156174loss in batch 83: 0.448868/0.159653loss in batch 84: 0.138611/0.159393loss in batch 85: 1.15094/0.170944loss in batch 86: 0.144699/0.170624loss in batch 87: 0.157669/0.170486loss in batch 88: 0.487717/0.174057loss in batch 89: 0.139313/0.17366loss in batch 90: 0.122055/0.173096loss in batch 91: 0.0677643/0.171936loss in batch 92: 1.21301/0.183151loss in batch 93: 0.0162811/0.181366loss in batch 94: 0.0218506/0.179703loss in batch 95: 0.498596/0.183014loss in batch 96: 0.184052/0.183014loss in batch 97: 0.0194702/0.181351loss in batch 98: 0.122894/0.180771loss in batch 99: 0.397552/0.182938loss in batch 100: 0.254272/0.18364loss in batch 101: 0.0343323/0.182159loss in batch 102: 0.0190735/0.180588loss in batch 103: 0.0186615/0.179031loss in batch 104: 0.0147552/0.177475loss in batch 105: 0.0966339/0.176712loss in batch 106: 0.212677/0.177032loss in batch 107: 0.0503998/0.175858loss in batch 108: 0.494919/0.178802loss in batch 109: 0.0998688/0.17807loss in batch 110: 0.00994873/0.176559loss in batch 111: 0.624878/0.180573loss in batch 112: 0.112793/0.179977loss in batch 113: 0.376846/0.181686loss in batch 114: 0.0644531/0.180664loss in batch 115: 0.0847473/0.17984loss in batch 116: 0.0684204/0.178894loss in batch 117: 0.154602/0.17868loss in batch 118: 0.0411224/0.177536loss in batch 119: 0.106216/0.176941loss in batch 120: 0.0278625/0.175705loss in batch 121: 0.0558167/0.174728loss in batch 122: 0.327972/0.17598loss in batch 123: 0.106476/0.1754loss in batch 124: 0.424332/0.177399loss in batch 125: 0.33699/0.178665loss in batch 126: 0.0382385/0.177567loss in batch 127: 0.636261/0.181137loss in batch 128: 0.0911102/0.18045loss in batch 129: 0.101822/0.17984loss in batch 130: 0.0235748/0.17865loss in batch 131: 0.245834/0.179153loss in batch 132: 0.522293/0.181732loss in batch 133: 0.106583/0.181183loss in batch 134: 0.0176544/0.179962loss in batch 135: 0.0601959/0.179077loss in batch 136: 0.0515594/0.178162loss in batch 137: 0.0846252/0.177475loss in batch 138: 0.14183/0.177231loss in batch 139: 0.0153351/0.176071loss in batch 140: 0.0413971/0.17511loss in batch 141: 0.142242/0.174881loss in batch 142: 0.259705/0.175461loss in batch 143: 0.096283/0.174927loss in batch 144: 0.102631/0.174423loss in batch 145: 0.85437/0.179077loss in batch 146: 0.0831909/0.178436loss in batch 147: 0.00038147/0.177216loss in batch 148: 0.0477753/0.176361loss in batch 149: 0.147324/0.176163loss in batch 150: 0.458832/0.178024loss in batch 151: 0.141678/0.177795loss in batch 152: 0.0321198/0.176834loss in batch 153: 0.0932159/0.1763loss in batch 154: 1.28169/0.183426loss in batch 155: 0.365707/0.184601loss in batch 156: 0.0342865/0.18364loss in batch 157: 0.0653992/0.182892loss in batch 158: 0.00973511/0.181808loss in batch 159: 0.037735/0.180893loss in batch 160: 0.170013/0.180847loss in batch 161: 0.304062/0.181595loss in batch 162: 0.840042/0.185638loss in batch 163: 0.0658264/0.184906loss in batch 164: 0.0167999/0.183899loss in batch 165: 0.247345/0.184265loss in batch 166: 0.06073/0.183533loss in batch 167: 0.400085/0.184814loss in batch 168: 0.125351/0.184464loss in batch 169: 0.0838623/0.183884loss in batch 170: 0.927887/0.188232loss in batch 171: 0.0224762/0.187271loss in batch 172: 0.30101/0.187927loss in batch 173: 0.0210419/0.186966loss in batch 174: 0.00686646/0.185928loss in batch 175: 0.112061/0.185516loss in batch 176: 0.162109/0.185394loss in batch 177: 0.141754/0.185135loss in batch 178: 0.151871/0.184952loss in batch 179: 0.0832825/0.184387loss in batch 180: 0.0776215/0.183807loss in batch 181: 0.241364/0.184113loss in batch 182: 0.0534058/0.183395loss in batch 183: 0.0922241/0.182907loss in batch 184: 0.00724792/0.181946loss in batch 185: 0.277145/0.182465loss in batch 186: 0.0579681/0.181808loss in batch 187: 0.0497894/0.181091loss in batch 188: 0.0332642/0.180313loss in batch 189: 0.301102/0.180954loss in batch 190: 0.0184021/0.180099loss in batch 191: 0.223022/0.180328loss in batch 192: 0.15094/0.180176loss in batch 193: 0.263321/0.180588loss in batch 194: 0.04599/0.179916loss in batch 195: 0.204178/0.180038loss in batch 196: 0.126083/0.179764loss in batch 197: 0.0320282/0.179016loss in batch 198: 0.137604/0.178802loss in batch 199: 0.320587/0.179504loss in batch 200: 0.0594788/0.178925loss in batch 201: 0.120926/0.178635loss in batch 202: 0.0202332/0.177841loss in batch 203: 0.129807/0.177612loss in batch 204: 0.0162659/0.176834loss in batch 205: 0.0364227/0.176147loss in batch 206: 0.276871/0.176636loss in batch 207: 0.000610352/0.175781loss in batch 208: 0.084671/0.175354loss in batch 209: 0.235687/0.175644loss in batch 210: 0.0102081/0.17485loss in batch 211: 0.0346222/0.174194loss in batch 212: 0.216675/0.174393
done with epoch 12
train_acc: 0.938967 (400/426)
test loss: 0.216675
acc: 0.944056 (135/143)
loss in batch 0: 0.0874023/0.0874023loss in batch 1: 0.0236816/0.055542loss in batch 2: 0.164612/0.0919037loss in batch 3: 0.0127106/0.0720978loss in batch 4: 0.072113/0.0720978loss in batch 5: 0.0202026/0.063446loss in batch 6: 0.0334778/0.0591736loss in batch 7: 0.14711/0.0701752loss in batch 8: 0.0751801/0.0707092loss in batch 9: 0.0429993/0.0679474loss in batch 10: 0.00361633/0.0621033loss in batch 11: 0.586578/0.105804loss in batch 12: 0.0470123/0.101273loss in batch 13: 0.0135193/0.0950165loss in batch 14: 0.0612183/0.0927582loss in batch 15: 0.103333/0.0934143loss in batch 16: 0.136078/0.095932loss in batch 17: 0.390182/0.112289loss in batch 18: 0.0874481/0.110977loss in batch 19: 0.100189/0.110443loss in batch 20: 0.386368/0.123566loss in batch 21: 0.303543/0.13176loss in batch 22: 0.0187836/0.126846loss in batch 23: 0.135178/0.127182loss in batch 24: 0.159332/0.128464loss in batch 25: 0.0124817/0.124008loss in batch 26: 0.131485/0.124298loss in batch 27: 0.0552063/0.121826loss in batch 28: 0.00737/0.117874loss in batch 29: 0.0715637/0.116333loss in batch 30: 0.0349579/0.113708loss in batch 31: 0.046524/0.111618loss in batch 32: 0.518082/0.123932loss in batch 33: 0.0684967/0.122299loss in batch 34: 0.2323/0.125443loss in batch 35: 0.0561981/0.12352loss in batch 36: 0.353302/0.12973loss in batch 37: 0.216034/0.131989loss in batch 38: 0.0158539/0.129013loss in batch 39: 0.529846/0.139038loss in batch 40: 0.172684/0.139862loss in batch 41: 0.254013/0.142578loss in batch 42: 0.0312042/0.139999loss in batch 43: 0.62114/0.150925loss in batch 44: 0.00849915/0.147751loss in batch 45: 0.0414429/0.145447loss in batch 46: 0.139282/0.145325loss in batch 47: 0.113556/0.144653loss in batch 48: 0.0446167/0.142609loss in batch 49: 0.485657/0.14946loss in batch 50: 0.193726/0.150345loss in batch 51: 0.0499115/0.148407loss in batch 52: 0.113312/0.147751loss in batch 53: 0.123642/0.147308loss in batch 54: 0.0617828/0.145752loss in batch 55: 0.38797/0.15007loss in batch 56: 0.0403137/0.148148loss in batch 57: 0.0169983/0.145889loss in batch 58: 0.00773621/0.143555loss in batch 59: 0.0477753/0.141937loss in batch 60: 0.359711/0.145508loss in batch 61: 0.00926208/0.143311loss in batch 62: 0.0593719/0.141983loss in batch 63: 0.0624847/0.140747loss in batch 64: 0.0128021/0.138779loss in batch 65: 0.0568085/0.137543loss in batch 66: 0.250732/0.139221loss in batch 67: 0.0552826/0.137985loss in batch 68: 0.407364/0.141891loss in batch 69: 0.429398/0.145996loss in batch 70: 0.101807/0.14537loss in batch 71: 0.0771027/0.144424loss in batch 72: 0.0800781/0.143555loss in batch 73: 0.120804/0.14325loss in batch 74: 0.0434875/0.141922loss in batch 75: 0.14328/0.141922loss in batch 76: 0.453217/0.145966loss in batch 77: 0.00674438/0.14418loss in batch 78: 0.0917664/0.143509loss in batch 79: 0.114456/0.143158loss in batch 80: 0.231812/0.144257loss in batch 81: 0.179245/0.144684loss in batch 82: 0.277313/0.146271loss in batch 83: 0.130051/0.146088loss in batch 84: 0.143799/0.146057loss in batch 85: 0.23555/0.14711loss in batch 86: 0.449463/0.150574loss in batch 87: 0.0219421/0.149124loss in batch 88: 0.227173/0.149994loss in batch 89: 0.244354/0.151031loss in batch 90: 0.0894165/0.15036loss in batch 91: 0.0593109/0.149368loss in batch 92: 0.762665/0.15596loss in batch 93: 0.00338745/0.154343loss in batch 94: 0.0217896/0.152939loss in batch 95: 0.0030365/0.151398loss in batch 96: 0.0255127/0.150085loss in batch 97: 0.036972/0.148941loss in batch 98: 0.00694275/0.147491loss in batch 99: 0.0289612/0.146317loss in batch 100: 0.022644/0.145096loss in batch 101: 0.0820465/0.144485loss in batch 102: 0.0201569/0.143265loss in batch 103: 0.116104/0.143005loss in batch 104: 0.0580292/0.142197loss in batch 105: 0.236938/0.143082loss in batch 106: 0.0685272/0.14238loss in batch 107: 0.0376129/0.141418loss in batch 108: 0.0114746/0.140228loss in batch 109: 0.0550842/0.13945loss in batch 110: 0.0366516/0.138519loss in batch 111: 0.0404205/0.13765loss in batch 112: 0.0366211/0.136765loss in batch 113: 0.316437/0.138336loss in batch 114: 0.0291748/0.13739loss in batch 115: 0.0574646/0.136688loss in batch 116: 0.0878296/0.136276loss in batch 117: 0.12674/0.1362loss in batch 118: 0.301224/0.137589loss in batch 119: 0.181091/0.137939loss in batch 120: 0.108063/0.137695loss in batch 121: 0.0334167/0.136841loss in batch 122: 1.10411/0.144699loss in batch 123: 0.0507507/0.143951loss in batch 124: 0.0350647/0.143082loss in batch 125: 0.134064/0.143005loss in batch 126: 0.0906067/0.142609loss in batch 127: 0.166351/0.142776loss in batch 128: 0.0904236/0.14238loss in batch 129: 0.0706787/0.141815loss in batch 130: 0.43924/0.144089loss in batch 131: 0.252472/0.144913loss in batch 132: 0.322464/0.146255loss in batch 133: 0.334671/0.147644loss in batch 134: 0.868103/0.152985loss in batch 135: 0.0769501/0.152435loss in batch 136: 0.164261/0.152527loss in batch 137: 0.377594/0.154144loss in batch 138: 0.0953979/0.153732loss in batch 139: 0.0439453/0.152939loss in batch 140: 0.0304108/0.152069loss in batch 141: 0.0331116/0.15123loss in batch 142: 0.0589447/0.150589loss in batch 143: 0.101303/0.150238loss in batch 144: 0.0351563/0.14946loss in batch 145: 0.353333/0.150848loss in batch 146: 0.0124664/0.149918loss in batch 147: 0.375122/0.151428loss in batch 148: 0.0628967/0.150848loss in batch 149: 0.0140381/0.149918loss in batch 150: 0.0362396/0.14917loss in batch 151: 1.00719/0.154831loss in batch 152: 0.0257568/0.153976loss in batch 153: 0.0390472/0.153229loss in batch 154: 0.0978088/0.152863loss in batch 155: 0.0238342/0.152039loss in batch 156: 0.156448/0.152069loss in batch 157: 0.408249/0.153687loss in batch 158: 0.20314/0.154007loss in batch 159: 1.50258/0.162445loss in batch 160: 0.0936584/0.162003loss in batch 161: 0.657898/0.16507loss in batch 162: 1.64928/0.174179loss in batch 163: 0.0506744/0.173416loss in batch 164: 0.196198/0.173553loss in batch 165: 0.0233765/0.172653loss in batch 166: 0.354065/0.173737loss in batch 167: 0.097168/0.173294loss in batch 168: 0.409592/0.174683loss in batch 169: 0.0419922/0.173904loss in batch 170: 0.033783/0.17308loss in batch 171: 0.0967712/0.172638loss in batch 172: 0.140701/0.17244loss in batch 173: 0.0331421/0.171646loss in batch 174: 0.408386/0.173004loss in batch 175: 0.126633/0.172745loss in batch 176: 0.0751953/0.172195loss in batch 177: 0.387375/0.173401loss in batch 178: 0.306839/0.174149loss in batch 179: 0.0294952/0.17334loss in batch 180: 0.0396423/0.172592loss in batch 181: 0.0526276/0.171951loss in batch 182: 0.181473/0.171997loss in batch 183: 0.00793457/0.171097loss in batch 184: 0.0785675/0.170609loss in batch 185: 0.0141602/0.169769loss in batch 186: 0.0210876/0.168976loss in batch 187: 0.0540771/0.168365loss in batch 188: 0.142899/0.168228loss in batch 189: 0.0538635/0.167618loss in batch 190: 0.400497/0.168839loss in batch 191: 0.353775/0.1698loss in batch 192: 0.351074/0.170746loss in batch 193: 0.126678/0.170517loss in batch 194: 0.0828705/0.170074loss in batch 195: 0.522095/0.17186loss in batch 196: 0.0776062/0.171387loss in batch 197: 0.485992/0.172974loss in batch 198: 0.244385/0.173325loss in batch 199: 0.247223/0.173706loss in batch 200: 0.191849/0.173798loss in batch 201: 0.179688/0.173813loss in batch 202: 0.0836792/0.17337loss in batch 203: 0.106903/0.17305loss in batch 204: 0.121643/0.172791loss in batch 205: 0.0353546/0.172134loss in batch 206: 0.0439911/0.171509loss in batch 207: 0.580307/0.173477loss in batch 208: 0.356705/0.174362loss in batch 209: 0.019165/0.173615loss in batch 210: 0.0439606/0.173004loss in batch 211: 0.0300446/0.172333loss in batch 212: 0.00183105/0.171524
done with epoch 13
train_acc: 0.941315 (401/426)
test loss: 0.00183105
acc: 0.93007 (133/143)
loss in batch 0: 0.0272064/0.0272064loss in batch 1: 0.00354004/0.0153656loss in batch 2: 0.104553/0.045105loss in batch 3: 0.0328674/0.042038loss in batch 4: 0.00215149/0.0340576loss in batch 5: 0.0369263/0.0345459loss in batch 6: 0.042572/0.0356903loss in batch 7: 0.0517273/0.0376892loss in batch 8: 0.310425/0.0679932loss in batch 9: 0.386078/0.0998077loss in batch 10: 0.589432/0.144318loss in batch 11: 0.0147552/0.13353loss in batch 12: 0.11264/0.131912loss in batch 13: 0.553452/0.162018loss in batch 14: 0.355347/0.174911loss in batch 15: 0.00317383/0.164185loss in batch 16: 0.375946/0.176636loss in batch 17: 0.0279694/0.168381loss in batch 18: 0.0669403/0.16304loss in batch 19: 0.0200958/0.155884loss in batch 20: 0.273361/0.161484loss in batch 21: 0.409775/0.17276loss in batch 22: 0.0688629/0.168259loss in batch 23: 0.3181/0.1745loss in batch 24: 0.00540161/0.16774loss in batch 25: 0.0456238/0.16304loss in batch 26: 0.0877228/0.160248loss in batch 27: 0.0189667/0.155197loss in batch 28: 0.0617065/0.151978loss in batch 29: 0.0293427/0.147888loss in batch 30: 0.116577/0.146881loss in batch 31: 0.153946/0.14711loss in batch 32: 0.54631/0.159195loss in batch 33: 0.630676/0.173065loss in batch 34: 0.0307465/0.169006loss in batch 35: 0.373199/0.174667loss in batch 36: 0.0425262/0.171112loss in batch 37: 0.173157/0.171158loss in batch 38: 0.028595/0.167496loss in batch 39: 0.138809/0.166779loss in batch 40: 1.11809/0.189972loss in batch 41: 0.135513/0.18869loss in batch 42: 0.0809021/0.186188loss in batch 43: 0.0153198/0.182297loss in batch 44: 0.0745087/0.179901loss in batch 45: 0.0644989/0.177399loss in batch 46: 0.0736389/0.175186loss in batch 47: 0.0238037/0.172028loss in batch 48: 0.021225/0.168961loss in batch 49: 0.100113/0.167572loss in batch 50: 0.133881/0.166916loss in batch 51: 0.0349579/0.164383loss in batch 52: 0.0985413/0.163147loss in batch 53: 0.074585/0.161484loss in batch 54: 0.156067/0.161392loss in batch 55: 0.0158997/0.158813loss in batch 56: 0.00592041/0.156128loss in batch 57: 0.131012/0.155685loss in batch 58: 0.0510559/0.153915loss in batch 59: 0.000671387/0.151352loss in batch 60: 0.0246124/0.149277loss in batch 61: 0.330353/0.152191loss in batch 62: 0.290344/0.154388loss in batch 63: 0.358795/0.157578loss in batch 64: 0.206696/0.158356loss in batch 65: 0.0507202/0.156708loss in batch 66: 0.0594177/0.155258loss in batch 67: 0.038559/0.153549loss in batch 68: 0.0847626/0.152542loss in batch 69: 0.0362549/0.150879loss in batch 70: 0.0657806/0.149673loss in batch 71: 0.0341949/0.148071loss in batch 72: 0.063858/0.146927loss in batch 73: 0.144974/0.146912loss in batch 74: 0.0302277/0.145355loss in batch 75: 0.174957/0.145721loss in batch 76: 0.0249481/0.144165loss in batch 77: 0.535278/0.14917loss in batch 78: 0.00617981/0.147369loss in batch 79: 0.00419617/0.145569loss in batch 80: 0.00370789/0.143829loss in batch 81: 0.334198/0.146164loss in batch 82: 0.0721893/0.145264loss in batch 83: 0.232315/0.146286loss in batch 84: 0.165649/0.146515loss in batch 85: 0.0750885/0.145706loss in batch 86: 0.393539/0.148544loss in batch 87: 0.0888672/0.147858loss in batch 88: 0.365509/0.150314loss in batch 89: 0.068573/0.149399loss in batch 90: 0.0359497/0.148148loss in batch 91: 0.0452423/0.147034loss in batch 92: 0.044693/0.145935loss in batch 93: 0.0215149/0.144623loss in batch 94: 0.0540771/0.143661loss in batch 95: 0.0830383/0.143021loss in batch 96: 0.0470581/0.142044loss in batch 97: 0.0710602/0.141312loss in batch 98: 0.205704/0.141968loss in batch 99: 0.0764008/0.141312loss in batch 100: 0.103409/0.14093loss in batch 101: 0.047287/0.140015loss in batch 102: 0.0486908/0.13913loss in batch 103: 0.038269/0.138153loss in batch 104: 0.0144501/0.136978loss in batch 105: 0.117813/0.136795loss in batch 106: 0.0559845/0.136047loss in batch 107: 0.0282135/0.135056loss in batch 108: 0.138657/0.135086loss in batch 109: 0.0813751/0.134598loss in batch 110: 0.0810242/0.134109loss in batch 111: 0.0180206/0.133072loss in batch 112: 0.0122528/0.132004loss in batch 113: 0.198212/0.132584loss in batch 114: 0.291046/0.133972loss in batch 115: 0.0395203/0.133148loss in batch 116: 0.0814209/0.132706loss in batch 117: 0.0491486/0.132004loss in batch 118: 0.0253754/0.131104loss in batch 119: 0.0447083/0.130386loss in batch 120: 0.321594/0.131958loss in batch 121: 0.867722/0.137985loss in batch 122: 0.20401/0.138535loss in batch 123: 0.0239258/0.137604loss in batch 124: 0.0343933/0.13678loss in batch 125: 0.0952454/0.136444loss in batch 126: 0.0323486/0.135635loss in batch 127: 0.19194/0.136063loss in batch 128: 0.142929/0.136124loss in batch 129: 0.0778809/0.135666loss in batch 130: 0.100784/0.135406loss in batch 131: 0.0805206/0.134995loss in batch 132: 0.0122681/0.134064loss in batch 133: 0.0172424/0.133194loss in batch 134: 0.234955/0.133957loss in batch 135: 0.0581818/0.133392loss in batch 136: 0.139053/0.133438loss in batch 137: 0.329575/0.134857loss in batch 138: 0.0584412/0.134308loss in batch 139: 0.0329132/0.133575loss in batch 140: 0.0692291/0.133133loss in batch 141: 0.0258026/0.13237loss in batch 142: 0.0419006/0.131744loss in batch 143: 0.043396/0.131119loss in batch 144: 0.683167/0.134933loss in batch 145: 0.0570679/0.134399loss in batch 146: 0.273956/0.135345loss in batch 147: 0.0422974/0.13472loss in batch 148: 0.0534058/0.134171loss in batch 149: 0.0271454/0.133453loss in batch 150: 0.0484619/0.132889loss in batch 151: 0.292297/0.133942loss in batch 152: 0.0389862/0.133331loss in batch 153: 0.526016/0.13588loss in batch 154: 0.019165/0.135117loss in batch 155: 0.407806/0.136871loss in batch 156: 0.0413971/0.136261loss in batch 157: 0.00164795/0.135406loss in batch 158: 0.0156097/0.134659loss in batch 159: 0.0449371/0.134094loss in batch 160: 0.0584259/0.133621loss in batch 161: 0.105484/0.133453loss in batch 162: 0.16243/0.133621loss in batch 163: 0.0914917/0.133362loss in batch 164: 0.0539246/0.132889loss in batch 165: 0.273331/0.133743loss in batch 166: 0.880615/0.138214loss in batch 167: 1.29736/0.145111loss in batch 168: 0.0132751/0.144333loss in batch 169: 0.06427/0.14386loss in batch 170: 0.972/0.148697loss in batch 171: 0.255325/0.149323loss in batch 172: 0.134201/0.149231loss in batch 173: 0.015976/0.148468loss in batch 174: 0.312592/0.149414loss in batch 175: 0.0679321/0.148956loss in batch 176: 1.27663/0.155319loss in batch 177: 0.138657/0.155228loss in batch 178: 0.00492859/0.154373loss in batch 179: 0.212402/0.154694loss in batch 180: 0.0953979/0.154373loss in batch 181: 0.298035/0.155151loss in batch 182: 0.0658569/0.154678loss in batch 183: 0.202057/0.154938loss in batch 184: 0.281815/0.155624loss in batch 185: 0.524887/0.157608loss in batch 186: 0.378326/0.158798loss in batch 187: 0.158707/0.158783loss in batch 188: 0.239594/0.159225loss in batch 189: 0.102951/0.15892loss in batch 190: 0.0874329/0.158554loss in batch 191: 0.540848/0.160522loss in batch 192: 0.479965/0.162201loss in batch 193: 0.0144348/0.161423loss in batch 194: 0.225937/0.161758loss in batch 195: 0.0521698/0.161209loss in batch 196: 0.575714/0.163315loss in batch 197: 0.110382/0.16304loss in batch 198: 0.0172882/0.162308loss in batch 199: 0.0487366/0.161743loss in batch 200: 0.119888/0.16153loss in batch 201: 0.209915/0.161758loss in batch 202: 0.103867/0.161469loss in batch 203: 0.0358429/0.160858loss in batch 204: 0.559067/0.162811loss in batch 205: 0.20195/0.16301loss in batch 206: 0.396271/0.164124loss in batch 207: 0.648834/0.166458loss in batch 208: 0.382233/0.167496loss in batch 209: 0.0489655/0.166931loss in batch 210: 0.0873718/0.16655loss in batch 211: 0.419876/0.16774loss in batch 212: 0.0517578/0.167191
done with epoch 14
train_acc: 0.946009 (403/426)
test loss: 0.0517578
acc: 0.937063 (134/143)
loss in batch 0: 0.0569763/0.0569763loss in batch 1: 0.176178/0.116577loss in batch 2: 0.144485/0.125885loss in batch 3: 0.021225/0.0997162loss in batch 4: 0.232437/0.126251loss in batch 5: 0.0298615/0.110184loss in batch 6: 0.323792/0.140701loss in batch 7: 0.326767/0.163956loss in batch 8: 0.33226/0.182663loss in batch 9: 0.306229/0.195023loss in batch 10: 0.0435028/0.181259loss in batch 11: 0.031311/0.168762loss in batch 12: 0.0233459/0.157562loss in batch 13: 0.491684/0.181427loss in batch 14: 0.0796204/0.174637loss in batch 15: 0.207626/0.176712loss in batch 16: 0.0751495/0.170731loss in batch 17: 0.82309/0.20697loss in batch 18: 0.0230713/0.197296loss in batch 19: 0.0222931/0.188538loss in batch 20: 0.0455475/0.181732loss in batch 21: 0.0341034/0.175018loss in batch 22: 0.0364838/0.169006loss in batch 23: 0.0378723/0.163544loss in batch 24: 0.024826/0.15799loss in batch 25: 0.0133057/0.15242loss in batch 26: 0.10527/0.150681loss in batch 27: 0.123367/0.149704loss in batch 28: 0.0274963/0.145493loss in batch 29: 0.102844/0.144058loss in batch 30: 0.537872/0.156769loss in batch 31: 0.0665741/0.153946loss in batch 32: 0.161499/0.15419loss in batch 33: 0.110107/0.152893loss in batch 34: 0.226654/0.154984loss in batch 35: 0.00436401/0.150803loss in batch 36: 1.47408/0.186584loss in batch 37: 0.0220642/0.182236loss in batch 38: 0.43367/0.18869loss in batch 39: 0.0487671/0.185196loss in batch 40: 1.18401/0.209549loss in batch 41: 0.0372925/0.205444loss in batch 42: 0.0282135/0.20134loss in batch 43: 0.0691223/0.198318loss in batch 44: 0.127182/0.196747loss in batch 45: 0.118057/0.195023loss in batch 46: 0.0211639/0.19133loss in batch 47: 0.0310059/0.187988loss in batch 48: 0.367249/0.19165loss in batch 49: 0.0348969/0.188522loss in batch 50: 0.19516/0.188644loss in batch 51: 0.0888672/0.186722loss in batch 52: 0.0488281/0.184128loss in batch 53: 0.0197906/0.181076loss in batch 54: 0.015152/0.17807loss in batch 55: 0.210709/0.17865loss in batch 56: 0.0882721/0.177063loss in batch 57: 0.0484161/0.174835loss in batch 58: 0.0105438/0.172058loss in batch 59: 0.0477905/0.169998loss in batch 60: 0.0335388/0.167755loss in batch 61: 0.0675201/0.166138loss in batch 62: 0.216034/0.166931loss in batch 63: 0.00453186/0.164398loss in batch 64: 0.164505/0.164398loss in batch 65: 0.106689/0.163513loss in batch 66: 0.332932/0.166046loss in batch 67: 0.112198/0.165268loss in batch 68: 0.101425/0.164322loss in batch 69: 0.0348511/0.162491loss in batch 70: 0.0881805/0.161423loss in batch 71: 0.245529/0.162613loss in batch 72: 0.0565186/0.161148loss in batch 73: 0.193054/0.161575loss in batch 74: 0.123077/0.161072loss in batch 75: 0.367188/0.163773loss in batch 76: 0.0381775/0.16214loss in batch 77: 0.288239/0.163773loss in batch 78: 0.0904541/0.162827loss in batch 79: 0.29686/0.16452loss in batch 80: 0.0977783/0.163681loss in batch 81: 0.011795/0.161835loss in batch 82: 0.123001/0.161362loss in batch 83: 0.182846/0.161636loss in batch 84: 0.0507202/0.160309loss in batch 85: 0.372635/0.162796loss in batch 86: 0.0432739/0.161407loss in batch 87: 0.187805/0.161713loss in batch 88: 0.468277/0.165146loss in batch 89: 0.0432587/0.163803loss in batch 90: 0.101501/0.163132loss in batch 91: 0.110764/0.162552loss in batch 92: 0.0359039/0.161179loss in batch 93: 0.226288/0.16188loss in batch 94: 0.0552979/0.160751loss in batch 95: 0.0895691/0.160019loss in batch 96: 0.0556946/0.158951loss in batch 97: 0.0687256/0.15802loss in batch 98: 0.193268/0.158371loss in batch 99: 0.0884705/0.157684loss in batch 100: 0.108627/0.157196loss in batch 101: 0.213028/0.15773loss in batch 102: 0.135361/0.157516loss in batch 103: 0.0949554/0.156921loss in batch 104: 0.0625153/0.156021loss in batch 105: 0.11853/0.155655loss in batch 106: 0.355591/0.157532loss in batch 107: 0.21106/0.158035loss in batch 108: 0.0135651/0.156708loss in batch 109: 0.107559/0.15625loss in batch 110: 0.138107/0.156097loss in batch 111: 0.052475/0.155167loss in batch 112: 0.0214081/0.153992loss in batch 113: 0.219681/0.154556loss in batch 114: 1.13029/0.16304loss in batch 115: 0.206711/0.163422loss in batch 116: 0.164307/0.163437loss in batch 117: 0.370834/0.165192loss in batch 118: 0.00561523/0.163849loss in batch 119: 0.291916/0.164917loss in batch 120: 0.116577/0.164505loss in batch 121: 0.0848236/0.163864loss in batch 122: 0.0429993/0.162872loss in batch 123: 0.115692/0.162491loss in batch 124: 0.334549/0.163879loss in batch 125: 0.0301514/0.162827loss in batch 126: 0.070694/0.162094loss in batch 127: 0.405792/0.163986loss in batch 128: 0.0252838/0.162918loss in batch 129: 0.218781/0.163361loss in batch 130: 0.0169983/0.162231loss in batch 131: 0.104538/0.161789loss in batch 132: 0.102859/0.161346loss in batch 133: 0.303787/0.162415loss in batch 134: 0.179184/0.162537loss in batch 135: 0.00613403/0.161377loss in batch 136: 0.176819/0.161499loss in batch 137: 0.0109558/0.160416loss in batch 138: 0.267914/0.161179loss in batch 139: 0.682068/0.164902loss in batch 140: 0.0364838/0.163986loss in batch 141: 0.0393372/0.163116loss in batch 142: 0.103241/0.162704loss in batch 143: 0.126663/0.162445loss in batch 144: 0.235672/0.162949loss in batch 145: 0.272568/0.163696loss in batch 146: 0.0179749/0.16272loss in batch 147: 0.0328674/0.161819loss in batch 148: 0.445435/0.163727loss in batch 149: 0.0613708/0.163055loss in batch 150: 0.809097/0.167328loss in batch 151: 0.104889/0.166916loss in batch 152: 0.447815/0.168762loss in batch 153: 0.208603/0.169022loss in batch 154: 0.0385742/0.168167loss in batch 155: 0.433487/0.169876loss in batch 156: 0.0300446/0.168976loss in batch 157: 0.724808/0.172501loss in batch 158: 0.0493164/0.171738loss in batch 159: 0.0117645/0.170715loss in batch 160: 0.435852/0.172379loss in batch 161: 0.012085/0.171387loss in batch 162: 0.0335541/0.170547loss in batch 163: 0.18576/0.170639loss in batch 164: 0.0852509/0.170105loss in batch 165: 0.299911/0.170898loss in batch 166: 0.0363922/0.17009loss in batch 167: 0.0127106/0.169159loss in batch 168: 0.00480652/0.168182loss in batch 169: 0.0669556/0.167587loss in batch 170: 0.00996399/0.166656loss in batch 171: 0.134384/0.166489loss in batch 172: 0.116974/0.166199loss in batch 173: 0.0113983/0.165298loss in batch 174: 0.0459747/0.164612loss in batch 175: 0.0975037/0.164246loss in batch 176: 0.0356445/0.163513loss in batch 177: 0.0676727/0.162964loss in batch 178: 0.0494385/0.162354loss in batch 179: 0.0441284/0.161682loss in batch 180: 0.212219/0.161957loss in batch 181: 0.0648041/0.161438loss in batch 182: 0.100418/0.161102loss in batch 183: 0.019989/0.160324loss in batch 184: 0.0714417/0.159836loss in batch 185: 0.0811615/0.159424loss in batch 186: 0.0057373/0.158615loss in batch 187: 0.0684814/0.158127loss in batch 188: 0.28299/0.158783loss in batch 189: 0.323715/0.159653loss in batch 190: 0.0288696/0.158966loss in batch 191: 0.0804596/0.158554loss in batch 192: 0.284103/0.15921loss in batch 193: 0.0441132/0.158615loss in batch 194: 0.29306/0.159317loss in batch 195: 0.0616608/0.158798loss in batch 196: 0.307755/0.159561loss in batch 197: 0.448227/0.161011loss in batch 198: 0.0591278/0.160507loss in batch 199: 0.033905/0.159882loss in batch 200: 0.459839/0.161377loss in batch 201: 0.0389252/0.160767loss in batch 202: 0.371338/0.161789loss in batch 203: 0.0129395/0.161057loss in batch 204: 0.0704346/0.160629loss in batch 205: 0.0406647/0.160034loss in batch 206: 0.324646/0.160843loss in batch 207: 0.00744629/0.16011loss in batch 208: 0.187134/0.160233loss in batch 209: 0.315033/0.16098loss in batch 210: 0.0153503/0.160278loss in batch 211: 0.042923/0.159714loss in batch 212: 0.795532/0.16272
done with epoch 15
train_acc: 0.950704 (405/426)
test loss: 0.795532
acc: 0.944056 (135/143)
loss in batch 0: 0.0218201/0.0218201loss in batch 1: 0.0112/0.01651loss in batch 2: 0.0712128/0.0347443loss in batch 3: 0.0119324/0.0290375loss in batch 4: 1.08882/0.240997loss in batch 5: 0.0119781/0.202835loss in batch 6: 0.0974274/0.187759loss in batch 7: 0.389252/0.212967loss in batch 8: 0.11731/0.202316loss in batch 9: 0.0475922/0.186859loss in batch 10: 0.104385/0.179367loss in batch 11: 0.039093/0.167679loss in batch 12: 0.090271/0.161713loss in batch 13: 0.0861359/0.156311loss in batch 14: 0.0805969/0.15126loss in batch 15: 0.0588531/0.145493loss in batch 16: 0.432205/0.162354loss in batch 17: 0.180847/0.163391loss in batch 18: 0.138824/0.162094loss in batch 19: 0.0139008/0.154678loss in batch 20: 0.0614471/0.150238loss in batch 21: 0.0103302/0.14389loss in batch 22: 0.101074/0.142014loss in batch 23: 0.139252/0.141907loss in batch 24: 0.112656/0.140747loss in batch 25: 0.300201/0.146866loss in batch 26: 0.0906525/0.144791loss in batch 27: 0.00460815/0.139771loss in batch 28: 0.0181122/0.135574loss in batch 29: 0.0602417/0.133072loss in batch 30: 0.24472/0.136673loss in batch 31: 0.0513763/0.134003loss in batch 32: 0.193497/0.135803loss in batch 33: 0.0509338/0.133316loss in batch 34: 0.011734/0.129837loss in batch 35: 0.0536499/0.127731loss in batch 36: 0.0903168/0.126709loss in batch 37: 0.0999451/0.126007loss in batch 38: 0.00238037/0.122849loss in batch 39: 0.536362/0.133179loss in batch 40: 0.0505371/0.131165loss in batch 41: 0.0351257/0.128876loss in batch 42: 0.0596466/0.127274loss in batch 43: 0.261444/0.130325loss in batch 44: 0.065506/0.128876loss in batch 45: 0.210571/0.130646loss in batch 46: 0.469421/0.137848loss in batch 47: 0.0205841/0.135406loss in batch 48: 1.30873/0.159363loss in batch 49: 0.145752/0.159088loss in batch 50: 0.0310516/0.15657loss in batch 51: 0.0495148/0.15451loss in batch 52: 0.447998/0.160049loss in batch 53: 1.97557/0.19368loss in batch 54: 0.542511/0.200012loss in batch 55: 0.0314941/0.197006loss in batch 56: 0.0489807/0.194412loss in batch 57: 0.0525665/0.191956loss in batch 58: 0.443161/0.196228loss in batch 59: 0.123276/0.195007loss in batch 60: 0.217072/0.195374loss in batch 61: 0.180145/0.195114loss in batch 62: 0.0744324/0.193207loss in batch 63: 0.00494385/0.190262loss in batch 64: 0.00846863/0.187469loss in batch 65: 0.401749/0.19072loss in batch 66: 0.0655365/0.188858loss in batch 67: 0.125824/0.187927loss in batch 68: 0.186493/0.187897loss in batch 69: 0.880707/0.1978loss in batch 70: 0.286362/0.199051loss in batch 71: 0.0280914/0.196671loss in batch 72: 0.300583/0.198105loss in batch 73: 0.00575256/0.195496loss in batch 74: 0.0935059/0.194138loss in batch 75: 0.571152/0.199097loss in batch 76: 0.270706/0.200027loss in batch 77: 0.0464325/0.198059loss in batch 78: 0.0679474/0.196411loss in batch 79: 0.42276/0.199234loss in batch 80: 0.0177155/0.196991loss in batch 81: 0.0336761/0.195023loss in batch 82: 0.0453491/0.193192loss in batch 83: 0.00511169/0.190964loss in batch 84: 0.00648499/0.188797loss in batch 85: 0.0851898/0.187592loss in batch 86: 0.461197/0.190735loss in batch 87: 0.0106812/0.18869loss in batch 88: 0.0512848/0.187149loss in batch 89: 0.476746/0.190369loss in batch 90: 0.057251/0.188904loss in batch 91: 0.226288/0.189301loss in batch 92: 0.0690002/0.188019loss in batch 93: 0.0381775/0.186417loss in batch 94: 0.0217438/0.184692loss in batch 95: 0.00970459/0.182861loss in batch 96: 0.0531464/0.181519loss in batch 97: 0.0439453/0.18013loss in batch 98: 0.3591/0.181931loss in batch 99: 0.107101/0.181183loss in batch 100: 0.48204/0.184158loss in batch 101: 0.0304871/0.182648loss in batch 102: 0.109146/0.181946loss in batch 103: 0.077652/0.180939loss in batch 104: 0.0370483/0.179565loss in batch 105: 0.144043/0.17923loss in batch 106: 0.0945129/0.178436loss in batch 107: 0.550842/0.181885loss in batch 108: 0.00149536/0.180237loss in batch 109: 0.12468/0.179718loss in batch 110: 0.11734/0.179169loss in batch 111: 0.25975/0.179886loss in batch 112: 0.237747/0.180405loss in batch 113: 0.181351/0.180405loss in batch 114: 0.0522308/0.179291loss in batch 115: 0.0642548/0.178299loss in batch 116: 0.101929/0.177643loss in batch 117: 0.00263977/0.176163loss in batch 118: 0.128448/0.175766loss in batch 119: 0.0203094/0.174469loss in batch 120: 0.0063324/0.17308loss in batch 121: 0.0182495/0.171814loss in batch 122: 0.284378/0.172714loss in batch 123: 0.0721741/0.171906loss in batch 124: 0.463699/0.17424loss in batch 125: 0.0832214/0.173523loss in batch 126: 0.101517/0.172958loss in batch 127: 0.0524597/0.172012loss in batch 128: 0.0678406/0.171219loss in batch 129: 0.556519/0.174179loss in batch 130: 0.00717163/0.172897loss in batch 131: 0.450424/0.175003loss in batch 132: 0.0758362/0.174255loss in batch 133: 0.0897064/0.17363loss in batch 134: 0.145874/0.173431loss in batch 135: 0.0454559/0.172485loss in batch 136: 0.291397/0.173355loss in batch 137: 1.40552/0.182281loss in batch 138: 0.266373/0.182877loss in batch 139: 0.230759/0.183228loss in batch 140: 0.0400391/0.182205loss in batch 141: 0.181732/0.18219loss in batch 142: 0.0715637/0.181442loss in batch 143: 0.0427094/0.180466loss in batch 144: 0.102188/0.179916loss in batch 145: 0.00590515/0.178741loss in batch 146: 0.147217/0.178528loss in batch 147: 0.382278/0.179901loss in batch 148: 0.0287323/0.178879loss in batch 149: 0.290344/0.179611loss in batch 150: 0.00939941/0.178497loss in batch 151: 0.279282/0.179153loss in batch 152: 0.226608/0.179459loss in batch 153: 0.0838928/0.178848loss in batch 154: 0.0147095/0.17778loss in batch 155: 0.105103/0.177322loss in batch 156: 0.330032/0.178299loss in batch 157: 0.175613/0.178284loss in batch 158: 0.184601/0.178314loss in batch 159: 0.194153/0.178421loss in batch 160: 0.132156/0.178131loss in batch 161: 0.0593414/0.177399loss in batch 162: 0.0112915/0.176376loss in batch 163: 0.0131378/0.175385loss in batch 164: 0.0500793/0.174622loss in batch 165: 0.0240326/0.173721loss in batch 166: 0.0630035/0.17305loss in batch 167: 0.0692139/0.17244loss in batch 168: 0.253632/0.172913loss in batch 169: 0.0589447/0.172256loss in batch 170: 0.172897/0.172241loss in batch 171: 0.149063/0.172119loss in batch 172: 0.164154/0.172058loss in batch 173: 0.0797424/0.171539loss in batch 174: 0.095459/0.171112loss in batch 175: 0.116211/0.170792loss in batch 176: 0.050354/0.17012loss in batch 177: 0.0266571/0.169296loss in batch 178: 0.0796967/0.168808loss in batch 179: 0.065155/0.168228loss in batch 180: 0.0511169/0.167572loss in batch 181: 0.0263977/0.166809loss in batch 182: 0.0270081/0.166046loss in batch 183: 0.0474854/0.16539loss in batch 184: 0.136841/0.165253loss in batch 185: 0.510727/0.167099loss in batch 186: 0.0281219/0.166351loss in batch 187: 0.0599213/0.165802loss in batch 188: 0.0738678/0.165298loss in batch 189: 0.0301819/0.164597loss in batch 190: 0.0206909/0.163849loss in batch 191: 0.0469971/0.163223loss in batch 192: 0.140274/0.163116loss in batch 193: 0.0673218/0.162628loss in batch 194: 0.29628/0.163315loss in batch 195: 0.0160675/0.162552loss in batch 196: 0.251038/0.162994loss in batch 197: 0.165421/0.16301loss in batch 198: 0.0377502/0.162399loss in batch 199: 0.0543213/0.16185loss in batch 200: 0.233994/0.162216loss in batch 201: 0.0832062/0.161804loss in batch 202: 0.0614471/0.161316loss in batch 203: 0.146301/0.16124loss in batch 204: 0.0221558/0.160568loss in batch 205: 0.311813/0.161316loss in batch 206: 0.0331116/0.16069loss in batch 207: 0.621109/0.162903loss in batch 208: 0.106628/0.162628loss in batch 209: 0.214401/0.162872loss in batch 210: 0.0434723/0.162308loss in batch 211: 0.0258179/0.161667loss in batch 212: 0.071228/0.16124
done with epoch 16
train_acc: 0.943662 (402/426)
test loss: 0.071228
acc: 0.944056 (135/143)
loss in batch 0: 0.0140228/0.0140228loss in batch 1: 0.476227/0.245117loss in batch 2: 0.032547/0.174255loss in batch 3: 0.0165863/0.134857loss in batch 4: 0.518463/0.211563loss in batch 5: 0.0716095/0.188248loss in batch 6: 0.0178986/0.16391loss in batch 7: 0.529465/0.20961loss in batch 8: 0.408478/0.231705loss in batch 9: 0.0933228/0.21785loss in batch 10: 0.106781/0.207764loss in batch 11: 0.0192719/0.192062loss in batch 12: 0.0163116/0.178543loss in batch 13: 0.0320435/0.168076loss in batch 14: 0.0215149/0.158295loss in batch 15: 0.0154114/0.149368loss in batch 16: 0.0174866/0.141617loss in batch 17: 0.242844/0.147232loss in batch 18: 0.142807/0.147003loss in batch 19: 0.019577/0.140625loss in batch 20: 0.0873718/0.138092loss in batch 21: 0.32637/0.146652loss in batch 22: 0.00469971/0.140472loss in batch 23: 0.0396423/0.136276loss in batch 24: 0.0276947/0.131927loss in batch 25: 0.0362701/0.12825loss in batch 26: 0.295288/0.134445loss in batch 27: 0.0317535/0.130783loss in batch 28: 0.0672913/0.128586loss in batch 29: 0.0837402/0.12709loss in batch 30: 0.0399628/0.124283loss in batch 31: 0.024826/0.12117loss in batch 32: 0.234421/0.124603loss in batch 33: 0.0341339/0.121948loss in batch 34: 0.0114899/0.11879loss in batch 35: 0.0425568/0.116669loss in batch 36: 0.00158691/0.113556loss in batch 37: 0.140106/0.114258loss in batch 38: 0.0458984/0.112503loss in batch 39: 0.0212555/0.110229loss in batch 40: 0.046936/0.108688loss in batch 41: 0.501404/0.118027loss in batch 42: 0.486252/0.126602loss in batch 43: 0.0251007/0.124283loss in batch 44: 0.288391/0.12793loss in batch 45: 0.303802/0.13176loss in batch 46: 0.0169983/0.129318loss in batch 47: 0.420685/0.135391loss in batch 48: 0.0286102/0.133209loss in batch 49: 0.521759/0.140976loss in batch 50: 0.083252/0.139847loss in batch 51: 0.0981293/0.139038loss in batch 52: 0.197891/0.140152loss in batch 53: 0.0740967/0.138931loss in batch 54: 0.0422821/0.137177loss in batch 55: 0.0771942/0.136108loss in batch 56: 0.0204468/0.134079loss in batch 57: 0.0977936/0.133453loss in batch 58: 0.138519/0.13353loss in batch 59: 0.0446625/0.13205loss in batch 60: 0.0128174/0.130096loss in batch 61: 0.0131989/0.128204loss in batch 62: 0.0200348/0.126495loss in batch 63: 0.0128937/0.124725loss in batch 64: 0.0504913/0.123581loss in batch 65: 0.0875397/0.123032loss in batch 66: 0.0195618/0.12149loss in batch 67: 0.0773773/0.120834loss in batch 68: 0.0299835/0.119537loss in batch 69: 0.00622559/0.117905loss in batch 70: 0.0139923/0.11644loss in batch 71: 0.924622/0.12767loss in batch 72: 0.266296/0.129562loss in batch 73: 0.0482788/0.128464loss in batch 74: 0.333267/0.131195loss in batch 75: 0.0797119/0.130524loss in batch 76: 0.0630341/0.129639loss in batch 77: 0.0116272/0.128128loss in batch 78: 0.275711/0.130005loss in batch 79: 0.167236/0.130463loss in batch 80: 0.033844/0.129272loss in batch 81: 0.264877/0.13092loss in batch 82: 0.155258/0.13121loss in batch 83: 0.0861053/0.130676loss in batch 84: 0.217224/0.131699loss in batch 85: 0.151779/0.131943loss in batch 86: 0.442169/0.135498loss in batch 87: 0.117371/0.1353loss in batch 88: 0.0370789/0.134186loss in batch 89: 0.204224/0.134964loss in batch 90: 0.135727/0.134979loss in batch 91: 0.304993/0.136826loss in batch 92: 0.0605164/0.136002loss in batch 93: 0.0626831/0.135223loss in batch 94: 0.373962/0.137741loss in batch 95: 0.0208282/0.13652loss in batch 96: 0.0209503/0.13533loss in batch 97: 0.137085/0.135345loss in batch 98: 0.514023/0.13916loss in batch 99: 0.128372/0.139069loss in batch 100: 0.459015/0.142227loss in batch 101: 0.0351563/0.141174loss in batch 102: 0.0266571/0.140076loss in batch 103: 0.346191/0.142044loss in batch 104: 0.165375/0.142273loss in batch 105: 0.0263672/0.141174loss in batch 106: 0.0198822/0.140045loss in batch 107: 0.128845/0.139938loss in batch 108: 0.480667/0.143066loss in batch 109: 0.207886/0.143646loss in batch 110: 0.0394745/0.142731loss in batch 111: 0.0267029/0.141678loss in batch 112: 0.0361938/0.140747loss in batch 113: 0.0329437/0.139801loss in batch 114: 0.237427/0.140656loss in batch 115: 0.15271/0.140762loss in batch 116: 0.17865/0.141083loss in batch 117: 0.0105743/0.139969loss in batch 118: 0.182083/0.14032loss in batch 119: 0.0779877/0.139801loss in batch 120: 0.0946655/0.139435loss in batch 121: 0.316132/0.140884loss in batch 122: 0.198364/0.141342loss in batch 123: 1.30348/0.150726loss in batch 124: 0.00933838/0.149582loss in batch 125: 0.688339/0.153854loss in batch 126: 0.0910492/0.153381loss in batch 127: 0.338409/0.154816loss in batch 128: 0.070282/0.154175loss in batch 129: 0.0560455/0.153397loss in batch 130: 0.0825043/0.152878loss in batch 131: 0.762436/0.157486loss in batch 132: 0.219513/0.157959loss in batch 133: 0.339981/0.159317loss in batch 134: 0.157684/0.159302loss in batch 135: 0.046051/0.158463loss in batch 136: 0.000259399/0.157318loss in batch 137: 0.00654602/0.156219loss in batch 138: 0.0885162/0.155731loss in batch 139: 0.0549622/0.154999loss in batch 140: 0.0891113/0.154541loss in batch 141: 0.362244/0.156006loss in batch 142: 0.0506897/0.155273loss in batch 143: 0.184799/0.155472loss in batch 144: 0.286072/0.156372loss in batch 145: 0.545624/0.159042loss in batch 146: 0.112457/0.158722loss in batch 147: 0.223923/0.159164loss in batch 148: 0.111252/0.158844loss in batch 149: 0.0959473/0.158432loss in batch 150: 0.0862732/0.157944loss in batch 151: 0.101944/0.157578loss in batch 152: 0.13208/0.15741loss in batch 153: 0.0265045/0.156555loss in batch 154: 0.88681/0.16127loss in batch 155: 0.0327911/0.160461loss in batch 156: 0.0736389/0.159897loss in batch 157: 0.223663/0.160309loss in batch 158: 0.0390167/0.159531loss in batch 159: 0.0162048/0.15863loss in batch 160: 0.0606995/0.15802loss in batch 161: 0.0339508/0.157272loss in batch 162: 0.136017/0.15715loss in batch 163: 0.118088/0.156906loss in batch 164: 0.0193634/0.156067loss in batch 165: 0.0662842/0.155518loss in batch 166: 0.195007/0.155762loss in batch 167: 0.00578308/0.154861loss in batch 168: 1.84094/0.164841loss in batch 169: 0.0549316/0.1642loss in batch 170: 0.00305176/0.163254loss in batch 171: 0.0974121/0.162872loss in batch 172: 0.100586/0.162506loss in batch 173: 0.527557/0.164597loss in batch 174: 0.141678/0.164474loss in batch 175: 0.232986/0.164871loss in batch 176: 0.257095/0.16539loss in batch 177: 0.156464/0.165329loss in batch 178: 0.241089/0.165771loss in batch 179: 0.00428772/0.164871loss in batch 180: 0.082016/0.164398loss in batch 181: 0.24559/0.164856loss in batch 182: 0.00382996/0.163986loss in batch 183: 0.00630188/0.163116loss in batch 184: 0.023056/0.162354loss in batch 185: 0.0929871/0.161987loss in batch 186: 0.409271/0.163315loss in batch 187: 0.216492/0.163589loss in batch 188: 0.605759/0.165939loss in batch 189: 0.023468/0.165176loss in batch 190: 0.0414886/0.164536loss in batch 191: 0.286606/0.165176loss in batch 192: 0.0259247/0.164444loss in batch 193: 0.166/0.164459loss in batch 194: 0.37738/0.165558loss in batch 195: 0.256653/0.166016loss in batch 196: 0.0875702/0.165619loss in batch 197: 0.0795898/0.165176loss in batch 198: 0.0267487/0.16449loss in batch 199: 0.178848/0.164551loss in batch 200: 0.0138245/0.163818loss in batch 201: 0.145615/0.163712loss in batch 202: 0.0695648/0.163254loss in batch 203: 0.0287781/0.162582loss in batch 204: 0.0745087/0.16217loss in batch 205: 0.222168/0.16246loss in batch 206: 0.0200043/0.161774loss in batch 207: 0.0733948/0.161346loss in batch 208: 0.0453949/0.160782loss in batch 209: 0.265274/0.161285loss in batch 210: 0.0228271/0.160629loss in batch 211: 0.0532379/0.16011loss in batch 212: 0.0526123/0.159622
done with epoch 17
train_acc: 0.948357 (404/426)
test loss: 0.0526123
acc: 0.944056 (135/143)
loss in batch 0: 0.021759/0.021759loss in batch 1: 0.128235/0.0749969loss in batch 2: 0.0296021/0.0598602loss in batch 3: 0.0268402/0.0516052loss in batch 4: 0.297653/0.100815loss in batch 5: 0.0561829/0.0933685loss in batch 6: 0.386703/0.135284loss in batch 7: 0.137497/0.135559loss in batch 8: 0.155334/0.137756loss in batch 9: 0.0800018/0.131989loss in batch 10: 0.351746/0.151947loss in batch 11: 0.251144/0.160233loss in batch 12: 0.0280151/0.150055loss in batch 13: 0.143845/0.149612loss in batch 14: 0.0182343/0.140854loss in batch 15: 0.0367737/0.134338loss in batch 16: 0.0216675/0.127716loss in batch 17: 0.571503/0.152374loss in batch 18: 0.0774231/0.148438loss in batch 19: 0.0474396/0.143372loss in batch 20: 0.0280457/0.137878loss in batch 21: 0.26561/0.143692loss in batch 22: 0.0372772/0.139069loss in batch 23: 0.0551605/0.135574loss in batch 24: 0.18483/0.137543loss in batch 25: 0.237534/0.141388loss in batch 26: 0.0372925/0.137527loss in batch 27: 0.0767975/0.135361loss in batch 28: 0.14711/0.135757loss in batch 29: 0.347366/0.142822loss in batch 30: 0.112076/0.14183loss in batch 31: 0.0498962/0.138947loss in batch 32: 0.0789795/0.137131loss in batch 33: 0.0983429/0.135986loss in batch 34: 0.0573578/0.133743loss in batch 35: 0.0363007/0.131042loss in batch 36: 0.0140076/0.127884loss in batch 37: 0.0616913/0.126144loss in batch 38: 0.0256958/0.123566loss in batch 39: 0.0222931/0.121033loss in batch 40: 0.0753021/0.119919loss in batch 41: 0.0501251/0.118256loss in batch 42: 0.154266/0.11908loss in batch 43: 0.402939/0.125549loss in batch 44: 0.0173187/0.123138loss in batch 45: 0.0566864/0.121689loss in batch 46: 0.188416/0.123108loss in batch 47: 0.0211487/0.120987loss in batch 48: 0.0180969/0.118896loss in batch 49: 0.459076/0.125702loss in batch 50: 0.0387573/0.123993loss in batch 51: 0.119522/0.123901loss in batch 52: 0.0900421/0.12326loss in batch 53: 0.870544/0.1371loss in batch 54: 0.44957/0.142776loss in batch 55: 0.0445404/0.141022loss in batch 56: 0.0207672/0.138916loss in batch 57: 0.412476/0.143646loss in batch 58: 0.076004/0.142502loss in batch 59: 0.025528/0.140533loss in batch 60: 0.0380249/0.138855loss in batch 61: 0.00737/0.136734loss in batch 62: 0.19017/0.137589loss in batch 63: 0.0270996/0.135864loss in batch 64: 0.0700836/0.134842loss in batch 65: 0.0222015/0.133148loss in batch 66: 0.104523/0.132706loss in batch 67: 0.0205536/0.131073loss in batch 68: 0.203156/0.132111loss in batch 69: 0.0479889/0.130905loss in batch 70: 0.168839/0.131439loss in batch 71: 0.0688324/0.130569loss in batch 72: 0.0387878/0.129318loss in batch 73: 0.0085907/0.127686loss in batch 74: 1.20351/0.142029loss in batch 75: 0.925644/0.152344loss in batch 76: 0.647308/0.158768loss in batch 77: 0.10376/0.158066loss in batch 78: 0.0295258/0.156433loss in batch 79: 0.00953674/0.154587loss in batch 80: 0.2836/0.156189loss in batch 81: 0.249084/0.157333loss in batch 82: 0.0045929/0.155487loss in batch 83: 0.274307/0.156906loss in batch 84: 0.028656/0.15538loss in batch 85: 0.0785675/0.154495loss in batch 86: 0.0263062/0.15303loss in batch 87: 0.0207977/0.15152loss in batch 88: 0.468185/0.155075loss in batch 89: 0.0232391/0.15361loss in batch 90: 0.212845/0.154251loss in batch 91: 0.37294/0.156647loss in batch 92: 0.0182495/0.155151loss in batch 93: 0.277405/0.156464loss in batch 94: 0.540222/0.160492loss in batch 95: 0.025589/0.159088loss in batch 96: 0.0217285/0.157684loss in batch 97: 0.0141144/0.156204loss in batch 98: 0.0576935/0.155212loss in batch 99: 0.0331268/0.153992loss in batch 100: 0.0650024/0.153107loss in batch 101: 0.174408/0.15332loss in batch 102: 0.00263977/0.151855loss in batch 103: 0.225693/0.152557loss in batch 104: 0.410019/0.155014loss in batch 105: 0.0686798/0.154205loss in batch 106: 0.318161/0.155731loss in batch 107: 0.0204468/0.154495loss in batch 108: 0.00544739/0.153107loss in batch 109: 0.171387/0.153275loss in batch 110: 0.0294037/0.152161loss in batch 111: 0.147308/0.152115loss in batch 112: 0.085144/0.15152loss in batch 113: 0.0487213/0.15062loss in batch 114: 0.0779419/0.149994loss in batch 115: 0.019043/0.148865loss in batch 116: 0.00308228/0.147614loss in batch 117: 0.0505676/0.146805loss in batch 118: 0.0177307/0.145721loss in batch 119: 0.113129/0.145432loss in batch 120: 0.522858/0.14856loss in batch 121: 0.0976105/0.148148loss in batch 122: 0.028717/0.147171loss in batch 123: 0.193787/0.147537loss in batch 124: 0.0175323/0.1465loss in batch 125: 0.084671/0.146011loss in batch 126: 0.0532532/0.145294loss in batch 127: 0.0797119/0.14476loss in batch 128: 0.0122986/0.143753loss in batch 129: 0.238312/0.14447loss in batch 130: 0.00857544/0.143433loss in batch 131: 0.0419312/0.14267loss in batch 132: 0.0115204/0.141678loss in batch 133: 0.0822144/0.141235loss in batch 134: 0.109528/0.141006loss in batch 135: 0.129654/0.140915loss in batch 136: 0.162704/0.141083loss in batch 137: 0.0837402/0.140656loss in batch 138: 0.00645447/0.139694loss in batch 139: 0.224182/0.140305loss in batch 140: 0.297836/0.141418loss in batch 141: 0.0475616/0.140747loss in batch 142: 0.0383301/0.140045loss in batch 143: 0.561707/0.14296loss in batch 144: 0.226105/0.143539loss in batch 145: 0.100967/0.14325loss in batch 146: 0.00894165/0.142334loss in batch 147: 0.0448608/0.141678loss in batch 148: 0.0528564/0.141083loss in batch 149: 0.32077/0.142273loss in batch 150: 0.163239/0.14241loss in batch 151: 0.0269623/0.141663loss in batch 152: 0.0418091/0.141006loss in batch 153: 0.0120239/0.140167loss in batch 154: 0.673111/0.1436loss in batch 155: 0.330215/0.144806loss in batch 156: 0.0546417/0.144241loss in batch 157: 0.0510406/0.143646loss in batch 158: 0.0411835/0.14299loss in batch 159: 0.193024/0.143311loss in batch 160: 0.0488892/0.142715loss in batch 161: 0.0430603/0.142105loss in batch 162: 0.273071/0.142899loss in batch 163: 0.212646/0.143326loss in batch 164: 0.0260925/0.142624loss in batch 165: 0.0865173/0.142288loss in batch 166: 0.053299/0.141754loss in batch 167: 0.00592041/0.140945loss in batch 168: 0.046051/0.140396loss in batch 169: 0.0468292/0.139832loss in batch 170: 0.631516/0.142715loss in batch 171: 0.703964/0.145981loss in batch 172: 0.441254/0.147675loss in batch 173: 0.0250397/0.146973loss in batch 174: 0.306793/0.147888loss in batch 175: 0.0434875/0.147293loss in batch 176: 0.195343/0.147568loss in batch 177: 0.0176849/0.146835loss in batch 178: 0.0380402/0.146225loss in batch 179: 0.275513/0.146942loss in batch 180: 0.0188293/0.14624loss in batch 181: 0.02948/0.145599loss in batch 182: 0.015213/0.144882loss in batch 183: 0.0808868/0.144547loss in batch 184: 0.00169373/0.143768loss in batch 185: 0.0114441/0.143051loss in batch 186: 0.0272369/0.142426loss in batch 187: 0.893661/0.146423loss in batch 188: 0.000396729/0.14566loss in batch 189: 0.0379944/0.145081loss in batch 190: 0.0106506/0.144379loss in batch 191: 0.307205/0.145233loss in batch 192: 1.07323/0.15004loss in batch 193: 0.0656586/0.149612loss in batch 194: 0.0253906/0.148972loss in batch 195: 0.0568542/0.148514loss in batch 196: 0.210144/0.148819loss in batch 197: 0.0558777/0.148346loss in batch 198: 0.0648041/0.147934loss in batch 199: 0.135895/0.147858loss in batch 200: 1.44643/0.154327loss in batch 201: 0.0615997/0.15387loss in batch 202: 0.0386505/0.153305loss in batch 203: 0.168823/0.153381loss in batch 204: 0.146881/0.153351loss in batch 205: 0.0157318/0.152664loss in batch 206: 0.445099/0.154083loss in batch 207: 0.491913/0.155701loss in batch 208: 0.0865021/0.15538loss in batch 209: 0.079483/0.155014loss in batch 210: 0.210281/0.155289loss in batch 211: 0.31871/0.156052loss in batch 212: 0.0209808/0.155411
done with epoch 18
train_acc: 0.950704 (405/426)
test loss: 0.0209808
acc: 0.937063 (134/143)
loss in batch 0: 0.0689545/0.0689545loss in batch 1: 0.0785522/0.0737457loss in batch 2: 0.0248718/0.0574646loss in batch 3: 0.104309/0.0691681loss in batch 4: 0.320938/0.119522loss in batch 5: 0.0229645/0.103424loss in batch 6: 0.0345612/0.0935974loss in batch 7: 0.262314/0.114685loss in batch 8: 0.58873/0.167358loss in batch 9: 0.0771332/0.15834loss in batch 10: 0.0131531/0.145126loss in batch 11: 0.0218048/0.134857loss in batch 12: 0.143066/0.135483loss in batch 13: 0.362854/0.151718loss in batch 14: 0.118073/0.14949loss in batch 15: 0.100433/0.146408loss in batch 16: 0.0805054/0.142548loss in batch 17: 0.029068/0.13623loss in batch 18: 0.0434418/0.131363loss in batch 19: 0.00952148/0.125259loss in batch 20: 0.00756836/0.119659loss in batch 21: 0.00398254/0.114395loss in batch 22: 0.042038/0.111252loss in batch 23: 0.324509/0.120148loss in batch 24: 0.00157166/0.115387loss in batch 25: 0.172577/0.117599loss in batch 26: 0.101395/0.117004loss in batch 27: 0.0705261/0.115341loss in batch 28: 0.353119/0.123535loss in batch 29: 0.167007/0.124985loss in batch 30: 0.200745/0.127426loss in batch 31: 0.141266/0.127853loss in batch 32: 0.113449/0.127426loss in batch 33: 0.00991821/0.123962loss in batch 34: 0.0929108/0.123077loss in batch 35: 0.0128326/0.120026loss in batch 36: 0.109818/0.119736loss in batch 37: 0.416595/0.127548loss in batch 38: 0.359818/0.133514loss in batch 39: 0.0424347/0.131226loss in batch 40: 0.395676/0.13768loss in batch 41: 0.177475/0.138626loss in batch 42: 0.0767365/0.137192loss in batch 43: 0.00547791/0.134201loss in batch 44: 0.00563049/0.131332loss in batch 45: 0.0749359/0.130112loss in batch 46: 0.185028/0.131271loss in batch 47: 0.213181/0.13298loss in batch 48: 0.0685272/0.131668loss in batch 49: 0.019516/0.12944loss in batch 50: 0.542084/0.137512loss in batch 51: 0.0224609/0.1353loss in batch 52: 1.25984/0.156525loss in batch 53: 0.0718536/0.154953loss in batch 54: 1.0956/0.172058loss in batch 55: 0.0739746/0.170319loss in batch 56: 0.00210571/0.167358loss in batch 57: 0.127869/0.166672loss in batch 58: 0.068222/0.165009loss in batch 59: 0.0804138/0.163605loss in batch 60: 0.0153351/0.161179loss in batch 61: 0.199387/0.161774loss in batch 62: 0.242416/0.163071loss in batch 63: 0.0135803/0.160721loss in batch 64: 0.0547333/0.159103loss in batch 65: 0.492599/0.164139loss in batch 66: 0.0274658/0.162109loss in batch 67: 0.0311584/0.160187loss in batch 68: 0.153809/0.160095loss in batch 69: 0.0480957/0.158493loss in batch 70: 0.471375/0.162888loss in batch 71: 0.00473022/0.160706loss in batch 72: 0.0881348/0.159714loss in batch 73: 0.144028/0.159485loss in batch 74: 0.0248108/0.1577loss in batch 75: 0.0635681/0.156464loss in batch 76: 0.338654/0.158829loss in batch 77: 0.253479/0.160049loss in batch 78: 0.114716/0.15947loss in batch 79: 0.108566/0.158844loss in batch 80: 0.0758667/0.157806loss in batch 81: 0.0576172/0.156586loss in batch 82: 0.437607/0.159973loss in batch 83: 0.0556793/0.158737loss in batch 84: 0.312515/0.160538loss in batch 85: 0.0996094/0.159836loss in batch 86: 0.00189209/0.15802loss in batch 87: 0.0291748/0.156555loss in batch 88: 0.0354309/0.155197loss in batch 89: 0.847748/0.162888loss in batch 90: 0.033905/0.161469loss in batch 91: 0.220749/0.162109loss in batch 92: 0.0769501/0.161194loss in batch 93: 0.0547943/0.160065loss in batch 94: 0.231445/0.160812loss in batch 95: 0.422684/0.163544loss in batch 96: 0.223495/0.164169loss in batch 97: 0.100723/0.163513loss in batch 98: 0.0779877/0.162659loss in batch 99: 0.371399/0.164734loss in batch 100: 0.204437/0.165146loss in batch 101: 0.198944/0.165466loss in batch 102: 0.1828/0.165634loss in batch 103: 0.0249481/0.164291loss in batch 104: 0.0328522/0.16304loss in batch 105: 0.172272/0.163116loss in batch 106: 0.082901/0.162354loss in batch 107: 0.056366/0.161377loss in batch 108: 0.228867/0.162003loss in batch 109: 0.145874/0.16185loss in batch 110: 0.145996/0.161713loss in batch 111: 0.0101318/0.16037loss in batch 112: 0.109924/0.159912loss in batch 113: 0.0833893/0.159241loss in batch 114: 0.00593567/0.157913loss in batch 115: 0.0912781/0.157333loss in batch 116: 0.0449066/0.156387loss in batch 117: 0.0461273/0.155441loss in batch 118: 0.111771/0.15506loss in batch 119: 0.481354/0.157791loss in batch 120: 0.0133514/0.156601loss in batch 121: 0.0273895/0.155533loss in batch 122: 0.0155792/0.154388loss in batch 123: 0.402649/0.156403loss in batch 124: 0.0373077/0.155457loss in batch 125: 0.417435/0.157532loss in batch 126: 0.0362701/0.156586loss in batch 127: 0.106506/0.156189loss in batch 128: 0.044281/0.155319loss in batch 129: 0.127319/0.15509loss in batch 130: 0.268158/0.155975loss in batch 131: 0.0308838/0.155014loss in batch 132: 0.266495/0.155853loss in batch 133: 0.0285034/0.154907loss in batch 134: 0.124466/0.154678loss in batch 135: 0.0565491/0.153946loss in batch 136: 0.0444336/0.153168loss in batch 137: 0.0101166/0.152115loss in batch 138: 1.0549/0.158615loss in batch 139: 0.0558014/0.157883loss in batch 140: 0.0862427/0.157379loss in batch 141: 0.358414/0.158798loss in batch 142: 0.514359/0.161285loss in batch 143: 0.0422363/0.160461loss in batch 144: 0.0919037/0.159988loss in batch 145: 0.263077/0.16069loss in batch 146: 0.0557861/0.159958loss in batch 147: 0.116974/0.159668loss in batch 148: 0.00683594/0.158661loss in batch 149: 0.0257568/0.157761loss in batch 150: 0.0267639/0.156891loss in batch 151: 0.117081/0.156647loss in batch 152: 0.224075/0.157074loss in batch 153: 0.0172272/0.156158loss in batch 154: 0.109329/0.155869loss in batch 155: 0.0201874/0.154984loss in batch 156: 0.0919342/0.154587loss in batch 157: 0.711334/0.158127loss in batch 158: 0.0476532/0.157425loss in batch 159: 0.208176/0.157745loss in batch 160: 0.108612/0.157425loss in batch 161: 0.0267029/0.156631loss in batch 162: 0.150482/0.156601loss in batch 163: 0.0175781/0.155746loss in batch 164: 0.0723877/0.155243loss in batch 165: 0.0574341/0.154648loss in batch 166: 1.44305/0.162369loss in batch 167: 0.519775/0.164505loss in batch 168: 0.259964/0.165054loss in batch 169: 0.235931/0.165466loss in batch 170: 0.316223/0.166351loss in batch 171: 0.0222931/0.165527loss in batch 172: 0.0193024/0.164673loss in batch 173: 0.0281372/0.163879loss in batch 174: 0.340942/0.164902loss in batch 175: 0.407486/0.16629loss in batch 176: 0.0643005/0.16571loss in batch 177: 0.0993347/0.165329loss in batch 178: 0.0909119/0.164917loss in batch 179: 0.440796/0.166458loss in batch 180: 0.143295/0.166321loss in batch 181: 0.0328674/0.165588loss in batch 182: 0.222549/0.165909loss in batch 183: 0.00566101/0.165024loss in batch 184: 0.0221252/0.164261loss in batch 185: 0.0632324/0.163696loss in batch 186: 0.264709/0.164246loss in batch 187: 0.398865/0.165512loss in batch 188: 0.0352173/0.164795loss in batch 189: 0.0315704/0.164108loss in batch 190: 0.0270844/0.163391loss in batch 191: 0.0880432/0.162994loss in batch 192: 0.0525513/0.16243loss in batch 193: 0.103851/0.162125loss in batch 194: 0.0348969/0.161484loss in batch 195: 0.0447693/0.160889loss in batch 196: 0.00469971/0.16008loss in batch 197: 0.0181427/0.159363loss in batch 198: 0.050354/0.158813loss in batch 199: 0.0382385/0.158203loss in batch 200: 0.0487823/0.157684loss in batch 201: 0.00602722/0.156921loss in batch 202: 0.0110931/0.156189loss in batch 203: 0.263779/0.156723loss in batch 204: 0.00895691/0.156006loss in batch 205: 0.0962677/0.155716loss in batch 206: 0.0226288/0.155075loss in batch 207: 0.0340881/0.15448loss in batch 208: 0.0245514/0.15387loss in batch 209: 0.0594635/0.153412loss in batch 210: 0.137436/0.153336loss in batch 211: 0.0578308/0.152893loss in batch 212: 0.0637665/0.152481
done with epoch 19
train_acc: 0.953052 (406/426)
test loss: 0.0637665
acc: 0.944056 (135/143)
[0.171066, -0.0512085, -0.0587616, -0.100845, -0.0185242, -0.0265045, -0.0882568, -0.012619, -0.000991821, -0.00254822, -0.288483, -0.093399, -0.00498962, -0.315262, -0.385544, 0.0899353, -0.0496063, 0.0156403, 0.0464783, 0.00109863, 0.438019, 0.0844879, -0.148117, -0.0266266, 0.0140533, -0.0321503, -0.00811768, 0.0920715, -0.0399933, 0.0230713, -0.009552, 0.0491638, -0.313965, 0.286331, -0.00123596, 0.31871, -0.0856781, 0.137466, -0.0839386, 0.310684, 0.853287, -0.00691223, 0.671295, -0.00389099, -0.389267, 0.00106812, -0.00088501, -0.0567169, -0.0100098, 0.113281, 0.0404205, 0.164352, 0.0852509, -0.0315399, -0.0469513, -0.0126801, -0.0127258, -0.00776672, -0.0407104, 0.000900269, 0.420914, 0.201279, -0.00933838, -0.0261078, 0.0205383, -0.0418701, 0, 0.0167999, 0.00894165, -0.00682068, -0.368881, 0.0350342, -0.0377808, 0.564255, 0.0055542, -0.0427704, -0.00627136, -0.144592, -0.0122681, -0.00106812, 0.233826, 0.000854492, 0.0348969, -0.00985718, 0.103287, -0.0363159, -0.0146027, -0.00221252, 0.0659943, 9.15527e-05, -0.00506592, 0.618408, -0.407516, 0.00559998, -0.00389099, -0.0346527, 0.0147858, -0.511551, -0.0288544, -0.0149536, -0.00117493, -0.0551758, -0.0587769, -0.157898, 0.0640564, -0.00801086, 0.0052948, -0.132538, 0.727036, 0.749313, -0.0378113, 0.0103455, 0.0611877, -0.262222, -0.0112915, -0.030014, -0.28186, -0.0144958, -0.0367889, -0.106064, -0.0272369, -0.0205841, 0.295166, -0.0840912, 0.00482178, -0.138702, -0.207306, -0.0213928, -0.0114594, -0.00334167, 0.144852, -0.00265503, -0.0101929, -0.195679, -0.13797, -0.00814819, -0.205811, 0.597168, 0.00012207, -0.00735474, -0.0687866, -0.0281525, 0.000976563]
Compiler: ./compile.py breast_logistic
	750 triples of gfp left
	804 dabits of gfp left
	920 triples of gfp left
	166 dabits of gfp left
2 threads spent a total of 178.457 seconds (785.672 MB, 2735208 rounds) on the online phase, 799.022 seconds (75425.3 MB, 705015 rounds) on the preprocessing/offline phase, and 978.612 seconds idling.
Join timer: 0 978044
Finish timer: 0.00268008
Join timer: 1 970278
Finish timer: 0.00268008
Communication details (rounds in parallel threads counted double):
Exchanging one-to-one 47680 MB in 69855 rounds, taking 102.517 seconds
Receiving directly 785.672 MB in 1367604 rounds, taking 142.083 seconds
Receiving one-to-one 28664.5 MB in 317580 rounds, taking 42.0135 seconds
Sending directly 785.672 MB in 1367604 rounds, taking 16.7969 seconds
Sending one-to-one 27745.2 MB in 317580 rounds, taking 4.13228 seconds
CPU time = 669.838 (overall core time)
The following benchmarks are including preprocessing (offline phase).
Time = 978.047 seconds 
Data sent = 76210.9 MB in ~3440223 rounds (party 0 only)
Global data sent = 153341 MB (all parties)
Actual cost of program:
  Type int
      23279330        Triples
     229817398           Bits
This program might benefit from some protocol options.
Consider adding the following at the beginning of your code:
	program.use_trunc_pr = True
	program.use_edabit(True)
This virtual machine was compiled with GCC. Recompile with 'CXX = clang++' in 'CONFIG.mine' for optimal performance.
Command line: ./semi-party.x -N 2 -e --ip-file-name /HOST -p 0 -v breast_logistic
