Using SPDZ2k security parameter 64
Using statistical security parameter 40
Trying to run 64-bit computation
Current working directory: "/root/MP-SPDZ"
Current working directory: "/root/MP-SPDZ"
Using SGD
done with epoch 0done with epoch 1done with epoch 2done with epoch 3done with epoch 4done with epoch 5done with epoch 6done with epoch 7done with epoch 8done with epoch 9done with epoch 10done with epoch 11done with epoch 12done with epoch 13done with epoch 14done with epoch 15done with epoch 16done with epoch 17done with epoch 18done with epoch 19
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Using SGD
loss in batch 0: 0.452011/0.452011loss in batch 1: 0.78006/0.616028loss in batch 2: 0.436752/0.556274loss in batch 3: 0.795212/0.615997loss in batch 4: 0.459351/0.584686loss in batch 5: 0.839279/0.627121loss in batch 6: 0.460068/0.603241loss in batch 7: 0.413025/0.579468loss in batch 8: 0.414886/0.561188loss in batch 9: 0.814209/0.586487loss in batch 10: 0.771072/0.603256loss in batch 11: 0.881927/0.626495loss in batch 12: 1.3316/0.680725loss in batch 13: 0.395508/0.660355loss in batch 14: 0.369019/0.64093loss in batch 15: 0.373734/0.624237loss in batch 16: 0.818298/0.635651loss in batch 17: 0.811249/0.645401loss in batch 18: 0.788666/0.652939loss in batch 19: 0.781662/0.659393loss in batch 20: 0.809341/0.666519loss in batch 21: 0.756165/0.670593loss in batch 22: 0.424805/0.659912loss in batch 23: 0.425049/0.650116loss in batch 24: 0.445953/0.641953loss in batch 25: 0.933853/0.653183loss in batch 26: 0.66539/0.653641loss in batch 27: 0.793427/0.65863loss in batch 28: 0.43129/0.650787loss in batch 29: 0.500076/0.645767loss in batch 30: 0.44223/0.639191loss in batch 31: 0.607315/0.638199loss in batch 32: 0.847641/0.644547loss in batch 33: 0.837799/0.650238loss in batch 34: 0.733856/0.652618loss in batch 35: 0.630295/0.651993loss in batch 36: 0.771896/0.655243loss in batch 37: 0.702103/0.656479loss in batch 38: 0.686264/0.657242loss in batch 39: 0.737961/0.659271loss in batch 40: 0.755112/0.661591loss in batch 41: 0.790466/0.664673loss in batch 42: 0.652496/0.664383loss in batch 43: 0.684631/0.664841loss in batch 44: 0.603149/0.663483loss in batch 45: 0.743042/0.665192loss in batch 46: 0.51886/0.662094loss in batch 47: 0.603546/0.660873loss in batch 48: 0.541336/0.658432loss in batch 49: 0.763275/0.660522loss in batch 50: 0.60994/0.659531loss in batch 51: 0.582321/0.658051loss in batch 52: 0.597794/0.656906loss in batch 53: 0.645096/0.656693loss in batch 54: 0.549042/0.654724loss in batch 55: 0.691711/0.65538loss in batch 56: 0.546768/0.653488loss in batch 57: 0.659256/0.65358loss in batch 58: 0.554337/0.651901loss in batch 59: 0.514725/0.649612loss in batch 60: 0.539902/0.647812loss in batch 61: 0.799072/0.650253loss in batch 62: 0.555298/0.648743loss in batch 63: 0.432022/0.645355loss in batch 64: 0.646469/0.645386loss in batch 65: 0.399643/0.641663loss in batch 66: 0.680435/0.642227loss in batch 67: 0.707016/0.643188loss in batch 68: 0.644684/0.643204loss in batch 69: 0.790512/0.645309loss in batch 70: 0.438904/0.64241loss in batch 71: 0.553116/0.641159loss in batch 72: 0.507858/0.639343loss in batch 73: 0.580948/0.63855loss in batch 74: 0.514145/0.636902loss in batch 75: 0.436752/0.634262loss in batch 76: 0.52179/0.632813loss in batch 77: 0.459335/0.630569loss in batch 78: 0.48291/0.628708loss in batch 79: 0.46524/0.626663loss in batch 80: 0.53569/0.625534loss in batch 81: 0.593933/0.625153loss in batch 82: 0.598236/0.624832loss in batch 83: 0.507477/0.623444loss in batch 84: 0.555649/0.62265loss in batch 85: 0.604462/0.622421loss in batch 86: 0.476898/0.620758loss in batch 87: 0.469742/0.619049loss in batch 88: 0.511383/0.617828loss in batch 89: 0.410553/0.615524loss in batch 90: 0.52153/0.614502loss in batch 91: 0.404129/0.612213loss in batch 92: 0.562729/0.611679loss in batch 93: 0.570831/0.611237loss in batch 94: 0.334946/0.608322loss in batch 95: 0.674545/0.609024loss in batch 96: 0.622025/0.609146loss in batch 97: 0.672592/0.609802loss in batch 98: 0.826736/0.611984loss in batch 99: 0.487488/0.610748loss in batch 100: 0.530411/0.609955loss in batch 101: 0.887543/0.612671loss in batch 102: 0.355316/0.610168loss in batch 103: 0.698242/0.611023loss in batch 104: 0.379272/0.608826loss in batch 105: 0.408844/0.606934loss in batch 106: 0.44426/0.605408loss in batch 107: 0.384399/0.603363loss in batch 108: 0.50676/0.602478loss in batch 109: 0.615005/0.6026loss in batch 110: 0.397064/0.600739loss in batch 111: 0.360107/0.598602loss in batch 112: 0.539673/0.598068loss in batch 113: 0.508362/0.59729loss in batch 114: 0.539032/0.596771loss in batch 115: 0.311508/0.594315loss in batch 116: 0.480316/0.593338loss in batch 117: 0.497971/0.592529loss in batch 118: 0.44632/0.591309loss in batch 119: 0.578766/0.591187loss in batch 120: 0.614639/0.591385loss in batch 121: 0.562637/0.591156loss in batch 122: 0.707916/0.592102loss in batch 123: 0.573288/0.591965loss in batch 124: 0.480408/0.591064loss in batch 125: 0.439331/0.589859loss in batch 126: 0.435547/0.588638loss in batch 127: 0.294067/0.586334loss in batch 128: 0.52951/0.585892loss in batch 129: 0.444321/0.584808loss in batch 130: 0.624344/0.585114loss in batch 131: 0.745605/0.586319loss in batch 132: 0.239838/0.583725loss in batch 133: 0.530365/0.583328loss in batch 134: 0.854111/0.585342loss in batch 135: 0.577713/0.585266loss in batch 136: 0.309128/0.583252loss in batch 137: 0.501358/0.582657loss in batch 138: 0.56044/0.582504loss in batch 139: 0.438278/0.581467loss in batch 140: 0.557083/0.581299loss in batch 141: 0.48822/0.580643loss in batch 142: 0.585815/0.580673loss in batch 143: 0.353333/0.579102loss in batch 144: 0.595551/0.579208loss in batch 145: 0.396057/0.577972loss in batch 146: 0.43718/0.577011loss in batch 147: 0.44548/0.576111loss in batch 148: 0.351776/0.574615loss in batch 149: 0.509674/0.574173loss in batch 150: 0.483109/0.573578loss in batch 151: 0.333923/0.572006loss in batch 152: 0.317383/0.570343loss in batch 153: 0.386688/0.569138loss in batch 154: 0.45607/0.568405loss in batch 155: 0.516418/0.568085loss in batch 156: 0.656281/0.568634loss in batch 157: 0.365799/0.567352loss in batch 158: 0.444687/0.566589loss in batch 159: 0.402817/0.565552loss in batch 160: 0.259384/0.56366loss in batch 161: 0.551819/0.563599loss in batch 162: 0.683685/0.564331loss in batch 163: 0.598694/0.564529loss in batch 164: 0.714081/0.565445loss in batch 165: 0.61322/0.565735loss in batch 166: 0.445099/0.565018loss in batch 167: 0.328293/0.563599loss in batch 168: 0.336716/0.562256loss in batch 169: 0.617706/0.562576loss in batch 170: 0.409348/0.561676loss in batch 171: 0.435318/0.560944loss in batch 172: 0.436859/0.560226loss in batch 173: 0.342087/0.558975loss in batch 174: 0.440552/0.558304loss in batch 175: 0.604385/0.558563loss in batch 176: 0.473068/0.55809loss in batch 177: 0.293274/0.556595loss in batch 178: 0.37207/0.555557loss in batch 179: 0.563736/0.555603loss in batch 180: 0.220016/0.553757loss in batch 181: 0.516495/0.553558loss in batch 182: 0.845642/0.555145loss in batch 183: 0.445328/0.55455loss in batch 184: 0.171585/0.552475loss in batch 185: 0.591812/0.552689loss in batch 186: 0.657394/0.553253loss in batch 187: 0.542252/0.553192loss in batch 188: 0.313049/0.55191loss in batch 189: 0.396866/0.551102loss in batch 190: 0.488495/0.550781loss in batch 191: 0.276291/0.549347loss in batch 192: 0.468292/0.548935loss in batch 193: 0.577087/0.549072loss in batch 194: 0.486862/0.548752loss in batch 195: 0.45903/0.548294loss in batch 196: 0.394363/0.547516loss in batch 197: 0.366486/0.546616loss in batch 198: 0.370575/0.545715loss in batch 199: 0.360474/0.544785loss in batch 200: 0.571976/0.544922loss in batch 201: 0.210602/0.543274loss in batch 202: 0.38588/0.542496loss in batch 203: 0.477905/0.542175loss in batch 204: 0.36235/0.54129loss in batch 205: 0.469376/0.540955loss in batch 206: 0.412369/0.540329loss in batch 207: 0.302963/0.5392loss in batch 208: 0.589905/0.539429loss in batch 209: 0.290039/0.538254loss in batch 210: 0.476852/0.537964loss in batch 211: 0.351349/0.537079loss in batch 212: 0.49884/0.536896
done with epoch 0
train_acc: 0.788732 (336/426)
test loss: 0.498839
acc: 0.930068 (133/143)
loss in batch 0: 0.316132/0.316132loss in batch 1: 0.333481/0.324814loss in batch 2: 0.730743/0.460114loss in batch 3: 0.380386/0.440186loss in batch 4: 0.300888/0.412323loss in batch 5: 0.271133/0.388794loss in batch 6: 0.432449/0.395035loss in batch 7: 0.378952/0.393021loss in batch 8: 0.511963/0.406235loss in batch 9: 0.285782/0.39418loss in batch 10: 0.421692/0.396698loss in batch 11: 0.239532/0.383606loss in batch 12: 0.326096/0.379166loss in batch 13: 0.453857/0.384506loss in batch 14: 0.429718/0.387512loss in batch 15: 0.463898/0.392288loss in batch 16: 0.522842/0.399963loss in batch 17: 0.469589/0.403839loss in batch 18: 0.579773/0.413101loss in batch 19: 0.496384/0.417267loss in batch 20: 0.47908/0.420212loss in batch 21: 0.187698/0.409637loss in batch 22: 0.491898/0.413223loss in batch 23: 0.566971/0.419632loss in batch 24: 0.333344/0.416183loss in batch 25: 0.146133/0.405777loss in batch 26: 0.475525/0.408371loss in batch 27: 0.562958/0.413895loss in batch 28: 0.254578/0.408386loss in batch 29: 0.340042/0.406113loss in batch 30: 0.209015/0.399765loss in batch 31: 0.314453/0.397095loss in batch 32: 0.500443/0.400223loss in batch 33: 0.19783/0.394272loss in batch 34: 0.607529/0.40036loss in batch 35: 0.2435/0.395996loss in batch 36: 0.47496/0.398148loss in batch 37: 0.329178/0.396332loss in batch 38: 0.638733/0.402542loss in batch 39: 0.394958/0.402344loss in batch 40: 0.872131/0.413803loss in batch 41: 0.291382/0.410904loss in batch 42: 0.308884/0.408524loss in batch 43: 0.444656/0.409348loss in batch 44: 0.345001/0.407928loss in batch 45: 0.608124/0.412262loss in batch 46: 0.199539/0.40773loss in batch 47: 0.592941/0.411606loss in batch 48: 0.322479/0.409775loss in batch 49: 0.290482/0.407394loss in batch 50: 0.29097/0.405106loss in batch 51: 0.190598/0.400986loss in batch 52: 0.185043/0.396912loss in batch 53: 0.393417/0.396835loss in batch 54: 0.161819/0.392578loss in batch 55: 0.345413/0.391724loss in batch 56: 0.222229/0.388763loss in batch 57: 0.621643/0.392776loss in batch 58: 0.389038/0.392715loss in batch 59: 0.730438/0.398331loss in batch 60: 0.241058/0.395767loss in batch 61: 0.283539/0.393951loss in batch 62: 0.56572/0.396667loss in batch 63: 0.208267/0.393738loss in batch 64: 0.239868/0.391357loss in batch 65: 0.591278/0.394394loss in batch 66: 0.31694/0.393234loss in batch 67: 0.32164/0.392181loss in batch 68: 0.309052/0.390976loss in batch 69: 0.243484/0.38887loss in batch 70: 0.545792/0.391083loss in batch 71: 0.25145/0.389145loss in batch 72: 0.486572/0.390472loss in batch 73: 0.67868/0.394379loss in batch 74: 0.303894/0.393173loss in batch 75: 0.12645/0.389648loss in batch 76: 0.454727/0.390518loss in batch 77: 0.238861/0.38855loss in batch 78: 0.520401/0.390228loss in batch 79: 0.319382/0.389343loss in batch 80: 0.511108/0.390854loss in batch 81: 0.345047/0.390289loss in batch 82: 0.209747/0.388107loss in batch 83: 0.129333/0.385025loss in batch 84: 0.436157/0.38562loss in batch 85: 0.234665/0.383881loss in batch 86: 0.318161/0.383133loss in batch 87: 0.328079/0.382492loss in batch 88: 0.320206/0.38179loss in batch 89: 0.261566/0.380463loss in batch 90: 0.257706/0.379105loss in batch 91: 0.211105/0.377289loss in batch 92: 0.403397/0.377563loss in batch 93: 0.217087/0.37587loss in batch 94: 0.346893/0.375549loss in batch 95: 0.433899/0.37616loss in batch 96: 0.464813/0.377075loss in batch 97: 0.411865/0.377426loss in batch 98: 0.268143/0.376312loss in batch 99: 0.315659/0.375732loss in batch 100: 0.669037/0.378616loss in batch 101: 0.628433/0.381073loss in batch 102: 0.399445/0.381256loss in batch 103: 0.301117/0.380478loss in batch 104: 0.231766/0.379059loss in batch 105: 0.371658/0.378998loss in batch 106: 0.198425/0.377319loss in batch 107: 0.154434/0.375244loss in batch 108: 0.622726/0.377518loss in batch 109: 0.241287/0.376266loss in batch 110: 0.302887/0.37561loss in batch 111: 0.418259/0.375992loss in batch 112: 0.296738/0.37529loss in batch 113: 0.198547/0.373734loss in batch 114: 0.29425/0.373062loss in batch 115: 0.107529/0.370758loss in batch 116: 0.424164/0.371216loss in batch 117: 0.353897/0.371078loss in batch 118: 0.573624/0.372772loss in batch 119: 0.159836/0.371002loss in batch 120: 0.156006/0.369232loss in batch 121: 0.191528/0.367767loss in batch 122: 0.365601/0.367752loss in batch 123: 0.347519/0.367599loss in batch 124: 0.401398/0.367859loss in batch 125: 0.679825/0.370331loss in batch 126: 0.103302/0.368225loss in batch 127: 0.317215/0.367828loss in batch 128: 0.532166/0.369095loss in batch 129: 0.393341/0.369293loss in batch 130: 0.170334/0.367767loss in batch 131: 0.173325/0.366287loss in batch 132: 0.332718/0.366058loss in batch 133: 0.189774/0.364731loss in batch 134: 0.452133/0.365372loss in batch 135: 0.221924/0.364334loss in batch 136: 0.224701/0.363297loss in batch 137: 0.458435/0.363998loss in batch 138: 0.189529/0.362732loss in batch 139: 0.222595/0.36174loss in batch 140: 0.407227/0.362061loss in batch 141: 0.485992/0.36293loss in batch 142: 0.20697/0.361847loss in batch 143: 0.170883/0.360519loss in batch 144: 0.089447/0.358643loss in batch 145: 0.259018/0.357971loss in batch 146: 0.118011/0.356339loss in batch 147: 0.298889/0.355957loss in batch 148: 0.223694/0.355057loss in batch 149: 0.171829/0.353836loss in batch 150: 0.533386/0.355026loss in batch 151: 1.10968/0.360001loss in batch 152: 0.301239/0.359604loss in batch 153: 0.796783/0.362442loss in batch 154: 0.338058/0.362289loss in batch 155: 0.44249/0.362808loss in batch 156: 0.381226/0.362915loss in batch 157: 0.251038/0.362213loss in batch 158: 0.347275/0.362122loss in batch 159: 0.352631/0.362061loss in batch 160: 0.836014/0.36499loss in batch 161: 0.58194/0.366333loss in batch 162: 0.29068/0.365875loss in batch 163: 0.238342/0.365097loss in batch 164: 0.235703/0.364304loss in batch 165: 0.267059/0.363724loss in batch 166: 0.720657/0.36586loss in batch 167: 0.169754/0.3647loss in batch 168: 0.20517/0.363754loss in batch 169: 0.309052/0.363434loss in batch 170: 0.242767/0.362732loss in batch 171: 0.577164/0.363968loss in batch 172: 0.364822/0.363983loss in batch 173: 0.263947/0.363403loss in batch 174: 0.34845/0.363312loss in batch 175: 0.721405/0.365341loss in batch 176: 0.560394/0.366455loss in batch 177: 0.398743/0.366638loss in batch 178: 0.358368/0.366592loss in batch 179: 0.125992/0.36525loss in batch 180: 0.177948/0.364212loss in batch 181: 0.557632/0.36528loss in batch 182: 0.269028/0.364761loss in batch 183: 0.666321/0.366394loss in batch 184: 0.25531/0.365799loss in batch 185: 0.259323/0.365219loss in batch 186: 0.366562/0.365219loss in batch 187: 0.241516/0.364578loss in batch 188: 0.247879/0.363953loss in batch 189: 0.208252/0.363129loss in batch 190: 0.471893/0.363693loss in batch 191: 0.320206/0.36348loss in batch 192: 0.207016/0.362656loss in batch 193: 0.171112/0.361679loss in batch 194: 0.412003/0.361938loss in batch 195: 0.550262/0.3629loss in batch 196: 0.431381/0.363251loss in batch 197: 0.324005/0.363037loss in batch 198: 0.257294/0.362518loss in batch 199: 0.456894/0.362976loss in batch 200: 0.210602/0.362228loss in batch 201: 0.168488/0.361267loss in batch 202: 0.661499/0.362747loss in batch 203: 0.125122/0.361588loss in batch 204: 0.142136/0.360519loss in batch 205: 0.295654/0.360199loss in batch 206: 0.197937/0.359406loss in batch 207: 0.219437/0.358749loss in batch 208: 0.387375/0.358887loss in batch 209: 0.280258/0.358505loss in batch 210: 0.483124/0.359085loss in batch 211: 0.524734/0.359879loss in batch 212: 0.173538/0.358994
done with epoch 1
train_acc: 0.906103 (386/426)
test loss: 0.173538
acc: 0.902096 (129/143)
loss in batch 0: 0.426727/0.426727loss in batch 1: 0.472641/0.449692loss in batch 2: 0.240402/0.379913loss in batch 3: 0.250916/0.347672loss in batch 4: 0.425583/0.363251loss in batch 5: 0.214661/0.338486loss in batch 6: 0.24968/0.325806loss in batch 7: 0.247665/0.31604loss in batch 8: 0.156723/0.298325loss in batch 9: 0.848694/0.353363loss in batch 10: 0.209869/0.340317loss in batch 11: 0.483093/0.352219loss in batch 12: 0.135315/0.335541loss in batch 13: 0.310364/0.33374loss in batch 14: 0.175415/0.323181loss in batch 15: 0.334305/0.323883loss in batch 16: 0.0782928/0.309433loss in batch 17: 0.297089/0.308746loss in batch 18: 0.21434/0.303787loss in batch 19: 0.171844/0.29718loss in batch 20: 0.284546/0.296585loss in batch 21: 0.143814/0.289642loss in batch 22: 0.537125/0.3004loss in batch 23: 1.11781/0.334442loss in batch 24: 0.480453/0.340302loss in batch 25: 0.211807/0.335342loss in batch 26: 0.617447/0.345795loss in batch 27: 0.327774/0.345169loss in batch 28: 0.603271/0.354065loss in batch 29: 0.133698/0.346725loss in batch 30: 0.25267/0.343689loss in batch 31: 0.247986/0.340683loss in batch 32: 0.377213/0.341797loss in batch 33: 0.209152/0.337891loss in batch 34: 0.229202/0.334778loss in batch 35: 0.251404/0.332474loss in batch 36: 0.246246/0.330139loss in batch 37: 0.0738983/0.323395loss in batch 38: 0.178223/0.319672loss in batch 39: 0.275146/0.318558loss in batch 40: 0.328064/0.318802loss in batch 41: 0.195053/0.315842loss in batch 42: 0.203049/0.313217loss in batch 43: 0.130173/0.309067loss in batch 44: 0.483704/0.312943loss in batch 45: 0.265442/0.31192loss in batch 46: 0.26004/0.310822loss in batch 47: 0.887482/0.322815loss in batch 48: 0.591934/0.328323loss in batch 49: 0.145279/0.324661loss in batch 50: 0.300491/0.324173loss in batch 51: 0.316147/0.32402loss in batch 52: 0.181458/0.321335loss in batch 53: 0.142731/0.318039loss in batch 54: 0.219879/0.316254loss in batch 55: 0.310608/0.316147loss in batch 56: 0.363953/0.316986loss in batch 57: 0.260239/0.31601loss in batch 58: 0.319672/0.316071loss in batch 59: 0.313095/0.31601loss in batch 60: 0.136276/0.313065loss in batch 61: 0.18222/0.310959loss in batch 62: 0.389099/0.312195loss in batch 63: 0.456467/0.314453loss in batch 64: 0.420303/0.316086loss in batch 65: 0.520706/0.319183loss in batch 66: 0.07341/0.315506loss in batch 67: 0.233612/0.314301loss in batch 68: 0.281708/0.313828loss in batch 69: 0.247223/0.312881loss in batch 70: 0.232605/0.311752loss in batch 71: 0.438446/0.313507loss in batch 72: 0.18808/0.311798loss in batch 73: 0.237717/0.310791loss in batch 74: 0.178513/0.309036loss in batch 75: 0.22316/0.307892loss in batch 76: 0.6866/0.31282loss in batch 77: 0.118729/0.310333loss in batch 78: 0.411774/0.311615loss in batch 79: 0.571564/0.314865loss in batch 80: 0.18103/0.313217loss in batch 81: 0.296341/0.313019loss in batch 82: 0.661392/0.317215loss in batch 83: 0.35994/0.317719loss in batch 84: 0.215012/0.316498loss in batch 85: 0.323242/0.316574loss in batch 86: 0.3983/0.31752loss in batch 87: 0.376129/0.318176loss in batch 88: 0.251434/0.317429loss in batch 89: 0.83963/0.323242loss in batch 90: 0.41008/0.324188loss in batch 91: 0.380356/0.324799loss in batch 92: 0.234436/0.323837loss in batch 93: 0.12146/0.321686loss in batch 94: 0.114548/0.319504loss in batch 95: 0.0963745/0.317169loss in batch 96: 0.226013/0.316238loss in batch 97: 0.254715/0.315613loss in batch 98: 0.283066/0.315292loss in batch 99: 0.293076/0.315048loss in batch 100: 0.641251/0.318283loss in batch 101: 0.258514/0.317703loss in batch 102: 0.211166/0.316666loss in batch 103: 0.410843/0.317566loss in batch 104: 0.276993/0.317184loss in batch 105: 0.652252/0.320343loss in batch 106: 0.400894/0.321106loss in batch 107: 0.188095/0.31987loss in batch 108: 0.418304/0.32077loss in batch 109: 0.24118/0.320038loss in batch 110: 0.137024/0.318405loss in batch 111: 0.415009/0.31926loss in batch 112: 0.167999/0.317932loss in batch 113: 0.95253/0.323486loss in batch 114: 0.125732/0.321762loss in batch 115: 0.464081/0.322998loss in batch 116: 0.314636/0.322922loss in batch 117: 0.296982/0.322708loss in batch 118: 0.178772/0.321503loss in batch 119: 0.59259/0.323746loss in batch 120: 0.118073/0.322052loss in batch 121: 0.136765/0.320541loss in batch 122: 0.0761719/0.318558loss in batch 123: 0.13887/0.317093loss in batch 124: 0.175827/0.315964loss in batch 125: 0.162628/0.314758loss in batch 126: 0.187195/0.313751loss in batch 127: 0.126328/0.312286loss in batch 128: 0.308517/0.312256loss in batch 129: 0.186661/0.311295loss in batch 130: 0.239304/0.31073loss in batch 131: 0.293991/0.310608loss in batch 132: 0.201675/0.309799loss in batch 133: 0.145523/0.308563loss in batch 134: 0.299347/0.308502loss in batch 135: 0.1716/0.307495loss in batch 136: 0.295593/0.307404loss in batch 137: 0.43576/0.308334loss in batch 138: 0.3862/0.308899loss in batch 139: 0.284393/0.308716loss in batch 140: 0.192902/0.307892loss in batch 141: 0.0628052/0.306183loss in batch 142: 0.119049/0.304871loss in batch 143: 0.0755615/0.303268loss in batch 144: 0.32074/0.303391loss in batch 145: 0.115906/0.302109loss in batch 146: 0.531723/0.30368loss in batch 147: 0.0336456/0.301849loss in batch 148: 0.538986/0.303436loss in batch 149: 0.185989/0.302658loss in batch 150: 0.100967/0.301331loss in batch 151: 0.0681152/0.299789loss in batch 152: 0.191467/0.299088loss in batch 153: 0.129227/0.297974loss in batch 154: 0.632614/0.30014loss in batch 155: 0.469604/0.301224loss in batch 156: 0.16243/0.300339loss in batch 157: 0.404373/0.300995loss in batch 158: 0.186981/0.300278loss in batch 159: 0.109344/0.299088loss in batch 160: 0.353516/0.299423loss in batch 161: 0.37001/0.29985loss in batch 162: 0.148346/0.298935loss in batch 163: 0.282532/0.298828loss in batch 164: 0.123993/0.29776loss in batch 165: 0.23262/0.297379loss in batch 166: 0.134583/0.296402loss in batch 167: 0.110168/0.295303loss in batch 168: 0.857117/0.298615loss in batch 169: 0.127258/0.297623loss in batch 170: 0.386002/0.298126loss in batch 171: 0.253235/0.297867loss in batch 172: 0.308044/0.297913loss in batch 173: 0.222504/0.297485loss in batch 174: 0.114319/0.296432loss in batch 175: 0.184525/0.295807loss in batch 176: 0.0961914/0.294693loss in batch 177: 0.175491/0.294006loss in batch 178: 0.50824/0.295197loss in batch 179: 0.395493/0.295776loss in batch 180: 0.341675/0.296021loss in batch 181: 0.388535/0.296524loss in batch 182: 0.189011/0.295944loss in batch 183: 0.773621/0.298538loss in batch 184: 0.401047/0.299088loss in batch 185: 0.289093/0.299026loss in batch 186: 0.0860748/0.297897loss in batch 187: 0.209839/0.297424loss in batch 188: 0.265121/0.297256loss in batch 189: 0.157913/0.296524loss in batch 190: 0.155487/0.295792loss in batch 191: 0.584747/0.297287loss in batch 192: 0.253494/0.297058loss in batch 193: 0.304794/0.297104loss in batch 194: 0.203857/0.296631loss in batch 195: 0.402176/0.297165loss in batch 196: 0.134872/0.296341loss in batch 197: 0.523346/0.297485loss in batch 198: 0.562912/0.298813loss in batch 199: 0.235336/0.298508loss in batch 200: 0.241516/0.298218loss in batch 201: 0.134521/0.297409loss in batch 202: 0.255905/0.297195loss in batch 203: 0.169739/0.29657loss in batch 204: 0.0590515/0.295425loss in batch 205: 0.305649/0.295471loss in batch 206: 0.115509/0.294586loss in batch 207: 0.424255/0.295227loss in batch 208: 0.176987/0.294662loss in batch 209: 0.24678/0.294434loss in batch 210: 0.171951/0.293854loss in batch 211: 0.568893/0.295151loss in batch 212: 0.151001/0.294479
done with epoch 2
train_acc: 0.915493 (390/426)
test loss: 0.151001
acc: 0.923075 (132/143)
loss in batch 0: 0.240585/0.240585loss in batch 1: 0.471619/0.35611loss in batch 2: 0.0331116/0.248428loss in batch 3: 0.101288/0.211655loss in batch 4: 0.461853/0.261688loss in batch 5: 0.693634/0.333679loss in batch 6: 0.234848/0.319565loss in batch 7: 0.488754/0.340714loss in batch 8: 0.346451/0.341339loss in batch 9: 0.187561/0.325974loss in batch 10: 0.197464/0.314285loss in batch 11: 0.45047/0.325638loss in batch 12: 0.179413/0.314392loss in batch 13: 0.139313/0.301895loss in batch 14: 0.5466/0.318192loss in batch 15: 0.349213/0.320129loss in batch 16: 0.0864868/0.306381loss in batch 17: 0.0973206/0.294785loss in batch 18: 0.225723/0.291138loss in batch 19: 0.178741/0.285522loss in batch 20: 0.28891/0.28569loss in batch 21: 0.258621/0.284454loss in batch 22: 0.157761/0.278946loss in batch 23: 0.103561/0.271637loss in batch 24: 0.546951/0.282654loss in batch 25: 0.595474/0.294693loss in batch 26: 0.128647/0.288528loss in batch 27: 0.0822144/0.281158loss in batch 28: 0.264908/0.280609loss in batch 29: 1.17551/0.31044loss in batch 30: 0.103394/0.303757loss in batch 31: 0.240753/0.301788loss in batch 32: 0.294632/0.301559loss in batch 33: 0.157883/0.297333loss in batch 34: 0.357529/0.299072loss in batch 35: 0.40625/0.302048loss in batch 36: 0.25885/0.300873loss in batch 37: 0.286514/0.300491loss in batch 38: 0.395203/0.302933loss in batch 39: 0.1595/0.299347loss in batch 40: 0.527908/0.304901loss in batch 41: 0.174606/0.301819loss in batch 42: 0.179871/0.298965loss in batch 43: 0.268143/0.298279loss in batch 44: 0.06987/0.293198loss in batch 45: 0.122009/0.289474loss in batch 46: 0.182175/0.287186loss in batch 47: 0.160614/0.284561loss in batch 48: 0.0834198/0.280441loss in batch 49: 0.228424/0.279404loss in batch 50: 0.144653/0.276764loss in batch 51: 0.209915/0.275482loss in batch 52: 0.120483/0.272568loss in batch 53: 0.31192/0.273285loss in batch 54: 0.0717773/0.269623loss in batch 55: 0.13562/0.267227loss in batch 56: 0.0931244/0.264175loss in batch 57: 0.622437/0.270355loss in batch 58: 0.120163/0.267807loss in batch 59: 0.293655/0.268234loss in batch 60: 0.102798/0.265533loss in batch 61: 0.490784/0.269165loss in batch 62: 0.214294/0.26828loss in batch 63: 0.110474/0.265823loss in batch 64: 0.0944672/0.263184loss in batch 65: 0.139069/0.261307loss in batch 66: 0.108017/0.259018loss in batch 67: 0.105927/0.25676loss in batch 68: 0.188934/0.255783loss in batch 69: 0.502258/0.259308loss in batch 70: 0.0914154/0.256943loss in batch 71: 0.131943/0.255203loss in batch 72: 0.389542/0.25705loss in batch 73: 0.4646/0.259857loss in batch 74: 0.25621/0.259796loss in batch 75: 0.14769/0.258316loss in batch 76: 0.143356/0.256821loss in batch 77: 0.0941315/0.254745loss in batch 78: 1.29265/0.267883loss in batch 79: 0.0977325/0.265747loss in batch 80: 0.138824/0.264191loss in batch 81: 0.260757/0.264145loss in batch 82: 0.0730286/0.261841loss in batch 83: 0.679764/0.266815loss in batch 84: 0.455246/0.269028loss in batch 85: 0.298477/0.269379loss in batch 86: 0.340012/0.270203loss in batch 87: 0.249115/0.269943loss in batch 88: 0.0315094/0.267273loss in batch 89: 0.134415/0.265793loss in batch 90: 0.392426/0.267181loss in batch 91: 1.03851/0.275574loss in batch 92: 0.0633698/0.273285loss in batch 93: 0.0508423/0.27092loss in batch 94: 0.526184/0.273605loss in batch 95: 0.116943/0.271973loss in batch 96: 0.186646/0.271103loss in batch 97: 0.0820923/0.269165loss in batch 98: 0.152267/0.26799loss in batch 99: 0.201202/0.267334loss in batch 100: 0.132584/0.265976loss in batch 101: 0.152893/0.264877loss in batch 102: 0.270401/0.264923loss in batch 103: 0.209061/0.264389loss in batch 104: 0.110916/0.262924loss in batch 105: 0.335785/0.263611loss in batch 106: 0.344406/0.264374loss in batch 107: 0.0481262/0.262375loss in batch 108: 0.263336/0.262375loss in batch 109: 0.0807953/0.260727loss in batch 110: 0.144485/0.259689loss in batch 111: 0.384247/0.260788loss in batch 112: 0.128922/0.259628loss in batch 113: 0.0821228/0.258072loss in batch 114: 0.556686/0.260666loss in batch 115: 0.117966/0.259445loss in batch 116: 0.357224/0.260269loss in batch 117: 0.0643616/0.258621loss in batch 118: 0.18338/0.25798loss in batch 119: 0.143723/0.257034loss in batch 120: 0.065918/0.255447loss in batch 121: 0.147873/0.254578loss in batch 122: 0.111053/0.253403loss in batch 123: 0.446182/0.254959loss in batch 124: 0.78035/0.259155loss in batch 125: 0.231888/0.258942loss in batch 126: 0.125916/0.257889loss in batch 127: 0.132217/0.256912loss in batch 128: 0.137161/0.255981loss in batch 129: 0.379898/0.256943loss in batch 130: 0.0374756/0.255264loss in batch 131: 0.68689/0.258545loss in batch 132: 0.659668/0.261551loss in batch 133: 0.254807/0.261505loss in batch 134: 0.209747/0.261124loss in batch 135: 0.132538/0.260162loss in batch 136: 0.332443/0.260696loss in batch 137: 0.23465/0.260498loss in batch 138: 0.165878/0.259827loss in batch 139: 0.282547/0.259995loss in batch 140: 0.349579/0.26062loss in batch 141: 0.122086/0.259644loss in batch 142: 0.0540924/0.258209loss in batch 143: 0.236557/0.258057loss in batch 144: 0.136765/0.257217loss in batch 145: 0.505249/0.258926loss in batch 146: 0.420761/0.260025loss in batch 147: 0.0880432/0.258865loss in batch 148: 0.0808563/0.25766loss in batch 149: 0.420883/0.258759loss in batch 150: 0.120255/0.257843loss in batch 151: 0.171051/0.257263loss in batch 152: 0.0904999/0.25618loss in batch 153: 0.13942/0.255417loss in batch 154: 0.322357/0.255844loss in batch 155: 0.156815/0.255219loss in batch 156: 0.176117/0.254715loss in batch 157: 1.27957/0.2612loss in batch 158: 0.288788/0.261368loss in batch 159: 0.0856171/0.260269loss in batch 160: 0.353531/0.260849loss in batch 161: 0.189026/0.260406loss in batch 162: 0.754791/0.263443loss in batch 163: 0.500336/0.264877loss in batch 164: 0.0549927/0.263611loss in batch 165: 0.355865/0.264175loss in batch 166: 0.308685/0.264435loss in batch 167: 0.0967102/0.263443loss in batch 168: 0.145981/0.262741loss in batch 169: 0.431259/0.263748loss in batch 170: 0.167542/0.263168loss in batch 171: 0.174927/0.262665loss in batch 172: 0.123322/0.261856loss in batch 173: 0.33017/0.262238loss in batch 174: 0.449234/0.263321loss in batch 175: 0.391373/0.264053loss in batch 176: 0.169632/0.263504loss in batch 177: 0.429764/0.264435loss in batch 178: 0.333893/0.264832loss in batch 179: 0.136002/0.264114loss in batch 180: 0.273056/0.264175loss in batch 181: 0.511703/0.265518loss in batch 182: 0.14917/0.264893loss in batch 183: 0.0500793/0.263718loss in batch 184: 0.12384/0.26297loss in batch 185: 0.122513/0.262222loss in batch 186: 0.370361/0.262802loss in batch 187: 0.0721283/0.261765loss in batch 188: 0.251511/0.261719loss in batch 189: 0.524841/0.263107loss in batch 190: 0.180573/0.26268loss in batch 191: 0.142685/0.262054loss in batch 192: 0.548859/0.263535loss in batch 193: 0.236145/0.263397loss in batch 194: 0.046402/0.262283loss in batch 195: 0.212723/0.262024loss in batch 196: 0.0623932/0.261002loss in batch 197: 0.249435/0.260956loss in batch 198: 0.390686/0.261612loss in batch 199: 0.161179/0.261093loss in batch 200: 0.259979/0.261108loss in batch 201: 0.176834/0.260681loss in batch 202: 0.298569/0.260864loss in batch 203: 0.210556/0.260635loss in batch 204: 0.375153/0.261185loss in batch 205: 0.140915/0.26059loss in batch 206: 0.0900269/0.259766loss in batch 207: 0.156281/0.259277loss in batch 208: 0.0498352/0.258286loss in batch 209: 0.0435333/0.257248loss in batch 210: 0.308029/0.257492loss in batch 211: 0.277496/0.257584loss in batch 212: 0.110275/0.256897
done with epoch 3
train_acc: 0.92723 (395/426)
test loss: 0.110275
acc: 0.923075 (132/143)
loss in batch 0: 0.319107/0.319107loss in batch 1: 0.339523/0.329315loss in batch 2: 0.203964/0.287521loss in batch 3: 0.15564/0.254562loss in batch 4: 0.189392/0.241516loss in batch 5: 0.331345/0.2565loss in batch 6: 0.109451/0.235489loss in batch 7: 0.128448/0.222107loss in batch 8: 0.0921631/0.207657loss in batch 9: 0.0601959/0.192917loss in batch 10: 0.292038/0.201935loss in batch 11: 0.62767/0.237411loss in batch 12: 0.3936/0.249435loss in batch 13: 0.00566101/0.23201loss in batch 14: 0.338715/0.23912loss in batch 15: 0.232712/0.238724loss in batch 16: 0.431702/0.250076loss in batch 17: 0.0664368/0.239883loss in batch 18: 0.146515/0.234955loss in batch 19: 0.350128/0.240723loss in batch 20: 0.362778/0.246536loss in batch 21: 0.169739/0.243042loss in batch 22: 0.126236/0.237961loss in batch 23: 0.0953979/0.232025loss in batch 24: 0.0495911/0.224716loss in batch 25: 0.126923/0.220978loss in batch 26: 0.218613/0.220886loss in batch 27: 0.278168/0.222931loss in batch 28: 0.429672/0.230057loss in batch 29: 0.127655/0.226639loss in batch 30: 0.138809/0.223801loss in batch 31: 0.0549164/0.218521loss in batch 32: 0.943375/0.240494loss in batch 33: 0.0792389/0.235748loss in batch 34: 0.321228/0.23819loss in batch 35: 0.196213/0.23703loss in batch 36: 0.187637/0.235687loss in batch 37: 0.22937/0.235535loss in batch 38: 0.404724/0.239868loss in batch 39: 0.191635/0.238663loss in batch 40: 0.153503/0.236572loss in batch 41: 0.22522/0.236313loss in batch 42: 0.0839081/0.232773loss in batch 43: 0.495926/0.238754loss in batch 44: 0.507339/0.24472loss in batch 45: 0.258286/0.24501loss in batch 46: 0.498703/0.250412loss in batch 47: 0.259567/0.250595loss in batch 48: 0.0778961/0.247086loss in batch 49: 0.0596161/0.243332loss in batch 50: 0.257019/0.243591loss in batch 51: 0.0442505/0.239761loss in batch 52: 0.340515/0.241669loss in batch 53: 0.0980988/0.238998loss in batch 54: 0.213257/0.238525loss in batch 55: 0.171677/0.23735loss in batch 56: 0.270004/0.237915loss in batch 57: 0.22435/0.237686loss in batch 58: 0.0858459/0.235107loss in batch 59: 0.12532/0.233276loss in batch 60: 0.0600891/0.230438loss in batch 61: 0.311172/0.231735loss in batch 62: 0.513885/0.236221loss in batch 63: 0.148865/0.234848loss in batch 64: 0.0979919/0.232758loss in batch 65: 0.318558/0.234055loss in batch 66: 0.199875/0.233536loss in batch 67: 0.210449/0.2332loss in batch 68: 0.078598/0.230957loss in batch 69: 0.0727692/0.228699loss in batch 70: 0.332352/0.230164loss in batch 71: 0.143051/0.228958loss in batch 72: 0.13118/0.227615loss in batch 73: 0.073349/0.225525loss in batch 74: 0.18515/0.224991loss in batch 75: 0.171494/0.224274loss in batch 76: 0.328079/0.225632loss in batch 77: 0.105637/0.224106loss in batch 78: 0.109421/0.222641loss in batch 79: 0.192413/0.222275loss in batch 80: 0.32608/0.223557loss in batch 81: 0.194122/0.22319loss in batch 82: 0.456863/0.225998loss in batch 83: 0.444489/0.228607loss in batch 84: 0.202988/0.228302loss in batch 85: 0.118484/0.22702loss in batch 86: 0.639038/0.231766loss in batch 87: 0.129272/0.230606loss in batch 88: 0.304291/0.231415loss in batch 89: 0.0796814/0.229736loss in batch 90: 0.369797/0.231277loss in batch 91: 0.0909271/0.229752loss in batch 92: 0.393539/0.231522loss in batch 93: 0.950897/0.239166loss in batch 94: 0.159012/0.238312loss in batch 95: 0.718979/0.243332loss in batch 96: 0.0885315/0.24173loss in batch 97: 0.148178/0.240768loss in batch 98: 0.413681/0.242523loss in batch 99: 0.32637/0.243362loss in batch 100: 0.113831/0.242081loss in batch 101: 0.228302/0.241959loss in batch 102: 0.547043/0.244904loss in batch 103: 0.221298/0.244675loss in batch 104: 0.423965/0.246384loss in batch 105: 0.102097/0.245026loss in batch 106: 0.519699/0.247589loss in batch 107: 0.0915527/0.246155loss in batch 108: 0.756119/0.250824loss in batch 109: 0.578171/0.253799loss in batch 110: 0.0314178/0.251801loss in batch 111: 0.77121/0.256439loss in batch 112: 0.324051/0.257034loss in batch 113: 0.138916/0.256012loss in batch 114: 0.168686/0.255249loss in batch 115: 0.0653381/0.253601loss in batch 116: 0.145981/0.252686loss in batch 117: 0.157883/0.251892loss in batch 118: 0.21553/0.251587loss in batch 119: 0.12645/0.250534loss in batch 120: 0.195679/0.250076loss in batch 121: 0.274719/0.25029loss in batch 122: 0.18718/0.249771loss in batch 123: 0.139374/0.248871loss in batch 124: 0.315674/0.249405loss in batch 125: 0.152542/0.248642loss in batch 126: 0.551651/0.251022loss in batch 127: 0.0752258/0.249649loss in batch 128: 0.200836/0.249283loss in batch 129: 1.31158/0.257446loss in batch 130: 0.129883/0.25647loss in batch 131: 0.165146/0.255783loss in batch 132: 0.214294/0.255478loss in batch 133: 0.0732269/0.25412loss in batch 134: 0.0427399/0.252533loss in batch 135: 0.198669/0.252151loss in batch 136: 0.0289307/0.250519loss in batch 137: 0.103226/0.249451loss in batch 138: 0.0433197/0.247971loss in batch 139: 0.158936/0.24733loss in batch 140: 0.720947/0.250687loss in batch 141: 0.0910034/0.249557loss in batch 142: 0.165512/0.248978loss in batch 143: 0.0667877/0.247711loss in batch 144: 0.0774536/0.246536loss in batch 145: 0.469635/0.248077loss in batch 146: 0.686264/0.251038loss in batch 147: 0.0931244/0.249985loss in batch 148: 0.0551147/0.248672loss in batch 149: 0.0639038/0.247452loss in batch 150: 0.0641174/0.246216loss in batch 151: 0.265289/0.246353loss in batch 152: 0.0476074/0.245056loss in batch 153: 0.198212/0.244751loss in batch 154: 0.332199/0.245316loss in batch 155: 0.833206/0.249084loss in batch 156: 0.245712/0.249069loss in batch 157: 0.0616913/0.247879loss in batch 158: 0.281769/0.248093loss in batch 159: 0.0673981/0.246964loss in batch 160: 0.230423/0.246857loss in batch 161: 0.315964/0.247284loss in batch 162: 0.423416/0.248352loss in batch 163: 0.539612/0.250137loss in batch 164: 0.0067749/0.248672loss in batch 165: 0.18779/0.248306loss in batch 166: 0.0631409/0.247192loss in batch 167: 0.157455/0.246658loss in batch 168: 0.357224/0.247314loss in batch 169: 0.706039/0.25loss in batch 170: 0.223129/0.249847loss in batch 171: 0.123337/0.249115loss in batch 172: 0.144501/0.248505loss in batch 173: 0.228729/0.248398loss in batch 174: 0.398911/0.249252loss in batch 175: 0.147797/0.248688loss in batch 176: 0.663071/0.251022loss in batch 177: 0.16394/0.250534loss in batch 178: 0.155289/0.25loss in batch 179: 0.0360107/0.24881loss in batch 180: 0.039505/0.24765loss in batch 181: 0.13945/0.247055loss in batch 182: 0.441635/0.248123loss in batch 183: 0.101059/0.24733loss in batch 184: 0.132233/0.246704loss in batch 185: 0.178345/0.246338loss in batch 186: 0.583328/0.248138loss in batch 187: 0.112305/0.247421loss in batch 188: 0.326035/0.247833loss in batch 189: 0.495941/0.249146loss in batch 190: 0.0831757/0.248276loss in batch 191: 0.0967712/0.247482loss in batch 192: 0.0902252/0.246658loss in batch 193: 0.177841/0.246307loss in batch 194: 0.0153656/0.245132loss in batch 195: 0.258408/0.245193loss in batch 196: 0.245041/0.245193loss in batch 197: 0.37442/0.245834loss in batch 198: 0.288544/0.246063loss in batch 199: 0.511749/0.247391loss in batch 200: 0.274033/0.247513loss in batch 201: 0.205093/0.247314loss in batch 202: 0.112915/0.246643loss in batch 203: 0.143311/0.24614loss in batch 204: 0.0489044/0.245163loss in batch 205: 0.0805054/0.244385loss in batch 206: 0.170059/0.244019loss in batch 207: 0.211853/0.243866loss in batch 208: 0.0421295/0.242905loss in batch 209: 0.135193/0.242386loss in batch 210: 0.512253/0.243668loss in batch 211: 0.133148/0.243149loss in batch 212: 0.324875/0.24353
done with epoch 4
train_acc: 0.922535 (393/426)
test loss: 0.324874
acc: 0.937061 (134/143)
loss in batch 0: 0.203537/0.203537loss in batch 1: 0.292801/0.248169loss in batch 2: 0.359222/0.285187loss in batch 3: 0.235275/0.272705loss in batch 4: 0.119095/0.241989loss in batch 5: 0.0917358/0.216949loss in batch 6: 0.113968/0.202225loss in batch 7: 0.164932/0.197571loss in batch 8: 0.444046/0.22496loss in batch 9: 0.0755768/0.210022loss in batch 10: 0.112595/0.201172loss in batch 11: 0.31778/0.210876loss in batch 12: 0.0964355/0.202087loss in batch 13: 0.463501/0.220749loss in batch 14: 0.25679/0.223145loss in batch 15: 0.0419922/0.211823loss in batch 16: 0.159988/0.208771loss in batch 17: 0.463562/0.222946loss in batch 18: 0.237518/0.223694loss in batch 19: 0.148849/0.219971loss in batch 20: 0.0607452/0.212372loss in batch 21: 0.583984/0.229279loss in batch 22: 0.401688/0.236771loss in batch 23: 0.0714111/0.229874loss in batch 24: 0.382492/0.235992loss in batch 25: 0.100983/0.230789loss in batch 26: 0.110382/0.226334loss in batch 27: 0.509872/0.236465loss in batch 28: 0.151459/0.233521loss in batch 29: 0.232773/0.233505loss in batch 30: 0.578781/0.244629loss in batch 31: 0.143265/0.24147loss in batch 32: 0.353928/0.244888loss in batch 33: 0.4702/0.251511loss in batch 34: 0.355469/0.254471loss in batch 35: 0.811707/0.269943loss in batch 36: 0.0588226/0.264236loss in batch 37: 0.207016/0.262741loss in batch 38: 0.28685/0.263351loss in batch 39: 0.151596/0.260559loss in batch 40: 0.168854/0.258331loss in batch 41: 0.0846863/0.254196loss in batch 42: 0.491058/0.259705loss in batch 43: 0.123184/0.256592loss in batch 44: 0.147781/0.254181loss in batch 45: 0.048996/0.249725loss in batch 46: 0.143661/0.247452loss in batch 47: 0.0535583/0.243423loss in batch 48: 0.144104/0.241409loss in batch 49: 0.0401459/0.237381loss in batch 50: 0.0643158/0.233978loss in batch 51: 0.141815/0.232208loss in batch 52: 0.0220642/0.228241loss in batch 53: 0.10025/0.225876loss in batch 54: 0.115906/0.223877loss in batch 55: 0.092926/0.221542loss in batch 56: 0.119888/0.219742loss in batch 57: 0.146133/0.218491loss in batch 58: 0.220749/0.218521loss in batch 59: 0.207016/0.218323loss in batch 60: 0.0790405/0.216049loss in batch 61: 0.482086/0.220337loss in batch 62: 0.0319061/0.217331loss in batch 63: 0.136276/0.21608loss in batch 64: 0.251953/0.216629loss in batch 65: 0.470428/0.220474loss in batch 66: 1.24229/0.235718loss in batch 67: 0.121353/0.234039loss in batch 68: 1.35553/0.250305loss in batch 69: 0.380859/0.252167loss in batch 70: 0.153778/0.250778loss in batch 71: 0.994293/0.261108loss in batch 72: 0.177231/0.259949loss in batch 73: 0.0389404/0.256973loss in batch 74: 0.475906/0.259888loss in batch 75: 0.0202789/0.256729loss in batch 76: 0.0346985/0.253845loss in batch 77: 0.0793762/0.251617loss in batch 78: 0.0579071/0.249161loss in batch 79: 0.101379/0.247314loss in batch 80: 0.0708771/0.245148loss in batch 81: 0.0957794/0.243317loss in batch 82: 0.0770111/0.241318loss in batch 83: 0.0867157/0.239471loss in batch 84: 0.106293/0.2379loss in batch 85: 0.0516052/0.235733loss in batch 86: 0.135895/0.234589loss in batch 87: 0.085556/0.232895loss in batch 88: 1.27917/0.244659loss in batch 89: 0.28418/0.245087loss in batch 90: 0.12645/0.24379loss in batch 91: 0.185486/0.243164loss in batch 92: 0.0855103/0.241455loss in batch 93: 0.279556/0.241867loss in batch 94: 0.122818/0.240601loss in batch 95: 0.127365/0.239426loss in batch 96: 0.156906/0.238586loss in batch 97: 0.627853/0.242554loss in batch 98: 0.221695/0.24234loss in batch 99: 0.104263/0.240967loss in batch 100: 0.198288/0.24054loss in batch 101: 0.0540009/0.238708loss in batch 102: 0.327194/0.239578loss in batch 103: 0.0849152/0.238083loss in batch 104: 0.367599/0.239319loss in batch 105: 0.133972/0.238327loss in batch 106: 0.102966/0.237045loss in batch 107: 0.225327/0.236954loss in batch 108: 0.124435/0.235916loss in batch 109: 0.340927/0.236862loss in batch 110: 0.178925/0.236359loss in batch 111: 0.0661621/0.234833loss in batch 112: 0.14534/0.234024loss in batch 113: 0.0276489/0.232224loss in batch 114: 0.0726471/0.230835loss in batch 115: 0.11113/0.229813loss in batch 116: 0.167007/0.229279loss in batch 117: 0.0269623/0.227554loss in batch 118: 0.366364/0.228729loss in batch 119: 0.180725/0.228333loss in batch 120: 0.182358/0.227936loss in batch 121: 0.191879/0.227646loss in batch 122: 0.240356/0.227753loss in batch 123: 0.134964/0.227005loss in batch 124: 0.170242/0.226547loss in batch 125: 0.116776/0.225677loss in batch 126: 0.184448/0.225357loss in batch 127: 0.114426/0.224487loss in batch 128: 0.225327/0.224487loss in batch 129: 0.0840607/0.223419loss in batch 130: 0.100006/0.222473loss in batch 131: 0.10672/0.221588loss in batch 132: 0.389832/0.222855loss in batch 133: 0.0246735/0.221375loss in batch 134: 0.286377/0.221863loss in batch 135: 0.370956/0.222961loss in batch 136: 0.12146/0.222214loss in batch 137: 0.327042/0.222977loss in batch 138: 0.0847015/0.221985loss in batch 139: 0.154678/0.221512loss in batch 140: 0.0572357/0.220337loss in batch 141: 0.313141/0.220978loss in batch 142: 0.1427/0.220444loss in batch 143: 0.0907135/0.219543loss in batch 144: 0.42482/0.220947loss in batch 145: 0.762878/0.22467loss in batch 146: 0.179703/0.224365loss in batch 147: 0.0826263/0.223404loss in batch 148: 0.214401/0.223343loss in batch 149: 0.227783/0.223373loss in batch 150: 0.282867/0.223755loss in batch 151: 0.182632/0.223495loss in batch 152: 0.178879/0.223206loss in batch 153: 0.136871/0.222641loss in batch 154: 0.294128/0.223099loss in batch 155: 0.107285/0.222366loss in batch 156: 0.108337/0.221634loss in batch 157: 0.296356/0.222107loss in batch 158: 0.422394/0.223373loss in batch 159: 0.203476/0.223236loss in batch 160: 0.223236/0.223251loss in batch 161: 0.0951538/0.222458loss in batch 162: 0.262161/0.222702loss in batch 163: 0.131241/0.222137loss in batch 164: 0.335678/0.222824loss in batch 165: 0.528656/0.22467loss in batch 166: 0.317673/0.22522loss in batch 167: 0.106995/0.224533loss in batch 168: 0.267517/0.224777loss in batch 169: 0.0168762/0.223557loss in batch 170: 0.322372/0.224136loss in batch 171: 0.0364075/0.223038loss in batch 172: 1.01364/0.2276loss in batch 173: 0.217163/0.227539loss in batch 174: 0.630722/0.229858loss in batch 175: 0.114334/0.229202loss in batch 176: 0.26445/0.229401loss in batch 177: 0.147202/0.228928loss in batch 178: 0.0695343/0.228043loss in batch 179: 0.124115/0.227478loss in batch 180: 0.0865631/0.226685loss in batch 181: 0.113983/0.226074loss in batch 182: 0.0305481/0.225006loss in batch 183: 0.0980988/0.224304loss in batch 184: 0.523453/0.225922loss in batch 185: 0.122803/0.225372loss in batch 186: 0.141983/0.22493loss in batch 187: 0.0581512/0.224045loss in batch 188: 0.0976257/0.223373loss in batch 189: 0.22821/0.223389loss in batch 190: 0.244476/0.223511loss in batch 191: 0.20639/0.223419loss in batch 192: 0.141174/0.222992loss in batch 193: 0.364883/0.223724loss in batch 194: 0.544128/0.225372loss in batch 195: 0.135895/0.224915loss in batch 196: 0.396851/0.225784loss in batch 197: 0.0237579/0.224762loss in batch 198: 0.349594/0.225403loss in batch 199: 0.182831/0.225189loss in batch 200: 0.256744/0.225327loss in batch 201: 0.31868/0.2258loss in batch 202: 0.161743/0.225479loss in batch 203: 0.177078/0.22525loss in batch 204: 0.108185/0.224686loss in batch 205: 0.24823/0.224777loss in batch 206: 0.197968/0.224655loss in batch 207: 0.113052/0.224136loss in batch 208: 0.0604553/0.223343loss in batch 209: 0.639755/0.225327loss in batch 210: 0.0813599/0.22464loss in batch 211: 0.04599/0.223801loss in batch 212: 0.020401/0.222839
done with epoch 5
train_acc: 0.93662 (399/426)
test loss: 0.020401
acc: 0.937061 (134/143)
loss in batch 0: 0.0840302/0.0840302loss in batch 1: 0.0800476/0.0820313loss in batch 2: 0.145355/0.103149loss in batch 3: 0.0605316/0.0924835loss in batch 4: 0.230423/0.120071loss in batch 5: 0.0524292/0.108795loss in batch 6: 0.168716/0.117355loss in batch 7: 0.144073/0.120697loss in batch 8: 0.00396729/0.107727loss in batch 9: 0.0453186/0.101501loss in batch 10: 0.109024/0.102173loss in batch 11: 0.155136/0.106583loss in batch 12: 0.0955963/0.105743loss in batch 13: 0.100449/0.105362loss in batch 14: 0.127594/0.106842loss in batch 15: 0.161957/0.110291loss in batch 16: 0.106659/0.110077loss in batch 17: 0.0422363/0.106308loss in batch 18: 0.571991/0.130829loss in batch 19: 0.145798/0.131561loss in batch 20: 0.918167/0.169022loss in batch 21: 0.384552/0.178833loss in batch 22: 0.0642853/0.173843loss in batch 23: 0.0617065/0.169174loss in batch 24: 0.141388/0.16806loss in batch 25: 0.407745/0.177277loss in batch 26: 0.503174/0.189346loss in batch 27: 0.134995/0.187408loss in batch 28: 0.0885162/0.184006loss in batch 29: 0.0731201/0.180298loss in batch 30: 0.0917206/0.177429loss in batch 31: 0.590805/0.190353loss in batch 32: 0.114288/0.188065loss in batch 33: 0.135651/0.186508loss in batch 34: 0.197861/0.186844loss in batch 35: 0.416229/0.193207loss in batch 36: 0.0797272/0.19014loss in batch 37: 0.0948944/0.187637loss in batch 38: 0.0755005/0.184769loss in batch 39: 0.964096/0.204239loss in batch 40: 0.445587/0.210129loss in batch 41: 0.0844879/0.207138loss in batch 42: 0.503342/0.214035loss in batch 43: 0.0617981/0.210571loss in batch 44: 0.183304/0.209961loss in batch 45: 0.163895/0.208969loss in batch 46: 0.0943909/0.206528loss in batch 47: 0.335663/0.209213loss in batch 48: 0.175827/0.208542loss in batch 49: 0.420303/0.212769loss in batch 50: 0.16803/0.211884loss in batch 51: 0.0279694/0.208344loss in batch 52: 0.153687/0.207321loss in batch 53: 0.0524139/0.204453loss in batch 54: 0.302902/0.206238loss in batch 55: 0.112015/0.204559loss in batch 56: 0.371475/0.207489loss in batch 57: 0.0509949/0.204788loss in batch 58: 0.0738373/0.20256loss in batch 59: 0.0498505/0.200027loss in batch 60: 0.230377/0.200516loss in batch 61: 0.215897/0.20076loss in batch 62: 0.218964/0.20105loss in batch 63: 0.27803/0.202271loss in batch 64: 1.04216/0.215179loss in batch 65: 0.0398254/0.21254loss in batch 66: 0.043335/0.210007loss in batch 67: 0.414551/0.213013loss in batch 68: 0.417496/0.215973loss in batch 69: 0.189514/0.215591loss in batch 70: 0.0428925/0.213165loss in batch 71: 0.570557/0.218124loss in batch 72: 0.669846/0.224304loss in batch 73: 0.111115/0.222778loss in batch 74: 0.0551758/0.220551loss in batch 75: 0.11734/0.219193loss in batch 76: 0.075119/0.217316loss in batch 77: 0.397064/0.219635loss in batch 78: 0.0731049/0.217773loss in batch 79: 0.200409/0.217545loss in batch 80: 0.288376/0.21843loss in batch 81: 0.159775/0.217712loss in batch 82: 0.0263672/0.215408loss in batch 83: 0.128387/0.214371loss in batch 84: 0.155777/0.213684loss in batch 85: 0.473572/0.216705loss in batch 86: 0.0391998/0.214661loss in batch 87: 0.378769/0.216537loss in batch 88: 0.30098/0.217484loss in batch 89: 0.0912781/0.216064loss in batch 90: 0.0259094/0.213974loss in batch 91: 0.065918/0.212372loss in batch 92: 0.150223/0.211716loss in batch 93: 0.11145/0.210648loss in batch 94: 0.53804/0.214081loss in batch 95: 0.205505/0.214005loss in batch 96: 0.0565338/0.212372loss in batch 97: 0.0169678/0.210373loss in batch 98: 0.0576477/0.208832loss in batch 99: 0.0764008/0.207504loss in batch 100: 0.0549469/0.205994loss in batch 101: 0.0968628/0.204941loss in batch 102: 0.256256/0.205429loss in batch 103: 0.200333/0.205383loss in batch 104: 0.0402374/0.203812loss in batch 105: 0.536591/0.206955loss in batch 106: 0.146301/0.20639loss in batch 107: 0.1483/0.205841loss in batch 108: 0.0995789/0.20488loss in batch 109: 0.313446/0.205856loss in batch 110: 0.0498047/0.204437loss in batch 111: 0.329712/0.205566loss in batch 112: 0.590256/0.208984loss in batch 113: 0.201981/0.208908loss in batch 114: 0.175064/0.208618loss in batch 115: 0.116882/0.207825loss in batch 116: 0.451492/0.2099loss in batch 117: 0.156204/0.209457loss in batch 118: 0.0540924/0.208145loss in batch 119: 0.0473022/0.206802loss in batch 120: 0.442322/0.208755loss in batch 121: 0.192078/0.208618loss in batch 122: 0.208939/0.208618loss in batch 123: 0.459229/0.210632loss in batch 124: 0.55574/0.213409loss in batch 125: 0.0989227/0.212494loss in batch 126: 0.279984/0.213028loss in batch 127: 0.0399475/0.21167loss in batch 128: 0.175385/0.211395loss in batch 129: 0.344055/0.212402loss in batch 130: 0.138702/0.211853loss in batch 131: 0.0286102/0.210449loss in batch 132: 0.596069/0.213348loss in batch 133: 0.207977/0.213318loss in batch 134: 0.282196/0.213837loss in batch 135: 0.0905914/0.212936loss in batch 136: 0.040741/0.21167loss in batch 137: 0.11763/0.210983loss in batch 138: 0.0358429/0.209732loss in batch 139: 0.176147/0.209488loss in batch 140: 0.328064/0.210327loss in batch 141: 0.69754/0.21376loss in batch 142: 0.161789/0.213394loss in batch 143: 0.111893/0.212677loss in batch 144: 0.0594177/0.211639loss in batch 145: 0.296051/0.212204loss in batch 146: 0.0707092/0.211258loss in batch 147: 0.840378/0.2155loss in batch 148: 0.0812073/0.2146loss in batch 149: 0.551819/0.216843loss in batch 150: 0.475586/0.218552loss in batch 151: 0.051651/0.217453loss in batch 152: 0.143021/0.216965loss in batch 153: 0.260239/0.217255loss in batch 154: 0.0953827/0.216461loss in batch 155: 0.0730133/0.215546loss in batch 156: 0.0368195/0.214401loss in batch 157: 0.120651/0.213806loss in batch 158: 0.215454/0.213837loss in batch 159: 0.246094/0.214035loss in batch 160: 0.159225/0.213684loss in batch 161: 0.119446/0.21312loss in batch 162: 0.545258/0.215149loss in batch 163: 0.402878/0.216293loss in batch 164: 0.220856/0.216324loss in batch 165: 0.103088/0.215622loss in batch 166: 0.271347/0.215973loss in batch 167: 0.0745544/0.215118loss in batch 168: 0.428284/0.2164loss in batch 169: 0.0545349/0.215439loss in batch 170: 0.0652466/0.214554loss in batch 171: 0.0895233/0.213837loss in batch 172: 0.493469/0.215454loss in batch 173: 0.0296173/0.214386loss in batch 174: 0.488571/0.215958loss in batch 175: 0.130646/0.215469loss in batch 176: 0.147736/0.215088loss in batch 177: 0.0274811/0.214035loss in batch 178: 0.938797/0.218079loss in batch 179: 0.0507355/0.217133loss in batch 180: 0.31369/0.217682loss in batch 181: 0.0231171/0.216599loss in batch 182: 0.119156/0.21608loss in batch 183: 0.0821228/0.215347loss in batch 184: 0.0984497/0.214722loss in batch 185: 0.110382/0.214157loss in batch 186: 0.369568/0.214981loss in batch 187: 0.217056/0.214996loss in batch 188: 0.389832/0.215927loss in batch 189: 1.33527/0.221817loss in batch 190: 0.115311/0.221252loss in batch 191: 0.0705414/0.220474loss in batch 192: 0.315018/0.220963loss in batch 193: 0.0506592/0.220093loss in batch 194: 0.0267487/0.219101loss in batch 195: 0.0734863/0.218353loss in batch 196: 0.345352/0.218994loss in batch 197: 0.233749/0.21907loss in batch 198: 0.298401/0.219467loss in batch 199: 0.0979004/0.218857loss in batch 200: 0.229401/0.218903loss in batch 201: 0.247742/0.219055loss in batch 202: 0.23732/0.219147loss in batch 203: 0.0332489/0.218231loss in batch 204: 0.131958/0.217819loss in batch 205: 0.104416/0.217255loss in batch 206: 0.0769501/0.216583loss in batch 207: 0.330215/0.217117loss in batch 208: 0.151138/0.216812loss in batch 209: 0.146713/0.216476loss in batch 210: 0.178223/0.216293loss in batch 211: 0.0776215/0.215652loss in batch 212: 0.0744171/0.214981
done with epoch 6
train_acc: 0.929577 (396/426)
test loss: 0.074417
acc: 0.937061 (134/143)
loss in batch 0: 0.0892181/0.0892181loss in batch 1: 0.363892/0.226547loss in batch 2: 0.114471/0.189194loss in batch 3: 0.0852966/0.163223loss in batch 4: 0.10025/0.15062loss in batch 5: 0.32724/0.180069loss in batch 6: 0.120728/0.171585loss in batch 7: 0.0792847/0.160049loss in batch 8: 0.0529785/0.148148loss in batch 9: 0.165314/0.149872loss in batch 10: 0.0696259/0.142563loss in batch 11: 0.0701904/0.136551loss in batch 12: 0.0187073/0.127472loss in batch 13: 0.031601/0.120621loss in batch 14: 0.421967/0.140717loss in batch 15: 0.754837/0.179108loss in batch 16: 0.155701/0.177719loss in batch 17: 0.404221/0.190308loss in batch 18: 0.446014/0.203766loss in batch 19: 0.126389/0.19989loss in batch 20: 0.326721/0.205933loss in batch 21: 0.0531464/0.19899loss in batch 22: 0.39122/0.207352loss in batch 23: 0.220963/0.207916loss in batch 24: 0.0563202/0.201843loss in batch 25: 0.105133/0.198135loss in batch 26: 0.133102/0.195724loss in batch 27: 0.613495/0.210632loss in batch 28: 0.0359039/0.20462loss in batch 29: 0.0517578/0.199524loss in batch 30: 0.437027/0.207184loss in batch 31: 0.627167/0.220306loss in batch 32: 0.111633/0.217026loss in batch 33: 0.00686646/0.210831loss in batch 34: 0.0629883/0.206604loss in batch 35: 0.0578613/0.202469loss in batch 36: 0.0259552/0.197708loss in batch 37: 0.0594482/0.194077loss in batch 38: 0.0704651/0.190903loss in batch 39: 0.69368/0.203461loss in batch 40: 0.0895081/0.200699loss in batch 41: 1.15102/0.223328loss in batch 42: 0.091507/0.220261loss in batch 43: 0.0353088/0.216049loss in batch 44: 0.103195/0.213531loss in batch 45: 0.0145416/0.209213loss in batch 46: 0.220123/0.209457loss in batch 47: 0.063446/0.206406loss in batch 48: 0.410995/0.210587loss in batch 49: 0.0354462/0.207077loss in batch 50: 0.0898438/0.204773loss in batch 51: 0.36499/0.207855loss in batch 52: 0.13591/0.206512loss in batch 53: 0.172089/0.205872loss in batch 54: 0.088028/0.20372loss in batch 55: 0.0949707/0.201782loss in batch 56: 0.293335/0.203384loss in batch 57: 0.0111389/0.200073loss in batch 58: 0.389359/0.203278loss in batch 59: 0.0335236/0.200455loss in batch 60: 0.136444/0.199402loss in batch 61: 0.273544/0.200592loss in batch 62: 0.0684509/0.198502loss in batch 63: 0.35614/0.200958loss in batch 64: 0.154984/0.200256loss in batch 65: 0.168808/0.199783loss in batch 66: 0.101501/0.198318loss in batch 67: 0.452225/0.202042loss in batch 68: 0.254639/0.202805loss in batch 69: 0.12085/0.20163loss in batch 70: 0.113297/0.200394loss in batch 71: 0.0800781/0.198715loss in batch 72: 0.34874/0.200775loss in batch 73: 0.0566864/0.198822loss in batch 74: 0.412735/0.201675loss in batch 75: 0.0761566/0.200027loss in batch 76: 0.074646/0.19841loss in batch 77: 0.162109/0.197937loss in batch 78: 0.120773/0.19696loss in batch 79: 0.0431366/0.195038loss in batch 80: 0.008255/0.192734loss in batch 81: 0.116089/0.191788loss in batch 82: 0.0931396/0.190598loss in batch 83: 0.534088/0.194702loss in batch 84: 0.184601/0.19458loss in batch 85: 0.0484772/0.192886loss in batch 86: 1.15581/0.203949loss in batch 87: 0.0649261/0.202362loss in batch 88: 0.77475/0.208801loss in batch 89: 0.213348/0.208847loss in batch 90: 0.0978546/0.207626loss in batch 91: 0.118408/0.206665loss in batch 92: 0.258545/0.207214loss in batch 93: 0.197617/0.207123loss in batch 94: 0.361206/0.20874loss in batch 95: 0.347137/0.210175loss in batch 96: 0.20813/0.210159loss in batch 97: 0.4879/0.212982loss in batch 98: 0.141296/0.21228loss in batch 99: 0.102386/0.211182loss in batch 100: 0.265411/0.211716loss in batch 101: 0.387283/0.213425loss in batch 102: 0.0967865/0.212296loss in batch 103: 0.282257/0.212967loss in batch 104: 0.0812225/0.2117loss in batch 105: 0.492447/0.214355loss in batch 106: 0.0418091/0.212753loss in batch 107: 0.0749664/0.211487loss in batch 108: 0.479935/0.213943loss in batch 109: 0.263519/0.214386loss in batch 110: 0.148407/0.213791loss in batch 111: 0.129242/0.213028loss in batch 112: 0.438324/0.215027loss in batch 113: 0.217926/0.215042loss in batch 114: 0.309357/0.215881loss in batch 115: 0.341721/0.216965loss in batch 116: 0.0840912/0.21582loss in batch 117: 0.0388794/0.214325loss in batch 118: 0.054718/0.212982loss in batch 119: 0.0909424/0.21196loss in batch 120: 0.12883/0.211288loss in batch 121: 0.0865326/0.210266loss in batch 122: 0.0490875/0.208954loss in batch 123: 0.0430756/0.207611loss in batch 124: 0.0795441/0.206573loss in batch 125: 0.169098/0.206284loss in batch 126: 0.192032/0.206177loss in batch 127: 0.129761/0.205582loss in batch 128: 0.426697/0.207291loss in batch 129: 0.374985/0.208572loss in batch 130: 0.408066/0.210114loss in batch 131: 0.159073/0.209717loss in batch 132: 0.191574/0.209579loss in batch 133: 0.0234375/0.208191loss in batch 134: 0.47258/0.210159loss in batch 135: 0.0681458/0.209106loss in batch 136: 0.180023/0.208893loss in batch 137: 0.0164337/0.207504loss in batch 138: 0.33284/0.208405loss in batch 139: 0.0283508/0.207108loss in batch 140: 0.175613/0.206894loss in batch 141: 0.0632019/0.205872loss in batch 142: 0.905869/0.210785loss in batch 143: 0.131683/0.21022loss in batch 144: 0.265472/0.210617loss in batch 145: 0.257263/0.210922loss in batch 146: 0.363083/0.211975loss in batch 147: 0.10347/0.211227loss in batch 148: 0.0172272/0.20993loss in batch 149: 0.102646/0.209213loss in batch 150: 0.0436554/0.208115loss in batch 151: 0.0906982/0.207352loss in batch 152: 0.175125/0.207138loss in batch 153: 0.172028/0.206909loss in batch 154: 0.191528/0.206802loss in batch 155: 0.436035/0.208267loss in batch 156: 0.959854/0.213058loss in batch 157: 0.363632/0.214005loss in batch 158: 0.0655823/0.213089loss in batch 159: 0.105453/0.212402loss in batch 160: 0.119156/0.211838loss in batch 161: 0.253983/0.212097loss in batch 162: 0.439178/0.213486loss in batch 163: 0.0934601/0.212753loss in batch 164: 0.437988/0.214127loss in batch 165: 0.165497/0.213821loss in batch 166: 0.106094/0.213181loss in batch 167: 0.0558167/0.212234loss in batch 168: 0.0954895/0.211548loss in batch 169: 0.543884/0.213501loss in batch 170: 0.250427/0.213715loss in batch 171: 0.0374298/0.212692loss in batch 172: 0.151138/0.212341loss in batch 173: 0.0380402/0.211349loss in batch 174: 0.273682/0.211685loss in batch 175: 0.0292816/0.210648loss in batch 176: 0.106995/0.210083loss in batch 177: 0.09021/0.209412loss in batch 178: 0.0450134/0.208481loss in batch 179: 0.0506439/0.207596loss in batch 180: 0.0143433/0.206543loss in batch 181: 0.110352/0.206009loss in batch 182: 0.327774/0.20668loss in batch 183: 0.0435486/0.205795loss in batch 184: 0.0595398/0.204987loss in batch 185: 0.0738525/0.204285loss in batch 186: 0.0714111/0.203568loss in batch 187: 0.260605/0.203888loss in batch 188: 0.127731/0.203476loss in batch 189: 0.0163116/0.202499loss in batch 190: 0.141113/0.202179loss in batch 191: 0.227097/0.202301loss in batch 192: 0.0322266/0.201431loss in batch 193: 0.181473/0.201324loss in batch 194: 0.545258/0.203079loss in batch 195: 0.0838165/0.202484loss in batch 196: 0.0456848/0.201691loss in batch 197: 0.134064/0.201324loss in batch 198: 0.0497437/0.200577loss in batch 199: 0.0569611/0.19986loss in batch 200: 0.0565643/0.199142loss in batch 201: 0.704544/0.201645loss in batch 202: 0.127579/0.201279loss in batch 203: 0.080368/0.200684loss in batch 204: 0.246857/0.200912loss in batch 205: 0.0458069/0.200165loss in batch 206: 0.233627/0.200333loss in batch 207: 0.0428772/0.19957loss in batch 208: 0.0559387/0.198883loss in batch 209: 0.0789795/0.198303loss in batch 210: 0.124557/0.197968loss in batch 211: 1.28529/0.203094loss in batch 212: 0.0390472/0.202316
done with epoch 7
train_acc: 0.93662 (399/426)
test loss: 0.0390472
acc: 0.923075 (132/143)
loss in batch 0: 0.0695648/0.0695648loss in batch 1: 0.0155487/0.0425568loss in batch 2: 0.0503387/0.0451508loss in batch 3: 0.435501/0.142746loss in batch 4: 0.399063/0.194loss in batch 5: 0.531708/0.25029loss in batch 6: 0.189072/0.241531loss in batch 7: 0.0806274/0.221436loss in batch 8: 0.126999/0.210922loss in batch 9: 0.0545197/0.195297loss in batch 10: 0.31839/0.206482loss in batch 11: 0.629974/0.241776loss in batch 12: 0.0803833/0.22937loss in batch 13: 0.433624/0.243942loss in batch 14: 0.0550079/0.231354loss in batch 15: 0.279831/0.23439loss in batch 16: 0.0710907/0.224777loss in batch 17: 0.0915222/0.217361loss in batch 18: 0.126251/0.212585loss in batch 19: 0.302582/0.217072loss in batch 20: 0.567978/0.233795loss in batch 21: 0.0734711/0.226517loss in batch 22: 0.151001/0.223221loss in batch 23: 0.104294/0.218262loss in batch 24: 0.0870056/0.213013loss in batch 25: 0.656174/0.230057loss in batch 26: 0.219772/0.229691loss in batch 27: 0.23053/0.229706loss in batch 28: 0.15065/0.22699loss in batch 29: 0.623337/0.240189loss in batch 30: 0.152756/0.237366loss in batch 31: 0.172546/0.235352loss in batch 32: 0.0968323/0.23114loss in batch 33: 0.253433/0.231796loss in batch 34: 0.075882/0.227356loss in batch 35: 0.304672/0.229507loss in batch 36: 0.0361023/0.224274loss in batch 37: 1.10156/0.24736loss in batch 38: 0.187683/0.245834loss in batch 39: 0.218521/0.245148loss in batch 40: 0.224823/0.244644loss in batch 41: 0.0246277/0.23941loss in batch 42: 0.118362/0.236603loss in batch 43: 0.265198/0.237244loss in batch 44: 0.0363464/0.232788loss in batch 45: 0.546417/0.239609loss in batch 46: 0.0206757/0.23494loss in batch 47: 0.0118866/0.230301loss in batch 48: 0.0731049/0.227081loss in batch 49: 0.106979/0.224686loss in batch 50: 0.607819/0.232193loss in batch 51: 0.11409/0.229935loss in batch 52: 0.22171/0.229767loss in batch 53: 0.0379486/0.226227loss in batch 54: 0.0571136/0.223145loss in batch 55: 0.1651/0.222107loss in batch 56: 0.0756683/0.219528loss in batch 57: 0.468964/0.223831loss in batch 58: 0.212479/0.223648loss in batch 59: 0.379211/0.226242loss in batch 60: 0.149414/0.224976loss in batch 61: 0.168182/0.22406loss in batch 62: 0.0539246/0.221375loss in batch 63: 0.171158/0.220581loss in batch 64: 0.116348/0.218979loss in batch 65: 0.0569153/0.216522loss in batch 66: 0.0443726/0.213943loss in batch 67: 0.0572205/0.211639loss in batch 68: 0.412933/0.214569loss in batch 69: 0.0803375/0.212631loss in batch 70: 0.0164337/0.209885loss in batch 71: 0.340652/0.211685loss in batch 72: 0.226074/0.211899loss in batch 73: 0.0476837/0.209686loss in batch 74: 0.0951996/0.20816loss in batch 75: 0.292786/0.209259loss in batch 76: 0.127426/0.208206loss in batch 77: 0.0692902/0.206421loss in batch 78: 0.0827942/0.204849loss in batch 79: 0.0267181/0.202621loss in batch 80: 0.0714111/0.201004loss in batch 81: 0.335159/0.202652loss in batch 82: 0.0196991/0.200439loss in batch 83: 0.227875/0.20076loss in batch 84: 0.543732/0.204803loss in batch 85: 0.390289/0.206955loss in batch 86: 0.0062561/0.204651loss in batch 87: 0.229492/0.204926loss in batch 88: 0.0196686/0.20285loss in batch 89: 0.400986/0.205048loss in batch 90: 0.123184/0.204147loss in batch 91: 0.0186005/0.202133loss in batch 92: 0.0270386/0.200256loss in batch 93: 0.612473/0.204651loss in batch 94: 0.109299/0.203629loss in batch 95: 0.0307159/0.201828loss in batch 96: 0.0864868/0.200638loss in batch 97: 0.0893555/0.199509loss in batch 98: 0.0677643/0.198181loss in batch 99: 0.4758/0.200958loss in batch 100: 0.0526886/0.199493loss in batch 101: 0.0164337/0.197693loss in batch 102: 0.0877686/0.19664loss in batch 103: 0.0319366/0.195038loss in batch 104: 0.096405/0.194107loss in batch 105: 0.0685883/0.192917loss in batch 106: 0.368393/0.19455loss in batch 107: 0.332809/0.195831loss in batch 108: 0.136749/0.195297loss in batch 109: 0.0571136/0.194046loss in batch 110: 0.0437012/0.192688loss in batch 111: 0.11908/0.192032loss in batch 112: 0.0479126/0.19075loss in batch 113: 0.300858/0.191711loss in batch 114: 0.149063/0.191345loss in batch 115: 0.0612488/0.190216loss in batch 116: 0.0482941/0.189011loss in batch 117: 0.546371/0.192047loss in batch 118: 0.0764313/0.191071loss in batch 119: 0.182007/0.190994loss in batch 120: 0.246872/0.191452loss in batch 121: 0.0736847/0.190491loss in batch 122: 0.200912/0.190582loss in batch 123: 0.00335693/0.189072loss in batch 124: 0.0534668/0.187988loss in batch 125: 0.120346/0.187439loss in batch 126: 0.0359344/0.186249loss in batch 127: 0.0762024/0.185394loss in batch 128: 0.0119476/0.184052loss in batch 129: 0.0356445/0.182907loss in batch 130: 0.00926208/0.18158loss in batch 131: 0.559204/0.184448loss in batch 132: 0.235779/0.184814loss in batch 133: 0.175095/0.184753loss in batch 134: 0.00914001/0.183456loss in batch 135: 0.0907593/0.18277loss in batch 136: 0.0497589/0.181793loss in batch 137: 0.0476532/0.180817loss in batch 138: 0.0979462/0.180237loss in batch 139: 0.256241/0.180771loss in batch 140: 0.0541382/0.179871loss in batch 141: 0.035141/0.178848loss in batch 142: 0.215256/0.179123loss in batch 143: 0.230011/0.179474loss in batch 144: 0.617783/0.182495loss in batch 145: 0.652924/0.185715loss in batch 146: 0.0149384/0.18454loss in batch 147: 0.088562/0.183899loss in batch 148: 0.630753/0.18689loss in batch 149: 0.357117/0.188034loss in batch 150: 0.0783691/0.187302loss in batch 151: 0.0309753/0.186279loss in batch 152: 0.0752869/0.185562loss in batch 153: 0.283569/0.186188loss in batch 154: 0.220474/0.186401loss in batch 155: 0.118286/0.185974loss in batch 156: 0.124908/0.185577loss in batch 157: 0.483109/0.187469loss in batch 158: 0.0381317/0.186523loss in batch 159: 0.191055/0.186554loss in batch 160: 0.212784/0.186722loss in batch 161: 0.228409/0.186966loss in batch 162: 0.213882/0.187149loss in batch 163: 0.927719/0.19165loss in batch 164: 0.108459/0.191162loss in batch 165: 0.321686/0.19194loss in batch 166: 0.274429/0.192444loss in batch 167: 0.311859/0.193146loss in batch 168: 0.239914/0.19342loss in batch 169: 0.13353/0.193069loss in batch 170: 0.402023/0.19429loss in batch 171: 0.0674438/0.193558loss in batch 172: 0.440369/0.194992loss in batch 173: 0.219635/0.195129loss in batch 174: 0.0325928/0.194199loss in batch 175: 0.264053/0.19458loss in batch 176: 0.115952/0.194153loss in batch 177: 0.0551605/0.193359loss in batch 178: 0.096405/0.192825loss in batch 179: 1.13182/0.198044loss in batch 180: 0.114319/0.197586loss in batch 181: 0.0488892/0.196762loss in batch 182: 0.096283/0.196213loss in batch 183: 0.0964966/0.195663loss in batch 184: 0.142593/0.195389loss in batch 185: 0.111069/0.194916loss in batch 186: 0.332779/0.195663loss in batch 187: 0.166153/0.195511loss in batch 188: 0.100479/0.195007loss in batch 189: 0.0496521/0.194244loss in batch 190: 0.0478058/0.193466loss in batch 191: 1.14113/0.19841loss in batch 192: 0.0553894/0.197678loss in batch 193: 0.0186005/0.196747loss in batch 194: 0.236557/0.196945loss in batch 195: 0.135345/0.19664loss in batch 196: 0.0957794/0.196121loss in batch 197: 0.0314941/0.195297loss in batch 198: 0.0913849/0.194778loss in batch 199: 0.125305/0.194412loss in batch 200: 0.135376/0.194138loss in batch 201: 0.304581/0.194672loss in batch 202: 0.507019/0.196213loss in batch 203: 0.0722961/0.195602loss in batch 204: 0.123978/0.195251loss in batch 205: 0.224823/0.195389loss in batch 206: 0.05159/0.194702loss in batch 207: 0.0257568/0.193893loss in batch 208: 0.260178/0.194214loss in batch 209: 0.358688/0.194992loss in batch 210: 0.216873/0.195099loss in batch 211: 0.0306549/0.194321loss in batch 212: 0.400757/0.195282
done with epoch 8
train_acc: 0.931925 (397/426)
test loss: 0.400756
acc: 0.944054 (135/143)
loss in batch 0: 0.0287781/0.0287781loss in batch 1: 0.0720062/0.0503845loss in batch 2: 0.0151215/0.0386353loss in batch 3: 0.0297699/0.0364227loss in batch 4: 0.0688934/0.0429077loss in batch 5: 0.131393/0.057663loss in batch 6: 0.174789/0.0743866loss in batch 7: 0.138382/0.0823975loss in batch 8: 0.0317841/0.0767822loss in batch 9: 0.183487/0.0874481loss in batch 10: 1.39795/0.206573loss in batch 11: 0.0615692/0.194504loss in batch 12: 0.0453033/0.183029loss in batch 13: 0.684219/0.218811loss in batch 14: 0.016571/0.205338loss in batch 15: 0.0673218/0.196701loss in batch 16: 0.108566/0.191528loss in batch 17: 0.0955505/0.186188loss in batch 18: 0.245117/0.189301loss in batch 19: 0.159714/0.18782loss in batch 20: 0.238617/0.190231loss in batch 21: 0.218536/0.191528loss in batch 22: 0.324921/0.197327loss in batch 23: 0.09198/0.192932loss in batch 24: 0.0178223/0.185928loss in batch 25: 0.23204/0.187698loss in batch 26: 0.0946808/0.18425loss in batch 27: 0.0719299/0.180252loss in batch 28: 0.322983/0.185165loss in batch 29: 0.0376282/0.180252loss in batch 30: 0.356049/0.185913loss in batch 31: 0.0369873/0.181274loss in batch 32: 0.0517578/0.177338loss in batch 33: 0.162262/0.176895loss in batch 34: 0.0664978/0.173752loss in batch 35: 0.166321/0.173538loss in batch 36: 0.115738/0.171982loss in batch 37: 0.0191193/0.167953loss in batch 38: 0.141983/0.167297loss in batch 39: 0.106949/0.165787loss in batch 40: 0.598572/0.176331loss in batch 41: 0.0839996/0.174133loss in batch 42: 0.129517/0.173096loss in batch 43: 0.109268/0.171646loss in batch 44: 0.214066/0.172592loss in batch 45: 0.98143/0.19017loss in batch 46: 0.091156/0.188065loss in batch 47: 0.0123901/0.184402loss in batch 48: 0.154572/0.183792loss in batch 49: 0.00950623/0.180298loss in batch 50: 0.13768/0.179474loss in batch 51: 0.0209503/0.176422loss in batch 52: 0.122849/0.175415loss in batch 53: 0.106567/0.174149loss in batch 54: 0.274994/0.175964loss in batch 55: 0.452652/0.180923loss in batch 56: 0.121033/0.179871loss in batch 57: 0.154861/0.179443loss in batch 58: 0.0295868/0.176895loss in batch 59: 0.0286865/0.174423loss in batch 60: 0.183182/0.174576loss in batch 61: 0.520416/0.180145loss in batch 62: 0.514816/0.185455loss in batch 63: 0.0403748/0.183197loss in batch 64: 0.312424/0.185181loss in batch 65: 0.0698242/0.183441loss in batch 66: 0.0856018/0.181976loss in batch 67: 0.013382/0.179489loss in batch 68: 0.287018/0.181046loss in batch 69: 0.283936/0.182526loss in batch 70: 0.0521545/0.180695loss in batch 71: 0.0746613/0.179214loss in batch 72: 0.0949249/0.178055loss in batch 73: 0.168594/0.177933loss in batch 74: 0.239624/0.178757loss in batch 75: 0.152893/0.178406loss in batch 76: 0.249466/0.179337loss in batch 77: 0.0643768/0.177856loss in batch 78: 0.104416/0.176926loss in batch 79: 0.373657/0.179398loss in batch 80: 0.0521698/0.177811loss in batch 81: 0.427078/0.180862loss in batch 82: 0.224319/0.181381loss in batch 83: 0.0616302/0.179962loss in batch 84: 0.130783/0.179382loss in batch 85: 0.185303/0.179459loss in batch 86: 0.246063/0.180222loss in batch 87: 0.164764/0.180038loss in batch 88: 0.0600128/0.178696loss in batch 89: 0.0624542/0.177399loss in batch 90: 0.216965/0.177841loss in batch 91: 0.00753784/0.17598loss in batch 92: 0.184509/0.176071loss in batch 93: 0.0605164/0.174835loss in batch 94: 0.516037/0.178421loss in batch 95: 0.0497437/0.177094loss in batch 96: 0.283997/0.178192loss in batch 97: 0.0897675/0.177292loss in batch 98: 0.132523/0.176834loss in batch 99: 0.0741119/0.175812loss in batch 100: 0.0580597/0.174637loss in batch 101: 0.00395203/0.172974loss in batch 102: 0.394073/0.175125loss in batch 103: 0.059021/0.174011loss in batch 104: 0.297409/0.175186loss in batch 105: 0.102005/0.1745loss in batch 106: 0.44873/0.177048loss in batch 107: 0.519058/0.180222loss in batch 108: 0.0618744/0.179138loss in batch 109: 0.0580444/0.178024loss in batch 110: 0.0844574/0.177185loss in batch 111: 0.0193024/0.175781loss in batch 112: 0.18837/0.175888loss in batch 113: 0.0662842/0.174927loss in batch 114: 0.0531006/0.173874loss in batch 115: 0.0108795/0.172455loss in batch 116: 0.133316/0.172134loss in batch 117: 0.122513/0.171707loss in batch 118: 0.0359039/0.170578loss in batch 119: 0.088623/0.169891loss in batch 120: 0.180069/0.169968loss in batch 121: 0.0359802/0.168884loss in batch 122: 0.166824/0.168854loss in batch 123: 0.00808716/0.167557loss in batch 124: 0.46759/0.169952loss in batch 125: 0.0404663/0.16893loss in batch 126: 0.014801/0.167725loss in batch 127: 0.0827026/0.167053loss in batch 128: 0.259613/0.16777loss in batch 129: 0.374237/0.169357loss in batch 130: 0.0560303/0.168488loss in batch 131: 0.047699/0.167587loss in batch 132: 0.197693/0.167816loss in batch 133: 0.0880737/0.167221loss in batch 134: 0.0544739/0.166367loss in batch 135: 0.520813/0.168976loss in batch 136: 0.0814362/0.168335loss in batch 137: 0.0135193/0.167221loss in batch 138: 0.580002/0.170181loss in batch 139: 0.290894/0.171051loss in batch 140: 0.15126/0.170914loss in batch 141: 0.438904/0.172806loss in batch 142: 0.223297/0.173141loss in batch 143: 0.266373/0.173798loss in batch 144: 0.374222/0.175186loss in batch 145: 0.104263/0.174698loss in batch 146: 0.0298767/0.173706loss in batch 147: 0.082077/0.173096loss in batch 148: 0.366013/0.174393loss in batch 149: 0.0518188/0.173569loss in batch 150: 0.0349731/0.172653loss in batch 151: 0.0490112/0.171844loss in batch 152: 0.107391/0.171417loss in batch 153: 0.0158539/0.17041loss in batch 154: 0.110703/0.170013loss in batch 155: 0.0688629/0.169373loss in batch 156: 0.067215/0.168716loss in batch 157: 0.306824/0.169601loss in batch 158: 0.0163574/0.16864loss in batch 159: 0.019516/0.167694loss in batch 160: 0.0439911/0.166946loss in batch 161: 0.417526/0.168488loss in batch 162: 0.101059/0.168076loss in batch 163: 0.419846/0.169601loss in batch 164: 0.0510712/0.168884loss in batch 165: 0.301163/0.169678loss in batch 166: 0.170578/0.169678loss in batch 167: 0.42955/0.171234loss in batch 168: 0.499313/0.173172loss in batch 169: 0.816116/0.176956loss in batch 170: 0.177078/0.176956loss in batch 171: 0.351395/0.177979loss in batch 172: 1.04532/0.182983loss in batch 173: 0.121735/0.182632loss in batch 174: 0.74292/0.185837loss in batch 175: 0.136566/0.185562loss in batch 176: 0.224045/0.185776loss in batch 177: 0.10054/0.185287loss in batch 178: 0.666626/0.187973loss in batch 179: 0.41835/0.189255loss in batch 180: 0.111008/0.188828loss in batch 181: 0.0565491/0.188095loss in batch 182: 0.322418/0.188828loss in batch 183: 0.899567/0.192703loss in batch 184: 0.118805/0.192307loss in batch 185: 0.0761261/0.191666loss in batch 186: 0.575943/0.193741loss in batch 187: 0.0222321/0.192825loss in batch 188: 0.0582123/0.192108loss in batch 189: 0.216034/0.19223loss in batch 190: 0.137039/0.19194loss in batch 191: 0.133926/0.191635loss in batch 192: 0.219574/0.191788loss in batch 193: 0.116898/0.191391loss in batch 194: 0.170685/0.191284loss in batch 195: 0.0411377/0.190536loss in batch 196: 0.195404/0.190552loss in batch 197: 0.297531/0.191086loss in batch 198: 0.0546417/0.190399loss in batch 199: 0.0343933/0.189636loss in batch 200: 0.241577/0.18988loss in batch 201: 0.209839/0.189987loss in batch 202: 0.12706/0.189667loss in batch 203: 0.0386658/0.188934loss in batch 204: 0.633621/0.191101loss in batch 205: 0.048172/0.190399loss in batch 206: 0.0881348/0.189926loss in batch 207: 0.0770569/0.189377loss in batch 208: 0.121338/0.189041loss in batch 209: 0.0567322/0.188416loss in batch 210: 0.860031/0.191589loss in batch 211: 0.0697784/0.191025loss in batch 212: 0.0603027/0.190414
done with epoch 9
train_acc: 0.934272 (398/426)
test loss: 0.0603026
acc: 0.944054 (135/143)
loss in batch 0: 0.0948029/0.0948029loss in batch 1: 0.216385/0.155594loss in batch 2: 0.047348/0.119507loss in batch 3: 0.125107/0.120911loss in batch 4: 0.442215/0.185165loss in batch 5: 0.225479/0.191895loss in batch 6: 0.0245514/0.167984loss in batch 7: 0.0477753/0.152954loss in batch 8: 0.222168/0.160645loss in batch 9: 0.089035/0.153488loss in batch 10: 0.0346222/0.14267loss in batch 11: 0.523346/0.174408loss in batch 12: 0.0580597/0.165451loss in batch 13: 0.0712433/0.158722loss in batch 14: 0.097702/0.154648loss in batch 15: 0.160446/0.155014loss in batch 16: 0.0747833/0.150299loss in batch 17: 0.130188/0.14917loss in batch 18: 0.0563354/0.144302loss in batch 19: 0.405945/0.157379loss in batch 20: 0.0362854/0.151611loss in batch 21: 0.0453033/0.146774loss in batch 22: 0.117493/0.145508loss in batch 23: 0.386505/0.155533loss in batch 24: 0.0498047/0.151321loss in batch 25: 0.0402832/0.147034loss in batch 26: 0.0214539/0.142395loss in batch 27: 0.0620117/0.139526loss in batch 28: 0.316849/0.145645loss in batch 29: 0.158325/0.146057loss in batch 30: 0.0785675/0.143875loss in batch 31: 0.668533/0.160278loss in batch 32: 0.0594635/0.157227loss in batch 33: 0.124283/0.156265loss in batch 34: 0.433594/0.164185loss in batch 35: 0.513397/0.173889loss in batch 36: 0.117447/0.172363loss in batch 37: 0.015976/0.168243loss in batch 38: 0.0649719/0.165604loss in batch 39: 0.223907/0.167053loss in batch 40: 0.200562/0.167862loss in batch 41: 0.147369/0.167389loss in batch 42: 0.260345/0.16954loss in batch 43: 0.125565/0.168533loss in batch 44: 0.0244293/0.165344loss in batch 45: 0.409256/0.170639loss in batch 46: 0.146912/0.170135loss in batch 47: 0.217422/0.171127loss in batch 48: 0.0356598/0.168365loss in batch 49: 0.0729828/0.166443loss in batch 50: 0.0319214/0.163803loss in batch 51: 0.0408325/0.161453loss in batch 52: 0.483856/0.167542loss in batch 53: 0.0692596/0.16571loss in batch 54: 0.101044/0.164536loss in batch 55: 0.12204/0.163788loss in batch 56: 0.276749/0.165756loss in batch 57: 1.02034/0.180496loss in batch 58: 0.294418/0.182419loss in batch 59: 0.169434/0.182205loss in batch 60: 0.0502167/0.180038loss in batch 61: 0.242889/0.181046loss in batch 62: 0.575577/0.187317loss in batch 63: 0.106461/0.18605loss in batch 64: 0.373032/0.188934loss in batch 65: 0.27121/0.19017loss in batch 66: 0.19017/0.190186loss in batch 67: 0.221649/0.190643loss in batch 68: 0.159027/0.19017loss in batch 69: 0.0483856/0.188156loss in batch 70: 0.558273/0.193375loss in batch 71: 0.0214691/0.190979loss in batch 72: 0.0215149/0.18866loss in batch 73: 0.143265/0.188049loss in batch 74: 0.0496216/0.186203loss in batch 75: 0.0486145/0.184387loss in batch 76: 0.37178/0.186829loss in batch 77: 0.137756/0.186203loss in batch 78: 0.129288/0.185471loss in batch 79: 0.353333/0.187576loss in batch 80: 0.0108032/0.185394loss in batch 81: 0.0275421/0.183456loss in batch 82: 0.109879/0.182587loss in batch 83: 0.0189972/0.180634loss in batch 84: 0.184616/0.180679loss in batch 85: 0.479477/0.184158loss in batch 86: 0.0574188/0.182709loss in batch 87: 0.070343/0.181427loss in batch 88: 0.199295/0.181625loss in batch 89: 0.0717163/0.180405loss in batch 90: 0.13591/0.179916loss in batch 91: 0.13913/0.179474loss in batch 92: 0.496796/0.182892loss in batch 93: 0.342209/0.18457loss in batch 94: 0.125824/0.18396loss in batch 95: 0.236176/0.184509loss in batch 96: 0.0788422/0.183411loss in batch 97: 0.349152/0.185104loss in batch 98: 0.0234833/0.183472loss in batch 99: 0.205154/0.183685loss in batch 100: 0.462082/0.186432loss in batch 101: 0.062088/0.185226loss in batch 102: 0.0739288/0.184143loss in batch 103: 0.0970764/0.183304loss in batch 104: 0.345566/0.184845loss in batch 105: 0.0396729/0.183487loss in batch 106: 0.552292/0.186935loss in batch 107: 0.0521545/0.185669loss in batch 108: 0.0866852/0.184769loss in batch 109: 0.0517426/0.183563loss in batch 110: 0.119202/0.182983loss in batch 111: 0.242142/0.183517loss in batch 112: 0.188736/0.183563loss in batch 113: 0.0425415/0.182312loss in batch 114: 0.00527954/0.180771loss in batch 115: 0.10437/0.180115loss in batch 116: 0.349045/0.181564loss in batch 117: 0.0502319/0.18045loss in batch 118: 0.400864/0.182312loss in batch 119: 0.081192/0.181458loss in batch 120: 0.10881/0.180862loss in batch 121: 0.261749/0.181519loss in batch 122: 0.0413055/0.180389loss in batch 123: 0.0503235/0.179337loss in batch 124: 0.00267029/0.177917loss in batch 125: 0.0344238/0.176788loss in batch 126: 0.0710297/0.175949loss in batch 127: 0.0425262/0.174896loss in batch 128: 0.316284/0.17601loss in batch 129: 0.085083/0.175308loss in batch 130: 0.126694/0.174927loss in batch 131: 0.215073/0.175232loss in batch 132: 0.01651/0.174042loss in batch 133: 0.0403137/0.17305loss in batch 134: 0.0468292/0.172104loss in batch 135: 0.176132/0.17215loss in batch 136: 0.041748/0.171204loss in batch 137: 0.577164/0.174133loss in batch 138: 0.0263214/0.17308loss in batch 139: 0.578522/0.175964loss in batch 140: 0.358688/0.177261loss in batch 141: 0.027359/0.176208loss in batch 142: 0.740585/0.180161loss in batch 143: 0.0311127/0.179123loss in batch 144: 0.118393/0.178696loss in batch 145: 0.0926971/0.178116loss in batch 146: 0.601013/0.180984loss in batch 147: 0.0277863/0.179962loss in batch 148: 0.109985/0.179474loss in batch 149: 0.0310059/0.178482loss in batch 150: 1.7847/0.189117loss in batch 151: 0.0569611/0.188263loss in batch 152: 0.122849/0.187836loss in batch 153: 0.0253143/0.186783loss in batch 154: 0.161484/0.1866loss in batch 155: 0.1371/0.186295loss in batch 156: 0.35318/0.187363loss in batch 157: 0.147629/0.187103loss in batch 158: 0.152466/0.18689loss in batch 159: 0.240158/0.187225loss in batch 160: 0.0481567/0.186371loss in batch 161: 0.140488/0.186066loss in batch 162: 0.0901031/0.185486loss in batch 163: 1.10962/0.191116loss in batch 164: 0.358215/0.192139loss in batch 165: 0.0748138/0.191422loss in batch 166: 0.118362/0.190994loss in batch 167: 0.0956116/0.190414loss in batch 168: 0.348373/0.191345loss in batch 169: 0.0776062/0.190689loss in batch 170: 0.210632/0.190796loss in batch 171: 0.058609/0.190033loss in batch 172: 0.03508/0.189148loss in batch 173: 0.0894928/0.188568loss in batch 174: 0.0789642/0.187943loss in batch 175: 0.61676/0.190369loss in batch 176: 0.0437317/0.18956loss in batch 177: 0.0294952/0.188644loss in batch 178: 0.0206451/0.187714loss in batch 179: 0.037796/0.186874loss in batch 180: 0.0520477/0.186142loss in batch 181: 0.0442963/0.185349loss in batch 182: 0.0514679/0.184616loss in batch 183: 0.565338/0.186691loss in batch 184: 0.0723877/0.186081loss in batch 185: 0.159561/0.185928loss in batch 186: 0.264099/0.186356loss in batch 187: 0.0463409/0.185608loss in batch 188: 0.552444/0.187546loss in batch 189: 0.0161591/0.186646loss in batch 190: 0.0697937/0.18602loss in batch 191: 0.126587/0.18573loss in batch 192: 0.281769/0.186218loss in batch 193: 0.286911/0.186737loss in batch 194: 0.241333/0.187012loss in batch 195: 0.538086/0.188812loss in batch 196: 0.328903/0.189514loss in batch 197: 0.0387726/0.188766loss in batch 198: 0.27269/0.189178loss in batch 199: 0.328583/0.18988loss in batch 200: 0.123383/0.189545loss in batch 201: 0.0593109/0.188904loss in batch 202: 0.0122223/0.188034loss in batch 203: 0.158951/0.187897loss in batch 204: 0.0712738/0.187317loss in batch 205: 0.13269/0.187057loss in batch 206: 0.0297546/0.186295loss in batch 207: 0.0462189/0.185623loss in batch 208: 0.060257/0.185028loss in batch 209: 0.379715/0.185944loss in batch 210: 0.0427704/0.185272loss in batch 211: 0.0731201/0.184738loss in batch 212: 0.0103149/0.183929
done with epoch 10
train_acc: 0.943662 (402/426)
test loss: 0.0103149
acc: 0.944054 (135/143)
loss in batch 0: 0.0419617/0.0419617loss in batch 1: 1.05998/0.550964loss in batch 2: 0.363647/0.488525loss in batch 3: 0.125473/0.397766loss in batch 4: 0.22467/0.363144loss in batch 5: 0.0352631/0.308502loss in batch 6: 0.17276/0.289108loss in batch 7: 0.365082/0.298599loss in batch 8: 0.602371/0.332352loss in batch 9: 0.0761108/0.306732loss in batch 10: 0.30954/0.306992loss in batch 11: 0.226563/0.300278loss in batch 12: 0.315735/0.301468loss in batch 13: 0.213898/0.295227loss in batch 14: 0.143005/0.285065loss in batch 15: 0.0137787/0.268112loss in batch 16: 0.0550537/0.255585loss in batch 17: 0.0483704/0.244064loss in batch 18: 0.06987/0.234894loss in batch 19: 0.0669098/0.226501loss in batch 20: 0.519867/0.240479loss in batch 21: 0.0605164/0.2323loss in batch 22: 0.0519714/0.224457loss in batch 23: 0.0831604/0.218552loss in batch 24: 0.315491/0.222443loss in batch 25: 0.177826/0.220734loss in batch 26: 0.056015/0.21463loss in batch 27: 0.219864/0.214813loss in batch 28: 0.406281/0.22142loss in batch 29: 0.10379/0.217484loss in batch 30: 0.101395/0.213745loss in batch 31: 0.116547/0.210709loss in batch 32: 0.0583801/0.206085loss in batch 33: 0.192841/0.205704loss in batch 34: 0.092392/0.202469loss in batch 35: 0.421661/0.208557loss in batch 36: 0.0909271/0.205383loss in batch 37: 0.0661469/0.201721loss in batch 38: 0.161942/0.200684loss in batch 39: 0.119232/0.198654loss in batch 40: 0.0141144/0.194153loss in batch 41: 0.371521/0.19838loss in batch 42: 0.00349426/0.193848loss in batch 43: 0.0333405/0.190201loss in batch 44: 0.0887909/0.187943loss in batch 45: 0.101364/0.186066loss in batch 46: 0.210709/0.186584loss in batch 47: 0.195709/0.186783loss in batch 48: 0.0463104/0.183914loss in batch 49: 0.0712433/0.181671loss in batch 50: 0.0760651/0.179581loss in batch 51: 0.35585/0.182983loss in batch 52: 0.0378723/0.180237loss in batch 53: 0.0562592/0.177948loss in batch 54: 0.45079/0.182907loss in batch 55: 0.0716095/0.180923loss in batch 56: 0.40239/0.184799loss in batch 57: 0.308731/0.186935loss in batch 58: 0.121063/0.185837loss in batch 59: 0.118134/0.184692loss in batch 60: 0.0931549/0.183197loss in batch 61: 0.432358/0.18721loss in batch 62: 0.0890198/0.185654loss in batch 63: 0.0549774/0.183609loss in batch 64: 0.0694275/0.181854loss in batch 65: 0.23381/0.182648loss in batch 66: 0.128204/0.181839loss in batch 67: 0.0182495/0.179413loss in batch 68: 0.171753/0.179321loss in batch 69: 0.193298/0.17952loss in batch 70: 0.367401/0.182159loss in batch 71: 0.0237732/0.179962loss in batch 72: 0.0553741/0.178253loss in batch 73: 0.0885773/0.177032loss in batch 74: 0.123657/0.176331loss in batch 75: 0.0731812/0.174973loss in batch 76: 0.0469055/0.173309loss in batch 77: 0.244537/0.174225loss in batch 78: 0.0224915/0.172302loss in batch 79: 0.0301819/0.170532loss in batch 80: 0.0468903/0.168991loss in batch 81: 0.328918/0.170944loss in batch 82: 0.262711/0.172043loss in batch 83: 0.0601349/0.170731loss in batch 84: 0.0450134/0.169235loss in batch 85: 0.0865479/0.168289loss in batch 86: 0.0643616/0.167084loss in batch 87: 0.0606232/0.165878loss in batch 88: 0.588593/0.170624loss in batch 89: 0.271378/0.171753loss in batch 90: 0.105164/0.171021loss in batch 91: 0.0326843/0.16951loss in batch 92: 0.0579681/0.16832loss in batch 93: 0.0141602/0.166672loss in batch 94: 0.0242462/0.165161loss in batch 95: 0.533218/0.169006loss in batch 96: 1.08067/0.178406loss in batch 97: 0.259277/0.17923loss in batch 98: 0.11908/0.178619loss in batch 99: 0.0597229/0.177429loss in batch 100: 0.0501709/0.176163loss in batch 101: 0.0637512/0.175064loss in batch 102: 0.0880127/0.174225loss in batch 103: 0.26741/0.175125loss in batch 104: 0.457077/0.177811loss in batch 105: 0.371628/0.179642loss in batch 106: 0.0549622/0.178467loss in batch 107: 0.0737457/0.177505loss in batch 108: 0.0897064/0.176697loss in batch 109: 0.388626/0.178619loss in batch 110: 0.0219727/0.177216loss in batch 111: 0.0822144/0.176361loss in batch 112: 1.341/0.186661loss in batch 113: 0.0344849/0.185349loss in batch 114: 0.376129/0.186996loss in batch 115: 0.209869/0.187195loss in batch 116: 0.0135193/0.185699loss in batch 117: 0.666794/0.189789loss in batch 118: 0.137619/0.189346loss in batch 119: 0.0566101/0.188232loss in batch 120: 0.15506/0.187958loss in batch 121: 0.226852/0.188278loss in batch 122: 0.376358/0.189819loss in batch 123: 0.0252533/0.188492loss in batch 124: 0.311737/0.189468loss in batch 125: 0.0567474/0.188416loss in batch 126: 0.214264/0.188629loss in batch 127: 0.360596/0.189957loss in batch 128: 0.307526/0.190887loss in batch 129: 0.348145/0.192078loss in batch 130: 0.158676/0.191833loss in batch 131: 0.0218048/0.190536loss in batch 132: 0.0616913/0.189575loss in batch 133: 0.380325/0.190994loss in batch 134: 0.26889/0.191574loss in batch 135: 0.00805664/0.190231loss in batch 136: 0.0386047/0.189117loss in batch 137: 0.00323486/0.187775loss in batch 138: 0.345032/0.188904loss in batch 139: 0.134613/0.188522loss in batch 140: 0.820435/0.193008loss in batch 141: 0.0698395/0.192123loss in batch 142: 0.0654755/0.191238loss in batch 143: 0.186081/0.191208loss in batch 144: 0.288635/0.191879loss in batch 145: 0.0782166/0.191101loss in batch 146: 0.106415/0.190521loss in batch 147: 0.0478973/0.18956loss in batch 148: 0.0848999/0.188858loss in batch 149: 0.332413/0.189819loss in batch 150: 0.424072/0.19136loss in batch 151: 0.388412/0.192673loss in batch 152: 0.0488739/0.191727loss in batch 153: 0.0274353/0.190659loss in batch 154: 0.207413/0.190765loss in batch 155: 0.0490112/0.18985loss in batch 156: 0.0741425/0.189117loss in batch 157: 0.0844421/0.188446loss in batch 158: 0.255035/0.188873loss in batch 159: 0.0596008/0.188065loss in batch 160: 0.293762/0.188736loss in batch 161: 0.039917/0.18782loss in batch 162: 0.0209198/0.186783loss in batch 163: 0.0808716/0.186142loss in batch 164: 0.383926/0.187332loss in batch 165: 0.138748/0.187042loss in batch 166: 0.377884/0.188187loss in batch 167: 0.0422211/0.187317loss in batch 168: 0.0150604/0.186295loss in batch 169: 0.0987091/0.185776loss in batch 170: 0.0486908/0.184982loss in batch 171: 0.000915527/0.183914loss in batch 172: 0.11142/0.183487loss in batch 173: 0.138504/0.183228loss in batch 174: 0.0927734/0.182724loss in batch 175: 0.0465088/0.181946loss in batch 176: 0.0175629/0.181015loss in batch 177: 0.182968/0.18103loss in batch 178: 0.0183716/0.180115loss in batch 179: 0.597794/0.182434loss in batch 180: 0.106491/0.182022loss in batch 181: 0.12384/0.181702loss in batch 182: 0.102386/0.181259loss in batch 183: 0.917984/0.185272loss in batch 184: 0.0144958/0.184341loss in batch 185: 0.258209/0.184738loss in batch 186: 0.0982513/0.18428loss in batch 187: 0.385818/0.185349loss in batch 188: 0.0759277/0.184769loss in batch 189: 0.265762/0.185211loss in batch 190: 0.0764008/0.184631loss in batch 191: 0.0460052/0.183914loss in batch 192: 0.0652618/0.183289loss in batch 193: 0.0384064/0.182541loss in batch 194: 0.10672/0.182159loss in batch 195: 0.0177917/0.18132loss in batch 196: 0.133926/0.181076loss in batch 197: 0.482346/0.182602loss in batch 198: 0.174408/0.182556loss in batch 199: 0.0952301/0.182129loss in batch 200: 0.0280457/0.181351loss in batch 201: 0.0110779/0.180511loss in batch 202: 0.0514221/0.179886loss in batch 203: 0.349274/0.180695loss in batch 204: 0.148499/0.180542loss in batch 205: 0.0646973/0.179993loss in batch 206: 0.0621643/0.179413loss in batch 207: 0.0139008/0.178619loss in batch 208: 0.0952301/0.178223loss in batch 209: 0.0169678/0.17746loss in batch 210: 0.035614/0.176773loss in batch 211: 0.554672/0.178558loss in batch 212: 0.0214386/0.177826
done with epoch 11
train_acc: 0.938967 (400/426)
test loss: 0.0214386
acc: 0.951047 (136/143)
loss in batch 0: 0.0621338/0.0621338loss in batch 1: 0.175797/0.118958loss in batch 2: 0.0632172/0.100388loss in batch 3: 0.0694733/0.0926514loss in batch 4: 0.0591431/0.0859528loss in batch 5: 0.470856/0.150116loss in batch 6: 0.264297/0.166412loss in batch 7: 0.121307/0.160767loss in batch 8: 0.112228/0.15538loss in batch 9: 0.462616/0.186111loss in batch 10: 0.0231323/0.171295loss in batch 11: 0.0558014/0.161667loss in batch 12: 0.109726/0.157669loss in batch 13: 0.142075/0.156555loss in batch 14: 0.0782166/0.151337loss in batch 15: 0.223923/0.155869loss in batch 16: 0.114868/0.153458loss in batch 17: 0.244293/0.158508loss in batch 18: 0.101837/0.155533loss in batch 19: 0.287628/0.16214loss in batch 20: 0.0837555/0.158401loss in batch 21: 0.346588/0.166946loss in batch 22: 0.113525/0.164627loss in batch 23: 0.190659/0.16571loss in batch 24: 0.0316467/0.160355loss in batch 25: 0.261749/0.164261loss in batch 26: 0.452148/0.174911loss in batch 27: 0.211777/0.176239loss in batch 28: 0.182007/0.176422loss in batch 29: 0.057785/0.17247loss in batch 30: 0.0140381/0.167358loss in batch 31: 0.0280914/0.16301loss in batch 32: 0.0439758/0.159409loss in batch 33: 0.0853119/0.157227loss in batch 34: 0.179337/0.157867loss in batch 35: 0.326385/0.162537loss in batch 36: 0.0262451/0.158844loss in batch 37: 0.325073/0.163223loss in batch 38: 0.112289/0.161926loss in batch 39: 0.119766/0.160873loss in batch 40: 0.300583/0.164276loss in batch 41: 0.0695801/0.162033loss in batch 42: 0.0622559/0.159714loss in batch 43: 0.351761/0.164063loss in batch 44: 0.015564/0.160767loss in batch 45: 0.497314/0.168076loss in batch 46: 0.0890808/0.166397loss in batch 47: 0.0143127/0.163239loss in batch 48: 0.0914307/0.161774loss in batch 49: 0.0307159/0.159149loss in batch 50: 0.0682831/0.157364loss in batch 51: 0.191574/0.158035loss in batch 52: 0.038681/0.155762loss in batch 53: 0.302246/0.158478loss in batch 54: 0.143494/0.158218loss in batch 55: 0.0161285/0.15567loss in batch 56: 0.660873/0.164536loss in batch 57: 0.0170746/0.162003loss in batch 58: 0.0984802/0.160919loss in batch 59: 0.0730591/0.159454loss in batch 60: 0.032608/0.157364loss in batch 61: 0.0478973/0.155609loss in batch 62: 0.139694/0.15535loss in batch 63: 0.53595/0.161301loss in batch 64: 0.00704956/0.158936loss in batch 65: 0.100021/0.158035loss in batch 66: 0.0670929/0.156693loss in batch 67: 0.0635529/0.155304loss in batch 68: 0.385788/0.158661loss in batch 69: 0.0797272/0.157532loss in batch 70: 0.0926971/0.156601loss in batch 71: 0.168777/0.156784loss in batch 72: 0.338348/0.159271loss in batch 73: 0.202362/0.159836loss in batch 74: 0.367218/0.162613loss in batch 75: 0.365112/0.165283loss in batch 76: 0.0267029/0.163467loss in batch 77: 0.0246735/0.161697loss in batch 78: 0.0248718/0.159958loss in batch 79: 0.0888062/0.159088loss in batch 80: 0.0124207/0.157272loss in batch 81: 0.145401/0.15712loss in batch 82: 0.0290527/0.155579loss in batch 83: 0.247421/0.156677loss in batch 84: 0.299316/0.15834loss in batch 85: 0.0826569/0.157471loss in batch 86: 0.421982/0.160507loss in batch 87: 0.0298767/0.159027loss in batch 88: 0.196533/0.159439loss in batch 89: 0.57193/0.164032loss in batch 90: 0.0531921/0.162811loss in batch 91: 0.109055/0.162231loss in batch 92: 0.148575/0.162079loss in batch 93: 0.292389/0.163467loss in batch 94: 0.469635/0.166687loss in batch 95: 0.463196/0.169785loss in batch 96: 0.129669/0.169357loss in batch 97: 0.0354767/0.167999loss in batch 98: 0.0577393/0.166885loss in batch 99: 0.13945/0.166611loss in batch 100: 0.0863495/0.165817loss in batch 101: 0.698349/0.171036loss in batch 102: 0.307037/0.172363loss in batch 103: 0.140244/0.172058loss in batch 104: 0.0395813/0.170792loss in batch 105: 0.17659/0.170853loss in batch 106: 0.364044/0.172653loss in batch 107: 0.00332642/0.171082loss in batch 108: 0.176376/0.171127loss in batch 109: 0.0286865/0.169846loss in batch 110: 0.127792/0.169464loss in batch 111: 0.00856018/0.16803loss in batch 112: 0.0761719/0.167206loss in batch 113: 0.00608826/0.165802loss in batch 114: 0.143677/0.165604loss in batch 115: 0.0297394/0.164429loss in batch 116: 0.0914764/0.163818loss in batch 117: 0.181961/0.163971loss in batch 118: 0.0772095/0.163223loss in batch 119: 0.250183/0.163956loss in batch 120: 0.105194/0.163467loss in batch 121: 1.87517/0.177505loss in batch 122: 0.121552/0.177048loss in batch 123: 0.194626/0.177185loss in batch 124: 0.121002/0.176727loss in batch 125: 0.0900726/0.176056loss in batch 126: 0.0463409/0.175034loss in batch 127: 0.0502777/0.174042loss in batch 128: 0.0535889/0.173126loss in batch 129: 0.302673/0.174118loss in batch 130: 0.189163/0.174225loss in batch 131: 0.00483704/0.172943loss in batch 132: 0.46254/0.17511loss in batch 133: 0.0163269/0.17395loss in batch 134: 0.0575409/0.17308loss in batch 135: 0.53775/0.175751loss in batch 136: 0.0332184/0.174728loss in batch 137: 0.0288086/0.17366loss in batch 138: 0.642624/0.177032loss in batch 139: 0.0704956/0.17627loss in batch 140: 0.0466461/0.175354loss in batch 141: 0.0700684/0.174622loss in batch 142: 0.0214386/0.173538loss in batch 143: 0.0143738/0.17244loss in batch 144: 0.432022/0.17424loss in batch 145: 0.115417/0.173813loss in batch 146: 0.106445/0.17337loss in batch 147: 0.188568/0.173477loss in batch 148: 0.0461121/0.172623loss in batch 149: 0.314682/0.173569loss in batch 150: 0.0314484/0.172607loss in batch 151: 0.0829773/0.172028loss in batch 152: 0.352036/0.173203loss in batch 153: 0.326233/0.174194loss in batch 154: 0.046402/0.173386loss in batch 155: 0.0150757/0.172363loss in batch 156: 0.0678101/0.171692loss in batch 157: 0.158722/0.171616loss in batch 158: 0.365723/0.172836loss in batch 159: 0.796722/0.176727loss in batch 160: 0.582504/0.17926loss in batch 161: 0.239044/0.179626loss in batch 162: 0.0755157/0.178986loss in batch 163: 0.099472/0.178497loss in batch 164: 0.0593109/0.17778loss in batch 165: 0.259384/0.178253loss in batch 166: 0.0192566/0.177322loss in batch 167: 0.545715/0.179504loss in batch 168: 0.0654755/0.178833loss in batch 169: 0.021286/0.177902loss in batch 170: 0.137787/0.177673loss in batch 171: 0.0223389/0.176773loss in batch 172: 0.110825/0.176392loss in batch 173: 0.242157/0.176773loss in batch 174: 0.0900116/0.17627loss in batch 175: 0.075943/0.17569loss in batch 176: 0.405563/0.177002loss in batch 177: 0.0762634/0.176437loss in batch 178: 0.755371/0.179672loss in batch 179: 0.0376587/0.178879loss in batch 180: 0.199142/0.179001loss in batch 181: 0.121826/0.17868loss in batch 182: 0.0441895/0.177948loss in batch 183: 0.0673523/0.177353loss in batch 184: 0.0701294/0.176758loss in batch 185: 0.0905609/0.1763loss in batch 186: 0.0564117/0.175659loss in batch 187: 0.111191/0.175323loss in batch 188: 0.225571/0.175583loss in batch 189: 1.06769/0.180283loss in batch 190: 0.123337/0.179977loss in batch 191: 0.0138702/0.179108loss in batch 192: 0.0657654/0.178513loss in batch 193: 0.119492/0.178223loss in batch 194: 1.02444/0.182556loss in batch 195: 0.0706787/0.181992loss in batch 196: 0.0442963/0.18129loss in batch 197: 0.0294952/0.180511loss in batch 198: 0.0516357/0.179886loss in batch 199: 0.0862579/0.179398loss in batch 200: 0.0178986/0.178604loss in batch 201: 0.0434723/0.177933loss in batch 202: 0.0655518/0.177383loss in batch 203: 0.368637/0.178314loss in batch 204: 0.0547943/0.177719loss in batch 205: 0.00694275/0.176895loss in batch 206: 0.0682831/0.176361loss in batch 207: 0.0249023/0.175644loss in batch 208: 0.067215/0.175125loss in batch 209: 0.011795/0.174347loss in batch 210: 0.0198059/0.173615loss in batch 211: 0.0389557/0.172974loss in batch 212: 0.0417328/0.172348
done with epoch 12
train_acc: 0.941315 (401/426)
test loss: 0.0417327
acc: 0.930068 (133/143)
loss in batch 0: 0.147568/0.147568loss in batch 1: 0.18634/0.166946loss in batch 2: 0.331604/0.221832loss in batch 3: 0.0526886/0.17955loss in batch 4: 0.32782/0.209198loss in batch 5: 0.496399/0.257065loss in batch 6: 0.0750275/0.231064loss in batch 7: 0.0916748/0.213638loss in batch 8: 0.0500793/0.195465loss in batch 9: 0.460022/0.221924loss in batch 10: 0.445984/0.242294loss in batch 11: 0.350235/0.251282loss in batch 12: 0.0392609/0.234985loss in batch 13: 0.0190735/0.219543loss in batch 14: 0.620621/0.246292loss in batch 15: 0.0627289/0.234818loss in batch 16: 0.0875092/0.226151loss in batch 17: 0.0128937/0.214294loss in batch 18: 0.0915222/0.20784loss in batch 19: 0.185608/0.206726loss in batch 20: 0.245438/0.208572loss in batch 21: 0.0447845/0.201126loss in batch 22: 0.229935/0.202393loss in batch 23: 0.317551/0.207184loss in batch 24: 0.265244/0.209503loss in batch 25: 0.0184021/0.202148loss in batch 26: 0.11232/0.198822loss in batch 27: 0.0273285/0.192703loss in batch 28: 0.103455/0.189636loss in batch 29: 0.043457/0.184753loss in batch 30: 0.150894/0.183655loss in batch 31: 0.450607/0.192001loss in batch 32: 0.104553/0.189362loss in batch 33: 0.0363464/0.184845loss in batch 34: 0.423126/0.191666loss in batch 35: 0.117416/0.18959loss in batch 36: 0.22374/0.190521loss in batch 37: 0.0773315/0.187531loss in batch 38: 0.0497894/0.184006loss in batch 39: 0.891342/0.201691loss in batch 40: 0.0272675/0.197449loss in batch 41: 0.177429/0.19696loss in batch 42: 0.27449/0.198761loss in batch 43: 0.259094/0.200134loss in batch 44: 0.224838/0.200684loss in batch 45: 0.313934/0.20314loss in batch 46: 0.0470276/0.199829loss in batch 47: 0.0483551/0.196671loss in batch 48: 0.0262451/0.193192loss in batch 49: 0.0233765/0.189804loss in batch 50: 0.161407/0.18924loss in batch 51: 0.00299072/0.185654loss in batch 52: 0.199677/0.185913loss in batch 53: 0.0409241/0.183228loss in batch 54: 0.197052/0.183487loss in batch 55: 0.0884247/0.181793loss in batch 56: 0.244766/0.182892loss in batch 57: 0.279221/0.184555loss in batch 58: 0.240601/0.185516loss in batch 59: 0.313721/0.187637loss in batch 60: 0.0303345/0.185059loss in batch 61: 0.0417633/0.182755loss in batch 62: 0.463196/0.18721loss in batch 63: 0.0133057/0.184494loss in batch 64: 0.314285/0.186478loss in batch 65: 0.217087/0.186951loss in batch 66: 0.302032/0.18866loss in batch 67: 0.393829/0.191681loss in batch 68: 0.123962/0.190689loss in batch 69: 0.398987/0.19368loss in batch 70: 0.0715332/0.191956loss in batch 71: 0.352982/0.194183loss in batch 72: 0.0248566/0.191879loss in batch 73: 0.203796/0.192032loss in batch 74: 0.0383911/0.189987loss in batch 75: 0.0348053/0.187958loss in batch 76: 0.103027/0.186844loss in batch 77: 0.0866852/0.185562loss in batch 78: 0.17897/0.185471loss in batch 79: 0.0956116/0.184357loss in batch 80: 0.0805511/0.18306loss in batch 81: 0.0572205/0.181534loss in batch 82: 0.490219/0.185257loss in batch 83: 0.0372772/0.183487loss in batch 84: 0.0847168/0.182327loss in batch 85: 0.0219574/0.180466loss in batch 86: 0.27829/0.181595loss in batch 87: 0.0252228/0.17981loss in batch 88: 0.174728/0.179764loss in batch 89: 0.0318604/0.178116loss in batch 90: 0.128845/0.177567loss in batch 91: 0.0482635/0.176163loss in batch 92: 0.174622/0.176147loss in batch 93: 0.0195007/0.174484loss in batch 94: 0.0350952/0.173019loss in batch 95: 0.425156/0.175644loss in batch 96: 0.0620575/0.174469loss in batch 97: 0.0715485/0.173416loss in batch 98: 0.138245/0.173065loss in batch 99: 0.0222321/0.171555loss in batch 100: 0.0856323/0.1707loss in batch 101: 0.148773/0.170486loss in batch 102: 0.021225/0.169052loss in batch 103: 0.242737/0.169754loss in batch 104: 0.132721/0.169403loss in batch 105: 0.128571/0.169022loss in batch 106: 0.0214233/0.167633loss in batch 107: 0.0423889/0.166473loss in batch 108: 0.00808716/0.165009loss in batch 109: 0.0780334/0.16423loss in batch 110: 0.0743103/0.163422loss in batch 111: 0.0480652/0.162399loss in batch 112: 0.0075531/0.161026loss in batch 113: 0.168808/0.161087loss in batch 114: 0.163757/0.161102loss in batch 115: 0.371231/0.162918loss in batch 116: 0.0289764/0.161789loss in batch 117: 0.106537/0.161301loss in batch 118: 0.100662/0.160797loss in batch 119: 0.103882/0.160324loss in batch 120: 0.0333557/0.159271loss in batch 121: 0.485138/0.161957loss in batch 122: 0.0115051/0.160721loss in batch 123: 0.0274811/0.159653loss in batch 124: 0.506729/0.16243loss in batch 125: 0.0418243/0.161469loss in batch 126: 0.258743/0.162247loss in batch 127: 1.76213/0.174728loss in batch 128: 0.192093/0.174866loss in batch 129: 0.0323029/0.173782loss in batch 130: 0.128891/0.173431loss in batch 131: 0.403854/0.175171loss in batch 132: 0.0564423/0.174271loss in batch 133: 0.0354614/0.173248loss in batch 134: 0.102264/0.172729loss in batch 135: 0.526657/0.175323loss in batch 136: 0.0885315/0.174698loss in batch 137: 0.0281982/0.173615loss in batch 138: 0.0203705/0.172531loss in batch 139: 0.0172882/0.171417loss in batch 140: 0.0428925/0.170502loss in batch 141: 0.0515747/0.169678loss in batch 142: 0.0143738/0.168579loss in batch 143: 0.225967/0.168991loss in batch 144: 0.0137939/0.167908loss in batch 145: 0.0556183/0.16713loss in batch 146: 0.59053/0.170029loss in batch 147: 0.412064/0.171661loss in batch 148: 0.0331116/0.170715loss in batch 149: 0.279114/0.171448loss in batch 150: 0.0154266/0.17041loss in batch 151: 0.293961/0.171219loss in batch 152: 0.0487061/0.170425loss in batch 153: 0.500961/0.172577loss in batch 154: 0.00817871/0.171524loss in batch 155: 1.06334/0.177231loss in batch 156: 0.362289/0.178406loss in batch 157: 0.0749512/0.17775loss in batch 158: 0.0787811/0.177139loss in batch 159: 0.309372/0.177963loss in batch 160: 0.0657959/0.177261loss in batch 161: 0.258987/0.177765loss in batch 162: 0.263977/0.178299loss in batch 163: 0.0569611/0.177551loss in batch 164: 0.0336456/0.176682loss in batch 165: 0.124725/0.176361loss in batch 166: 0.473679/0.178146loss in batch 167: 0.091156/0.177628loss in batch 168: 0.430054/0.179123loss in batch 169: 0.181305/0.179138loss in batch 170: 0.227142/0.179428loss in batch 171: 0.299103/0.180115loss in batch 172: 0.0204315/0.179184loss in batch 173: 0.015274/0.178253loss in batch 174: 0.00779724/0.177277loss in batch 175: 0.0536041/0.176575loss in batch 176: 0.168488/0.176529loss in batch 177: 0.154099/0.176407loss in batch 178: 0.0432434/0.175659loss in batch 179: 0.0438538/0.174927loss in batch 180: 0.125946/0.174652loss in batch 181: 0.0683594/0.174072loss in batch 182: 0.0441284/0.17337loss in batch 183: 0.0371094/0.172623loss in batch 184: 0.0755768/0.172104loss in batch 185: 0.122864/0.171829loss in batch 186: 0.335968/0.172714loss in batch 187: 0.0136414/0.17186loss in batch 188: 0.117386/0.17157loss in batch 189: 0.378967/0.172653loss in batch 190: 0.220703/0.172913loss in batch 191: 0.27652/0.173462loss in batch 192: 0.14946/0.17334loss in batch 193: 0.0265808/0.172577loss in batch 194: 0.374664/0.173615loss in batch 195: 0.0610046/0.173035loss in batch 196: 0.0448914/0.172394loss in batch 197: 0.0358124/0.171707loss in batch 198: 0.0976868/0.171326loss in batch 199: 0.0700226/0.170822loss in batch 200: 0.119598/0.170563loss in batch 201: 0.0408783/0.169922loss in batch 202: 1.20956/0.175049loss in batch 203: 0.167831/0.175003loss in batch 204: 0.0207367/0.174255loss in batch 205: 0.00836182/0.173447loss in batch 206: 0.0154266/0.172684loss in batch 207: 0.114517/0.172409loss in batch 208: 0.0574951/0.171844loss in batch 209: 0.0352173/0.171204loss in batch 210: 0.106354/0.170898loss in batch 211: 0.0581512/0.170364loss in batch 212: 0.0254364/0.169693
done with epoch 13
train_acc: 0.943662 (402/426)
test loss: 0.0254363
acc: 0.944054 (135/143)
loss in batch 0: 0.0379791/0.0379791loss in batch 1: 0.123718/0.0808411loss in batch 2: 0.0232239/0.0616455loss in batch 3: 0.0419769/0.0567169loss in batch 4: 0.0351715/0.0524139loss in batch 5: 0.0728912/0.0558167loss in batch 6: 0.594696/0.132797loss in batch 7: 0.0672607/0.124619loss in batch 8: 0.00392151/0.111206loss in batch 9: 0.336868/0.133774loss in batch 10: 0.495361/0.166641loss in batch 11: 0.0175934/0.154221loss in batch 12: 0.194138/0.157288loss in batch 13: 0.0592651/0.150284loss in batch 14: 0.274506/0.158569loss in batch 15: 0.0502014/0.151794loss in batch 16: 0.28009/0.159348loss in batch 17: 0.392349/0.172287loss in batch 18: 0.0809021/0.16748loss in batch 19: 0.0197754/0.160095loss in batch 20: 0.257401/0.164734loss in batch 21: 0.0117188/0.157776loss in batch 22: 0.0214081/0.15184loss in batch 23: 0.289169/0.157578loss in batch 24: 1.08208/0.19455loss in batch 25: 0.0599976/0.189377loss in batch 26: 0.292664/0.193192loss in batch 27: 0.0658264/0.188644loss in batch 28: 0.0444641/0.183685loss in batch 29: 0.384506/0.190369loss in batch 30: 0.270096/0.192932loss in batch 31: 0.682373/0.208237loss in batch 32: 0.0374603/0.203064loss in batch 33: 0.233261/0.203949loss in batch 34: 0.0609131/0.19986loss in batch 35: 0.0205231/0.194885loss in batch 36: 0.271072/0.196945loss in batch 37: 0.126038/0.195084loss in batch 38: 0.0991821/0.192612loss in batch 39: 0.0680237/0.189499loss in batch 40: 0.0226898/0.185425loss in batch 41: 0.0413818/0.182007loss in batch 42: 0.0797882/0.179626loss in batch 43: 0.310532/0.182602loss in batch 44: 0.0172272/0.178925loss in batch 45: 0.495956/0.185822loss in batch 46: 0.527786/0.193085loss in batch 47: 0.24765/0.194229loss in batch 48: 1.02769/0.211243loss in batch 49: 0.0264893/0.207535loss in batch 50: 0.735016/0.21788loss in batch 51: 0.0644073/0.214935loss in batch 52: 0.536087/0.220993loss in batch 53: 0.0492096/0.217804loss in batch 54: 0.186676/0.217255loss in batch 55: 0.0955048/0.215073loss in batch 56: 0.028183/0.211792loss in batch 57: 0.137161/0.21051loss in batch 58: 0.118637/0.208954loss in batch 59: 0.0620575/0.206497loss in batch 60: 0.203049/0.206451loss in batch 61: 0.00453186/0.203186loss in batch 62: 0.0141754/0.20018loss in batch 63: 0.172241/0.199753loss in batch 64: 0.182251/0.199478loss in batch 65: 0.0570068/0.197327loss in batch 66: 0.0611572/0.195297loss in batch 67: 0.0529175/0.193192loss in batch 68: 0.0810699/0.191574loss in batch 69: 0.179382/0.191406loss in batch 70: 0.00740051/0.188812loss in batch 71: 0.0285645/0.186584loss in batch 72: 0.464035/0.190369loss in batch 73: 0.567947/0.19548loss in batch 74: 0.041275/0.193436loss in batch 75: 0.00476074/0.190948loss in batch 76: 0.0787964/0.189499loss in batch 77: 0.139252/0.188843loss in batch 78: 0.0443878/0.187012loss in batch 79: 0.251846/0.18782loss in batch 80: 0.0302734/0.185883loss in batch 81: 0.0583191/0.184326loss in batch 82: 0.176468/0.184235loss in batch 83: 0.0304871/0.182404loss in batch 84: 0.387405/0.184814loss in batch 85: 0.460754/0.188034loss in batch 86: 0.0472565/0.186417loss in batch 87: 0.0516205/0.184875loss in batch 88: 0.0894165/0.183807loss in batch 89: 0.0131073/0.1819loss in batch 90: 0.0549927/0.180511loss in batch 91: 0.109039/0.179733loss in batch 92: 0.0662842/0.178513loss in batch 93: 0.286667/0.179657loss in batch 94: 0.0274811/0.178055loss in batch 95: 0.0448151/0.176666loss in batch 96: 0.0285034/0.175156loss in batch 97: 0.457382/0.178024loss in batch 98: 0.04953/0.176727loss in batch 99: 0.0484314/0.175446loss in batch 100: 0.186172/0.175537loss in batch 101: 0.0330811/0.174149loss in batch 102: 0.0914764/0.173355loss in batch 103: 0.344131/0.175003loss in batch 104: 0.0586243/0.173889loss in batch 105: 0.0288086/0.172516loss in batch 106: 0.0611572/0.171478loss in batch 107: 0.0375519/0.170242loss in batch 108: 0.0163879/0.168839loss in batch 109: 0.361282/0.170578loss in batch 110: 0.225876/0.171082loss in batch 111: 0.0111084/0.169647loss in batch 112: 0.0789337/0.168839loss in batch 113: 0.0271149/0.167603loss in batch 114: 0.0562286/0.166626loss in batch 115: 0.0742493/0.165833loss in batch 116: 0.0577545/0.164917loss in batch 117: 0.758514/0.169937loss in batch 118: 0.110153/0.169449loss in batch 119: 0.0119476/0.168137loss in batch 120: 0.0359497/0.167038loss in batch 121: 0.00721741/0.165726loss in batch 122: 0.0252838/0.164581loss in batch 123: 0.124924/0.164276loss in batch 124: 0.326126/0.165558loss in batch 125: 0.213028/0.165939loss in batch 126: 0.216873/0.166336loss in batch 127: 0.00878906/0.1651loss in batch 128: 0.108551/0.164673loss in batch 129: 0.244675/0.165283loss in batch 130: 0.120331/0.164932loss in batch 131: 0.109314/0.16452loss in batch 132: 0.0584869/0.163712loss in batch 133: 0.951462/0.169601loss in batch 134: 0.0562592/0.168747loss in batch 135: 0.145752/0.168579loss in batch 136: 0.163513/0.168549loss in batch 137: 0.216629/0.1689loss in batch 138: 0.1633/0.168854loss in batch 139: 0.179214/0.168945loss in batch 140: 0.0704041/0.168243loss in batch 141: 0.269775/0.168945loss in batch 142: 0.014328/0.167862loss in batch 143: 0.0493927/0.167053loss in batch 144: 0.0271301/0.166077loss in batch 145: 0.0992279/0.165619loss in batch 146: 0.0388794/0.164764loss in batch 147: 1.37572/0.172943loss in batch 148: 0.0532532/0.172134loss in batch 149: 0.447021/0.173981loss in batch 150: 0.124786/0.173645loss in batch 151: 0.439133/0.175385loss in batch 152: 0.0357208/0.174484loss in batch 153: 0.123901/0.174149loss in batch 154: 0.00817871/0.17308loss in batch 155: 0.0125885/0.172058loss in batch 156: 0.127136/0.171753loss in batch 157: 0.0253296/0.170837loss in batch 158: 0.234238/0.171234loss in batch 159: 0.0498505/0.170471loss in batch 160: 0.00244141/0.169449loss in batch 161: 0.00650024/0.168427loss in batch 162: 0.130737/0.168198loss in batch 163: 0.0342102/0.167389loss in batch 164: 0.355179/0.168518loss in batch 165: 0.316513/0.169403loss in batch 166: 0.0425415/0.168655loss in batch 167: 0.542679/0.170883loss in batch 168: 0.199509/0.171051loss in batch 169: 0.0607605/0.17041loss in batch 170: 0.188187/0.170502loss in batch 171: 0.0625/0.169876loss in batch 172: 0.13559/0.169678loss in batch 173: 0.108551/0.169327loss in batch 174: 0.0241547/0.168488loss in batch 175: 0.063736/0.167908loss in batch 176: 0.039978/0.167175loss in batch 177: 0.185898/0.167282loss in batch 178: 0.462952/0.168945loss in batch 179: 0.436508/0.170425loss in batch 180: 0.0861359/0.169952loss in batch 181: 0.0157928/0.169113loss in batch 182: 0.00994873/0.168243loss in batch 183: 0.269043/0.168777loss in batch 184: 0.114624/0.168503loss in batch 185: 0.0716858/0.167984loss in batch 186: 0.0474243/0.167328loss in batch 187: 0.539886/0.169312loss in batch 188: 0.00325012/0.168427loss in batch 189: 0.0992737/0.168076loss in batch 190: 0.104416/0.16774loss in batch 191: 0.418518/0.169037loss in batch 192: 0.0669556/0.168503loss in batch 193: 0.0247498/0.16777loss in batch 194: 0.333359/0.168625loss in batch 195: 0.0927277/0.168228loss in batch 196: 0.109833/0.167938loss in batch 197: 0.125717/0.167725loss in batch 198: 0.046875/0.167114loss in batch 199: 0.32753/0.167923loss in batch 200: 0.227524/0.168213loss in batch 201: 0.0883026/0.167816loss in batch 202: 0.0474548/0.167221loss in batch 203: 0.0177765/0.166504loss in batch 204: 0.0646973/0.165985loss in batch 205: 0.243546/0.166382loss in batch 206: 0.0378113/0.165756loss in batch 207: 0.0678558/0.165283loss in batch 208: 0.192703/0.165421loss in batch 209: 0.447357/0.166763loss in batch 210: 0.0993652/0.166443loss in batch 211: 0.0444946/0.165863loss in batch 212: 0.16481/0.165848
done with epoch 14
train_acc: 0.950704 (405/426)
test loss: 0.16481
acc: 0.944054 (135/143)
loss in batch 0: 0.235947/0.235947loss in batch 1: 0.24791/0.241928loss in batch 2: 0.0429688/0.175598loss in batch 3: 0.130524/0.164337loss in batch 4: 0.0160217/0.134674loss in batch 5: 0.00621033/0.113251loss in batch 6: 0.0303802/0.101425loss in batch 7: 0.396774/0.138336loss in batch 8: 0.0651855/0.130219loss in batch 9: 0.303879/0.147583loss in batch 10: 0.204453/0.15274loss in batch 11: 0.0384827/0.143234loss in batch 12: 0.042923/0.135513loss in batch 13: 0.0171356/0.12706loss in batch 14: 0.605072/0.15892loss in batch 15: 0.0615234/0.152832loss in batch 16: 0.337662/0.163712loss in batch 17: 0.0354004/0.156586loss in batch 18: 0.0417786/0.150543loss in batch 19: 0.0497437/0.145493loss in batch 20: 0.00526428/0.138824loss in batch 21: 0.364243/0.149063loss in batch 22: 0.0799408/0.146057loss in batch 23: 0.00881958/0.140335loss in batch 24: 0.143936/0.140488loss in batch 25: 0.664383/0.160645loss in batch 26: 0.0679779/0.157196loss in batch 27: 0.328125/0.163315loss in batch 28: 0.0509338/0.159439loss in batch 29: 0.549698/0.17244loss in batch 30: 0.127884/0.171005loss in batch 31: 0.430313/0.179108loss in batch 32: 0.181381/0.179184loss in batch 33: 0.0486908/0.175339loss in batch 34: 0.0416107/0.171524loss in batch 35: 0.105667/0.169693loss in batch 36: 0.110611/0.168106loss in batch 37: 0.0365601/0.164627loss in batch 38: 0.171356/0.16481loss in batch 39: 0.29007/0.167938loss in batch 40: 0.0236053/0.164413loss in batch 41: 0.123932/0.163452loss in batch 42: 0.028244/0.160309loss in batch 43: 0.454208/0.166977loss in batch 44: 0.0487671/0.164368loss in batch 45: 0.0878296/0.162689loss in batch 46: 0.065506/0.160629loss in batch 47: 0.0720825/0.158798loss in batch 48: 0.0231476/0.156021loss in batch 49: 0.0512085/0.153931loss in batch 50: 0.575928/0.162186loss in batch 51: 0.0422516/0.159897loss in batch 52: 0.316681/0.162857loss in batch 53: 0.478073/0.168686loss in batch 54: 0.0779114/0.167023loss in batch 55: 0.135696/0.166473loss in batch 56: 0.120911/0.165665loss in batch 57: 0.136292/0.165161loss in batch 58: 0.0318909/0.162903loss in batch 59: 0.0391846/0.160858loss in batch 60: 0.0213623/0.158554loss in batch 61: 0.0124817/0.156204loss in batch 62: 0.370087/0.159607loss in batch 63: 0.0579224/0.158005loss in batch 64: 0.00662231/0.15567loss in batch 65: 0.192841/0.15625loss in batch 66: 0.0252991/0.154297loss in batch 67: 0.0253296/0.15239loss in batch 68: 0.139709/0.152206loss in batch 69: 0.0539093/0.150803loss in batch 70: 0.0186005/0.148941loss in batch 71: 0.0504456/0.147568loss in batch 72: 0.474731/0.152054loss in batch 73: 0.357376/0.154831loss in batch 74: 0.184036/0.155212loss in batch 75: 0.0806122/0.154236loss in batch 76: 0.237427/0.155319loss in batch 77: 0.114975/0.1548loss in batch 78: 0.138397/0.154602loss in batch 79: 0.362274/0.157196loss in batch 80: 0.237946/0.158188loss in batch 81: 0.0288849/0.156616loss in batch 82: 0.0505371/0.155334loss in batch 83: 0.0231476/0.153748loss in batch 84: 0.00933838/0.152054loss in batch 85: 0.0171967/0.150497loss in batch 86: 0.0610352/0.14946loss in batch 87: 0.0254669/0.148041loss in batch 88: 0.0518646/0.146973loss in batch 89: 0.0507813/0.145905loss in batch 90: 0.555573/0.150406loss in batch 91: 0.0866547/0.149719loss in batch 92: 0.0637665/0.148804loss in batch 93: 0.150513/0.148819loss in batch 94: 0.0604553/0.147873loss in batch 95: 0.065033/0.147003loss in batch 96: 0.278183/0.148361loss in batch 97: 0.318405/0.150101loss in batch 98: 0.310379/0.151718loss in batch 99: 0.00688171/0.150269loss in batch 100: 0.6418/0.155151loss in batch 101: 0.171982/0.155304loss in batch 102: 0.0110168/0.1539loss in batch 103: 0.00823975/0.152496loss in batch 104: 0.152954/0.152496loss in batch 105: 0.0509338/0.15155loss in batch 106: 0.023056/0.150345loss in batch 107: 0.0419464/0.149338loss in batch 108: 0.111267/0.149002loss in batch 109: 0.0435181/0.148041loss in batch 110: 0.0439148/0.147095loss in batch 111: 0.267456/0.148163loss in batch 112: 0.151199/0.148193loss in batch 113: 0.0330963/0.147202loss in batch 114: 0.0337219/0.14621loss in batch 115: 0.182877/0.146515loss in batch 116: 0.077774/0.14592loss in batch 117: 1.17741/0.154678loss in batch 118: 0.352432/0.156326loss in batch 119: 0.0113983/0.155121loss in batch 120: 0.0108795/0.153946loss in batch 121: 0.00776672/0.152725loss in batch 122: 0.0357819/0.151779loss in batch 123: 0.113571/0.151489loss in batch 124: 0.0447083/0.15062loss in batch 125: 0.0431366/0.149765loss in batch 126: 0.0375366/0.148895loss in batch 127: 0.0785522/0.148331loss in batch 128: 0.0414276/0.147522loss in batch 129: 0.810104/0.152603loss in batch 130: 0.00799561/0.151489loss in batch 131: 0.0987854/0.151108loss in batch 132: 0.357147/0.152649loss in batch 133: 0.0635223/0.151993loss in batch 134: 0.0895538/0.15152loss in batch 135: 0.502029/0.154099loss in batch 136: 0.209045/0.15451loss in batch 137: 0.0403442/0.153671loss in batch 138: 0.403152/0.155457loss in batch 139: 0.0479736/0.154694loss in batch 140: 0.0514221/0.153961loss in batch 141: 0.108353/0.153641loss in batch 142: 0.332962/0.154907loss in batch 143: 0.106277/0.154556loss in batch 144: 0.019455/0.153641loss in batch 145: 0.0527802/0.152954loss in batch 146: 0.0513153/0.152237loss in batch 147: 0.0175781/0.151337loss in batch 148: 0.0628815/0.150757loss in batch 149: 0.100006/0.150406loss in batch 150: 1.01599/0.156143loss in batch 151: 0.291122/0.157028loss in batch 152: 0.45636/0.158981loss in batch 153: 0.0220337/0.158096loss in batch 154: 0.71524/0.161682loss in batch 155: 0.404495/0.163254loss in batch 156: 0.0696106/0.162659loss in batch 157: 0.0327301/0.161835loss in batch 158: 0.313202/0.162781loss in batch 159: 0.0974884/0.162384loss in batch 160: 0.0350342/0.161575loss in batch 161: 0.245743/0.162094loss in batch 162: 0.0578613/0.161469loss in batch 163: 0.10199/0.161102loss in batch 164: 0.0895386/0.16066loss in batch 165: 0.446594/0.162384loss in batch 166: 0.327866/0.163376loss in batch 167: 0.0730591/0.162842loss in batch 168: 0.0187378/0.162003loss in batch 169: 0.214539/0.162292loss in batch 170: 0.0865479/0.16185loss in batch 171: 0.026413/0.161072loss in batch 172: 0.122421/0.160843loss in batch 173: 0.0524445/0.160217loss in batch 174: 0.111572/0.159943loss in batch 175: 0.128815/0.15976loss in batch 176: 0.134689/0.159622loss in batch 177: 0.391281/0.160919loss in batch 178: 0.245178/0.161407loss in batch 179: 0.0251617/0.160645loss in batch 180: 0.131363/0.160477loss in batch 181: 0.252182/0.160995loss in batch 182: 0.0197906/0.160217loss in batch 183: 0.07724/0.15976loss in batch 184: 0.336044/0.160706loss in batch 185: 0.0529633/0.160126loss in batch 186: 0.0587769/0.159592loss in batch 187: 0.00979614/0.158798loss in batch 188: 0.434204/0.160248loss in batch 189: 0.0864868/0.159866loss in batch 190: 0.127396/0.159698loss in batch 191: 0.601501/0.161987loss in batch 192: 0.214523/0.162277loss in batch 193: 0.0527649/0.161697loss in batch 194: 0.0646057/0.161209loss in batch 195: 0.283417/0.161835loss in batch 196: 0.0988617/0.161499loss in batch 197: 0.105881/0.16124loss in batch 198: 0.0260925/0.160538loss in batch 199: 0.0489197/0.159988loss in batch 200: 0.248199/0.160431loss in batch 201: 0.0227509/0.159744loss in batch 202: 0.0332489/0.159119loss in batch 203: 0.154785/0.159103loss in batch 204: 0.0402222/0.158524loss in batch 205: 0.0486298/0.15799loss in batch 206: 0.0713348/0.157578loss in batch 207: 0.0102844/0.156876loss in batch 208: 0.0258331/0.156235loss in batch 209: 0.0615997/0.155777loss in batch 210: 0.374542/0.15683loss in batch 211: 1.47252/0.16304loss in batch 212: 0.0782318/0.162628
done with epoch 15
train_acc: 0.946009 (403/426)
test loss: 0.0782316
acc: 0.944054 (135/143)
loss in batch 0: 0.0521088/0.0521088loss in batch 1: 0.029953/0.0410309loss in batch 2: 0.117279/0.066452loss in batch 3: 0.0241089/0.0558624loss in batch 4: 0.0368652/0.052063loss in batch 5: 0.0692902/0.0549316loss in batch 6: 0.065506/0.0564423loss in batch 7: 0.0115814/0.050827loss in batch 8: 0.156006/0.0625153loss in batch 9: 0.0214539/0.0584106loss in batch 10: 0.404358/0.0898743loss in batch 11: 0.229858/0.101532loss in batch 12: 0.0952911/0.101044loss in batch 13: 0.0522003/0.0975647loss in batch 14: 0.0367432/0.0935059loss in batch 15: 0.0652008/0.0917358loss in batch 16: 0.00561523/0.0866699loss in batch 17: 0.124756/0.0887756loss in batch 18: 0.0773926/0.0881805loss in batch 19: 0.132629/0.0904083loss in batch 20: 0.00617981/0.0863953loss in batch 21: 0.231155/0.0929871loss in batch 22: 0.094986/0.0930634loss in batch 23: 0.316727/0.102386loss in batch 24: 0.057251/0.100571loss in batch 25: 0.240601/0.105972loss in batch 26: 0.0162964/0.102646loss in batch 27: 0.142014/0.10405loss in batch 28: 0.0278015/0.101425loss in batch 29: 0.474258/0.113846loss in batch 30: 0.612213/0.129929loss in batch 31: 0.0807648/0.128387loss in batch 32: 0.310028/0.133896loss in batch 33: 0.0594025/0.131699loss in batch 34: 0.0586548/0.129608loss in batch 35: 0.255829/0.133118loss in batch 36: 0.08638/0.131851loss in batch 37: 0.0388031/0.12941loss in batch 38: 0.0292816/0.126846loss in batch 39: 0.299683/0.131165loss in batch 40: 0.01297/0.128281loss in batch 41: 0.0899811/0.127365loss in batch 42: 0.122162/0.127258loss in batch 43: 0.0389404/0.125244loss in batch 44: 0.0659332/0.123917loss in batch 45: 0.025116/0.12178loss in batch 46: 0.474167/0.129272loss in batch 47: 0.0632935/0.127899loss in batch 48: 0.204788/0.129471loss in batch 49: 0.0200348/0.127289loss in batch 50: 0.0509949/0.125778loss in batch 51: 0.102859/0.125336loss in batch 52: 0.371841/0.130005loss in batch 53: 0.0683289/0.128845loss in batch 54: 0.00126648/0.126526