Using statistical security parameter 40
Trying to run 64-bit computation
Current working directory: "/root/MP-SPDZ"
Current working directory: "/root/MP-SPDZ"
Using SGD
done with epoch 0done with epoch 1done with epoch 2done with epoch 3done with epoch 4done with epoch 5done with epoch 6done with epoch 7done with epoch 8done with epoch 9done with epoch 10done with epoch 11done with epoch 12done with epoch 13done with epoch 14done with epoch 15done with epoch 16done with epoch 17done with epoch 18done with epoch 19
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Using SGD
loss in batch 0: 0.648178/0.648178loss in batch 1: 0.732391/0.690277loss in batch 2: 0.555618/0.645401loss in batch 3: 0.640945/0.644287loss in batch 4: 0.60643/0.636719loss in batch 5: 0.674988/0.643097loss in batch 6: 0.627991/0.64093loss in batch 7: 0.579163/0.633209loss in batch 8: 0.623734/0.632156loss in batch 9: 0.602036/0.62915loss in batch 10: 0.640778/0.630203loss in batch 11: 0.667084/0.633286loss in batch 12: 0.565231/0.628052loss in batch 13: 0.589554/0.62529loss in batch 14: 0.807281/0.637436loss in batch 15: 0.952011/0.657089loss in batch 16: 0.518005/0.648911loss in batch 17: 0.597977/0.646072loss in batch 18: 0.480667/0.637375loss in batch 19: 0.639267/0.637466loss in batch 20: 0.501968/0.631012loss in batch 21: 0.480896/0.624176loss in batch 22: 0.723755/0.628525loss in batch 23: 0.662369/0.629929loss in batch 24: 0.590958/0.628372loss in batch 25: 0.441849/0.621201loss in batch 26: 0.500031/0.616699loss in batch 27: 0.687256/0.619232loss in batch 28: 0.773682/0.624557loss in batch 29: 0.571228/0.622772loss in batch 30: 0.428192/0.616501loss in batch 31: 0.76886/0.621262loss in batch 32: 0.629913/0.621521loss in batch 33: 0.901016/0.629745loss in batch 34: 0.555206/0.627609loss in batch 35: 0.614365/0.627243loss in batch 36: 0.665298/0.628281loss in batch 37: 0.490448/0.624649loss in batch 38: 0.559052/0.622971loss in batch 39: 0.701157/0.624924loss in batch 40: 0.55777/0.623276loss in batch 41: 0.564178/0.621872loss in batch 42: 0.608368/0.621552loss in batch 43: 0.634552/0.621857loss in batch 44: 0.48259/0.618759loss in batch 45: 0.526794/0.61676loss in batch 46: 0.659134/0.617661loss in batch 47: 0.553528/0.616333loss in batch 48: 0.652344/0.617065loss in batch 49: 0.72406/0.619202loss in batch 50: 0.659058/0.619995loss in batch 51: 0.51474/0.617966loss in batch 52: 0.547409/0.616623loss in batch 53: 0.507797/0.614609loss in batch 54: 0.627258/0.614838loss in batch 55: 0.376221/0.61058loss in batch 56: 0.459564/0.607925loss in batch 57: 0.536163/0.606705loss in batch 58: 0.317764/0.601807loss in batch 59: 0.763214/0.604492loss in batch 60: 0.928909/0.609802loss in batch 61: 0.353638/0.605667loss in batch 62: 0.394043/0.602325loss in batch 63: 0.514343/0.600937loss in batch 64: 0.528229/0.599823loss in batch 65: 0.52272/0.598648loss in batch 66: 0.591934/0.598557loss in batch 67: 0.65744/0.599411loss in batch 68: 0.668991/0.600433loss in batch 69: 0.397095/0.597534loss in batch 70: 0.497757/0.596115loss in batch 71: 0.73822/0.598099loss in batch 72: 0.494247/0.59668loss in batch 73: 0.437439/0.594513loss in batch 74: 0.536148/0.593735loss in batch 75: 0.508911/0.592621loss in batch 76: 0.51178/0.591568loss in batch 77: 0.475525/0.590073loss in batch 78: 0.636948/0.590683loss in batch 79: 0.605804/0.590866loss in batch 80: 0.516296/0.589951loss in batch 81: 0.542618/0.589371loss in batch 82: 0.561325/0.589035loss in batch 83: 0.284348/0.585403loss in batch 84: 0.526077/0.584717loss in batch 85: 0.537811/0.584152loss in batch 86: 0.488419/0.583069loss in batch 87: 0.282806/0.579651loss in batch 88: 0.392288/0.57753loss in batch 89: 0.529663/0.577011loss in batch 90: 0.47789/0.575912loss in batch 91: 0.498245/0.575073loss in batch 92: 0.471939/0.573975loss in batch 93: 0.463821/0.5728loss in batch 94: 0.369156/0.570648loss in batch 95: 0.609573/0.57106loss in batch 96: 0.407639/0.569382loss in batch 97: 0.528/0.568954loss in batch 98: 0.364807/0.566895loss in batch 99: 0.427612/0.565491loss in batch 100: 0.446701/0.564331loss in batch 101: 0.516479/0.563843loss in batch 102: 0.5495/0.563721loss in batch 103: 0.591187/0.56398loss in batch 104: 0.350037/0.561935loss in batch 105: 0.508499/0.561447loss in batch 106: 0.489212/0.56076loss in batch 107: 0.572983/0.560867loss in batch 108: 0.462265/0.559967loss in batch 109: 0.497269/0.559402loss in batch 110: 0.390564/0.557877loss in batch 111: 0.609619/0.558334loss in batch 112: 0.509064/0.557907loss in batch 113: 0.425064/0.556747loss in batch 114: 0.356064/0.554993loss in batch 115: 0.636398/0.555695loss in batch 116: 0.329437/0.553757loss in batch 117: 0.37413/0.552231loss in batch 118: 0.453674/0.551407loss in batch 119: 0.390961/0.550079loss in batch 120: 0.346725/0.548386loss in batch 121: 0.575546/0.548615loss in batch 122: 0.5/0.548218loss in batch 123: 0.722153/0.549622loss in batch 124: 0.442078/0.548767loss in batch 125: 0.627579/0.549393loss in batch 126: 0.372681/0.547989loss in batch 127: 0.276215/0.545883loss in batch 128: 0.503647/0.545547loss in batch 129: 0.61377/0.546082loss in batch 130: 0.43959/0.545258loss in batch 131: 0.443024/0.544479loss in batch 132: 0.417297/0.543518loss in batch 133: 0.620697/0.544098loss in batch 134: 0.535919/0.544037loss in batch 135: 0.519379/0.543854loss in batch 136: 0.460403/0.543243loss in batch 137: 0.57547/0.543488loss in batch 138: 0.459122/0.542877loss in batch 139: 0.398422/0.541855loss in batch 140: 0.426331/0.541016loss in batch 141: 0.65033/0.541794loss in batch 142: 0.38147/0.54068loss in batch 143: 0.580917/0.540955loss in batch 144: 0.232605/0.538834loss in batch 145: 0.37323/0.537704loss in batch 146: 0.639633/0.538391loss in batch 147: 0.424271/0.537613loss in batch 148: 0.345642/0.536331loss in batch 149: 0.269058/0.534546loss in batch 150: 0.806305/0.536346loss in batch 151: 0.369781/0.535248loss in batch 152: 0.403931/0.534393loss in batch 153: 0.300903/0.532883loss in batch 154: 0.560715/0.533051loss in batch 155: 0.261841/0.531311loss in batch 156: 0.49379/0.531082loss in batch 157: 0.673233/0.531982loss in batch 158: 0.214218/0.529984loss in batch 159: 0.512238/0.529877loss in batch 160: 0.334335/0.528656loss in batch 161: 0.281403/0.527115loss in batch 162: 0.277374/0.525589loss in batch 163: 0.474274/0.525269loss in batch 164: 0.385574/0.524429loss in batch 165: 0.311462/0.523148loss in batch 166: 0.178207/0.521072loss in batch 167: 0.700974/0.522156loss in batch 168: 0.286545/0.520767loss in batch 169: 0.295761/0.51944loss in batch 170: 0.402573/0.518753loss in batch 171: 0.237213/0.51712loss in batch 172: 0.189209/0.515228loss in batch 173: 0.245422/0.513672loss in batch 174: 0.149414/0.511597loss in batch 175: 0.257614/0.510162loss in batch 176: 0.429169/0.509689loss in batch 177: 0.317398/0.508606loss in batch 178: 0.245346/0.507141loss in batch 179: 1.12453/0.510574loss in batch 180: 0.829178/0.512329loss in batch 181: 0.18013/0.510498loss in batch 182: 0.143982/0.508499loss in batch 183: 0.683731/0.509445loss in batch 184: 0.379868/0.508743loss in batch 185: 0.427109/0.508316loss in batch 186: 0.145493/0.506363loss in batch 187: 0.388611/0.505753loss in batch 188: 0.179306/0.504013loss in batch 189: 0.201584/0.502426loss in batch 190: 0.167496/0.500687loss in batch 191: 0.861359/0.502548loss in batch 192: 1.23825/0.506363loss in batch 193: 0.171692/0.504654loss in batch 194: 0.311035/0.503647loss in batch 195: 0.178467/0.501984loss in batch 196: 1.23465/0.505707loss in batch 197: 0.499664/0.505692loss in batch 198: 0.190475/0.504105loss in batch 199: 0.225189/0.502701loss in batch 200: 0.553711/0.50296loss in batch 201: 0.220352/0.501556loss in batch 202: 0.319336/0.500656loss in batch 203: 0.432877/0.50032loss in batch 204: 0.427963/0.499969loss in batch 205: 0.449402/0.499725loss in batch 206: 0.31105/0.498825loss in batch 207: 0.342896/0.498062loss in batch 208: 0.272766/0.496979loss in batch 209: 0.455048/0.496796loss in batch 210: 0.429352/0.49646loss in batch 211: 0.392212/0.495972loss in batch 212: 0.557709/0.496262
done with epoch 0
train_acc: 0.835681 (356/426)
test loss: 0.557708
acc: 0.923075 (132/143)
loss in batch 0: 0.588821/0.588821loss in batch 1: 0.411987/0.500397loss in batch 2: 0.500778/0.500534loss in batch 3: 0.419342/0.48024loss in batch 4: 0.433838/0.470947loss in batch 5: 0.589645/0.490738loss in batch 6: 0.320267/0.466385loss in batch 7: 0.376526/0.455154loss in batch 8: 0.391357/0.448059loss in batch 9: 0.331207/0.436386loss in batch 10: 0.178772/0.412964loss in batch 11: 0.455002/0.416473loss in batch 12: 0.447525/0.418854loss in batch 13: 0.434875/0.419998loss in batch 14: 0.303024/0.412186loss in batch 15: 0.179153/0.397629loss in batch 16: 0.198502/0.38591loss in batch 17: 0.31871/0.382187loss in batch 18: 0.329132/0.379395loss in batch 19: 0.22757/0.371796loss in batch 20: 0.146637/0.361084loss in batch 21: 0.351624/0.360641loss in batch 22: 0.72525/0.376495loss in batch 23: 0.503052/0.381775loss in batch 24: 0.268219/0.377228loss in batch 25: 0.746231/0.391418loss in batch 26: 0.27449/0.3871loss in batch 27: 0.20137/0.380463loss in batch 28: 0.472397/0.383636loss in batch 29: 0.310028/0.38118loss in batch 30: 0.381241/0.38118loss in batch 31: 0.594757/0.387848loss in batch 32: 0.221695/0.382828loss in batch 33: 0.345032/0.381714loss in batch 34: 0.502594/0.385162loss in batch 35: 0.813812/0.397064loss in batch 36: 0.263428/0.393463loss in batch 37: 0.232773/0.389221loss in batch 38: 0.318451/0.387405loss in batch 39: 0.312531/0.385544loss in batch 40: 0.255936/0.382385loss in batch 41: 0.548309/0.386322loss in batch 42: 0.338593/0.385223loss in batch 43: 0.68692/0.392075loss in batch 44: 0.252136/0.388962loss in batch 45: 0.566055/0.392822loss in batch 46: 0.347382/0.391846loss in batch 47: 0.265686/0.389221loss in batch 48: 0.436996/0.390198loss in batch 49: 0.274307/0.387878loss in batch 50: 0.217209/0.384521loss in batch 51: 0.21907/0.381348loss in batch 52: 0.207581/0.378082loss in batch 53: 0.205383/0.374878loss in batch 54: 0.214294/0.371964loss in batch 55: 0.29895/0.370651loss in batch 56: 0.459351/0.372208loss in batch 57: 0.59967/0.376129loss in batch 58: 0.358429/0.375824loss in batch 59: 0.465149/0.377319loss in batch 60: 0.371063/0.377213loss in batch 61: 0.158432/0.373688loss in batch 62: 0.162155/0.370331loss in batch 63: 0.267548/0.368729loss in batch 64: 0.11235/0.364777loss in batch 65: 0.328461/0.364227loss in batch 66: 0.405319/0.364853loss in batch 67: 0.340088/0.364471loss in batch 68: 0.136734/0.361176loss in batch 69: 0.629837/0.365021loss in batch 70: 0.6604/0.369171loss in batch 71: 0.307419/0.368317loss in batch 72: 0.248764/0.366684loss in batch 73: 0.230392/0.364838loss in batch 74: 0.230667/0.363052loss in batch 75: 0.163513/0.360428loss in batch 76: 0.360992/0.360428loss in batch 77: 0.321136/0.359924loss in batch 78: 0.222733/0.358185loss in batch 79: 0.294037/0.357391loss in batch 80: 0.498245/0.359131loss in batch 81: 0.560547/0.361588loss in batch 82: 0.200745/0.35965loss in batch 83: 0.663651/0.363266loss in batch 84: 0.23848/0.361786loss in batch 85: 0.270538/0.360733loss in batch 86: 0.693756/0.364563loss in batch 87: 0.28743/0.363678loss in batch 88: 0.147675/0.361267loss in batch 89: 0.371323/0.361374loss in batch 90: 0.484222/0.362732loss in batch 91: 0.333725/0.362411loss in batch 92: 0.248474/0.361176loss in batch 93: 0.259781/0.360092loss in batch 94: 0.406708/0.360596loss in batch 95: 0.158112/0.35849loss in batch 96: 0.603455/0.361008loss in batch 97: 0.232864/0.359711loss in batch 98: 0.246643/0.358566loss in batch 99: 0.330917/0.358292loss in batch 100: 0.54425/0.360123loss in batch 101: 0.510376/0.361603loss in batch 102: 0.829514/0.366135loss in batch 103: 0.709045/0.369446loss in batch 104: 0.491272/0.370605loss in batch 105: 0.289856/0.369827loss in batch 106: 0.211029/0.368347loss in batch 107: 0.728149/0.371689loss in batch 108: 0.710403/0.374786loss in batch 109: 0.365341/0.374695loss in batch 110: 0.360611/0.374573loss in batch 111: 0.381958/0.374649loss in batch 112: 0.29274/0.373917loss in batch 113: 0.196838/0.37236loss in batch 114: 0.163879/0.370544loss in batch 115: 0.443359/0.371185loss in batch 116: 0.344849/0.370956loss in batch 117: 0.503494/0.37207loss in batch 118: 0.325775/0.371689loss in batch 119: 0.230789/0.370514loss in batch 120: 0.558929/0.37207loss in batch 121: 0.299591/0.371475loss in batch 122: 0.249268/0.370483loss in batch 123: 0.2771/0.369736loss in batch 124: 0.712967/0.372467loss in batch 125: 0.362854/0.372406loss in batch 126: 0.470795/0.373169loss in batch 127: 0.225677/0.372025loss in batch 128: 0.135971/0.370193loss in batch 129: 0.219482/0.369034loss in batch 130: 0.142868/0.36731loss in batch 131: 0.32338/0.366974loss in batch 132: 0.288727/0.366394loss in batch 133: 0.284821/0.365784loss in batch 134: 0.160461/0.364258loss in batch 135: 0.347412/0.364136loss in batch 136: 0.38678/0.364304loss in batch 137: 0.25827/0.363525loss in batch 138: 0.1987/0.362335loss in batch 139: 0.467178/0.363098loss in batch 140: 0.473099/0.363876loss in batch 141: 0.630447/0.365753loss in batch 142: 0.230392/0.364807loss in batch 143: 0.219269/0.3638loss in batch 144: 0.110077/0.362045loss in batch 145: 0.588837/0.363586loss in batch 146: 0.341736/0.363449loss in batch 147: 0.152374/0.362015loss in batch 148: 0.156479/0.360641loss in batch 149: 0.207718/0.359619loss in batch 150: 0.164185/0.358337loss in batch 151: 0.32164/0.358093loss in batch 152: 0.273224/0.357544loss in batch 153: 0.52536/0.358627loss in batch 154: 0.488586/0.359451loss in batch 155: 0.19014/0.358383loss in batch 156: 0.512756/0.35936loss in batch 157: 0.173325/0.35817loss in batch 158: 0.4245/0.358597loss in batch 159: 0.43071/0.359039loss in batch 160: 0.176483/0.35791loss in batch 161: 0.398926/0.358154loss in batch 162: 0.774872/0.360733loss in batch 163: 0.290207/0.360291loss in batch 164: 0.575668/0.361603loss in batch 165: 0.0870819/0.35994loss in batch 166: 0.170639/0.35881loss in batch 167: 0.130966/0.357468loss in batch 168: 0.515427/0.358398loss in batch 169: 0.259598/0.357819loss in batch 170: 0.192627/0.356842loss in batch 171: 0.398209/0.357086loss in batch 172: 0.203308/0.356186loss in batch 173: 0.389267/0.356384loss in batch 174: 0.322266/0.356201loss in batch 175: 0.143143/0.35498loss in batch 176: 0.148209/0.353806loss in batch 177: 0.212784/0.353012loss in batch 178: 0.236954/0.352371loss in batch 179: 0.405289/0.352676loss in batch 180: 0.280807/0.352264loss in batch 181: 0.232391/0.351624loss in batch 182: 0.937012/0.354813loss in batch 183: 0.192886/0.353928loss in batch 184: 0.275345/0.3535loss in batch 185: 0.506165/0.354324loss in batch 186: 0.332397/0.354202loss in batch 187: 0.174713/0.353241loss in batch 188: 0.276352/0.352844loss in batch 189: 0.465439/0.353439loss in batch 190: 0.344925/0.353394loss in batch 191: 0.31987/0.353226loss in batch 192: 0.321594/0.353043loss in batch 193: 0.284561/0.352707loss in batch 194: 0.251205/0.352188loss in batch 195: 0.764709/0.354279loss in batch 196: 0.492859/0.354996loss in batch 197: 0.218521/0.354294loss in batch 198: 0.0681763/0.352859loss in batch 199: 0.36792/0.352936loss in batch 200: 0.134125/0.351852loss in batch 201: 0.288895/0.351532loss in batch 202: 0.309677/0.351334loss in batch 203: 0.475967/0.351944loss in batch 204: 0.229507/0.351334loss in batch 205: 0.180695/0.350525loss in batch 206: 0.489471/0.351181loss in batch 207: 0.372009/0.351288loss in batch 208: 0.595108/0.352463loss in batch 209: 0.153107/0.351517loss in batch 210: 0.151886/0.350555loss in batch 211: 0.374466/0.350677loss in batch 212: 0.322021/0.35054
done with epoch 1
train_acc: 0.913146 (389/426)
test loss: 0.322021
acc: 0.923075 (132/143)
loss in batch 0: 0.306641/0.306641loss in batch 1: 0.146744/0.226685loss in batch 2: 0.185272/0.212875loss in batch 3: 0.174438/0.203278loss in batch 4: 0.187317/0.200073loss in batch 5: 0.429718/0.238358loss in batch 6: 0.439407/0.26709loss in batch 7: 0.228531/0.262253loss in batch 8: 0.272659/0.263412loss in batch 9: 0.355469/0.272629loss in batch 10: 0.343918/0.279114loss in batch 11: 0.211243/0.273438loss in batch 12: 0.0489502/0.25618loss in batch 13: 0.317871/0.26059loss in batch 14: 0.118729/0.251129loss in batch 15: 0.261383/0.25177loss in batch 16: 0.0702515/0.241089loss in batch 17: 0.558548/0.258728loss in batch 18: 0.273636/0.259506loss in batch 19: 0.439026/0.268478loss in batch 20: 0.163177/0.263474loss in batch 21: 0.0950012/0.255814loss in batch 22: 0.224136/0.25444loss in batch 23: 0.207275/0.252472loss in batch 24: 0.698151/0.270294loss in batch 25: 0.612473/0.283463loss in batch 26: 0.743958/0.300507loss in batch 27: 0.157089/0.295395loss in batch 28: 1.07619/0.322311loss in batch 29: 0.149857/0.316574loss in batch 30: 0.0838318/0.309067loss in batch 31: 0.499985/0.315033loss in batch 32: 0.1436/0.309845loss in batch 33: 0.0978851/0.303589loss in batch 34: 0.32634/0.304245loss in batch 35: 0.210358/0.301636loss in batch 36: 0.120041/0.296738loss in batch 37: 0.276123/0.296188loss in batch 38: 0.202698/0.293793loss in batch 39: 0.143936/0.290054loss in batch 40: 0.219696/0.28833loss in batch 41: 0.166046/0.285431loss in batch 42: 0.279221/0.285278loss in batch 43: 0.708481/0.294891loss in batch 44: 0.873459/0.307755loss in batch 45: 0.159149/0.30452loss in batch 46: 0.675385/0.312424loss in batch 47: 0.0822144/0.307617loss in batch 48: 0.309128/0.307648loss in batch 49: 0.088089/0.303253loss in batch 50: 0.252182/0.302246loss in batch 51: 0.40213/0.304169loss in batch 52: 0.675934/0.311188loss in batch 53: 0.203125/0.309189loss in batch 54: 0.359589/0.310104loss in batch 55: 0.13559/0.306992loss in batch 56: 0.370941/0.308105loss in batch 57: 0.115417/0.304779loss in batch 58: 0.400436/0.306412loss in batch 59: 0.56601/0.31073loss in batch 60: 0.232071/0.309448loss in batch 61: 0.24501/0.308411loss in batch 62: 0.1073/0.305222loss in batch 63: 0.121017/0.302338loss in batch 64: 0.216568/0.301025loss in batch 65: 0.198456/0.299454loss in batch 66: 0.211746/0.298157loss in batch 67: 0.275192/0.297806loss in batch 68: 0.151596/0.295685loss in batch 69: 0.221725/0.294647loss in batch 70: 0.184357/0.293091loss in batch 71: 0.0887451/0.290253loss in batch 72: 0.343781/0.290985loss in batch 73: 1.11885/0.30217loss in batch 74: 0.163651/0.300323loss in batch 75: 0.163895/0.298523loss in batch 76: 0.326706/0.298904loss in batch 77: 0.532227/0.30188loss in batch 78: 0.149658/0.299957loss in batch 79: 0.358154/0.30069loss in batch 80: 0.23851/0.299911loss in batch 81: 0.09198/0.297379loss in batch 82: 0.476364/0.299545loss in batch 83: 0.165359/0.297943loss in batch 84: 0.301193/0.297974loss in batch 85: 0.1931/0.296753loss in batch 86: 0.546509/0.299622loss in batch 87: 0.306793/0.299713loss in batch 88: 0.876541/0.306198loss in batch 89: 0.424759/0.30751loss in batch 90: 0.568726/0.310379loss in batch 91: 0.266357/0.309906loss in batch 92: 0.315506/0.309967loss in batch 93: 0.231216/0.309128loss in batch 94: 0.366028/0.309723loss in batch 95: 0.0443115/0.306961loss in batch 96: 0.223038/0.306091loss in batch 97: 0.14064/0.304413loss in batch 98: 0.391647/0.305298loss in batch 99: 0.293274/0.305161loss in batch 100: 0.264008/0.304764loss in batch 101: 0.144119/0.303192loss in batch 102: 0.12149/0.301407loss in batch 103: 0.583145/0.304138loss in batch 104: 0.220795/0.303345loss in batch 105: 0.140244/0.301788loss in batch 106: 0.212402/0.300964loss in batch 107: 0.187607/0.299911loss in batch 108: 0.0882263/0.297974loss in batch 109: 0.217606/0.297241loss in batch 110: 0.112381/0.295578loss in batch 111: 0.616074/0.298431loss in batch 112: 0.275177/0.298218loss in batch 113: 0.631989/0.301163loss in batch 114: 0.398788/0.302002loss in batch 115: 0.220306/0.3013loss in batch 116: 0.214447/0.300568loss in batch 117: 0.204865/0.299759loss in batch 118: 0.425934/0.300812loss in batch 119: 0.279037/0.300629loss in batch 120: 0.738022/0.304245loss in batch 121: 0.464661/0.305557loss in batch 122: 0.194672/0.304642loss in batch 123: 0.0384979/0.302505loss in batch 124: 0.392349/0.303223loss in batch 125: 0.631958/0.305832loss in batch 126: 0.571732/0.307922loss in batch 127: 0.211029/0.307175loss in batch 128: 0.27356/0.306915loss in batch 129: 0.151184/0.30571loss in batch 130: 0.890213/0.310181loss in batch 131: 0.317886/0.310242loss in batch 132: 0.214355/0.309509loss in batch 133: 0.208282/0.308762loss in batch 134: 0.329742/0.308914loss in batch 135: 0.365845/0.309326loss in batch 136: 0.413956/0.310089loss in batch 137: 0.0873566/0.308472loss in batch 138: 0.294586/0.30838loss in batch 139: 0.711319/0.311264loss in batch 140: 0.137451/0.310028loss in batch 141: 0.148346/0.308899loss in batch 142: 0.37146/0.309326loss in batch 143: 0.391983/0.309906loss in batch 144: 0.246735/0.309464loss in batch 145: 0.28299/0.309296loss in batch 146: 0.368912/0.309677loss in batch 147: 0.202118/0.30896loss in batch 148: 0.233459/0.308441loss in batch 149: 0.369965/0.308868loss in batch 150: 0.195709/0.308121loss in batch 151: 0.166901/0.307175loss in batch 152: 0.476776/0.308289loss in batch 153: 0.257156/0.307953loss in batch 154: 0.131393/0.306824loss in batch 155: 0.198502/0.306137loss in batch 156: 0.169067/0.305252loss in batch 157: 0.155975/0.304321loss in batch 158: 0.116394/0.303116loss in batch 159: 0.124634/0.302017loss in batch 160: 0.472458/0.303085loss in batch 161: 0.145721/0.302109loss in batch 162: 0.170731/0.3013loss in batch 163: 0.249832/0.30098loss in batch 164: 0.0859222/0.299667loss in batch 165: 0.0967712/0.298447loss in batch 166: 0.302902/0.298477loss in batch 167: 0.216385/0.298004loss in batch 168: 0.158203/0.297165loss in batch 169: 0.47554/0.298218loss in batch 170: 0.394791/0.298782loss in batch 171: 0.211182/0.298264loss in batch 172: 0.101578/0.297134loss in batch 173: 1.06158/0.301529loss in batch 174: 0.126984/0.300537loss in batch 175: 0.280777/0.300415loss in batch 176: 0.182907/0.299759loss in batch 177: 0.351212/0.300049loss in batch 178: 0.190338/0.299438loss in batch 179: 0.215729/0.298965loss in batch 180: 0.210297/0.298477loss in batch 181: 0.140289/0.297607loss in batch 182: 0.232346/0.297256loss in batch 183: 0.191223/0.296661loss in batch 184: 0.0724335/0.295471loss in batch 185: 0.525742/0.296707loss in batch 186: 0.407532/0.297302loss in batch 187: 0.689499/0.299377loss in batch 188: 0.502991/0.300461loss in batch 189: 0.125534/0.299545loss in batch 190: 0.359299/0.29985loss in batch 191: 0.109238/0.298859loss in batch 192: 0.123337/0.297943loss in batch 193: 0.0496674/0.296661loss in batch 194: 0.145309/0.295898loss in batch 195: 0.250473/0.295654loss in batch 196: 0.233734/0.295349loss in batch 197: 0.144073/0.294586loss in batch 198: 0.32222/0.294724loss in batch 199: 0.204788/0.294266loss in batch 200: 0.231552/0.293961loss in batch 201: 0.140106/0.293198loss in batch 202: 0.459427/0.294022loss in batch 203: 0.111206/0.293121loss in batch 204: 0.273132/0.293015loss in batch 205: 0.223709/0.292679loss in batch 206: 0.183578/0.29216loss in batch 207: 0.156403/0.291504loss in batch 208: 0.152023/0.290848loss in batch 209: 0.337219/0.291061loss in batch 210: 0.466171/0.291885loss in batch 211: 0.168106/0.291306loss in batch 212: 0.492935/0.292252
done with epoch 2
train_acc: 0.920188 (392/426)
test loss: 0.492934
acc: 0.923075 (132/143)
loss in batch 0: 0.389053/0.389053loss in batch 1: 0.284836/0.336945loss in batch 2: 0.17897/0.284286loss in batch 3: 0.293854/0.286682loss in batch 4: 0.129578/0.255249loss in batch 5: 0.306656/0.263824loss in batch 6: 0.207611/0.255798loss in batch 7: 0.183533/0.246765loss in batch 8: 0.0972748/0.230164loss in batch 9: 0.102295/0.217361loss in batch 10: 0.302994/0.225143loss in batch 11: 0.122086/0.216568loss in batch 12: 0.19902/0.215225loss in batch 13: 0.185989/0.21312loss in batch 14: 0.114655/0.206558loss in batch 15: 0.299377/0.212357loss in batch 16: 0.134033/0.207748loss in batch 17: 0.168777/0.205582loss in batch 18: 0.337234/0.212509loss in batch 19: 0.0298157/0.203384loss in batch 20: 0.157196/0.201172loss in batch 21: 0.118408/0.197418loss in batch 22: 0.453568/0.208557loss in batch 23: 0.18367/0.20752loss in batch 24: 0.288925/0.21077loss in batch 25: 0.130005/0.207672loss in batch 26: 0.0958099/0.203522loss in batch 27: 0.119461/0.200516loss in batch 28: 0.166656/0.199356loss in batch 29: 0.165039/0.198212loss in batch 30: 0.145554/0.196503loss in batch 31: 0.111145/0.193848loss in batch 32: 0.312653/0.197449loss in batch 33: 0.117203/0.195099loss in batch 34: 0.335342/0.199097loss in batch 35: 0.0604248/0.195236loss in batch 36: 0.105148/0.19281loss in batch 37: 0.21434/0.193375loss in batch 38: 0.125122/0.19162loss in batch 39: 0.438858/0.1978loss in batch 40: 0.140976/0.196426loss in batch 41: 0.0971832/0.194061loss in batch 42: 0.625336/0.204086loss in batch 43: 0.143555/0.202713loss in batch 44: 0.360565/0.206207loss in batch 45: 0.198395/0.206055loss in batch 46: 0.704193/0.216644loss in batch 47: 0.102615/0.214264loss in batch 48: 0.121613/0.212387loss in batch 49: 0.198227/0.212097loss in batch 50: 0.349884/0.214798loss in batch 51: 0.30867/0.216599loss in batch 52: 0.370682/0.219513loss in batch 53: 0.358826/0.222092loss in batch 54: 0.123077/0.220291loss in batch 55: 0.12616/0.218597loss in batch 56: 0.424057/0.222214loss in batch 57: 0.571564/0.228226loss in batch 58: 0.237488/0.228394loss in batch 59: 0.111374/0.226456loss in batch 60: 0.520615/0.231262loss in batch 61: 0.485825/0.235367loss in batch 62: 0.418533/0.238281loss in batch 63: 0.584045/0.243683loss in batch 64: 0.361572/0.245499loss in batch 65: 0.211334/0.24498loss in batch 66: 0.518875/0.249054loss in batch 67: 0.109619/0.247025loss in batch 68: 0.122787/0.245209loss in batch 69: 0.158157/0.243973loss in batch 70: 0.0644684/0.24144loss in batch 71: 0.752991/0.24855loss in batch 72: 0.258728/0.248688loss in batch 73: 0.0697632/0.246262loss in batch 74: 0.162506/0.245148loss in batch 75: 0.0921326/0.243149loss in batch 76: 0.0970001/0.241241loss in batch 77: 0.574524/0.245514loss in batch 78: 0.62204/0.250275loss in batch 79: 0.635818/0.255096loss in batch 80: 0.205353/0.254486loss in batch 81: 0.458969/0.256989loss in batch 82: 0.081665/0.254852loss in batch 83: 0.289047/0.25528loss in batch 84: 0.39035/0.256851loss in batch 85: 0.113312/0.255188loss in batch 86: 0.270004/0.255356loss in batch 87: 0.157349/0.254242loss in batch 88: 0.191574/0.25354loss in batch 89: 0.122055/0.25209loss in batch 90: 0.38205/0.25351loss in batch 91: 0.221939/0.253174loss in batch 92: 0.0837402/0.251343loss in batch 93: 0.25325/0.251373loss in batch 94: 0.183105/0.250641loss in batch 95: 0.156219/0.249664loss in batch 96: 0.080307/0.247925loss in batch 97: 0.339554/0.248856loss in batch 98: 0.0721893/0.24707loss in batch 99: 0.198944/0.246597loss in batch 100: 0.407837/0.248184loss in batch 101: 1.17326/0.257248loss in batch 102: 0.312607/0.257782loss in batch 103: 0.0561371/0.255859loss in batch 104: 0.125748/0.254608loss in batch 105: 0.333603/0.255356loss in batch 106: 0.117371/0.254074loss in batch 107: 0.196106/0.253525loss in batch 108: 0.553589/0.256287loss in batch 109: 0.276001/0.25647loss in batch 110: 0.555069/0.259155loss in batch 111: 0.222519/0.25882loss in batch 112: 0.0753479/0.257202loss in batch 113: 0.121307/0.256012loss in batch 114: 0.135651/0.254974loss in batch 115: 0.507736/0.257141loss in batch 116: 0.127762/0.256042loss in batch 117: 0.0350189/0.254166loss in batch 118: 0.0775909/0.252686loss in batch 119: 0.421967/0.254089loss in batch 120: 0.134201/0.253098loss in batch 121: 0.297089/0.253464loss in batch 122: 0.121201/0.25238loss in batch 123: 0.470581/0.25415loss in batch 124: 0.202927/0.253738loss in batch 125: 0.233856/0.253586loss in batch 126: 0.153198/0.252792loss in batch 127: 0.062973/0.251297loss in batch 128: 0.194351/0.25087loss in batch 129: 0.639771/0.25386loss in batch 130: 0.137772/0.252975loss in batch 131: 0.0213928/0.251221loss in batch 132: 0.398605/0.252319loss in batch 133: 0.124481/0.251373loss in batch 134: 0.274445/0.251541loss in batch 135: 0.231812/0.251389loss in batch 136: 0.155548/0.250702loss in batch 137: 0.107605/0.249664loss in batch 138: 0.102585/0.248611loss in batch 139: 0.167892/0.248032loss in batch 140: 0.11412/0.247086loss in batch 141: 0.0012207/0.245346loss in batch 142: 0.55748/0.247513loss in batch 143: 0.404037/0.248611loss in batch 144: 0.120911/0.247726loss in batch 145: 0.108109/0.24678loss in batch 146: 0.667404/0.249634loss in batch 147: 0.122879/0.248779loss in batch 148: 0.109085/0.247849loss in batch 149: 0.260239/0.247925loss in batch 150: 0.648621/0.25058loss in batch 151: 0.12941/0.249786loss in batch 152: 0.132935/0.249023loss in batch 153: 0.432053/0.250198loss in batch 154: 0.107529/0.249283loss in batch 155: 0.228241/0.249146loss in batch 156: 0.22345/0.248993loss in batch 157: 0.0901337/0.247986loss in batch 158: 0.0288391/0.246613loss in batch 159: 0.0919037/0.245651loss in batch 160: 0.137421/0.244965loss in batch 161: 0.213043/0.244781loss in batch 162: 0.71521/0.24765loss in batch 163: 0.205597/0.247406loss in batch 164: 0.0971832/0.24649loss in batch 165: 0.331406/0.247009loss in batch 166: 0.252838/0.24704loss in batch 167: 0.354538/0.247681loss in batch 168: 0.192169/0.247345loss in batch 169: 0.122604/0.246613loss in batch 170: 0.155045/0.246078loss in batch 171: 0.38002/0.246857loss in batch 172: 0.176041/0.246445loss in batch 173: 0.0743866/0.245453loss in batch 174: 0.347488/0.246033loss in batch 175: 0.525299/0.247635loss in batch 176: 0.263916/0.247711loss in batch 177: 0.533844/0.249329loss in batch 178: 0.45192/0.250473loss in batch 179: 0.271194/0.25058loss in batch 180: 0.223846/0.250427loss in batch 181: 0.131042/0.249771loss in batch 182: 0.153412/0.249237loss in batch 183: 0.768219/0.25206loss in batch 184: 0.25412/0.252075loss in batch 185: 0.107605/0.251297loss in batch 186: 0.132477/0.250671loss in batch 187: 0.292435/0.250885loss in batch 188: 0.394653/0.251648loss in batch 189: 0.799316/0.254532loss in batch 190: 0.456848/0.255585loss in batch 191: 0.571091/0.257217loss in batch 192: 0.192078/0.256897loss in batch 193: 0.460464/0.25795loss in batch 194: 0.690857/0.260162loss in batch 195: 0.218124/0.259949loss in batch 196: 0.164871/0.25946loss in batch 197: 0.390289/0.260132loss in batch 198: 0.344559/0.260544loss in batch 199: 0.209824/0.260284loss in batch 200: 0.284592/0.260422loss in batch 201: 0.0942993/0.259598loss in batch 202: 0.087677/0.258743loss in batch 203: 0.44278/0.259644loss in batch 204: 0.0567932/0.258652loss in batch 205: 0.245316/0.258591loss in batch 206: 0.173157/0.258179loss in batch 207: 0.0821228/0.257339loss in batch 208: 0.178131/0.256958loss in batch 209: 0.227325/0.256821loss in batch 210: 0.212341/0.256607loss in batch 211: 0.15567/0.256134loss in batch 212: 0.956238/0.25943
done with epoch 3
train_acc: 0.931925 (397/426)
test loss: 0.956236
acc: 0.930068 (133/143)
loss in batch 0: 0.248657/0.248657loss in batch 1: 0.231354/0.240005loss in batch 2: 0.182709/0.220901loss in batch 3: 0.0647888/0.181885loss in batch 4: 0.126144/0.170731loss in batch 5: 0.0873566/0.156845loss in batch 6: 0.469788/0.201538loss in batch 7: 0.189301/0.200012loss in batch 8: 0.255875/0.206207loss in batch 9: 0.152161/0.200806loss in batch 10: 0.139069/0.195206loss in batch 11: 0.385193/0.211029loss in batch 12: 0.101013/0.202576loss in batch 13: 0.235123/0.204895loss in batch 14: 0.266281/0.208984loss in batch 15: 0.283432/0.213638loss in batch 16: 0.327377/0.220322loss in batch 17: 0.0771179/0.212372loss in batch 18: 0.181625/0.210754loss in batch 19: 0.229507/0.211685loss in batch 20: 0.320053/0.216843loss in batch 21: 0.113312/0.212143loss in batch 22: 0.347855/0.218048loss in batch 23: 0.135056/0.214584loss in batch 24: 0.309036/0.218369loss in batch 25: 0.127274/0.214859loss in batch 26: 0.422974/0.22258loss in batch 27: 0.070694/0.217133loss in batch 28: 0.335007/0.221222loss in batch 29: 0.219482/0.221161loss in batch 30: 0.273117/0.222824loss in batch 31: 0.105011/0.219147loss in batch 32: 0.0727692/0.214722loss in batch 33: 0.0699768/0.210464loss in batch 34: 0.312332/0.213364loss in batch 35: 0.332138/0.21666loss in batch 36: 0.110336/0.213791loss in batch 37: 0.112991/0.211136loss in batch 38: 0.120361/0.208801loss in batch 39: 0.264511/0.210205loss in batch 40: 0.159744/0.208984loss in batch 41: 0.722961/0.221222loss in batch 42: 0.193924/0.220581loss in batch 43: 0.678635/0.230988loss in batch 44: 0.0734406/0.227493loss in batch 45: 0.0932465/0.224564loss in batch 46: 0.223602/0.224548loss in batch 47: 0.0812836/0.221573loss in batch 48: 0.116028/0.219406loss in batch 49: 0.0995941/0.21701loss in batch 50: 0.49411/0.222443loss in batch 51: 0.327896/0.224487loss in batch 52: 0.546188/0.23053loss in batch 53: 0.320816/0.232224loss in batch 54: 0.102585/0.229858loss in batch 55: 0.194687/0.229233loss in batch 56: 0.0609894/0.226288loss in batch 57: 0.185913/0.225586loss in batch 58: 0.11554/0.223709loss in batch 59: 0.384781/0.22641loss in batch 60: 0.253815/0.226852loss in batch 61: 0.0736084/0.22438loss in batch 62: 0.0340118/0.221359loss in batch 63: 0.365768/0.223602loss in batch 64: 0.99823/0.235535loss in batch 65: 0.0152283/0.232193loss in batch 66: 0.126602/0.230606loss in batch 67: 0.460922/0.234009loss in batch 68: 0.118423/0.23233loss in batch 69: 0.367325/0.234268loss in batch 70: 0.194885/0.233704loss in batch 71: 0.140808/0.232422loss in batch 72: 0.18364/0.23175loss in batch 73: 0.377472/0.233719loss in batch 74: 0.639145/0.23912loss in batch 75: 0.17955/0.238342loss in batch 76: 0.316101/0.239349loss in batch 77: 0.429688/0.241791loss in batch 78: 0.198578/0.241241loss in batch 79: 0.417816/0.243454loss in batch 80: 0.142914/0.242203loss in batch 81: 0.158112/0.24118loss in batch 82: 0.214371/0.24086loss in batch 83: 0.185074/0.240189loss in batch 84: 0.207565/0.239807loss in batch 85: 0.157837/0.238861loss in batch 86: 0.230438/0.238754loss in batch 87: 0.107666/0.237274loss in batch 88: 0.559586/0.240891loss in batch 89: 0.49057/0.243668loss in batch 90: 0.360718/0.244949loss in batch 91: 0.082489/0.243195loss in batch 92: 0.19371/0.242661loss in batch 93: 0.0471954/0.24057loss in batch 94: 0.202469/0.240173loss in batch 95: 0.160324/0.239334loss in batch 96: 0.0811768/0.237701loss in batch 97: 0.193008/0.237259loss in batch 98: 0.0423431/0.235291loss in batch 99: 0.223557/0.235168loss in batch 100: 0.23587/0.235184loss in batch 101: 0.215393/0.234985loss in batch 102: 0.145981/0.234131loss in batch 103: 0.936722/0.240875loss in batch 104: 0.126328/0.239792loss in batch 105: 0.567657/0.242874loss in batch 106: 0.0672913/0.241241loss in batch 107: 0.215149/0.240997loss in batch 108: 1.24164/0.250183loss in batch 109: 0.120422/0.248993loss in batch 110: 0.0258026/0.246979loss in batch 111: 0.25531/0.247055loss in batch 112: 0.0993805/0.245758loss in batch 113: 0.198669/0.245331loss in batch 114: 0.0431213/0.243591loss in batch 115: 0.0830536/0.242188loss in batch 116: 0.118439/0.241135loss in batch 117: 0.0855255/0.239822loss in batch 118: 0.0623932/0.238327loss in batch 119: 0.29921/0.238831loss in batch 120: 0.089325/0.23761loss in batch 121: 0.30365/0.238144loss in batch 122: 0.170105/0.237595loss in batch 123: 0.576996/0.240326loss in batch 124: 0.40741/0.241669loss in batch 125: 0.180069/0.24118loss in batch 126: 0.173706/0.240646loss in batch 127: 0.575165/0.243256loss in batch 128: 0.0478821/0.241745loss in batch 129: 0.297623/0.242172loss in batch 130: 0.168701/0.241608loss in batch 131: 0.195709/0.241257loss in batch 132: 0.328354/0.241913loss in batch 133: 0.112885/0.240952loss in batch 134: 0.107452/0.239975loss in batch 135: 0.0406952/0.23851loss in batch 136: 0.0195618/0.236908loss in batch 137: 0.138794/0.236191loss in batch 138: 0.453339/0.237747loss in batch 139: 0.207382/0.237534loss in batch 140: 0.0387878/0.236115loss in batch 141: 0.22081/0.236008loss in batch 142: 0.120453/0.235214loss in batch 143: 0.234787/0.235214loss in batch 144: 0.0781555/0.234131loss in batch 145: 0.414703/0.235352loss in batch 146: 0.428879/0.236679loss in batch 147: 0.307922/0.237152loss in batch 148: 0.257858/0.237305loss in batch 149: 0.264694/0.237473loss in batch 150: 0.105896/0.236603loss in batch 151: 0.253586/0.236725loss in batch 152: 0.489426/0.238373loss in batch 153: 0.0428925/0.237091loss in batch 154: 0.193115/0.236816loss in batch 155: 0.0501709/0.235626loss in batch 156: 0.108322/0.234818loss in batch 157: 0.299042/0.235214loss in batch 158: 0.131439/0.234573loss in batch 159: 0.236069/0.234573loss in batch 160: 0.50708/0.236267loss in batch 161: 0.0673828/0.235229loss in batch 162: 0.093811/0.23436loss in batch 163: 0.104111/0.233551loss in batch 164: 0.187469/0.233276loss in batch 165: 0.115677/0.232574loss in batch 166: 0.053894/0.231506loss in batch 167: 0.200974/0.231323loss in batch 168: 0.250717/0.231445loss in batch 169: 0.184677/0.231171loss in batch 170: 0.137039/0.230621loss in batch 171: 0.25145/0.230728loss in batch 172: 0.510117/0.232346loss in batch 173: 0.0737305/0.231445loss in batch 174: 0.233536/0.231445loss in batch 175: 0.129868/0.230881loss in batch 176: 0.133972/0.230331loss in batch 177: 0.287262/0.230637loss in batch 178: 0.524719/0.232285loss in batch 179: 0.0497131/0.231277loss in batch 180: 0.459641/0.232529loss in batch 181: 0.16394/0.232162loss in batch 182: 0.569366/0.233994loss in batch 183: 0.203278/0.233826loss in batch 184: 0.204269/0.233673loss in batch 185: 0.154266/0.233246loss in batch 186: 1.13699/0.238068loss in batch 187: 0.168076/0.237717loss in batch 188: 0.101135/0.236984loss in batch 189: 0.34787/0.237564loss in batch 190: 0.235184/0.237549loss in batch 191: 0.0448456/0.236557loss in batch 192: 0.381195/0.237305loss in batch 193: 0.479004/0.238556loss in batch 194: 0.0711365/0.237686loss in batch 195: 0.139877/0.237183loss in batch 196: 0.137634/0.236679loss in batch 197: 0.104385/0.236008loss in batch 198: 0.268341/0.236176loss in batch 199: 0.131561/0.235657loss in batch 200: 0.128601/0.235123loss in batch 201: 1.34943/0.240631loss in batch 202: 0.0423737/0.23967loss in batch 203: 0.0952301/0.238953loss in batch 204: 0.204193/0.238785loss in batch 205: 0.332581/0.239243loss in batch 206: 0.080246/0.23848loss in batch 207: 0.111145/0.237869loss in batch 208: 0.0626068/0.237015loss in batch 209: 0.413254/0.237869loss in batch 210: 0.0725098/0.237076loss in batch 211: 1.38547/0.242493loss in batch 212: 0.0660248/0.241669
done with epoch 4
train_acc: 0.929577 (396/426)
test loss: 0.0660246
acc: 0.930068 (133/143)
loss in batch 0: 0.104401/0.104401loss in batch 1: 0.117508/0.110947loss in batch 2: 0.131271/0.117722loss in batch 3: 0.323685/0.16922loss in batch 4: 0.0563965/0.146652loss in batch 5: 0.483414/0.202789loss in batch 6: 0.100967/0.188232loss in batch 7: 0.0382538/0.169479loss in batch 8: 0.11702/0.163651loss in batch 9: 0.113449/0.15863loss in batch 10: 0.222504/0.164429loss in batch 11: 0.0644379/0.156113loss in batch 12: 0.280884/0.165695loss in batch 13: 0.328415/0.177322loss in batch 14: 0.0754852/0.170532loss in batch 15: 0.162552/0.170044loss in batch 16: 0.173264/0.170227loss in batch 17: 0.157806/0.16954loss in batch 18: 0.945953/0.210403loss in batch 19: 0.0316315/0.201462loss in batch 20: 0.0315399/0.193375loss in batch 21: 0.250092/0.195953loss in batch 22: 0.176651/0.195114loss in batch 23: 0.400528/0.203674loss in batch 24: 0.126038/0.200562loss in batch 25: 0.0508881/0.194809loss in batch 26: 0.117157/0.191925loss in batch 27: 0.249725/0.194loss in batch 28: 0.242493/0.195663loss in batch 29: 0.0730743/0.191589loss in batch 30: 0.189835/0.191513loss in batch 31: 0.122162/0.189362loss in batch 32: 0.399445/0.195724loss in batch 33: 0.0737915/0.192139loss in batch 34: 0.0744324/0.188782loss in batch 35: 0.115982/0.186752loss in batch 36: 0.746719/0.201889loss in batch 37: 0.06633/0.198318loss in batch 38: 0.606522/0.208786loss in batch 39: 0.112747/0.206375loss in batch 40: 0.560226/0.215012loss in batch 41: 0.388962/0.219162loss in batch 42: 0.146072/0.217453loss in batch 43: 0.367783/0.220871loss in batch 44: 0.0937653/0.218048loss in batch 45: 0.100922/0.2155loss in batch 46: 0.452774/0.220551loss in batch 47: 0.0225372/0.216415loss in batch 48: 0.319733/0.218536loss in batch 49: 0.196259/0.218094loss in batch 50: 0.0413666/0.214615loss in batch 51: 0.208389/0.214493loss in batch 52: 0.54628/0.220764loss in batch 53: 0.402588/0.224136loss in batch 54: 0.128357/0.222397loss in batch 55: 0.0716553/0.219711loss in batch 56: 0.164566/0.218719loss in batch 57: 0.0660095/0.21611loss in batch 58: 0.141922/0.214844loss in batch 59: 0.0793762/0.21257loss in batch 60: 0.40271/0.215698loss in batch 61: 0.144318/0.214539loss in batch 62: 0.124008/0.213104loss in batch 63: 0.314575/0.214706loss in batch 64: 0.094101/0.21283loss in batch 65: 0.283081/0.213913loss in batch 66: 0.336609/0.215729loss in batch 67: 0.40744/0.218552loss in batch 68: 0.172867/0.217896loss in batch 69: 0.0705261/0.215775loss in batch 70: 0.154892/0.214935loss in batch 71: 0.124115/0.213669loss in batch 72: 0.0532837/0.211472loss in batch 73: 0.198029/0.211288loss in batch 74: 0.180359/0.210876loss in batch 75: 0.329239/0.212433loss in batch 76: 0.0176697/0.209915loss in batch 77: 0.189529/0.209641loss in batch 78: 0.438187/0.21254loss in batch 79: 0.180008/0.212128loss in batch 80: 0.0240326/0.209808loss in batch 81: 0.0645905/0.208038loss in batch 82: 0.106125/0.206802loss in batch 83: 0.377243/0.208832loss in batch 84: 0.813232/0.215942loss in batch 85: 0.0748596/0.21431loss in batch 86: 0.645752/0.219269loss in batch 87: 0.40596/0.22139loss in batch 88: 0.230728/0.221497loss in batch 89: 0.0799713/0.219925loss in batch 90: 0.119705/0.218826loss in batch 91: 0.219193/0.218826loss in batch 92: 0.464783/0.221466loss in batch 93: 0.0825958/0.219986loss in batch 94: 0.197388/0.219757loss in batch 95: 0.0273285/0.217758loss in batch 96: 0.419022/0.219818loss in batch 97: 0.275879/0.220398loss in batch 98: 0.321121/0.221405loss in batch 99: 0.294708/0.222153loss in batch 100: 0.285446/0.222778loss in batch 101: 0.239548/0.222946loss in batch 102: 0.0647278/0.221405loss in batch 103: 0.439178/0.223511loss in batch 104: 0.140289/0.222717loss in batch 105: 0.569031/0.225967loss in batch 106: 0.193665/0.225677loss in batch 107: 0.268539/0.226074loss in batch 108: 0.0851898/0.224777loss in batch 109: 0.0622559/0.223297loss in batch 110: 0.147491/0.22261loss in batch 111: 0.199081/0.222412loss in batch 112: 0.00311279/0.220459loss in batch 113: 0.431824/0.222321loss in batch 114: 0.450012/0.224304loss in batch 115: 0.124191/0.223434loss in batch 116: 0.184158/0.223099loss in batch 117: 0.201508/0.222916loss in batch 118: 0.153564/0.222336loss in batch 119: 0.447769/0.224213loss in batch 120: 0.0892029/0.223099loss in batch 121: 0.0357971/0.221558loss in batch 122: 0.350235/0.222595loss in batch 123: 0.130219/0.221863loss in batch 124: 1.16629/0.229416loss in batch 125: 0.356094/0.230423loss in batch 126: 0.106949/0.229446loss in batch 127: 0.0378113/0.227951loss in batch 128: 0.107574/0.22702loss in batch 129: 0.155624/0.226471loss in batch 130: 0.206192/0.226318loss in batch 131: 0.241699/0.226425loss in batch 132: 0.78981/0.230667loss in batch 133: 0.0574951/0.22937loss in batch 134: 0.12915/0.228638loss in batch 135: 0.195084/0.228378loss in batch 136: 0.119949/0.227585loss in batch 137: 0.264557/0.227859loss in batch 138: 0.174286/0.227478loss in batch 139: 0.223267/0.227448loss in batch 140: 0.674622/0.230621loss in batch 141: 0.199432/0.230392loss in batch 142: 0.275162/0.230713loss in batch 143: 0.0687714/0.229599loss in batch 144: 0.458496/0.231171loss in batch 145: 0.0979767/0.23024loss in batch 146: 0.973328/0.235306loss in batch 147: 0.627838/0.237961loss in batch 148: 0.208817/0.237762loss in batch 149: 0.137207/0.237091loss in batch 150: 0.0927582/0.23613loss in batch 151: 0.455444/0.237579loss in batch 152: 0.137863/0.236923loss in batch 153: 0.339432/0.237595loss in batch 154: 0.20874/0.237411loss in batch 155: 0.101837/0.236542loss in batch 156: 0.40097/0.237595loss in batch 157: 0.0977173/0.236694loss in batch 158: 0.210831/0.236542loss in batch 159: 0.0890503/0.235626loss in batch 160: 0.211975/0.235474loss in batch 161: 0.343994/0.236145loss in batch 162: 0.106308/0.235336loss in batch 163: 0.0701447/0.234329loss in batch 164: 0.138855/0.233749loss in batch 165: 0.112579/0.233032loss in batch 166: 0.0408478/0.231888loss in batch 167: 0.109039/0.231155loss in batch 168: 0.212662/0.231049loss in batch 169: 0.0365295/0.229904loss in batch 170: 0.0679169/0.228943loss in batch 171: 0.138214/0.228424loss in batch 172: 0.231232/0.228424loss in batch 173: 0.620255/0.230682loss in batch 174: 0.0974579/0.229935loss in batch 175: 0.280548/0.230209loss in batch 176: 0.647171/0.232574loss in batch 177: 1.09793/0.237442loss in batch 178: 0.139999/0.236877loss in batch 179: 0.0687103/0.235962loss in batch 180: 0.215805/0.23584loss in batch 181: 0.175735/0.235519loss in batch 182: 0.119598/0.234879loss in batch 183: 0.0718536/0.233978loss in batch 184: 0.0444794/0.232971loss in batch 185: 0.101593/0.232269loss in batch 186: 0.143219/0.231781loss in batch 187: 0.14415/0.231323loss in batch 188: 0.0531616/0.230377loss in batch 189: 0.0498962/0.229416loss in batch 190: 0.0636444/0.228546loss in batch 191: 0.204117/0.228439loss in batch 192: 0.0610657/0.22757loss in batch 193: 0.165634/0.227249loss in batch 194: 0.0910797/0.226547loss in batch 195: 0.265976/0.226746loss in batch 196: 0.0682068/0.225937loss in batch 197: 0.49498/0.227295loss in batch 198: 0.265106/0.227493loss in batch 199: 0.166245/0.227188loss in batch 200: 0.0455475/0.226273loss in batch 201: 0.252853/0.22641loss in batch 202: 0.0809021/0.225693loss in batch 203: 0.107956/0.225128loss in batch 204: 0.120529/0.224609loss in batch 205: 0.505432/0.225967loss in batch 206: 0.32608/0.226456loss in batch 207: 0.0397186/0.225555loss in batch 208: 0.0361481/0.224655loss in batch 209: 0.0705109/0.223923loss in batch 210: 0.476349/0.225113loss in batch 211: 0.0762634/0.224411loss in batch 212: 0.211105/0.224335
done with epoch 5
train_acc: 0.92723 (395/426)
test loss: 0.211105
acc: 0.937061 (134/143)
loss in batch 0: 0.110687/0.110687loss in batch 1: 0.0711212/0.0908966loss in batch 2: 0.209091/0.130295loss in batch 3: 0.405243/0.199036loss in batch 4: 0.748962/0.309021loss in batch 5: 0.024826/0.261658loss in batch 6: 0.164963/0.247833loss in batch 7: 0.346329/0.260147loss in batch 8: 0.046814/0.23645loss in batch 9: 0.641876/0.276993loss in batch 10: 0.175674/0.267776loss in batch 11: 0.542953/0.29071loss in batch 12: 0.146744/0.279633loss in batch 13: 0.167877/0.271667loss in batch 14: 0.0415649/0.256317loss in batch 15: 0.417465/0.266388loss in batch 16: 0.164322/0.260376loss in batch 17: 0.719894/0.285919loss in batch 18: 0.0845184/0.275299loss in batch 19: 0.122742/0.267685loss in batch 20: 0.459274/0.276794loss in batch 21: 0.273422/0.276657loss in batch 22: 0.097641/0.26886loss in batch 23: 0.180405/0.265182loss in batch 24: 0.0881805/0.258102loss in batch 25: 0.348526/0.261581loss in batch 26: 0.0848083/0.255035loss in batch 27: 0.0312347/0.24704loss in batch 28: 0.301025/0.248901loss in batch 29: 0.136414/0.245148loss in batch 30: 0.330627/0.24791loss in batch 31: 0.0321045/0.241165loss in batch 32: 0.117218/0.237411loss in batch 33: 0.0861664/0.232956loss in batch 34: 0.319473/0.235443loss in batch 35: 0.56192/0.244507loss in batch 36: 0.0900116/0.240326loss in batch 37: 0.0480347/0.235275loss in batch 38: 0.0463562/0.230423loss in batch 39: 0.187363/0.229355loss in batch 40: 0.0375824/0.22467loss in batch 41: 0.430344/0.229568loss in batch 42: 0.0968628/0.226486loss in batch 43: 0.0474854/0.222412loss in batch 44: 0.0583649/0.218765loss in batch 45: 0.0736542/0.215607loss in batch 46: 0.176926/0.214798loss in batch 47: 0.56955/0.222183loss in batch 48: 0.0483093/0.218628loss in batch 49: 0.217636/0.218613loss in batch 50: 0.263382/0.219482loss in batch 51: 0.170288/0.218536loss in batch 52: 0.285477/0.219803loss in batch 53: 0.334747/0.221939loss in batch 54: 0.199692/0.221527loss in batch 55: 0.0881195/0.219162loss in batch 56: 0.318924/0.220886loss in batch 57: 0.522537/0.226089loss in batch 58: 0.455353/0.229996loss in batch 59: 0.138153/0.228455loss in batch 60: 0.196426/0.227936loss in batch 61: 0.548233/0.233093loss in batch 62: 0.337296/0.234756loss in batch 63: 0.173386/0.233795loss in batch 64: 0.150986/0.232513loss in batch 65: 0.0700531/0.230057loss in batch 66: 0.22905/0.230042loss in batch 67: 0.051651/0.227417loss in batch 68: 0.167465/0.226547loss in batch 69: 0.534256/0.230942loss in batch 70: 0.0540466/0.228455loss in batch 71: 0.0800476/0.226395loss in batch 72: 0.933273/0.236069loss in batch 73: 0.404007/0.238342loss in batch 74: 0.203873/0.237869loss in batch 75: 0.161331/0.236877loss in batch 76: 0.964325/0.246323loss in batch 77: 0.111984/0.244598loss in batch 78: 1.26266/0.257492loss in batch 79: 0.084259/0.255325loss in batch 80: 0.115723/0.253601loss in batch 81: 0.022583/0.250793loss in batch 82: 0.421127/0.252823loss in batch 83: 0.111176/0.251144loss in batch 84: 0.373276/0.252579loss in batch 85: 0.0733337/0.250504loss in batch 86: 0.404221/0.252258loss in batch 87: 0.515579/0.255249loss in batch 88: 0.378143/0.256638loss in batch 89: 0.248688/0.256546loss in batch 90: 0.126999/0.255127loss in batch 91: 0.0248871/0.252625loss in batch 92: 0.0352783/0.25029loss in batch 93: 0.265793/0.250458loss in batch 94: 0.0577545/0.248428loss in batch 95: 0.436462/0.250381loss in batch 96: 0.348343/0.251389loss in batch 97: 0.254837/0.251434loss in batch 98: 0.0333557/0.249222loss in batch 99: 0.346954/0.250198loss in batch 100: 0.0398254/0.248123loss in batch 101: 0.154999/0.247208loss in batch 102: 0.351532/0.248215loss in batch 103: 0.0954437/0.24675loss in batch 104: 0.00811768/0.244476loss in batch 105: 0.67067/0.248505loss in batch 106: 0.150116/0.247589loss in batch 107: 0.0929565/0.246155loss in batch 108: 0.0963135/0.244781loss in batch 109: 0.0834503/0.243301loss in batch 110: 0.280548/0.243637loss in batch 111: 0.184219/0.243118loss in batch 112: 0.04776/0.241379loss in batch 113: 0.32431/0.242111loss in batch 114: 0.226639/0.241974loss in batch 115: 0.139023/0.241089loss in batch 116: 0.141373/0.240234loss in batch 117: 0.557297/0.24292loss in batch 118: 0.161133/0.242233loss in batch 119: 0.127747/0.241287loss in batch 120: 0.180054/0.240768loss in batch 121: 0.238251/0.240753loss in batch 122: 0.138412/0.239929loss in batch 123: 0.0370636/0.238281loss in batch 124: 0.190094/0.237915loss in batch 125: 0.209625/0.237686loss in batch 126: 0.00653076/0.235855loss in batch 127: 0.0647125/0.234528loss in batch 128: 0.0715485/0.233261loss in batch 129: 0.114319/0.232346loss in batch 130: 0.188995/0.23201loss in batch 131: 0.0112152/0.230347loss in batch 132: 0.371475/0.2314loss in batch 133: 0.0729065/0.230209loss in batch 134: 0.116791/0.22937loss in batch 135: 0.0665436/0.22818loss in batch 136: 1.06761/0.234314loss in batch 137: 0.285248/0.23468loss in batch 138: 0.0663605/0.233475loss in batch 139: 0.797195/0.237488loss in batch 140: 0.158386/0.236923loss in batch 141: 0.143845/0.236267loss in batch 142: 0.0442963/0.23494loss in batch 143: 0.0603485/0.233719loss in batch 144: 0.0818481/0.232681loss in batch 145: 0.0638428/0.231522loss in batch 146: 0.120712/0.230759loss in batch 147: 0.135956/0.230133loss in batch 148: 0.00862122/0.228638loss in batch 149: 0.203781/0.22847loss in batch 150: 0.270737/0.228745loss in batch 151: 0.122116/0.228043loss in batch 152: 0.0856171/0.227127loss in batch 153: 0.123276/0.22644loss in batch 154: 0.0755005/0.225464loss in batch 155: 0.442749/0.226868loss in batch 156: 0.0337524/0.225632loss in batch 157: 0.0346069/0.224426loss in batch 158: 0.0363617/0.223236loss in batch 159: 0.101181/0.222473loss in batch 160: 0.102127/0.221725loss in batch 161: 0.228622/0.221771loss in batch 162: 0.0416412/0.220657loss in batch 163: 0.119385/0.220047loss in batch 164: 0.243896/0.2202loss in batch 165: 0.355927/0.221008loss in batch 166: 0.179047/0.220764loss in batch 167: 0.0800629/0.219925loss in batch 168: 0.169403/0.21962loss in batch 169: 0.0107422/0.218399loss in batch 170: 0.141876/0.217941loss in batch 171: 0.079361/0.217148loss in batch 172: 0.0285187/0.216049loss in batch 173: 0.15242/0.215683loss in batch 174: 0.123901/0.215149loss in batch 175: 0.500031/0.216782loss in batch 176: 0.0370178/0.215759loss in batch 177: 0.25972/0.216019loss in batch 178: 0.153458/0.215668loss in batch 179: 0.0231171/0.214584loss in batch 180: 0.364151/0.215424loss in batch 181: 0.0794678/0.214661loss in batch 182: 0.0363922/0.213699loss in batch 183: 0.103699/0.213104loss in batch 184: 0.157944/0.212799loss in batch 185: 0.186371/0.212662loss in batch 186: 0.0954742/0.212036loss in batch 187: 0.745346/0.214874loss in batch 188: 0.191315/0.214737loss in batch 189: 0.289124/0.215134loss in batch 190: 0.260422/0.215378loss in batch 191: 0.0869141/0.214706loss in batch 192: 0.0939178/0.214081loss in batch 193: 0.257278/0.21431loss in batch 194: 0.0499878/0.213455loss in batch 195: 0.481262/0.214813loss in batch 196: 0.0549316/0.214005loss in batch 197: 0.0548706/0.213211loss in batch 198: 0.0601349/0.212448loss in batch 199: 0.0860748/0.211807loss in batch 200: 0.106308/0.211288loss in batch 201: 0.113098/0.2108loss in batch 202: 0.512009/0.21228loss in batch 203: 0.133163/0.211899loss in batch 204: 0.057663/0.211151loss in batch 205: 0.499268/0.21254loss in batch 206: 0.0810242/0.211899loss in batch 207: 0.0872498/0.211304loss in batch 208: 0.0584106/0.210571loss in batch 209: 0.474075/0.211823loss in batch 210: 0.0380859/0.211014loss in batch 211: 0.227768/0.21109loss in batch 212: 0.543533/0.212646
done with epoch 6
train_acc: 0.93662 (399/426)
test loss: 0.543532
acc: 0.937061 (134/143)
loss in batch 0: 0.239822/0.239822loss in batch 1: 0.2854/0.262619loss in batch 2: 0.0698547/0.198349loss in batch 3: 0.146317/0.185349loss in batch 4: 0.340439/0.21637loss in batch 5: 0.114182/0.199341loss in batch 6: 0.0300293/0.17514loss in batch 7: 0.0903015/0.164536loss in batch 8: 0.187668/0.167114loss in batch 9: 0.125626/0.162964loss in batch 10: 0.101212/0.157349loss in batch 11: 0.147369/0.156525loss in batch 12: 0.423874/0.177094loss in batch 13: 0.104019/0.17186loss in batch 14: 0.244095/0.176682loss in batch 15: 0.643829/0.205872loss in batch 16: 0.581543/0.227966loss in batch 17: 0.00164795/0.215393loss in batch 18: 0.0880585/0.208694loss in batch 19: 0.0961304/0.203064loss in batch 20: 0.0582428/0.196167loss in batch 21: 0.0315704/0.18869loss in batch 22: 0.15947/0.187424loss in batch 23: 0.0237732/0.180603loss in batch 24: 0.225937/0.182419loss in batch 25: 0.0284424/0.176498loss in batch 26: 0.0356903/0.17128loss in batch 27: 0.303543/0.17601loss in batch 28: 0.340851/0.181702loss in batch 29: 0.0800629/0.178299loss in batch 30: 0.471252/0.187744loss in batch 31: 0.64325/0.201981loss in batch 32: 0.12355/0.1996loss in batch 33: 0.387741/0.205139loss in batch 34: 0.929855/0.225861loss in batch 35: 0.101807/0.222412loss in batch 36: 0.476791/0.229279loss in batch 37: 0.102844/0.225937loss in batch 38: 0.555542/0.234406loss in batch 39: 0.345673/0.237183loss in batch 40: 0.0889282/0.233566loss in batch 41: 0.246887/0.233887loss in batch 42: 0.0593719/0.229828loss in batch 43: 0.13353/0.227646loss in batch 44: 0.115463/0.225143loss in batch 45: 0.0797272/0.221985loss in batch 46: 0.0560608/0.21846loss in batch 47: 0.337723/0.220947loss in batch 48: 0.451462/0.225632loss in batch 49: 0.188248/0.224899loss in batch 50: 0.0669403/0.221786loss in batch 51: 0.363647/0.224533loss in batch 52: 0.247635/0.22496loss in batch 53: 0.099762/0.222656loss in batch 54: 0.0926666/0.220276loss in batch 55: 0.0847931/0.21785loss in batch 56: 0.0921021/0.215652loss in batch 57: 0.389023/0.218643loss in batch 58: 0.103149/0.21669loss in batch 59: 0.205627/0.216492loss in batch 60: 0.136948/0.215195loss in batch 61: 0.168289/0.214432loss in batch 62: 0.0475922/0.211792loss in batch 63: 0.11734/0.210312loss in batch 64: 0.0583954/0.207977loss in batch 65: 0.355652/0.210205loss in batch 66: 0.208664/0.21019loss in batch 67: 0.119537/0.208862loss in batch 68: 0.066925/0.206802loss in batch 69: 0.224579/0.207047loss in batch 70: 0.0876617/0.205383loss in batch 71: 0.357498/0.207489loss in batch 72: 0.119507/0.206284loss in batch 73: 0.106918/0.204941loss in batch 74: 0.0204926/0.202469loss in batch 75: 0.0265808/0.200165loss in batch 76: 0.156525/0.1996loss in batch 77: 0.107986/0.198425loss in batch 78: 0.0345306/0.19635loss in batch 79: 0.417618/0.199112loss in batch 80: 0.229538/0.199478loss in batch 81: 0.0203705/0.197311loss in batch 82: 0.0884552/0.195999loss in batch 83: 0.256104/0.196716loss in batch 84: 0.0213623/0.194641loss in batch 85: 0.0879364/0.193405loss in batch 86: 0.0759277/0.192062loss in batch 87: 0.146637/0.191544loss in batch 88: 0.0579071/0.190033loss in batch 89: 0.0597534/0.188599loss in batch 90: 0.0549622/0.187119loss in batch 91: 0.0541382/0.185684loss in batch 92: 0.116028/0.184937loss in batch 93: 0.154816/0.184601loss in batch 94: 0.0897064/0.183609loss in batch 95: 0.156616/0.183334loss in batch 96: 0.0491791/0.181946loss in batch 97: 0.0903778/0.181015loss in batch 98: 0.0506287/0.179688loss in batch 99: 0.0580444/0.178467loss in batch 100: 0.141922/0.178116loss in batch 101: 0.0670319/0.177017loss in batch 102: 0.351715/0.178726loss in batch 103: 0.0685577/0.177673loss in batch 104: 0.275864/0.178604loss in batch 105: 1.49892/0.191055loss in batch 106: 0.186386/0.19101loss in batch 107: 0.0332947/0.18956loss in batch 108: 0.187286/0.189529loss in batch 109: 0.0370178/0.188141loss in batch 110: 0.170547/0.187988loss in batch 111: 0.194992/0.188049loss in batch 112: 0.0318604/0.186661loss in batch 113: 0.0113831/0.18512loss in batch 114: 0.0279999/0.183762loss in batch 115: 0.251221/0.184341loss in batch 116: 0.169525/0.184219loss in batch 117: 0.0256653/0.182861loss in batch 118: 0.543579/0.185898loss in batch 119: 0.355591/0.187317loss in batch 120: 0.109436/0.186676loss in batch 121: 0.414413/0.188538loss in batch 122: 0.0546417/0.187454loss in batch 123: 0.113083/0.186859loss in batch 124: 0.521729/0.189529loss in batch 125: 0.162354/0.189316loss in batch 126: 0.0910645/0.188553loss in batch 127: 0.196274/0.188599loss in batch 128: 0.042923/0.187469loss in batch 129: 0.289749/0.188263loss in batch 130: 0.924942/0.193878loss in batch 131: 0.40271/0.195465loss in batch 132: 0.583893/0.198395loss in batch 133: 0.142563/0.197968loss in batch 134: 0.102066/0.197266loss in batch 135: 0.0677185/0.19632loss in batch 136: 0.134216/0.195862loss in batch 137: 0.0339966/0.194672loss in batch 138: 0.492676/0.196823loss in batch 139: 0.311584/0.197647loss in batch 140: 0.239807/0.197937loss in batch 141: 0.275009/0.198486loss in batch 142: 0.0571594/0.197495loss in batch 143: 0.0966187/0.196793loss in batch 144: 0.0264435/0.195633loss in batch 145: 0.0455017/0.194595loss in batch 146: 0.0591125/0.193665loss in batch 147: 0.0313721/0.192581loss in batch 148: 0.125549/0.192123loss in batch 149: 0.0592804/0.191238loss in batch 150: 0.113525/0.19072loss in batch 151: 0.217194/0.190903loss in batch 152: 0.103668/0.190338loss in batch 153: 0.124756/0.189896loss in batch 154: 0.296127/0.190582loss in batch 155: 0.148163/0.190323loss in batch 156: 0.0773773/0.189606loss in batch 157: 0.281937/0.190186loss in batch 158: 0.389786/0.191437loss in batch 159: 0.0669708/0.190659loss in batch 160: 0.14827/0.190399loss in batch 161: 0.459488/0.192062loss in batch 162: 0.0607147/0.191254loss in batch 163: 1.12717/0.19696loss in batch 164: 0.317398/0.197693loss in batch 165: 0.0167084/0.196594loss in batch 166: 0.147705/0.196304loss in batch 167: 0.513733/0.198196loss in batch 168: 0.10202/0.197617loss in batch 169: 0.095108/0.197021loss in batch 170: 0.0765228/0.196304loss in batch 171: 0.0844421/0.195663loss in batch 172: 0.0140991/0.194626loss in batch 173: 0.0925903/0.194031loss in batch 174: 0.505844/0.195816loss in batch 175: 0.0803528/0.195145loss in batch 176: 0.0464325/0.194321loss in batch 177: 0.144043/0.194031loss in batch 178: 0.677155/0.196732loss in batch 179: 0.298248/0.197296loss in batch 180: 0.291473/0.197815loss in batch 181: 0.357925/0.1987loss in batch 182: 0.136581/0.198349loss in batch 183: 0.601028/0.200546loss in batch 184: 0.230515/0.200699loss in batch 185: 0.469711/0.202148loss in batch 186: 0.0541534/0.201355loss in batch 187: 0.197495/0.201355loss in batch 188: 0.556137/0.203217loss in batch 189: 0.105026/0.202713loss in batch 190: 0.248734/0.202942loss in batch 191: 0.104294/0.202438loss in batch 192: 0.0173187/0.201477loss in batch 193: 0.0358734/0.200623loss in batch 194: 0.097702/0.200089loss in batch 195: 0.340729/0.200806loss in batch 196: 0.663406/0.203156loss in batch 197: 0.313843/0.20372loss in batch 198: 0.532547/0.205368loss in batch 199: 0.205795/0.205368loss in batch 200: 0.428116/0.206482loss in batch 201: 0.0736542/0.205826loss in batch 202: 0.246933/0.206024loss in batch 203: 0.327454/0.206619loss in batch 204: 0.108429/0.206131loss in batch 205: 0.106476/0.205658loss in batch 206: 0.0956726/0.205124loss in batch 207: 0.021637/0.204239loss in batch 208: 0.163513/0.204041loss in batch 209: 0.280853/0.204407loss in batch 210: 0.0507813/0.20369loss in batch 211: 0.0867462/0.20314loss in batch 212: 0.368027/0.203918
done with epoch 7
train_acc: 0.93662 (399/426)
test loss: 0.368026
acc: 0.937061 (134/143)
loss in batch 0: 0.183792/0.183792loss in batch 1: 0.0723267/0.128052loss in batch 2: 0.291916/0.182678loss in batch 3: 0.0733643/0.15535loss in batch 4: 0.155167/0.155304loss in batch 5: 0.356827/0.188904loss in batch 6: 0.136749/0.181442loss in batch 7: 0.0112152/0.160172loss in batch 8: 0.133957/0.157257loss in batch 9: 0.117111/0.153244loss in batch 10: 0.00708008/0.139954loss in batch 11: 0.162872/0.141876loss in batch 12: 0.486969/0.168411loss in batch 13: 0.228088/0.172668loss in batch 14: 0.159332/0.171783loss in batch 15: 0.375397/0.184509loss in batch 16: 0.101517/0.179626loss in batch 17: 0.425156/0.193268loss in batch 18: 0.0233154/0.184326loss in batch 19: 0.0406036/0.177139loss in batch 20: 0.404175/0.187943loss in batch 21: 0.355087/0.195541loss in batch 22: 0.129135/0.192657loss in batch 23: 0.169708/0.191696loss in batch 24: 0.193024/0.191757loss in batch 25: 0.141663/0.189835loss in batch 26: 0.0413818/0.184326loss in batch 27: 1.04491/0.215057loss in batch 28: 0.294739/0.217804loss in batch 29: 0.494202/0.227036loss in batch 30: 0.133606/0.223999loss in batch 31: 0.15683/0.221909loss in batch 32: 0.0388336/0.21637loss in batch 33: 0.273102/0.218033loss in batch 34: 0.485428/0.225677loss in batch 35: 0.24408/0.226196loss in batch 36: 0.365601/0.22995loss in batch 37: 0.0278931/0.224625loss in batch 38: 0.419708/0.229645loss in batch 39: 0.0576172/0.225342loss in batch 40: 0.368103/0.228821loss in batch 41: 0.30542/0.230652loss in batch 42: 0.0751038/0.22702loss in batch 43: 0.0874786/0.223846loss in batch 44: 0.0841827/0.220764loss in batch 45: 0.372009/0.22403loss in batch 46: 0.0684509/0.220734loss in batch 47: 0.284042/0.222061loss in batch 48: 0.212097/0.221848loss in batch 49: 0.122787/0.219864loss in batch 50: 0.372498/0.222855loss in batch 51: 0.0853424/0.220215loss in batch 52: 0.112778/0.218185loss in batch 53: 0.27977/0.219315loss in batch 54: 0.26326/0.220123loss in batch 55: 0.0535431/0.217148loss in batch 56: 0.0242004/0.21376loss in batch 57: 0.199539/0.213531loss in batch 58: 0.648254/0.220886loss in batch 59: 0.300674/0.222229loss in batch 60: 0.0763397/0.219833loss in batch 61: 0.195648/0.219421loss in batch 62: 0.529877/0.224365loss in batch 63: 0.225632/0.22438loss in batch 64: 0.0414581/0.221573loss in batch 65: 0.130707/0.220184loss in batch 66: 0.154221/0.219208loss in batch 67: 0.287567/0.2202loss in batch 68: 0.155045/0.219269loss in batch 69: 0.0460663/0.216782loss in batch 70: 0.0296936/0.214157loss in batch 71: 0.198654/0.213943loss in batch 72: 0.188095/0.213593loss in batch 73: 0.226273/0.21376loss in batch 74: 0.0993652/0.212234loss in batch 75: 0.0166626/0.209656loss in batch 76: 0.314484/0.211029loss in batch 77: 0.220062/0.211136loss in batch 78: 0.145264/0.210297loss in batch 79: 0.265823/0.210999loss in batch 80: 0.0271759/0.208725loss in batch 81: 0.25032/0.209244loss in batch 82: 0.0756989/0.207626loss in batch 83: 0.0361481/0.205582loss in batch 84: 0.14621/0.20488loss in batch 85: 0.372513/0.206833loss in batch 86: 0.115952/0.205795loss in batch 87: 0.355865/0.207489loss in batch 88: 0.0874481/0.206161loss in batch 89: 0.0788574/0.204727loss in batch 90: 0.0893707/0.203461loss in batch 91: 0.0601501/0.201904loss in batch 92: 0.0496826/0.200272loss in batch 93: 0.490967/0.203369loss in batch 94: 0.0619965/0.201874loss in batch 95: 0.120529/0.201035loss in batch 96: 0.524628/0.204361loss in batch 97: 0.182541/0.204147loss in batch 98: 0.0675659/0.202774loss in batch 99: 0.0493469/0.201233loss in batch 100: 0.0554657/0.199799loss in batch 101: 0.0964203/0.198761loss in batch 102: 0.0592041/0.197418loss in batch 103: 0.0653381/0.196152loss in batch 104: 0.0191345/0.194458loss in batch 105: 0.152054/0.194061loss in batch 106: 0.210541/0.194214loss in batch 107: 0.0391235/0.19278loss in batch 108: 0.0432739/0.191406loss in batch 109: 0.0371857/0.190002loss in batch 110: 0.0734406/0.188965loss in batch 111: 0.0619049/0.18782loss in batch 112: 0.157974/0.187546loss in batch 113: 0.0229492/0.186127loss in batch 114: 0.645142/0.190109loss in batch 115: 0.0819244/0.189163loss in batch 116: 0.0206299/0.187729loss in batch 117: 0.0477142/0.186554loss in batch 118: 0.0479584/0.185379loss in batch 119: 0.441833/0.187515loss in batch 120: 0.0832977/0.186661loss in batch 121: 0.0493011/0.185532loss in batch 122: 1.47054/0.195984loss in batch 123: 0.293167/0.196762loss in batch 124: 0.288544/0.197495loss in batch 125: 0.512329/0.199997loss in batch 126: 0.240997/0.200317loss in batch 127: 0.360168/0.201569loss in batch 128: 0.208344/0.201614loss in batch 129: 0.0509644/0.200455loss in batch 130: 0.129089/0.199905loss in batch 131: 0.0704803/0.198944loss in batch 132: 0.0637817/0.197922loss in batch 133: 0.47847/0.200012loss in batch 134: 0.0637054/0.199005loss in batch 135: 0.0430145/0.197861loss in batch 136: 0.0462189/0.196747loss in batch 137: 0.0585785/0.19574loss in batch 138: 0.0228271/0.194504loss in batch 139: 0.0966187/0.193802loss in batch 140: 0.103424/0.193161loss in batch 141: 0.149612/0.192856loss in batch 142: 0.202896/0.192932loss in batch 143: 0.223801/0.193146loss in batch 144: 0.0258179/0.191986loss in batch 145: 0.0471344/0.190994loss in batch 146: 0.0769196/0.190216loss in batch 147: 0.355865/0.191345loss in batch 148: 0.0276184/0.190247loss in batch 149: 0.0938263/0.189606loss in batch 150: 0.267075/0.190109loss in batch 151: 0.198517/0.19017loss in batch 152: 0.417664/0.19165loss in batch 153: 0.0302582/0.190598loss in batch 154: 0.0907288/0.189957loss in batch 155: 0.0412292/0.189011loss in batch 156: 0.0169678/0.187912loss in batch 157: 0.0189362/0.186844loss in batch 158: 0.0848541/0.186203loss in batch 159: 0.0924225/0.185623loss in batch 160: 0.360703/0.186707loss in batch 161: 0.0340118/0.18576loss in batch 162: 0.0179901/0.184738loss in batch 163: 0.0214081/0.183731loss in batch 164: 0.217804/0.183945loss in batch 165: 1.24716/0.190338loss in batch 166: 1.13185/0.195984loss in batch 167: 1.03983/0.201004loss in batch 168: 0.234421/0.201202loss in batch 169: 0.00665283/0.200058loss in batch 170: 0.0121307/0.198959loss in batch 171: 0.11145/0.198456loss in batch 172: 0.150101/0.198166loss in batch 173: 0.170715/0.198013loss in batch 174: 0.0741882/0.197311loss in batch 175: 0.0936737/0.196716loss in batch 176: 0.0679169/0.195984loss in batch 177: 0.0333405/0.195068loss in batch 178: 0.0329742/0.194168loss in batch 179: 0.0569153/0.193405loss in batch 180: 0.0869446/0.192825loss in batch 181: 0.0884857/0.192245loss in batch 182: 0.154282/0.192032loss in batch 183: 0.00672913/0.191025loss in batch 184: 0.190994/0.19104loss in batch 185: 0.239517/0.191284loss in batch 186: 0.0959167/0.190781loss in batch 187: 0.799057/0.194016loss in batch 188: 0.115112/0.193588loss in batch 189: 0.102631/0.193115loss in batch 190: 0.0767975/0.192505loss in batch 191: 0.168594/0.192383loss in batch 192: 0.106186/0.19194loss in batch 193: 0.0763702/0.191345loss in batch 194: 0.0759735/0.19075loss in batch 195: 0.242401/0.191025loss in batch 196: 0.0824585/0.19046loss in batch 197: 0.153198/0.190277loss in batch 198: 0.0998993/0.189819loss in batch 199: 0.480881/0.191284loss in batch 200: 0.289673/0.191772loss in batch 201: 0.0253754/0.190948loss in batch 202: 0.00689697/0.190033loss in batch 203: 0.650879/0.192291loss in batch 204: 0.393036/0.193268loss in batch 205: 0.202057/0.193314loss in batch 206: 0.365631/0.194153loss in batch 207: 0.0259399/0.193344loss in batch 208: 0.110565/0.192947loss in batch 209: 0.411392/0.193985loss in batch 210: 0.496323/0.195419loss in batch 211: 0.0693207/0.194824loss in batch 212: 0.153671/0.194626
done with epoch 8
train_acc: 0.941315 (401/426)
test loss: 0.153671
acc: 0.937061 (134/143)
loss in batch 0: 0.283035/0.283035loss in batch 1: 0.511505/0.397278loss in batch 2: 0.109726/0.301422loss in batch 3: 0.0749207/0.244797loss in batch 4: 0.107758/0.217392loss in batch 5: 0.0390472/0.187668loss in batch 6: 0.297165/0.203308loss in batch 7: 0.133469/0.19458loss in batch 8: 0.214249/0.196762loss in batch 9: 0.0869751/0.185776loss in batch 10: 0.0774384/0.175949loss in batch 11: 1.02086/0.246353loss in batch 12: 0.0269928/0.229477loss in batch 13: 0.164215/0.224808loss in batch 14: 0.185852/0.222214loss in batch 15: 0.106445/0.214966loss in batch 16: 0.12114/0.209457loss in batch 17: 0.281372/0.21344loss in batch 18: 0.118195/0.208435loss in batch 19: 0.0266571/0.199341loss in batch 20: 0.206558/0.199692loss in batch 21: 0.209213/0.200119loss in batch 22: 0.049118/0.193558loss in batch 23: 0.0927124/0.189362loss in batch 24: 0.0393677/0.183365loss in batch 25: 0.280212/0.187088loss in batch 26: 0.23642/0.188904loss in batch 27: 0.0483704/0.183899loss in batch 28: 0.0635681/0.179733loss in batch 29: 0.0518341/0.175476loss in batch 30: 0.0278015/0.170715loss in batch 31: 0.0478821/0.16687loss in batch 32: 0.380142/0.17334loss in batch 33: 0.0931244/0.170975loss in batch 34: 0.178085/0.171188loss in batch 35: 0.0632629/0.168198loss in batch 36: 0.0587463/0.165222loss in batch 37: 0.481415/0.173553loss in batch 38: 0.11026/0.171936loss in batch 39: 0.0245667/0.168243loss in batch 40: 0.439331/0.17485loss in batch 41: 0.0508575/0.171906loss in batch 42: 0.0721588/0.169586loss in batch 43: 0.0740204/0.167404loss in batch 44: 0.0523071/0.164856loss in batch 45: 0.0788269/0.162979loss in batch 46: 0.322998/0.166382loss in batch 47: 0.207672/0.167252loss in batch 48: 0.0123901/0.164093loss in batch 49: 0.0771179/0.162354loss in batch 50: 0.115479/0.161423loss in batch 51: 0.149857/0.161209loss in batch 52: 0.0200653/0.158539loss in batch 53: 0.275879/0.160706loss in batch 54: 0.0792847/0.159241loss in batch 55: 0.177475/0.159561loss in batch 56: 0.215195/0.160538loss in batch 57: 0.380508/0.164337loss in batch 58: 0.0432587/0.162277loss in batch 59: 0.463104/0.167297loss in batch 60: 0.839859/0.178314loss in batch 61: 0.0220184/0.175797loss in batch 62: 0.16658/0.175659loss in batch 63: 0.0962067/0.174408loss in batch 64: 0.0894928/0.173111loss in batch 65: 0.326431/0.175415loss in batch 66: 0.0749817/0.173935loss in batch 67: 0.103714/0.172897loss in batch 68: 0.3423/0.175354loss in batch 69: 0.593796/0.18132loss in batch 70: 0.0200043/0.179062loss in batch 71: 0.0448608/0.1772loss in batch 72: 0.0648804/0.175659loss in batch 73: 0.519241/0.180298loss in batch 74: 0.0944824/0.179153loss in batch 75: 0.0622559/0.177612loss in batch 76: 0.135986/0.177078loss in batch 77: 0.572327/0.182144loss in batch 78: 0.0330048/0.180252loss in batch 79: 0.170654/0.18013loss in batch 80: 0.157562/0.179855loss in batch 81: 0.0413208/0.178162loss in batch 82: 0.0186157/0.176254loss in batch 83: 0.149628/0.175934loss in batch 84: 0.100143/0.175034loss in batch 85: 0.0157166/0.173187loss in batch 86: 0.160416/0.173035loss in batch 87: 0.686691/0.178864loss in batch 88: 0.0735474/0.177689loss in batch 89: 0.0167694/0.175903loss in batch 90: 0.170776/0.175842loss in batch 91: 0.296585/0.177155loss in batch 92: 0.37088/0.17923loss in batch 93: 0.0241241/0.177582loss in batch 94: 0.205231/0.177872loss in batch 95: 0.0839233/0.176895loss in batch 96: 0.191376/0.177048loss in batch 97: 0.160248/0.17688loss in batch 98: 0.131241/0.176422loss in batch 99: 0.224686/0.176895loss in batch 100: 0.188812/0.177017loss in batch 101: 0.0405579/0.175674loss in batch 102: 0.623993/0.180038loss in batch 103: 0.0740051/0.179016loss in batch 104: 0.456268/0.181656loss in batch 105: 0.0223389/0.180145loss in batch 106: 0.636703/0.184418loss in batch 107: 0.110001/0.183731loss in batch 108: 0.305878/0.184845loss in batch 109: 0.387115/0.186691loss in batch 110: 0.0268555/0.185257loss in batch 111: 0.10585/0.18454loss in batch 112: 0.0545502/0.183395loss in batch 113: 0.0651398/0.182343loss in batch 114: 0.0460815/0.181168loss in batch 115: 0.0266266/0.179825loss in batch 116: 0.137299/0.179474loss in batch 117: 0.0302429/0.178207loss in batch 118: 0.239792/0.178711loss in batch 119: 0.179565/0.178726loss in batch 120: 0.603622/0.182251loss in batch 121: 0.319122/0.183365loss in batch 122: 0.0601349/0.182358loss in batch 123: 0.0506744/0.181305loss in batch 124: 0.0801697/0.180481loss in batch 125: 0.0854645/0.179733loss in batch 126: 0.0700989/0.178879loss in batch 127: 0.189926/0.178955loss in batch 128: 0.366867/0.18042loss in batch 129: 0.21347/0.180679loss in batch 130: 0.0427551/0.179611loss in batch 131: 0.0624542/0.178726loss in batch 132: 0.199081/0.178879loss in batch 133: 0.132935/0.178543loss in batch 134: 0.0197906/0.177368loss in batch 135: 0.0126038/0.176147loss in batch 136: 0.0062561/0.174911loss in batch 137: 0.24382/0.175415loss in batch 138: 0.0837097/0.174759loss in batch 139: 0.390778/0.1763loss in batch 140: 0.178665/0.176315loss in batch 141: 0.302124/0.1772loss in batch 142: 0.0318604/0.176178loss in batch 143: 0.931244/0.181427loss in batch 144: 0.0384369/0.180435loss in batch 145: 0.386627/0.181854loss in batch 146: 0.0844727/0.181198loss in batch 147: 0.074295/0.180466loss in batch 148: 0.140594/0.180191loss in batch 149: 0.0227356/0.179138loss in batch 150: 0.16655/0.179062loss in batch 151: 0.0849762/0.178436loss in batch 152: 0.144394/0.178223loss in batch 153: 0.0894775/0.177643loss in batch 154: 0.168564/0.177582loss in batch 155: 0.0464325/0.176758loss in batch 156: 0.60556/0.179474loss in batch 157: 0.404022/0.180893loss in batch 158: 0.426071/0.182449loss in batch 159: 0.298462/0.183182loss in batch 160: 0.167114/0.183075loss in batch 161: 0.403198/0.184418loss in batch 162: 0.0380707/0.183533loss in batch 163: 0.0686951/0.182831loss in batch 164: 0.417419/0.18425loss in batch 165: 0.179581/0.184219loss in batch 166: 0.232254/0.184509loss in batch 167: 0.274384/0.185043loss in batch 168: 0.0347748/0.184143loss in batch 169: 0.131577/0.183853loss in batch 170: 0.419769/0.185226loss in batch 171: 0.0990906/0.184723loss in batch 172: 1.43169/0.191925loss in batch 173: 0.0112305/0.190887loss in batch 174: 0.148987/0.190643loss in batch 175: 0.0792847/0.190018loss in batch 176: 0.117386/0.189621loss in batch 177: 0.18512/0.18959loss in batch 178: 0.0909576/0.189041loss in batch 179: 0.0693665/0.18837loss in batch 180: 0.87236/0.192154loss in batch 181: 0.0408936/0.191315loss in batch 182: 0.0763855/0.190689loss in batch 183: 0.0620117/0.189987loss in batch 184: 0.0284729/0.189117loss in batch 185: 0.0296478/0.188263loss in batch 186: 1.17111/0.193527loss in batch 187: 0.0838318/0.192947loss in batch 188: 0.0484467/0.192169loss in batch 189: 0.0305481/0.19133loss in batch 190: 0.282364/0.191788loss in batch 191: 0.0306702/0.190948loss in batch 192: 0.273972/0.191391loss in batch 193: 0.0716248/0.190765loss in batch 194: 0.067749/0.19014loss in batch 195: 0.0854797/0.189606loss in batch 196: 0.263062/0.189972loss in batch 197: 0.356171/0.190811loss in batch 198: 0.25238/0.191116loss in batch 199: 0.463272/0.19249loss in batch 200: 0.103088/0.192047loss in batch 201: 0.0420074/0.191299loss in batch 202: 0.0784912/0.190735loss in batch 203: 0.149796/0.190552loss in batch 204: 0.389862/0.191513loss in batch 205: 0.263809/0.191864loss in batch 206: 0.115173/0.191498loss in batch 207: 0.0717163/0.190918loss in batch 208: 0.0772552/0.190384loss in batch 209: 0.0297546/0.189606loss in batch 210: 0.0809326/0.189102loss in batch 211: 0.0238495/0.188309loss in batch 212: 0.0059967/0.187454
done with epoch 9
train_acc: 0.938967 (400/426)
test loss: 0.00599669
acc: 0.937061 (134/143)
loss in batch 0: 0.404739/0.404739loss in batch 1: 0.0633392/0.234039loss in batch 2: 0.0365448/0.168198loss in batch 3: 0.0271301/0.13295loss in batch 4: 0.01297/0.108948loss in batch 5: 0.221146/0.127655loss in batch 6: 0.0474701/0.11618loss in batch 7: 0.0537415/0.108383loss in batch 8: 0.0686493/0.103973loss in batch 9: 0.092865/0.102859loss in batch 10: 0.0421448/0.0973358loss in batch 11: 0.276047/0.112228loss in batch 12: 0.121872/0.112976loss in batch 13: 0.0242767/0.106628loss in batch 14: 0.537064/0.13533loss in batch 15: 0.129578/0.134964loss in batch 16: 0.0510406/0.130035loss in batch 17: 0.0267487/0.124298loss in batch 18: 0.0903015/0.122513loss in batch 19: 0.0156097/0.117172loss in batch 20: 0.116257/0.117126loss in batch 21: 0.0618896/0.114609loss in batch 22: 0.0612946/0.112289loss in batch 23: 0.0476837/0.109604loss in batch 24: 0.456039/0.123459loss in batch 25: 0.0959015/0.122406loss in batch 26: 0.0243683/0.118774loss in batch 27: 0.301483/0.12529loss in batch 28: 0.177338/0.12709loss in batch 29: 0.080246/0.125519loss in batch 30: 0.1008/0.124725loss in batch 31: 1.16986/0.157394loss in batch 32: 0.140121/0.15686loss in batch 33: 0.0835419/0.154709loss in batch 34: 1.39951/0.190277loss in batch 35: 0.0663757/0.186829loss in batch 36: 0.193817/0.187027loss in batch 37: 0.0278778/0.182831loss in batch 38: 0.0453491/0.179306loss in batch 39: 0.319946/0.182831loss in batch 40: 0.153137/0.182098loss in batch 41: 0.60939/0.192276loss in batch 42: 0.289429/0.194534loss in batch 43: 0.343369/0.197922loss in batch 44: 0.017395/0.193909loss in batch 45: 0.68689/0.204636loss in batch 46: 0.243561/0.20546loss in batch 47: 0.200912/0.205353loss in batch 48: 0.391602/0.209152loss in batch 49: 0.0579681/0.206131loss in batch 50: 0.169022/0.205399loss in batch 51: 0.0670166/0.202744loss in batch 52: 0.0968933/0.200745loss in batch 53: 0.315826/0.202881loss in batch 54: 0.157532/0.202042loss in batch 55: 0.0104675/0.198624loss in batch 56: 1.04095/0.213409loss in batch 57: 0.03302/0.210297loss in batch 58: 0.110931/0.208618loss in batch 59: 0.0200806/0.205475loss in batch 60: 0.164886/0.204819loss in batch 61: 0.177582/0.204361loss in batch 62: 0.0260162/0.201538loss in batch 63: 0.0350189/0.198944loss in batch 64: 0.587143/0.20491loss in batch 65: 0.0800018/0.203018loss in batch 66: 0.0331726/0.200485loss in batch 67: 0.00721741/0.197632loss in batch 68: 0.292389/0.199005loss in batch 69: 0.50032/0.203308loss in batch 70: 0.0466003/0.201111loss in batch 71: 0.534561/0.205734loss in batch 72: 0.12056/0.204575loss in batch 73: 0.390457/0.207092loss in batch 74: 0.0464783/0.204941loss in batch 75: 0.00624084/0.202332loss in batch 76: 0.442902/0.20546loss in batch 77: 0.037796/0.203293loss in batch 78: 0.29155/0.204422loss in batch 79: 0.0299683/0.20224loss in batch 80: 0.127502/0.201309loss in batch 81: 0.501892/0.204987loss in batch 82: 0.0576935/0.203217loss in batch 83: 0.331772/0.204742loss in batch 84: 0.24379/0.2052loss in batch 85: 0.0461731/0.203354loss in batch 86: 0.00814819/0.201111loss in batch 87: 0.165253/0.200699loss in batch 88: 0.0410461/0.198898loss in batch 89: 0.39093/0.201035loss in batch 90: 0.0940704/0.19986loss in batch 91: 0.364456/0.201645loss in batch 92: 0.194183/0.201569loss in batch 93: 0.0043335/0.199478loss in batch 94: 0.539124/0.203049loss in batch 95: 0.206253/0.203079loss in batch 96: 0.0312347/0.201309loss in batch 97: 1.01796/0.209641loss in batch 98: 0.144638/0.208984loss in batch 99: 0.378342/0.210693loss in batch 100: 0.283691/0.211411loss in batch 101: 0.0205383/0.209534loss in batch 102: 0.0714417/0.208191loss in batch 103: 0.114456/0.207291loss in batch 104: 0.0344086/0.205643loss in batch 105: 0.0248718/0.203949loss in batch 106: 0.525589/0.206955loss in batch 107: 0.03862/0.205383loss in batch 108: 0.0293579/0.203766loss in batch 109: 0.399399/0.205551loss in batch 110: 0.153122/0.205063loss in batch 111: 0.0733185/0.203903loss in batch 112: 0.253937/0.204346loss in batch 113: 0.0962524/0.203384loss in batch 114: 0.162949/0.203033loss in batch 115: 0.0232239/0.201492loss in batch 116: 0.1138/0.200745loss in batch 117: 0.171875/0.2005loss in batch 118: 0.0537109/0.199265loss in batch 119: 0.0603027/0.198105loss in batch 120: 0.121719/0.197479loss in batch 121: 0.0340729/0.196136loss in batch 122: 0.0347748/0.194824loss in batch 123: 0.0885162/0.193954loss in batch 124: 0.257126/0.194473loss in batch 125: 0.0751648/0.193527loss in batch 126: 0.5271/0.196152loss in batch 127: 0.3255/0.197159loss in batch 128: 0.141571/0.196732loss in batch 129: 0.0290375/0.195435loss in batch 130: 0.0946808/0.194672loss in batch 131: 0.0414276/0.193512loss in batch 132: 0.236282/0.193832loss in batch 133: 0.0549927/0.192795loss in batch 134: 0.0169678/0.191498loss in batch 135: 0.0918884/0.190765loss in batch 136: 0.0735016/0.189896loss in batch 137: 0.110672/0.189331loss in batch 138: 0.0631714/0.188431loss in batch 139: 0.0301361/0.187302loss in batch 140: 0.157196/0.187088loss in batch 141: 0.0784454/0.186325loss in batch 142: 0.0253601/0.185196loss in batch 143: 0.0518951/0.184265loss in batch 144: 0.111343/0.183762loss in batch 145: 0.108337/0.183243loss in batch 146: 0.0808411/0.182541loss in batch 147: 0.608002/0.185425loss in batch 148: 0.157974/0.185242loss in batch 149: 0.0518341/0.184341loss in batch 150: 0.0339966/0.18335loss in batch 151: 0.234207/0.183685loss in batch 152: 0.0875244/0.18306loss in batch 153: 0.0284424/0.182068loss in batch 154: 0.365189/0.183243loss in batch 155: 0.189545/0.183273loss in batch 156: 0.214676/0.183472loss in batch 157: 0.221939/0.183731loss in batch 158: 0.127228/0.183365loss in batch 159: 0.0796814/0.182724loss in batch 160: 0.056778/0.181946loss in batch 161: 0.129089/0.18161loss in batch 162: 0.445709/0.183228loss in batch 163: 0.408035/0.184601loss in batch 164: 0.415604/0.186005loss in batch 165: 0.0880127/0.18541loss in batch 166: 0.634216/0.18811loss in batch 167: 0.0366821/0.187195loss in batch 168: 0.445984/0.188721loss in batch 169: 0.0376892/0.187836loss in batch 170: 0.061203/0.187103loss in batch 171: 0.149185/0.186874loss in batch 172: 0.191101/0.18689loss in batch 173: 0.0418854/0.186066loss in batch 174: 0.0856323/0.185501loss in batch 175: 0.191116/0.185532loss in batch 176: 0.0238647/0.184601loss in batch 177: 0.345245/0.185516loss in batch 178: 0.0433044/0.184723loss in batch 179: 0.729355/0.187744loss in batch 180: 0.0716248/0.187119loss in batch 181: 0.182571/0.187088loss in batch 182: 0.27739/0.187576loss in batch 183: 0.0257721/0.186691loss in batch 184: 0.533768/0.188583loss in batch 185: 0.0399628/0.187775loss in batch 186: 0.256271/0.188141loss in batch 187: 0.138138/0.187866loss in batch 188: 0.304306/0.188492loss in batch 189: 0.111298/0.18808loss in batch 190: 0.0714722/0.187469loss in batch 191: 0.125076/0.187149loss in batch 192: 0.0225525/0.186295loss in batch 193: 0.0908051/0.185791loss in batch 194: 0.0587769/0.18515loss in batch 195: 0.049057/0.184448loss in batch 196: 0.0735474/0.183884loss in batch 197: 0.104156/0.183487loss in batch 198: 0.198181/0.183563loss in batch 199: 0.400879/0.184647loss in batch 200: 0.291977/0.185181loss in batch 201: 0.187683/0.185196loss in batch 202: 0.298065/0.185745loss in batch 203: 0.148422/0.185577loss in batch 204: 0.0805206/0.185059loss in batch 205: 0.331985/0.18576loss in batch 206: 0.166855/0.185684loss in batch 207: 0.0101624/0.18483loss in batch 208: 0.143524/0.184631loss in batch 209: 0.136902/0.184418loss in batch 210: 0.0726013/0.183884loss in batch 211: 0.0400238/0.183197loss in batch 212: 0.0858917/0.182739
done with epoch 10
train_acc: 0.93662 (399/426)
test loss: 0.0858915
acc: 0.937061 (134/143)
loss in batch 0: 0.0449371/0.0449371loss in batch 1: 0.0581665/0.0515442loss in batch 2: 0.129318/0.0774689loss in batch 3: 0.0566254/0.0722504loss in batch 4: 0.0718536/0.0721741loss in batch 5: 0.16394/0.0874634loss in batch 6: 0.612411/0.16246loss in batch 7: 0.48645/0.202972loss in batch 8: 0.362503/0.220703loss in batch 9: 0.257843/0.224396loss in batch 10: 0.96109/0.291382loss in batch 11: 0.345657/0.295898loss in batch 12: 0.083786/0.279587loss in batch 13: 0.0319672/0.261902loss in batch 14: 0.0580597/0.248306loss in batch 15: 0.0839081/0.238037loss in batch 16: 0.142258/0.232391loss in batch 17: 0.0591278/0.222778loss in batch 18: 0.244034/0.223892loss in batch 19: 0.29248/0.227325loss in batch 20: 0.00997925/0.216965loss in batch 21: 0.0495148/0.209351loss in batch 22: 0.0488739/0.202377loss in batch 23: 1.10246/0.239883loss in batch 24: 0.0542297/0.232468loss in batch 25: 0.884811/0.257553loss in batch 26: 0.0302734/0.24913loss in batch 27: 0.285919/0.250443loss in batch 28: 0.0449524/0.243362loss in batch 29: 0.0503998/0.236938loss in batch 30: 0.126572/0.233368loss in batch 31: 0.111328/0.229553loss in batch 32: 1.524/0.268768loss in batch 33: 0.0307922/0.26178loss in batch 34: 0.283157/0.26239loss in batch 35: 0.2715/0.262634loss in batch 36: 0.0363312/0.256516loss in batch 37: 0.158295/0.253937loss in batch 38: 0.0492859/0.248688loss in batch 39: 0.313538/0.25032loss in batch 40: 0.0203247/0.244705loss in batch 41: 0.0780945/0.240738loss in batch 42: 0.168152/0.239059loss in batch 43: 0.0628815/0.235046loss in batch 44: 0.224228/0.234818loss in batch 45: 0.17627/0.233521loss in batch 46: 0.1483/0.23172loss in batch 47: 0.452026/0.236313loss in batch 48: 0.0464935/0.232437loss in batch 49: 0.0733795/0.229248loss in batch 50: 0.0316162/0.225372loss in batch 51: 0.120956/0.223373loss in batch 52: 0.0127258/0.219391loss in batch 53: 0.0445404/0.216156loss in batch 54: 0.375595/0.219055loss in batch 55: 0.0984344/0.216904loss in batch 56: 0.0358887/0.21373loss in batch 57: 0.0571289/0.211029loss in batch 58: 0.117981/0.209457loss in batch 59: 0.0108032/0.206131loss in batch 60: 0.23027/0.206543loss in batch 61: 0.03125/0.203705loss in batch 62: 0.0656891/0.201508loss in batch 63: 0.296768/0.203003loss in batch 64: 0.0285187/0.200317loss in batch 65: 0.00750732/0.197403loss in batch 66: 0.79332/0.206299loss in batch 67: 0.0314178/0.203735loss in batch 68: 0.0767822/0.201889loss in batch 69: 0.217346/0.202103loss in batch 70: 1.11044/0.214905loss in batch 71: 0.111771/0.213455loss in batch 72: 0.288864/0.214508loss in batch 73: 0.0763245/0.212631loss in batch 74: 0.0722504/0.210754loss in batch 75: 0.042984/0.208557loss in batch 76: 0.0454254/0.206436loss in batch 77: 0.069397/0.204666loss in batch 78: 0.0374908/0.20256loss in batch 79: 0.443848/0.205566loss in batch 80: 0.263016/0.206284loss in batch 81: 0.0859222/0.204819loss in batch 82: 0.0830841/0.203354loss in batch 83: 0.265869/0.204086loss in batch 84: 0.215378/0.204224loss in batch 85: 0.0151367/0.202026loss in batch 86: 0.368149/0.203934loss in batch 87: 0.167572/0.203522loss in batch 88: 0.384262/0.205566loss in batch 89: 0.217056/0.205673loss in batch 90: 0.0554657/0.204025loss in batch 91: 0.204498/0.204041loss in batch 92: 0.286102/0.204926loss in batch 93: 0.0284882/0.203049loss in batch 94: 0.0765991/0.201706loss in batch 95: 0.110062/0.20076loss in batch 96: 0.319946/0.201981loss in batch 97: 0.0261993/0.20018loss in batch 98: 0.429871/0.202515loss in batch 99: 0.1418/0.201904loss in batch 100: 0.133942/0.201233loss in batch 101: 0.0424347/0.199677loss in batch 102: 0.144974/0.199158loss in batch 103: 0.0253296/0.197464loss in batch 104: 0.149597/0.197021loss in batch 105: 0.132248/0.196396loss in batch 106: 0.183228/0.196274loss in batch 107: 0.42804/0.198425loss in batch 108: 0.0942078/0.197464loss in batch 109: 0.386902/0.199188loss in batch 110: 0.0327911/0.197693loss in batch 111: 0.112885/0.19693loss in batch 112: 0.203781/0.196991loss in batch 113: 0.512955/0.199768loss in batch 114: 0.574097/0.203018loss in batch 115: 0.0856171/0.202026loss in batch 116: 0.113998/0.201263loss in batch 117: 0.0510254/0.199982loss in batch 118: 0.0456085/0.198685loss in batch 119: 0.164291/0.198395loss in batch 120: 0.348969/0.199646loss in batch 121: 0.087265/0.198715loss in batch 122: 0.197617/0.198715loss in batch 123: 0.0899353/0.197845loss in batch 124: 0.0991974/0.197052loss in batch 125: 0.075882/0.196091loss in batch 126: 0.0318756/0.194794loss in batch 127: 0.0383759/0.193573loss in batch 128: 0.0262604/0.192276loss in batch 129: 0.131927/0.191818loss in batch 130: 0.0796051/0.190964loss in batch 131: 0.0549774/0.189926loss in batch 132: 0.284485/0.190643loss in batch 133: 0.500732/0.192947loss in batch 134: 0.138245/0.192551loss in batch 135: 0.259872/0.193039loss in batch 136: 0.0472107/0.191986loss in batch 137: 0.323334/0.192932loss in batch 138: 0.059021/0.191971loss in batch 139: 0.36525/0.193207loss in batch 140: 0.155731/0.192947loss in batch 141: 0.085556/0.192184loss in batch 142: 0.0861511/0.191437loss in batch 143: 0.161514/0.191238loss in batch 144: 0.1642/0.19104loss in batch 145: 0.0439301/0.190033loss in batch 146: 0.0913086/0.189362loss in batch 147: 0.196793/0.189423loss in batch 148: 0.357285/0.190552loss in batch 149: 0.139755/0.190201loss in batch 150: 0.0280762/0.189133loss in batch 151: 0.0184631/0.188019loss in batch 152: 0.07724/0.187286loss in batch 153: 0.266052/0.18779loss in batch 154: 0.0133362/0.186661loss in batch 155: 0.766342/0.190384loss in batch 156: 0.291565/0.19104loss in batch 157: 0.0287781/0.190018loss in batch 158: 0.033905/0.189011loss in batch 159: 0.406113/0.190369loss in batch 160: 0.00730896/0.18924loss in batch 161: 0.0439911/0.188354loss in batch 162: 0.0243073/0.187347loss in batch 163: 0.0460663/0.186478loss in batch 164: 0.152344/0.186279loss in batch 165: 0.11676/0.185837loss in batch 166: 0.528015/0.187912loss in batch 167: 0.68457/0.190857loss in batch 168: 0.0595398/0.190079loss in batch 169: 0.387711/0.191238loss in batch 170: 0.323364/0.192017loss in batch 171: 0.0309448/0.191086loss in batch 172: 0.0367584/0.190186loss in batch 173: 0.217514/0.190338loss in batch 174: 0.125259/0.189972loss in batch 175: 0.144058/0.189713loss in batch 176: 0.189163/0.189713loss in batch 177: 0.0134277/0.188721loss in batch 178: 0.0505829/0.187943loss in batch 179: 0.4328/0.189301loss in batch 180: 0.1082/0.188858loss in batch 181: 0.0197449/0.187927loss in batch 182: 0.00183105/0.186905loss in batch 183: 0.157364/0.186752loss in batch 184: 0.084549/0.186203loss in batch 185: 0.0322723/0.185379loss in batch 186: 0.0747986/0.184784loss in batch 187: 0.156708/0.184631loss in batch 188: 0.119949/0.18428loss in batch 189: 1.09125/0.189056loss in batch 190: 0.0418701/0.188293loss in batch 191: 0.0387726/0.187515loss in batch 192: 0.0875397/0.186996loss in batch 193: 0.340607/0.18779loss in batch 194: 0.0642242/0.187149loss in batch 195: 0.0652466/0.186523loss in batch 196: 0.416351/0.187698loss in batch 197: 0.0815887/0.187164loss in batch 198: 0.147278/0.186966loss in batch 199: 0.0379639/0.186218loss in batch 200: 0.133911/0.185959loss in batch 201: 0.0188446/0.18512loss in batch 202: 0.00537109/0.184235loss in batch 203: 0.117783/0.183929loss in batch 204: 0.0282898/0.183167loss in batch 205: 0.0498505/0.18251loss in batch 206: 0.403488/0.183578loss in batch 207: 0.0315704/0.182846loss in batch 208: 0.519211/0.184464loss in batch 209: 0.0641327/0.183884loss in batch 210: 0.00332642/0.183029loss in batch 211: 0.0569763/0.182434loss in batch 212: 0.135483/0.182205
done with epoch 11
train_acc: 0.929577 (396/426)
test loss: 0.135483
acc: 0.937061 (134/143)
loss in batch 0: 0.0737152/0.0737152loss in batch 1: 0.110031/0.0918732loss in batch 2: 0.0394592/0.0744019loss in batch 3: 0.1633/0.0966187loss in batch 4: 0.0141449/0.0801239loss in batch 5: 0.325104/0.120956loss in batch 6: 0.121796/0.121078loss in batch 7: 0.130676/0.122284loss in batch 8: 0.0407257/0.11322loss in batch 9: 0.0652618/0.108429loss in batch 10: 0.273605/0.123444loss in batch 11: 0.188309/0.128845loss in batch 12: 0.070816/0.124374loss in batch 13: 0.010025/0.116211loss in batch 14: 0.0445557/0.111435loss in batch 15: 0.696533/0.147995loss in batch 16: 0.0656128/0.143158loss in batch 17: 0.205872/0.146637loss in batch 18: 0.0413971/0.141113loss in batch 19: 0.0564575/0.136871loss in batch 20: 0.0805206/0.134186loss in batch 21: 0.0687866/0.131226loss in batch 22: 0.0778351/0.128891loss in batch 23: 0.353851/0.13826loss in batch 24: 0.811707/0.165207loss in batch 25: 0.0562134/0.161026loss in batch 26: 0.0705719/0.157654loss in batch 27: 0.159729/0.157745loss in batch 28: 0.0316925/0.153397loss in batch 29: 0.399948/0.161621loss in batch 30: 0.304459/0.166214loss in batch 31: 0.167313/0.166245loss in batch 32: 0.0487823/0.162689loss in batch 33: 0.0184937/0.158447loss in batch 34: 0.0163269/0.154388loss in batch 35: 0.0666199/0.151947loss in batch 36: 0.0180054/0.148331loss in batch 37: 0.0613861/0.146057loss in batch 38: 0.0635529/0.143921loss in batch 39: 0.148819/0.144043loss in batch 40: 0.187531/0.145111loss in batch 41: 0.00653076/0.1418loss in batch 42: 0.00465393/0.138626loss in batch 43: 0.447952/0.14566loss in batch 44: 0.109131/0.144836loss in batch 45: 0.191513/0.145859loss in batch 46: 0.26062/0.1483loss in batch 47: 0.0755005/0.146774loss in batch 48: 0.16861/0.147217loss in batch 49: 0.000732422/0.144287loss in batch 50: 0.0555878/0.142548loss in batch 51: 0.303223/0.145645loss in batch 52: 0.0699463/0.144211loss in batch 53: 0.201462/0.145279loss in batch 54: 0.243927/0.147079loss in batch 55: 0.0502625/0.14534loss in batch 56: 0.0493164/0.143661loss in batch 57: 0.0450592/0.141968loss in batch 58: 0.0597992/0.140564loss in batch 59: 0.0419617/0.138916loss in batch 60: 0.123398/0.138672loss in batch 61: 0.127243/0.138474loss in batch 62: 0.0412903/0.136932loss in batch 63: 0.0324707/0.135315loss in batch 64: 0.0283508/0.133667loss in batch 65: 0.00747681/0.131744loss in batch 66: 0.0420074/0.130402loss in batch 67: 0.0115662/0.128662loss in batch 68: 0.0098877/0.126938loss in batch 69: 0.199371/0.127975loss in batch 70: 0.023941/0.126511loss in batch 71: 0.10907/0.126266loss in batch 72: 0.0771637/0.125595loss in batch 73: 0.234802/0.127075loss in batch 74: 0.161499/0.127533loss in batch 75: 0.0599365/0.126648loss in batch 76: 0.164261/0.127121loss in batch 77: 0.554794/0.132614loss in batch 78: 0.0820618/0.131973loss in batch 79: 0.100235/0.131577loss in batch 80: 0.0706177/0.130829loss in batch 81: 0.0607758/0.129959loss in batch 82: 0.0449371/0.128937loss in batch 83: 0.0740662/0.128296loss in batch 84: 0.025589/0.127075loss in batch 85: 1.23706/0.139999loss in batch 86: 0.579361/0.14505loss in batch 87: 0.0796661/0.144287loss in batch 88: 0.260147/0.145599loss in batch 89: 0.108826/0.145187loss in batch 90: 0.0395966/0.144028loss in batch 91: 0.0578918/0.143097loss in batch 92: 0.182983/0.143524loss in batch 93: 0.0358429/0.14238loss in batch 94: 0.0334015/0.14122loss in batch 95: 0.508667/0.14505loss in batch 96: 0.0109711/0.143677loss in batch 97: 0.62912/0.148636loss in batch 98: 0.212677/0.149261loss in batch 99: 0.0853882/0.148636loss in batch 100: 0.0413666/0.147583loss in batch 101: 0.0808105/0.146927loss in batch 102: 0.0333557/0.145813loss in batch 103: 0.203995/0.146378loss in batch 104: 0.106094/0.145996loss in batch 105: 0.264236/0.14711loss in batch 106: 0.514999/0.150543loss in batch 107: 0.129639/0.150345loss in batch 108: 0.0677795/0.149597loss in batch 109: 0.0863495/0.149017loss in batch 110: 0.464554/0.151871loss in batch 111: 0.196213/0.152252loss in batch 112: 0.28038/0.153397loss in batch 113: 0.27916/0.154495loss in batch 114: 0.193985/0.154846loss in batch 115: 0.0153656/0.153641loss in batch 116: 0.0141296/0.152435loss in batch 117: 0.38739/0.154434loss in batch 118: 0.115311/0.154099loss in batch 119: 0.187531/0.154373loss in batch 120: 0.0215302/0.153275loss in batch 121: 0.152725/0.153275loss in batch 122: 0.57164/0.156693loss in batch 123: 0.32312/0.15802loss in batch 124: 0.150742/0.157974loss in batch 125: 0.375305/0.159698loss in batch 126: 0.0922394/0.159164loss in batch 127: 0.223267/0.159653loss in batch 128: 0.00495911/0.158463loss in batch 129: 0.277328/0.159378loss in batch 130: 0.135529/0.159195loss in batch 131: 0.348785/0.160629loss in batch 132: 0.107681/0.160233loss in batch 133: 0.357635/0.161713loss in batch 134: 0.345261/0.163071loss in batch 135: 0.15358/0.16301loss in batch 136: 0.142456/0.162842loss in batch 137: 0.216843/0.163239loss in batch 138: 0.270218/0.164001loss in batch 139: 0.0199127/0.162979loss in batch 140: 0.0688324/0.162308loss in batch 141: 0.00314331/0.161194loss in batch 142: 0.0410767/0.160355loss in batch 143: 0.0270233/0.159439loss in batch 144: 0.069519/0.158798loss in batch 145: 0.0520782/0.158066loss in batch 146: 0.212189/0.158432loss in batch 147: 0.0828094/0.157928loss in batch 148: 0.608536/0.16095loss in batch 149: 1.34219/0.168823loss in batch 150: 0.148834/0.168686loss in batch 151: 0.0728455/0.16806loss in batch 152: 0.0301361/0.167175loss in batch 153: 0.00239563/0.166092loss in batch 154: 0.160858/0.166061loss in batch 155: 0.0292358/0.165192loss in batch 156: 0.365494/0.166458loss in batch 157: 1.60141/0.175552loss in batch 158: 0.0952911/0.175034loss in batch 159: 0.0405884/0.174194loss in batch 160: 0.534927/0.176437loss in batch 161: 0.067688/0.175766loss in batch 162: 0.124191/0.175461loss in batch 163: 0.0693665/0.174789loss in batch 164: 0.269165/0.175369loss in batch 165: 0.0096283/0.174377loss in batch 166: 0.0115509/0.173401loss in batch 167: 0.0669098/0.172775loss in batch 168: 0.182159/0.172821loss in batch 169: 0.0173798/0.171906loss in batch 170: 0.0469818/0.171173loss in batch 171: 0.512299/0.173172loss in batch 172: 0.0724335/0.172577loss in batch 173: 0.0143738/0.171661loss in batch 174: 0.108429/0.17131loss in batch 175: 0.208603/0.171524loss in batch 176: 0.085144/0.171036loss in batch 177: 0.29393/0.171722loss in batch 178: 0.0724487/0.171173loss in batch 179: 0.250381/0.171616loss in batch 180: 0.0767365/0.171082loss in batch 181: 0.0448303/0.170395loss in batch 182: 0.234833/0.170746loss in batch 183: 0.271896/0.17128loss in batch 184: 0.38765/0.17247loss in batch 185: 0.0409698/0.171753loss in batch 186: 0.39595/0.172958loss in batch 187: 0.181808/0.172989loss in batch 188: 0.0191498/0.17218loss in batch 189: 0.988083/0.176483loss in batch 190: 0.0407715/0.175766loss in batch 191: 0.324539/0.176544loss in batch 192: 0.0251923/0.175766loss in batch 193: 0.0228882/0.174973loss in batch 194: 0.0176544/0.174164loss in batch 195: 0.14241/0.174011loss in batch 196: 0.472946/0.175522loss in batch 197: 0.569183/0.177521loss in batch 198: 0.0843658/0.177032loss in batch 199: 0.0799866/0.176559loss in batch 200: 0.203247/0.176697loss in batch 201: 0.120483/0.176422loss in batch 202: 0.0349426/0.17572loss in batch 203: 0.103836/0.175354loss in batch 204: 0.169479/0.175339loss in batch 205: 0.0669708/0.174805loss in batch 206: 0.184921/0.174866loss in batch 207: 0.0848694/0.174423loss in batch 208: 0.525787/0.176102loss in batch 209: 0.0937958/0.17572loss in batch 210: 0.0509186/0.175125loss in batch 211: 0.118668/0.17485loss in batch 212: 0.0911255/0.174454
done with epoch 12
train_acc: 0.938967 (400/426)
test loss: 0.0911253
acc: 0.923075 (132/143)
loss in batch 0: 0.0239868/0.0239868loss in batch 1: 0.139572/0.0817719loss in batch 2: 0.153595/0.105713loss in batch 3: 0.411026/0.182053loss in batch 4: 0.151855/0.17601loss in batch 5: 0.343643/0.203949loss in batch 6: 0.122162/0.192261loss in batch 7: 0.110825/0.182083loss in batch 8: 0.213913/0.185608loss in batch 9: 0.19426/0.186478loss in batch 10: 0.314194/0.19809loss in batch 11: 0.109222/0.190689loss in batch 12: 0.0493622/0.179825loss in batch 13: 0.00540161/0.167358loss in batch 14: 0.957947/0.220062loss in batch 15: 0.127838/0.214294loss in batch 16: 0.0832825/0.206589loss in batch 17: 0.286942/0.211044loss in batch 18: 0.00500488/0.200211loss in batch 19: 0.223816/0.201385loss in batch 20: 0.0775146/0.195496loss in batch 21: 0.336212/0.201889loss in batch 22: 0.107117/0.197769loss in batch 23: 0.064621/0.192215loss in batch 24: 0.0961609/0.188385loss in batch 25: 0.0403137/0.182678loss in batch 26: 0.0184174/0.17659loss in batch 27: 0.306091/0.181229loss in batch 28: 0.432114/0.18988loss in batch 29: 0.179016/0.189514loss in batch 30: 0.353333/0.194794loss in batch 31: 0.280457/0.197464loss in batch 32: 0.0173035/0.192017loss in batch 33: 0.0168304/0.186859loss in batch 34: 0.00924683/0.181793loss in batch 35: 0.0273743/0.177505loss in batch 36: 0.0395203/0.173782loss in batch 37: 0.0448914/0.17038loss in batch 38: 0.158722/0.17009loss in batch 39: 0.0728455/0.167648loss in batch 40: 0.103165/0.166077loss in batch 41: 0.0406189/0.163101loss in batch 42: 0.0527039/0.160522loss in batch 43: 0.0440216/0.157867loss in batch 44: 0.229568/0.15947loss in batch 45: 0.039444/0.15686loss in batch 46: 0.049408/0.154572loss in batch 47: 0.0541687/0.152466loss in batch 48: 0.473373/0.159027loss in batch 49: 0.209229/0.160034loss in batch 50: 0.202637/0.160873loss in batch 51: 0.0315857/0.158386loss in batch 52: 0.0799866/0.156891loss in batch 53: 0.0621338/0.155151loss in batch 54: 0.0897369/0.153961loss in batch 55: 0.0867767/0.152756loss in batch 56: 0.589005/0.160416loss in batch 57: 0.112091/0.159592loss in batch 58: 0.0568237/0.157837loss in batch 59: 0.0986938/0.15686loss in batch 60: 0.00630188/0.154373loss in batch 61: 0.228165/0.155563loss in batch 62: 0.0970154/0.154633loss in batch 63: 0.356949/0.157806loss in batch 64: 0.147888/0.157654loss in batch 65: 0.0792847/0.156464loss in batch 66: 0.0517578/0.154907loss in batch 67: 0.0209961/0.152939loss in batch 68: 0.900314/0.163773loss in batch 69: 0.0335236/0.161911loss in batch 70: 0.0526123/0.160355loss in batch 71: 0.0758057/0.159195loss in batch 72: 0.30275/0.161163loss in batch 73: 0.013031/0.159149loss in batch 74: 0.0726013/0.158005loss in batch 75: 0.0788574/0.156967loss in batch 76: 0.047287/0.155533loss in batch 77: 0.408844/0.158783loss in batch 78: 0.0397644/0.157272loss in batch 79: 0.0312805/0.155701loss in batch 80: 0.122574/0.155289loss in batch 81: 0.053009/0.154053loss in batch 82: 0.0575409/0.152878loss in batch 83: 0.0545349/0.151703loss in batch 84: 0.264069/0.15303loss in batch 85: 0.0587921/0.151947loss in batch 86: 0.161346/0.152054loss in batch 87: 0.290237/0.153625loss in batch 88: 0.366135/0.156006loss in batch 89: 0.0550385/0.154877loss in batch 90: 0.114426/0.154434loss in batch 91: 0.00843811/0.152847loss in batch 92: 0.425095/0.155777loss in batch 93: 0.19545/0.156204loss in batch 94: 0.0135651/0.154709loss in batch 95: 0.034256/0.153442loss in batch 96: 0.486496/0.156891loss in batch 97: 0.0511169/0.155792loss in batch 98: 0.0609589/0.154846loss in batch 99: 0.022171/0.153519loss in batch 100: 0.0183563/0.152176loss in batch 101: 0.0347595/0.151031loss in batch 102: 0.0487976/0.150024loss in batch 103: 0.074295/0.149292loss in batch 104: 0.130524/0.149124loss in batch 105: 0.0948029/0.148605loss in batch 106: 0.161774/0.148743loss in batch 107: 0.0497742/0.147812loss in batch 108: 0.125931/0.147614loss in batch 109: 0.00567627/0.146332loss in batch 110: 0.436478/0.148941loss in batch 111: 0.153763/0.148987loss in batch 112: 0.14624/0.148956loss in batch 113: 0.0730743/0.1483loss in batch 114: 0.034256/0.147308loss in batch 115: 0.795425/0.152893loss in batch 116: 0.266037/0.153854loss in batch 117: 0.226303/0.154465loss in batch 118: 0.0748901/0.153793loss in batch 119: 0.0472412/0.152908loss in batch 120: 0.0115356/0.151733loss in batch 121: 0.214432/0.152252loss in batch 122: 0.324493/0.153656loss in batch 123: 0.124344/0.153427loss in batch 124: 0.132782/0.153259loss in batch 125: 0.0491486/0.15242loss in batch 126: 0.183472/0.152679loss in batch 127: 0.131332/0.152496loss in batch 128: 0.397186/0.154419loss in batch 129: 0.00512695/0.153259loss in batch 130: 0.0285187/0.152298loss in batch 131: 0.27536/0.153244loss in batch 132: 0.102936/0.152863loss in batch 133: 0.053421/0.15213loss in batch 134: 0.0533142/0.151382loss in batch 135: 0.0101318/0.150345loss in batch 136: 1.97331/0.163651loss in batch 137: 0.0274963/0.162659loss in batch 138: 0.0746307/0.162033loss in batch 139: 0.764481/0.166336loss in batch 140: 0.0469055/0.165497loss in batch 141: 0.036499/0.164581loss in batch 142: 0.0159912/0.163544loss in batch 143: 0.308853/0.164551loss in batch 144: 0.0093689/0.163483loss in batch 145: 0.120087/0.163193loss in batch 146: 0.123276/0.162918loss in batch 147: 0.26413/0.163605loss in batch 148: 0.00271606/0.162506loss in batch 149: 0.130798/0.162308loss in batch 150: 0.255951/0.162918loss in batch 151: 0.0209503/0.161987loss in batch 152: 0.0197449/0.161057loss in batch 153: 0.166199/0.161102loss in batch 154: 0.0890656/0.160629loss in batch 155: 0.369781/0.161972loss in batch 156: 0.0151672/0.161041loss in batch 157: 0.217987/0.161392loss in batch 158: 0.135681/0.16124loss in batch 159: 0.161346/0.161224loss in batch 160: 0.0722351/0.160675loss in batch 161: 0.00744629/0.159729loss in batch 162: 0.0111237/0.158829loss in batch 163: 0.155151/0.158798loss in batch 164: 0.100052/0.158447loss in batch 165: 0.309219/0.159348loss in batch 166: 0.0831909/0.15889loss in batch 167: 0.023941/0.158096loss in batch 168: 0.041687/0.15741loss in batch 169: 0.0188904/0.156586loss in batch 170: 0.415054/0.158112loss in batch 171: 0.0507965/0.157486loss in batch 172: 0.34285/0.158539loss in batch 173: 0.397049/0.159927loss in batch 174: 0.38681/0.161224loss in batch 175: 0.0210266/0.160416loss in batch 176: 0.0895233/0.160019loss in batch 177: 0.151443/0.159973loss in batch 178: 0.0749359/0.1595loss in batch 179: 0.0317078/0.158798loss in batch 180: 0.161194/0.158798loss in batch 181: 0.0263977/0.158081loss in batch 182: 0.175461/0.158157loss in batch 183: 0.101593/0.157867loss in batch 184: 0.122238/0.157669loss in batch 185: 0.28894/0.158371loss in batch 186: 0.355286/0.159439loss in batch 187: 0.382721/0.160614loss in batch 188: 0.0409088/0.159988loss in batch 189: 0.334839/0.160904loss in batch 190: 0.0968781/0.160568loss in batch 191: 0.169586/0.160614loss in batch 192: 0.0111389/0.159836loss in batch 193: 0.0556641/0.159302loss in batch 194: 0.337418/0.160217loss in batch 195: 0.209854/0.160477loss in batch 196: 0.025528/0.15979loss in batch 197: 0.0437164/0.159195loss in batch 198: 0.0611572/0.158707loss in batch 199: 0.0190125/0.158005loss in batch 200: 0.377625/0.159103loss in batch 201: 0.0343323/0.158478loss in batch 202: 0.299759/0.15918loss in batch 203: 0.342819/0.16008loss in batch 204: 1.20229/0.165161loss in batch 205: 0.036377/0.164536loss in batch 206: 0.623291/0.166763loss in batch 207: 0.0429688/0.166168loss in batch 208: 0.316116/0.16687loss in batch 209: 0.898102/0.170364loss in batch 210: 0.0437317/0.169754loss in batch 211: 0.0866241/0.169373loss in batch 212: 0.054184/0.168823
done with epoch 13
train_acc: 0.946009 (403/426)
test loss: 0.0541838
acc: 0.937061 (134/143)
loss in batch 0: 1.59387/1.59387loss in batch 1: 0.0379791/0.815933loss in batch 2: 0.0278168/0.553223loss in batch 3: 0.670639/0.582565loss in batch 4: 0.0535583/0.476761loss in batch 5: 0.41214/0.466003loss in batch 6: 0.0347595/0.404404loss in batch 7: 0.326324/0.394638loss in batch 8: 0.0184326/0.352829loss in batch 9: 0.594116/0.376953loss in batch 10: 0.285065/0.368607loss in batch 11: 0.126083/0.348389loss in batch 12: 0.128387/0.331482loss in batch 13: 0.368439/0.334122loss in batch 14: 0.125107/0.320175loss in batch 15: 0.0666962/0.304337loss in batch 16: 0.253113/0.301315loss in batch 17: 0.116882/0.291077loss in batch 18: 0.00883484/0.276215loss in batch 19: 0.115753/0.268188loss in batch 20: 0.051178/0.257858loss in batch 21: 0.0455933/0.248215loss in batch 22: 0.28009/0.249603loss in batch 23: 0.304626/0.251892loss in batch 24: 0.0158691/0.242462loss in batch 25: 0.134705/0.238312loss in batch 26: 0.28154/0.239914loss in batch 27: 0.084198/0.23436loss in batch 28: 0.207733/0.233429loss in batch 29: 0.144211/0.230469loss in batch 30: 0.182953/0.228912loss in batch 31: 0.0274963/0.222626loss in batch 32: 0.472916/0.230209loss in batch 33: 0.0862122/0.225983loss in batch 34: 0.0359497/0.220551loss in batch 35: 0.456482/0.227112loss in batch 36: 0.0906372/0.223419loss in batch 37: 0.510498/0.230972loss in batch 38: 0.0142822/0.225418loss in batch 39: 0.112198/0.222595loss in batch 40: 0.01828/0.217606loss in batch 41: 0.0451508/0.213486loss in batch 42: 0.0328369/0.20929loss in batch 43: 0.695053/0.220322loss in batch 44: 0.0500031/0.216537loss in batch 45: 0.187485/0.215912loss in batch 46: 0.0547333/0.212494loss in batch 47: 0.0678406/0.209473loss in batch 48: 0.112198/0.207489loss in batch 49: 0.038681/0.204102loss in batch 50: 0.289719/0.205795loss in batch 51: 0.0699158/0.203171loss in batch 52: 0.107544/0.201385loss in batch 53: 0.0699768/0.198944loss in batch 54: 0.099472/0.197128loss in batch 55: 0.0265656/0.194092loss in batch 56: 0.0957184/0.192352loss in batch 57: 0.0202789/0.189392loss in batch 58: 0.0878754/0.187683loss in batch 59: 0.0276642/0.185013loss in batch 60: 0.0384369/0.182602loss in batch 61: 0.353271/0.185349loss in batch 62: 0.16658/0.185059loss in batch 63: 0.0668488/0.183212loss in batch 64: 1.07959/0.197006loss in batch 65: 0.191422/0.19693loss in batch 66: 0.365311/0.199432loss in batch 67: 0.0366364/0.197037loss in batch 68: 0.0861969/0.195435loss in batch 69: 0.207855/0.195602loss in batch 70: 0.01828/0.193115loss in batch 71: 0.183517/0.192978loss in batch 72: 0.0917969/0.191605loss in batch 73: 1.52588e-05/0.189011loss in batch 74: 0.0709991/0.187424loss in batch 75: 0.0501251/0.185608loss in batch 76: 0.0568237/0.18396loss in batch 77: 0.155762/0.183594loss in batch 78: 0.548386/0.188202loss in batch 79: 0.442123/0.191376loss in batch 80: 0.152679/0.190903loss in batch 81: 0.216187/0.191208loss in batch 82: 0.0599213/0.189621loss in batch 83: 0.0594788/0.18808loss in batch 84: 0.204468/0.188263loss in batch 85: 0.152832/0.187866loss in batch 86: 0.0597076/0.186386loss in batch 87: 0.131317/0.18576loss in batch 88: 0.223038/0.186172loss in batch 89: 0.0107117/0.184235loss in batch 90: 0.120117/0.183533loss in batch 91: 0.0367126/0.181931loss in batch 92: 0.241028/0.182571loss in batch 93: 0.0538483/0.181198loss in batch 94: 0.136414/0.180725loss in batch 95: 0.081192/0.179672loss in batch 96: 0.513763/0.183136loss in batch 97: 0.040329/0.181671loss in batch 98: 0.0367737/0.180206loss in batch 99: 0.17366/0.180145loss in batch 100: 0.0366516/0.178726loss in batch 101: 0.121216/0.178162loss in batch 102: 0.513138/0.181412loss in batch 103: 0.0290833/0.179947loss in batch 104: 0.0203857/0.178436loss in batch 105: 0.179779/0.178436loss in batch 106: 0.100082/0.177704loss in batch 107: 0.0639648/0.176666loss in batch 108: 0.0814819/0.175781loss in batch 109: 0.114517/0.175232loss in batch 110: 0.0132446/0.173767loss in batch 111: 0.00942993/0.172302loss in batch 112: 0.420929/0.1745loss in batch 113: 0.054184/0.173431loss in batch 114: 0.12149/0.172989loss in batch 115: 0.0271454/0.171738loss in batch 116: 0.103973/0.171158loss in batch 117: 0.0254669/0.169922loss in batch 118: 0.0937347/0.169281loss in batch 119: 0.234756/0.16983loss in batch 120: 0.210831/0.170166loss in batch 121: 0.284241/0.171097loss in batch 122: 0.0377808/0.170013loss in batch 123: 0.577011/0.173294loss in batch 124: 0.127579/0.172928loss in batch 125: 0.160339/0.172836loss in batch 126: 0.0595856/0.171951loss in batch 127: 0.0834808/0.171249loss in batch 128: 0.0160217/0.170059loss in batch 129: 0.0883179/0.169418loss in batch 130: 0.0409698/0.168442loss in batch 131: 0.0753021/0.16774loss in batch 132: 0.0388031/0.166763loss in batch 133: 0.445374/0.168854loss in batch 134: 0.032547/0.167831loss in batch 135: 0.0539246/0.166992loss in batch 136: 0.0142059/0.165878loss in batch 137: 0.132294/0.165649loss in batch 138: 0.0964661/0.165131loss in batch 139: 0.0455475/0.164291loss in batch 140: 0.538101/0.166946loss in batch 141: 0.0411987/0.166046loss in batch 142: 0.292465/0.166931loss in batch 143: 0.0553589/0.166168loss in batch 144: 0.0914612/0.165634loss in batch 145: 0.0197601/0.164642loss in batch 146: 0.211487/0.164963loss in batch 147: 0.0597229/0.164261loss in batch 148: 0.0950012/0.163788loss in batch 149: 0.0520477/0.16304loss in batch 150: 0.458099/0.164993loss in batch 151: 0.505997/0.167236loss in batch 152: 0.0741882/0.166641loss in batch 153: 0.303955/0.167511loss in batch 154: 0.743988/0.171249loss in batch 155: 0.0316925/0.170349loss in batch 156: 0.205719/0.170563loss in batch 157: 0.370483/0.171829loss in batch 158: 0.227081/0.172195loss in batch 159: 0.401627/0.17363loss in batch 160: 0.0863037/0.17308loss in batch 161: 0.0578613/0.172363loss in batch 162: 1.11957/0.178177loss in batch 163: 0.00315857/0.177109loss in batch 164: 0.0651245/0.176422loss in batch 165: 0.0105743/0.17543loss in batch 166: 0.0202179/0.174515loss in batch 167: 0.0447998/0.173737loss in batch 168: 0.274292/0.174316loss in batch 169: 0.0359039/0.173508loss in batch 170: 0.470276/0.175247loss in batch 171: 0.0447845/0.1745loss in batch 172: 0.0105438/0.173553loss in batch 173: 0.202057/0.173691loss in batch 174: 0.0233002/0.172852loss in batch 175: 0.384155/0.174042loss in batch 176: 0.132645/0.173813loss in batch 177: 0.137985/0.173615loss in batch 178: 0.0225372/0.172775loss in batch 179: 0.156357/0.172684loss in batch 180: 0.0247498/0.17186loss in batch 181: 0.0690918/0.171295loss in batch 182: 0.031662/0.170532loss in batch 183: 0.0789795/0.170029loss in batch 184: 0.291992/0.1707loss in batch 185: 0.0323792/0.169952loss in batch 186: 0.00694275/0.169083loss in batch 187: 0.429016/0.170456loss in batch 188: 0.41098/0.171722loss in batch 189: 0.0938568/0.171326loss in batch 190: 0.382065/0.172424loss in batch 191: 0.050705/0.171799loss in batch 192: 0.106766/0.171463loss in batch 193: 0.0191803/0.17067loss in batch 194: 0.0424957/0.170013loss in batch 195: 0.0343475/0.169312loss in batch 196: 0.00604248/0.168488loss in batch 197: 0.026001/0.16777loss in batch 198: 0.725876/0.170578loss in batch 199: 0.0444946/0.169937loss in batch 200: 0.0618286/0.169418loss in batch 201: 0.0287933/0.168716loss in batch 202: 0.0589447/0.168182loss in batch 203: 0.0558167/0.167618loss in batch 204: 0.204285/0.167801loss in batch 205: 0.0258636/0.167114loss in batch 206: 0.0718384/0.166656loss in batch 207: 0.0469666/0.166077loss in batch 208: 0.0866852/0.165695loss in batch 209: 0.145432/0.165604loss in batch 210: 0.510361/0.167236loss in batch 211: 0.137878/0.167099loss in batch 212: 0.0298767/0.166443
done with epoch 14
train_acc: 0.93662 (399/426)
test loss: 0.0298766
acc: 0.944054 (135/143)
loss in batch 0: 0.07547/0.07547loss in batch 1: 0.531769/0.303619loss in batch 2: 0.105011/0.237411loss in batch 3: 0.146194/0.214615loss in batch 4: 0.171814/0.206055loss in batch 5: 0.0359192/0.177704loss in batch 6: 0.0411377/0.158188loss in batch 7: 0.155869/0.157898loss in batch 8: 0.0237885/0.14299loss in batch 9: 0.696136/0.198303loss in batch 10: 0.0950165/0.188919loss in batch 11: 0.0379333/0.176346loss in batch 12: 0.0686188/0.16806loss in batch 13: 0.499466/0.191711loss in batch 14: 0.0334625/0.181168loss in batch 15: 0.0421753/0.172485loss in batch 16: 0.231842/0.17598loss in batch 17: 0.0418701/0.168533loss in batch 18: 0.046875/0.162125loss in batch 19: 0.605133/0.18428loss in batch 20: 0.00263977/0.175629loss in batch 21: 1.2309/0.223587loss in batch 22: 0.508728/0.235992loss in batch 23: 0.00941467/0.226563loss in batch 24: 0.298508/0.229431loss in batch 25: 0.23085/0.229492loss in batch 26: 0.0433197/0.222595loss in batch 27: 0.142059/0.219711loss in batch 28: 0.208633/0.219315loss in batch 29: 0.00650024/0.212234loss in batch 30: 0.39064/0.217987loss in batch 31: 0.083374/0.213791loss in batch 32: 0.274338/0.215622loss in batch 33: 0.310684/0.218414loss in batch 34: 0.0971527/0.214951loss in batch 35: 0.0892792/0.211456loss in batch 36: 0.0332336/0.20665loss in batch 37: 0.23558/0.207397loss in batch 38: 0.104797/0.204773loss in batch 39: 0.0369873/0.200577loss in batch 40: 0.338409/0.203949loss in batch 41: 0.0231018/0.199631loss in batch 42: 0.0723419/0.196671loss in batch 43: 0.502502/0.203629loss in batch 44: 0.0424805/0.200043loss in batch 45: 0.0757141/0.197342loss in batch 46: 0.246109/0.19838loss in batch 47: 0.0558319/0.195404loss in batch 48: 0.102951/0.193512loss in batch 49: 0.0656433/0.190964loss in batch 50: 0.0176544/0.187561loss in batch 51: 0.0170898/0.184296loss in batch 52: 0.0399933/0.181564loss in batch 53: 0.508942/0.187622loss in batch 54: 0.0218048/0.184616loss in batch 55: 0.0620422/0.182419loss in batch 56: 0.341217/0.185211loss in batch 57: 0.217117/0.18576loss in batch 58: 0.070694/0.183807loss in batch 59: 0.958557/0.196716loss in batch 60: 0.0273743/0.193939loss in batch 61: 0.068573/0.191925loss in batch 62: 0.0563507/0.189774loss in batch 63: 0.263611/0.190918loss in batch 64: 0.169174/0.190598loss in batch 65: 0.0180054/0.187973loss in batch 66: 0.105728/0.186752loss in batch 67: 0.0188141/0.18428loss in batch 68: 0.158661/0.183899loss in batch 69: 0.375259/0.186646loss in batch 70: 0.368225/0.189209loss in batch 71: 0.0415955/0.187149loss in batch 72: 0.251495/0.188019loss in batch 73: 0.0368805/0.185989loss in batch 74: 0.165482/0.185715loss in batch 75: 0.0787659/0.184296loss in batch 76: 0.0218048/0.182205loss in batch 77: 0.141296/0.181671loss in batch 78: 0.0501251/0.180008loss in batch 79: 0.0836945/0.178802loss in batch 80: 0.234207/0.179489loss in batch 81: 0.100815/0.178528loss in batch 82: 0.337509/0.18045loss in batch 83: 0.0730896/0.179169loss in batch 84: 0.199326/0.179398loss in batch 85: 0.0260468/0.177628loss in batch 86: 0.0205383/0.175812loss in batch 87: 0.00413513/0.173859loss in batch 88: 0.0561981/0.172546loss in batch 89: 1.43849/0.1866loss in batch 90: 0.0323944/0.184906loss in batch 91: 0.0416718/0.18335loss in batch 92: 0.0683289/0.182129loss in batch 93: 0.0331573/0.180527loss in batch 94: 0.0750885/0.179428loss in batch 95: 0.0624847/0.178192loss in batch 96: 0.0373535/0.176758loss in batch 97: 0.0326843/0.175293loss in batch 98: 0.0840149/0.174362loss in batch 99: 0.0609131/0.173218loss in batch 100: 0.189575/0.173386loss in batch 101: 0.0507813/0.17218loss in batch 102: 0.151733/0.171997loss in batch 103: 0.438919/0.174561loss in batch 104: 0.354095/0.17627loss in batch 105: 0.0302429/0.174881loss in batch 106: 0.0746918/0.17395loss in batch 107: 0.414795/0.176178loss in batch 108: 0.109512/0.175568loss in batch 109: 0.05336/0.174454loss in batch 110: 0.311096/0.17569loss in batch 111: 0.0585022/0.174652loss in batch 112: 0.00323486/0.173126loss in batch 113: 0.698639/0.177734loss in batch 114: 0.0870056/0.176956loss in batch 115: 0.0550232/0.175903loss in batch 116: 0.303574/0.176987loss in batch 117: 0.0366821/0.175797loss in batch 118: 0.242889/0.176361loss in batch 119: 0.0225372/0.175079loss in batch 120: 0.0266876/0.173859loss in batch 121: 0.17865/0.173889loss in batch 122: 0.00813293/0.172546loss in batch 123: 0.0114594/0.171249loss in batch 124: 0.455399/0.173523loss in batch 125: 0.0558624/0.172592loss in batch 126: 0.663971/0.176453loss in batch 127: 0.0375214/0.175369loss in batch 128: 0.146515/0.175156loss in batch 129: 0.069458/0.174332loss in batch 130: 0.0392303/0.173294loss in batch 131: 0.0617065/0.172455loss in batch 132: 0.0429077/0.171478loss in batch 133: 0.0700836/0.170731loss in batch 134: 0.0343475/0.169708loss in batch 135: 0.000198364/0.168472loss in batch 136: 0.0163269/0.167358loss in batch 137: 0.15097/0.167236loss in batch 138: 0.181671/0.167343loss in batch 139: 0.164078/0.167328loss in batch 140: 0.18689/0.167465loss in batch 141: 0.312561/0.168472loss in batch 142: 0.0135498/0.167389loss in batch 143: 0.0129547/0.166336loss in batch 144: 0.013382/0.165268loss in batch 145: 0.0172882/0.164261loss in batch 146: 0.378525/0.16571loss in batch 147: 0.0909576/0.165207loss in batch 148: 0.12326/0.164932loss in batch 149: 0.0557098/0.1642loss in batch 150: 0.0180359/0.163223loss in batch 151: 0.0140228/0.162247loss in batch 152: 0.225571/0.162659loss in batch 153: 0.0125885/0.161697loss in batch 154: 0.0609283/0.161041loss in batch 155: 0.0220032/0.160156loss in batch 156: 0.378815/0.161545loss in batch 157: 0.0490875/0.160828loss in batch 158: 0.50354/0.162994loss in batch 159: 0.119965/0.16272loss in batch 160: 0.068634/0.162125loss in batch 161: 0.0319366/0.161331loss in batch 162: 0.138458/0.161194loss in batch 163: 0.0282745/0.16037loss in batch 164: 0.0451965/0.159683loss in batch 165: 0.306335/0.160553loss in batch 166: 0.068634/0.160019loss in batch 167: 0.0440826/0.159332loss in batch 168: 0.589203/0.16185loss in batch 169: 0.211136/0.162155loss in batch 170: 0.342209/0.163208loss in batch 171: 0.0972595/0.162827loss in batch 172: 0.244797/0.1633loss in batch 173: 0.131805/0.163116loss in batch 174: 0.517792/0.165146loss in batch 175: 0.0755005/0.164642loss in batch 176: 0.0296478/0.163879loss in batch 177: 0.0414276/0.163177loss in batch 178: 0.205841/0.163422loss in batch 179: 0.0338135/0.162704loss in batch 180: 0.505096/0.164597loss in batch 181: 0.503815/0.166458loss in batch 182: 0.0868225/0.166031loss in batch 183: 0.0334625/0.165298loss in batch 184: 0.149689/0.165222loss in batch 185: 0.290894/0.165894loss in batch 186: 0.0248566/0.165131loss in batch 187: 0.162018/0.165115loss in batch 188: 0.0418854/0.164459loss in batch 189: 0.0283051/0.163757loss in batch 190: 0.0130768/0.162964loss in batch 191: 0.0817413/0.162537loss in batch 192: 0.0519714/0.161972loss in batch 193: 0.0526276/0.161392loss in batch 194: 0.105194/0.161118loss in batch 195: 0.0469208/0.160538loss in batch 196: 0.146713/0.160461loss in batch 197: 0.112778/0.160217loss in batch 198: 0.114151/0.159988loss in batch 199: 0.349503/0.160934loss in batch 200: 0.210434/0.161194loss in batch 201: 0.624023/0.163467loss in batch 202: 0.574905/0.165497loss in batch 203: 0.135544/0.165359loss in batch 204: 0.00389099/0.164566loss in batch 205: 0.026062/0.16391loss in batch 206: 0.112/0.163635loss in batch 207: 0.112259/0.163406loss in batch 208: 0.258514/0.163849loss in batch 209: 0.00436401/0.163101loss in batch 210: 0.0444641/0.162521loss in batch 211: 0.142838/0.162445loss in batch 212: 0.0584412/0.161942
done with epoch 15
train_acc: 0.948357 (404/426)
test loss: 0.058441
acc: 0.937061 (134/143)
loss in batch 0: 0.164963/0.164963loss in batch 1: 0.324066/0.244507loss in batch 2: 0.134354/0.207794loss in batch 3: 0.0485687/0.167999loss in batch 4: 0.0818939/0.150772loss in batch 5: 0.014679/0.128098loss in batch 6: 0.178329/0.135254loss in batch 7: 0.0162811/0.120392loss in batch 8: 0.0134735/0.108521loss in batch 9: 0.103775/0.108047loss in batch 10: 0.0720215/0.104767loss in batch 11: 0.0263367/0.0982208loss in batch 12: 0.259888/0.110657loss in batch 13: 0.00888062/0.103394loss in batch 14: 0.177917/0.108353loss in batch 15: 0.232697/0.116135loss in batch 16: 0.153595/0.118332loss in batch 17: 0.0908356/0.116806loss in batch 18: 0.252136/0.123932loss in batch 19: 0.0839081/0.121933loss in batch 20: 0.257965/0.128403loss in batch 21: 0.0456085/0.124634loss in batch 22: 0.0166016/0.119949loss in batch 23: 0.129135/0.120331loss in batch 24: 0.0635071/0.118057loss in batch 25: 0.00917053/0.113876loss in batch 26: 0.0126343/0.110123loss in batch 27: 0.199432/0.113312loss in batch 28: 0.214539/0.116806loss in batch 29: 0.0386047/0.114197loss in batch 30: 0.169128/0.115967loss in batch 31: 0.0278778/0.11322loss in batch 32: 0.112228/0.11319loss in batch 33: 0.211319/0.116074loss in batch 34: 0.213409/0.118851loss in batch 35: 0.196548/0.121002loss in batch 36: 0.0849457/0.120026loss in batch 37: 0.0660706/0.118622loss in batch 38: 0.0292358/0.116318loss in batch 39: 0.43602/0.124313loss in batch 40: 0.0540466/0.122604loss in batch 41: 0.0423126/0.120697loss in batch 42: 0.145432/0.121262loss in batch 43: 0.0201263/0.118958loss in batch 44: 0.09935/0.11853loss in batch 45: 0.0869446/0.117844loss in batch 46: 0.0772552/0.116974loss in batch 47: 0.229965/0.119339loss in batch 48: 0.151794/0.119995loss in batch 49: 0.0547028/0.118683loss in batch 50: 0.288498/0.122025loss in batch 51: 0.0207062/0.120071loss in batch 52: 0.0705109/0.119125loss in batch 53: 1.78413/0.149963loss in batch 54: 0.473801/0.155853loss in batch 55: 0.194473/0.156555loss in batch 56: 0.00994873/0.153976loss in batch 57: 0.030304/0.15184loss in batch 58: 0.105209/0.151047loss in batch 59: 0.0151215/0.148788loss in batch 60: 0.193588/0.149521loss in batch 61: 0.0548706/0.147995loss in batch 62: 0.273376/0.149979loss in batch 63: 0.147568/0.149948loss in batch 64: 0.338593/0.152847loss in batch 65: 0.0652618/0.15152loss in batch 66: 0.0243835/0.149612loss in batch 67: 0.158127/0.14975loss in batch 68: 0.00553894/0.147659loss in batch 69: 0.0045929/0.145615loss in batch 70: 0.478806/0.150299loss in batch 71: 0.0159607/0.148438loss in batch 72: 0.0520477/0.147125loss in batch 73: 0.00765991/0.145248loss in batch 74: 0.0336761/0.143753loss in batch 75: 0.237656/0.144974loss in batch 76: 0.0153503/0.143295loss in batch 77: 0.562988/0.148682loss in batch 78: 0.11702/0.148285loss in batch 79: 0.1362/0.148132loss in batch 80: 0.0305023/0.146683loss in batch 81: 0.0303192/0.145264loss in batch 82: 0.0502167/0.144119loss in batch 83: 0.0270691/0.142715loss in batch 84: 0.122665/0.142487loss in batch 85: 0.102234/0.142029loss in batch 86: 0.0303345/0.140747loss in batch 87: 0.191071/0.141312loss in batch 88: 0.00144958/0.139725loss in batch 89: 0.00498962/0.138229loss in batch 90: 0.118164/0.138016loss in batch 91: 0.0447235/0.136993loss in batch 92: 0.38855/0.139709loss in batch 93: 0.870026/0.147476loss in batch 94: 0.0705414/0.146667loss in batch 95: 0.447998/0.149811loss in batch 96: 0.569/0.154114loss in batch 97: 0.355942/0.156189loss in batch 98: 0.0583344/0.155197loss in batch 99: 0.091156/0.154572loss in batch 100: 0.0209961/0.153229loss in batch 101: 0.0496521/0.152222loss in batch 102: 0.0597534/0.151321loss in batch 103: 0.446198/0.154144loss in batch 104: 0.023819/0.152908loss in batch 105: 0.00616455/0.151535loss in batch 106: 0.202026/0.152008loss in batch 107: 0.0361176/0.150925loss in batch 108: 0.518127/0.154297loss in batch 109: 0.372375/0.156281loss in batch 110: 0.015625/0.155014loss in batch 111: 0.00653076/0.153687loss in batch 112: 0.0167236/0.152481loss in batch 113: 0.051239/0.151581loss in batch 114: 0.09552/0.151093loss in batch 115: 0.021759/0.149994loss in batch 116: 0.0235443/0.148895loss in batch 117: 0.00715637/0.147705loss in batch 118: 0.00791931/0.14653loss in batch 119: 0.308426/0.147873loss in batch 120: 0.0142822/0.146774loss in batch 121: 0.00126648/0.145584loss in batch 122: 0.0424194/0.14473loss in batch 123: 0.189529/0.145096loss in batch 124: 0.040329/0.144257loss in batch 125: 0.0250244/0.143311loss in batch 126: 0.535294/0.146408loss in batch 127: 0.0464478/0.14563loss in batch 128: 0.0471649/0.144867loss in batch 129: 0.0783539/0.144348loss in batch 130: 0.0517883/0.143646loss in batch 131: 0.0286102/0.142776loss in batch 132: 0.0129089/0.1418loss in batch 133: 0.411057/0.143799loss in batch 134: 0.0796356/0.143341loss in batch 135: 1.05669/0.15004loss in batch 136: 0.0449982/0.149277loss in batch 137: 0.173462/0.149445loss in batch 138: 0.098465/0.149094loss in batch 139: 0.317001/0.150284loss in batch 140: 0.422684/0.152222loss in batch 141: 0.0661469/0.151611loss in batch 142: 0.0185089/0.150681loss in batch 143: 0.484207/0.152985loss in batch 144: 0.126068/0.152802loss in batch 145: 1.11618/0.159409loss in batch 146: 0.0302887/0.158524loss in batch 147: 0.332382/0.159698loss in batch 148: 0.351685/0.160995loss in batch 149: 0.0838776/0.160477loss in batch 150: 0.252411/0.161087loss in batch 151: 0.0578156/0.160416loss in batch 152: 0.969513/0.16571loss in batch 153: 0.162201/0.16568loss in batch 154: 0.0298767/0.16481loss in batch 155: 0.0498199/0.164063loss in batch 156: 0.225342/0.164459loss in batch 157: 0.0955505/0.164017loss in batch 158: 0.356232/0.165237loss in batch 159: 0.6474/0.168243loss in batch 160: 0.0451355/0.16748loss in batch 161: 0.385437/0.168823loss in batch 162: 0.0298462/0.167969loss in batch 163: 0.273209/0.16861loss in batch 164: 0.0380707/0.167816loss in batch 165: 0.115112/0.167496loss in batch 166: 0.346008/0.168564loss in batch 167: 0.126205/0.16832loss in batch 168: 0.0665283/0.167709loss in batch 169: 0.352859/0.168793loss in batch 170: 0.00306702/0.167831loss in batch 171: 0.0166931/0.166962loss in batch 172: 0.0862122/0.166489loss in batch 173: 0.046051/0.165802loss in batch 174: 0.209061/0.166046loss in batch 175: 0.050415/0.16539loss in batch 176: 0.124069/0.165161loss in batch 177: 0.0108948/0.164291loss in batch 178: 0.43988/0.165833loss in batch 179: 0.460022/0.167465loss in batch 180: 0.255112/0.167938loss in batch 181: 0.316879/0.168762loss in batch 182: 0.221634/0.169052loss in batch 183: 0.171417/0.169067loss in batch 184: 0.264816/0.169586loss in batch 185: 0.0150604/0.168762loss in batch 186: 0.00109863/0.167847loss in batch 187: 0.0408478/0.167191loss in batch 188: 0.0928955/0.166794loss in batch 189: 0.048996/0.166168loss in batch 190: 0.0581512/0.165604loss in batch 191: 0.0789032/0.165161loss in batch 192: 0.0853119/0.164734loss in batch 193: 0.035141/0.164078loss in batch 194: 0.0258942/0.163361loss in batch 195: 0.0038147/0.162552loss in batch 196: 0.0749817/0.162094loss in batch 197: 0.0596619/0.161591loss in batch 198: 0.138901/0.161469loss in batch 199: 0.0236969/0.160782loss in batch 200: 0.0186157/0.16008loss in batch 201: 0.0582275/0.159576loss in batch 202: 0.471451/0.161102loss in batch 203: 0.559296/0.163055loss in batch 204: 0.150833/0.162994loss in batch 205: 0.163498/0.16301loss in batch 206: 0.0467224/0.16243loss in batch 207: 0.016922/0.161743loss in batch 208: 0.0260925/0.161087loss in batch 209: 0.0791779/0.160706loss in batch 210: 0.0652924/0.160248loss in batch 211: 0.138382/0.160141loss in batch 212: 0.0257568/0.159515
done with epoch 16
train_acc: 0.955399 (407/426)
test loss: 0.0257568
acc: 0.951047 (136/143)
loss in batch 0: 0.0677795/0.0677795loss in batch 1: 0.0543518/0.0610657loss in batch 2: 0.0529327/0.0583496loss in batch 3: 0.258438/0.108368loss in batch 4: 0.877457/0.262192loss in batch 5: 0.0440674/0.225845loss in batch 6: 0.0927429/0.206818loss in batch 7: 0.0434418/0.186401loss in batch 8: 0.748062/0.24881loss in batch 9: 0.0491943/0.228836loss in batch 10: 0.0398254/0.21167loss in batch 11: 0.0188293/0.195602loss in batch 12: 0.1604/0.192886loss in batch 13: 0.0745392/0.184433loss in batch 14: 0.435455/0.201172loss in batch 15: 0.401169/0.213669loss in batch 16: 0.0461426/0.203812loss in batch 17: 0.444214/0.217163loss in batch 18: 0.0246887/0.207031loss in batch 19: 0.18576/0.205963loss in batch 20: 0.228958/0.207062loss in batch 21: 0.120026/0.20311loss in batch 22: 0.101761/0.198715loss in batch 23: 0.0370636/0.191971loss in batch 24: 0.0491486/0.186264loss in batch 25: 0.508362/0.198639loss in batch 26: 0.0707092/0.193909loss in batch 27: 0.0425568/0.188507loss in batch 28: 0.117935/0.186081loss in batch 29: 0.0853882/0.182709loss in batch 30: 0.0506744/0.178452loss in batch 31: 0.185898/0.178696loss in batch 32: 0.126236/0.177094loss in batch 33: 0.305176/0.180862loss in batch 34: 0.280838/0.183716loss in batch 35: 0.0149689/0.179031loss in batch 36: 0.0607605/0.175842loss in batch 37: 0.0833893/0.173401loss in batch 38: 1.26424/0.20137loss in batch 39: 0.0423126/0.197388loss in batch 40: 0.295303/0.199799loss in batch 41: 0.0321808/0.195786loss in batch 42: 0.271423/0.197556loss in batch 43: 0.00852966/0.193253loss in batch 44: 0.0317841/0.189667loss in batch 45: 0.244873/0.190872loss in batch 46: 0.0370636/0.187592loss in batch 47: 0.0158997/0.184021loss in batch 48: 0.00636292/0.180389loss in batch 49: 0.0236053/0.177246loss in batch 50: 0.0135345/0.174042loss in batch 51: 0.0398712/0.171478loss in batch 52: 0.0396881/0.168991loss in batch 53: 0.211395/0.169769loss in batch 54: 0.407654/0.174088loss in batch 55: 0.369217/0.177582loss in batch 56: 0.092926/0.176102loss in batch 57: 0.237015/0.177155loss in batch 58: 0.00813293/0.174271loss in batch 59: 0.217361/0.175003loss in batch 60: 0.011734/0.172318loss in batch 61: 0.141266/0.171814loss in batch 62: 0.0346527/0.169647loss in batch 63: 0.0598755/0.167923loss in batch 64: 0.017807/0.165619loss in batch 65: 0.0168304/0.163361loss in batch 66: 0.0758972/0.162064loss in batch 67: 0.0956573/0.161072loss in batch 68: 0.147461/0.160889loss in batch 69: 0.088028/0.159851loss in batch 70: 0.101227/0.159012loss in batch 71: 0.0837555/0.157974loss in batch 72: 0.372864/0.160919loss in batch 73: 0.382782/0.163925loss in batch 74: 0.0566864/0.162491loss in batch 75: 0.0272522/0.160706loss in batch 76: 0.287292/0.162338loss in batch 77: 0.222931/0.163132loss in batch 78: 0.370346/0.165741loss in batch 79: 0.0440216/0.16423loss in batch 80: 0.0375824/0.162659loss in batch 81: 0.363739/0.165115loss in batch 82: 0.0675964/0.16394loss in batch 83: 0.450317/0.167358loss in batch 84: 0.025589/0.16568loss in batch 85: 1.46004/0.18074loss in batch 86: 0.0313873/0.179001loss in batch 87: 0.032486/0.177338loss in batch 88: 0.213654/0.177765loss in batch 89: 0.0520325/0.176361loss in batch 90: 0.0802155/0.175308loss in batch 91: 0.260941/0.176239loss in batch 92: 0.22963/0.176804loss in batch 93: 0.304535/0.178162loss in batch 94: 0.366043/0.180145loss in batch 95: 0.0499573/0.178787loss in batch 96: 0.327759/0.180328loss in batch 97: 0.140823/0.179916loss in batch 98: 0.0331573/0.178436loss in batch 99: 0.0305634/0.176956loss in batch 100: 0.0972595/0.176163loss in batch 101: 0.0270081/0.174713loss in batch 102: 0.0136108/0.173141loss in batch 103: 0.024765/0.171722loss in batch 104: 0.275192/0.172714loss in batch 105: 0.0709839/0.171753loss in batch 106: 0.0154419/0.170288loss in batch 107: 0.061676/0.169281loss in batch 108: 0.264343/0.170151loss in batch 109: 0.284805/0.171204loss in batch 110: 0.0121613/0.169769loss in batch 111: 0.948364/0.176712loss in batch 112: 0.0343323/0.175446loss in batch 113: 0.0230408/0.174103loss in batch 114: 0.02034/0.172775loss in batch 115: 0.174103/0.172791loss in batch 116: 0.328339/0.174118loss in batch 117: 0.0871429/0.173386loss in batch 118: 0.585831/0.176849loss in batch 119: 0.0488892/0.175781loss in batch 120: 0.273087/0.176575loss in batch 121: 0.144928/0.176331loss in batch 122: 0.0389404/0.175201loss in batch 123: 0.00701904/0.173843loss in batch 124: 0.0212708/0.172623loss in batch 125: 0.016037/0.171387loss in batch 126: 0.0426636/0.17038loss in batch 127: 0.0355072/0.169327loss in batch 128: 0.386063/0.171005loss in batch 129: 0.378281/0.172607loss in batch 130: 0.036499/0.171555loss in batch 131: 0.257675/0.172211loss in batch 132: 0.246841/0.17276loss in batch 133: 0.0899506/0.172165loss in batch 134: 0.113007/0.171707loss in batch 135: 0.400818/0.173401loss in batch 136: 0.0102081/0.172211loss in batch 137: 0.0520935/0.171341loss in batch 138: 0.0326538/0.170334loss in batch 139: 0.0302734/0.169342loss in batch 140: 0.206223/0.169601loss in batch 141: 0.00761414/0.168457loss in batch 142: 0.0602722/0.167709loss in batch 143: 0.488449/0.169937loss in batch 144: 0.022644/0.168915loss in batch 145: 0.0374756/0.168015loss in batch 146: 0.0428009/0.16716loss in batch 147: 0.124527/0.16687loss in batch 148: 0.507584/0.169159loss in batch 149: 0.0835266/0.168594loss in batch 150: 0.0756531/0.167969loss in batch 151: 0.29985/0.168839loss in batch 152: 0.4086/0.17041loss in batch 153: 0.0140533/0.169403loss in batch 154: 0.128433/0.169128loss in batch 155: 0.122681/0.168839loss in batch 156: 0.0563202/0.168106loss in batch 157: 0.0371857/0.167297loss in batch 158: 0.0585938/0.166611loss in batch 159: 0.033905/0.165771loss in batch 160: 0.0355225/0.164963loss in batch 161: 0.00634766/0.163986loss in batch 162: 0.454788/0.165771loss in batch 163: 0.0734406/0.165207loss in batch 164: 0.0428162/0.164459loss in batch 165: 0.033432/0.163681loss in batch 166: 0.169571/0.163712loss in batch 167: 0.0615997/0.163116loss in batch 168: 0.484299/0.165009loss in batch 169: 0.0614166/0.164398loss in batch 170: 0.100204/0.164017loss in batch 171: 0.101913/0.163666loss in batch 172: 0.0915527/0.163239loss in batch 173: 0.0267792/0.16246loss in batch 174: 0.0291901/0.161697loss in batch 175: 0.159714/0.161682loss in batch 176: 0.226501/0.162048loss in batch 177: 0.0520477/0.161438loss in batch 178: 0.0387115/0.160751loss in batch 179: 0.182449/0.160873loss in batch 180: 0.033844/0.160156loss in batch 181: 0.243484/0.160629loss in batch 182: 0.256149/0.161148loss in batch 183: 0.052597/0.160553loss in batch 184: 0.0768433/0.160095loss in batch 185: 0.0137939/0.159317loss in batch 186: 0.360886/0.1604loss in batch 187: 0.130707/0.160233loss in batch 188: 0.0285034/0.159546loss in batch 189: 0.029892/0.158859loss in batch 190: 0.041153/0.158249loss in batch 191: 0.0126648/0.157486loss in batch 192: 0.00889587/0.156708loss in batch 193: 0.0323944/0.156082loss in batch 194: 0.101318/0.155777loss in batch 195: 0.353653/0.156799loss in batch 196: 0.0615692/0.156326loss in batch 197: 0.0369263/0.155701loss in batch 198: 0.205078/0.15596loss in batch 199: 0.241211/0.156387loss in batch 200: 0.698914/0.159088loss in batch 201: 0.0517578/0.158554loss in batch 202: 0.0123291/0.157837loss in batch 203: 0.510315/0.159561loss in batch 204: 0.102524/0.159286loss in batch 205: 0.0365906/0.158676loss in batch 206: 0.0845337/0.15834loss in batch 207: 0.00410461/0.157593loss in batch 208: 0.225555/0.157913loss in batch 209: 0.239182/0.15831loss in batch 210: 0.00700378/0.157593loss in batch 211: 0.0231476/0.156952loss in batch 212: 0.0312805/0.156372
done with epoch 17
train_acc: 0.950704 (405/426)
test loss: 0.0312805
acc: 0.944054 (135/143)
loss in batch 0: 0.209198/0.209198loss in batch 1: 1.40559/0.807404loss in batch 2: 0.0818787/0.565567loss in batch 3: 0.0292358/0.431488loss in batch 4: 0.0162506/0.348419loss in batch 5: 0.00337219/0.290924loss in batch 6: 0.164841/0.272919loss in batch 7: 0.163803/0.259277loss in batch 8: 0.110657/0.242767loss in batch 9: 0.0178833/0.220261loss in batch 10: 0.467926/0.242783loss in batch 11: 0.0584564/0.227432loss in batch 12: 0.231384/0.227737loss in batch 13: 0.0119476/0.212311loss in batch 14: 0.0216522/0.1996loss in batch 15: 0.686523/0.230042loss in batch 16: 0.490982/0.245377loss in batch 17: 0.018631/0.232788loss in batch 18: 0.154205/0.228653loss in batch 19: 0.0227509/0.218353loss in batch 20: 0.103714/0.212891loss in batch 21: 0.0122375/0.203781loss in batch 22: 0.0515137/0.197159loss in batch 23: 0.390015/0.205185loss in batch 24: 0.304199/0.209152loss in batch 25: 0.0335693/0.202393loss in batch 26: 0.039856/0.196381loss in batch 27: 0.032959/0.190536loss in batch 28: 0.033783/0.18515loss in batch 29: 0.0836945/0.181763loss in batch 30: 0.229767/0.183304loss in batch 31: 0.415924/0.190567loss in batch 32: 0.717712/0.206543loss in batch 33: 0.195801/0.206238loss in batch 34: 0.349777/0.210327loss in batch 35: 0.407349/0.215805loss in batch 36: 0.00775146/0.21019loss in batch 37: 0.058075/0.206177loss in batch 38: 0.296692/0.208496loss in batch 39: 0.298721/0.210754loss in batch 40: 0.014267/0.205963loss in batch 41: 0.0410309/0.202042loss in batch 42: 0.347504/0.205429loss in batch 43: 0.029953/0.201431loss in batch 44: 0.0757599/0.198639loss in batch 45: 0.0487518/0.195374loss in batch 46: 0.0252075/0.191757loss in batch 47: 0.03508/0.188492loss in batch 48: 0.265839/0.190063loss in batch 49: 0.0138397/0.186554loss in batch 50: 0.0846405/0.184555loss in batch 51: 0.0208435/0.181412loss in batch 52: 0.056488/0.179047loss in batch 53: 0.0309753/0.1763loss in batch 54: 0.308655/0.178711loss in batch 55: 0.0230713/0.175934loss in batch 56: 0.563095/0.182724loss in batch 57: 0.393448/0.186356loss in batch 58: 0.0741119/0.184448loss in batch 59: 0.097168/0.182999loss in batch 60: 0.0153809/0.180252loss in batch 61: 0.145187/0.179688loss in batch 62: 0.0760651/0.17804loss in batch 63: 0.014389/0.175476loss in batch 64: 0.218948/0.176163loss in batch 65: 0.0165253/0.173737loss in batch 66: 0.0415802/0.171768loss in batch 67: 0.0470276/0.169937loss in batch 68: 0.0850677/0.168701loss in batch 69: 0.536469/0.17395loss in batch 70: 0.313644/0.175934loss in batch 71: 0.146805/0.175522loss in batch 72: 0.0232239/0.173431loss in batch 73: 0.155884/0.173187loss in batch 74: 0.041214/0.171432loss in batch 75: 0.167221/0.171371loss in batch 76: 0.0951233/0.170395loss in batch 77: 0.0114594/0.16835loss in batch 78: 0.0929718/0.167404loss in batch 79: 0.563354/0.172348loss in batch 80: 0.0604248/0.170959loss in batch 81: 0.0452423/0.169434loss in batch 82: 0.0294189/0.16774loss in batch 83: 0.0247498/0.166046loss in batch 84: 0.0452576/0.164612loss in batch 85: 0.014679/0.162888loss in batch 86: 0.0155334/0.161179loss in batch 87: 0.0361176/0.15976loss in batch 88: 0.0114441/0.158096loss in batch 89: 0.00218201/0.156372loss in batch 90: 0.273209/0.157654loss in batch 91: 0.173767/0.157822loss in batch 92: 0.152496/0.157761loss in batch 93: 0.0179901/0.156281loss in batch 94: 0.470764/0.159592loss in batch 95: 0.00701904/0.15799loss in batch 96: 0.0305786/0.156693loss in batch 97: 0.0210266/0.155304loss in batch 98: 0.0730286/0.154465loss in batch 99: 0.364105/0.15657loss in batch 100: 0.0622253/0.15564loss in batch 101: 0.286697/0.156921loss in batch 102: 0.493927/0.160187loss in batch 103: 0.0594788/0.159225loss in batch 104: 0.399155/0.161514loss in batch 105: 0.0663452/0.160599loss in batch 106: 0.309555/0.161987loss in batch 107: 0.085495/0.161285loss in batch 108: 0.00352478/0.159836loss in batch 109: 0.558304/0.163467loss in batch 110: 0.00619507/0.162048loss in batch 111: 0.0895538/0.161407loss in batch 112: 0.0293427/0.160233loss in batch 113: 0.0706024/0.159454loss in batch 114: 0.149612/0.159363loss in batch 115: 0.231445/0.159988loss in batch 116: 0.0218658/0.158813loss in batch 117: 0.397644/0.160828loss in batch 118: 0.0749512/0.16011loss in batch 119: 0.01828/0.158936loss in batch 120: 0.057373/0.158081loss in batch 121: 0.0507813/0.157196loss in batch 122: 0.105377/0.156784loss in batch 123: 0.167252/0.15686loss in batch 124: 0.332474/0.158279loss in batch 125: 0.0572052/0.157471loss in batch 126: 0.0407562/0.156555loss in batch 127: 0.126282/0.156311loss in batch 128: 0.188919/0.15657loss in batch 129: 0.0673065/0.155869loss in batch 130: 0.135254/0.155731loss in batch 131: 0.0402985/0.154846loss in batch 132: 0.0185699/0.153824loss in batch 133: 1.1506/0.16127loss in batch 134: 0.0422363/0.160385loss in batch 135: 0.15715/0.160355loss in batch 136: 0.0597534/0.159622loss in batch 137: 0.0926208/0.159149loss in batch 138: 0.0134583/0.158096loss in batch 139: 0.0218506/0.15712loss in batch 140: 0.0797119/0.15657loss in batch 141: 0.379211/0.158142loss in batch 142: 0.0184784/0.157166loss in batch 143: 0.124786/0.156937loss in batch 144: 0.747162/0.161011loss in batch 145: 0.0977936/0.160583loss in batch 146: 0.0327911/0.159698loss in batch 147: 0.0862885/0.15921loss in batch 148: 0.0471039/0.158463loss in batch 149: 0.0652618/0.157822loss in batch 150: 0.257278/0.158493loss in batch 151: 0.0889587/0.158035loss in batch 152: 0.0860138/0.157562loss in batch 153: 0.0328217/0.156738loss in batch 154: 0.0317078/0.155945loss in batch 155: 0.398773/0.157516loss in batch 156: 0.113663/0.157227loss in batch 157: 0.0671844/0.156647loss in batch 158: 0.189392/0.15686loss in batch 159: 0.0250397/0.156036loss in batch 160: 0.140259/0.15593loss in batch 161: 0.101349/0.155609loss in batch 162: 0.0254822/0.154816loss in batch 163: 0.016037/0.153946loss in batch 164: 0.0617065/0.153397loss in batch 165: 0.206818/0.153717loss in batch 166: 0.0108948/0.152863loss in batch 167: 0.0455627/0.152222loss in batch 168: 0.0858459/0.151825loss in batch 169: 0.0746002/0.151382loss in batch 170: 0.087616/0.151001loss in batch 171: 0.00880432/0.150177loss in batch 172: 0.0275421/0.14946loss in batch 173: 0.0668182/0.149002loss in batch 174: 0.0748138/0.148575loss in batch 175: 0.0897522/0.148224loss in batch 176: 0.863922/0.152283loss in batch 177: 0.00195313/0.151443loss in batch 178: 0.198776/0.151688loss in batch 179: 0.0748444/0.15126loss in batch 180: 0.0276337/0.150589loss in batch 181: 0.128952/0.150467loss in batch 182: 0.0380096/0.149857loss in batch 183: 0.0405273/0.149261loss in batch 184: 0.424927/0.150757loss in batch 185: 0.0273438/0.150101loss in batch 186: 0.201935/0.15036loss in batch 187: 0.0884705/0.15004loss in batch 188: 0.518951/0.151993loss in batch 189: 0.0695801/0.151566loss in batch 190: 0.0357361/0.15094loss in batch 191: 0.069397/0.150528loss in batch 192: 0.154175/0.150543loss in batch 193: 0.0363464/0.149963loss in batch 194: 0.0206451/0.149292loss in batch 195: 0.0671539/0.14888loss in batch 196: 0.0180664/0.148209loss in batch 197: 0.822388/0.151611loss in batch 198: 0.0797119/0.15126loss in batch 199: 0.424255/0.152618loss in batch 200: 0.0677643/0.152206loss in batch 201: 0.0834961/0.151855loss in batch 202: 0.055542/0.151382loss in batch 203: 0.0301208/0.150787loss in batch 204: 0.0592194/0.15033loss in batch 205: 0.299011/0.151062loss in batch 206: 0.00598145/0.15036loss in batch 207: 0.0554352/0.149902loss in batch 208: 1.57562/0.156723loss in batch 209: 0.0729218/0.156326loss in batch 210: 0.0258026/0.155716loss in batch 211: 0.0151825/0.155045loss in batch 212: 0.0409088/0.154495
done with epoch 18
train_acc: 0.953052 (406/426)
test loss: 0.0409087
acc: 0.944054 (135/143)
loss in batch 0: 0.0473175/0.0473175loss in batch 1: 0.0732574/0.0602875loss in batch 2: 0.00823975/0.0429382loss in batch 3: 0.0321198/0.0402374loss in batch 4: 0.131943/0.0585785loss in batch 5: 0.0224915/0.0525513loss in batch 6: 0.0175781/0.0475616loss in batch 7: 0.196243/0.0661621loss in batch 8: 0.254135/0.0870361loss in batch 9: 0.190643/0.0973969loss in batch 10: 0.223145/0.108826loss in batch 11: 0.439804/0.136414loss in batch 12: 0.0313568/0.128326loss in batch 13: 0.0689697/0.124084loss in batch 14: 0.374405/0.140778loss in batch 15: 0.00642395/0.132385loss in batch 16: 0.00785828/0.125046loss in batch 17: 0.359497/0.138077loss in batch 18: 0.222595/0.142532loss in batch 19: 0.00500488/0.135651loss in batch 20: 0.132965/0.135513loss in batch 21: 0.030899/0.130768loss in batch 22: 0.338257/0.139786loss in batch 23: 0.0661621/0.136719loss in batch 24: 0.141006/0.136902loss in batch 25: 0.0240021/0.132553loss in batch 26: 0.40715/0.142715loss in batch 27: 0.0797424/0.140472loss in batch 28: 0.0193634/0.136292loss in batch 29: 0.319275/0.142395loss in batch 30: 0.145325/0.142487loss in batch 31: 0.115875/0.141663loss in batch 32: 1.00114/0.167694loss in batch 33: 0.122559/0.166382loss in batch 34: 0.425323/0.173782loss in batch 35: 0.00463867/0.169083loss in batch 36: 0.1017/0.167267loss in batch 37: 0.0140381/0.163223loss in batch 38: 0.0116272/0.159348loss in batch 39: 0.37706/0.16478loss in batch 40: 0.0266724/0.161407loss in batch 41: 0.0952301/0.159836loss in batch 42: 0.045105/0.157166loss in batch 43: 0.0266418/0.154205loss in batch 44: 0.0362701/0.151566loss in batch 45: 0.0636902/0.149673loss in batch 46: 0.151306/0.149704loss in batch 47: 0.032135/0.147247loss in batch 48: 0.282043/0.149994loss in batch 49: 0.0321503/0.147644loss in batch 50: 0.106415/0.146835loss in batch 51: 0.0236969/0.14447loss in batch 52: 0.109314/0.143814loss in batch 53: 0.266556/0.146088loss in batch 54: 0.0469818/0.144272loss in batch 55: 0.0298767/0.142227loss in batch 56: 0.238876/0.143936loss in batch 57: 0.413025/0.148575loss in batch 58: 0.310623/0.151321loss in batch 59: 0.162842/0.151505loss in batch 60: 0.245834/0.153046loss in batch 61: 0.174377/0.153397loss in batch 62: 0.180161/0.153824loss in batch 63: 0.0520782/0.152237loss in batch 64: 0.00802612/0.150009loss in batch 65: 0.115051/0.14949loss in batch 66: 0.0286407/0.147675loss in batch 67: 0.790421/0.157135loss in batch 68: 0.0801849/0.156021loss in batch 69: 0.0462189/0.154449loss in batch 70: 0.0336914/0.15274loss in batch 71: 0.120163/0.152298loss in batch 72: 0.0246429/0.150543loss in batch 73: 0.0619659/0.149353loss in batch 74: 0.19104/0.149902loss in batch 75: 0.172043/0.150208loss in batch 76: 0.382599/0.153214loss in batch 77: 0.00622559/0.151321loss in batch 78: 0.224213/0.152252loss in batch 79: 0.0335083/0.150772loss in batch 80: 0.0124207/0.149063loss in batch 81: 0.0437469/0.147781loss in batch 82: 0.00601196/0.146072loss in batch 83: 0.04422/0.144852loss in batch 84: 0.132874/0.144714loss in batch 85: 0.156616/0.144867loss in batch 86: 0.19632/0.145447loss in batch 87: 0.0802307/0.144699loss in batch 88: 0.169205/0.144989loss in batch 89: 0.319077/0.146912loss in batch 90: 0.145493/0.146896loss in batch 91: 0.427887/0.149963loss in batch 92: 0.0539246/0.148926loss in batch 93: 0.534576/0.15303loss in batch 94: 0.0389709/0.151825loss in batch 95: 0.165512/0.151978loss in batch 96: 0.11438/0.151581loss in batch 97: 0.0250092/0.150284loss in batch 98: 0.0198517/0.148956loss in batch 99: 0.0239868/0.14772loss in batch 100: 0.00613403/0.146317loss in batch 101: 0.0633698/0.145508loss in batch 102: 0.000335693/0.144089loss in batch 103: 0.0587463/0.143265loss in batch 104: 0.0229645/0.14212loss in batch 105: 0.046051/0.14122loss in batch 106: 0.0291595/0.140167loss in batch 107: 0.0146484/0.139008loss in batch 108: 0.0121765/0.137848loss in batch 109: 0.112228/0.137619loss in batch 110: 0.0254364/0.136597loss in batch 111: 0.0538177/0.135864loss in batch 112: 0.0384827/0.13501loss in batch 113: 0.277084/0.136261loss in batch 114: 0.0280762/0.135315loss in batch 115: 0.0149231/0.134277loss in batch 116: 0.00646973/0.133179loss in batch 117: 0.143539/0.13327loss in batch 118: 0.0530396/0.132584loss in batch 119: 0.274643/0.133774loss in batch 120: 0.00167847/0.13269loss in batch 121: 0.108902/0.132492loss in batch 122: 0.585648/0.136185loss in batch 123: 0.0927429/0.135834loss in batch 124: 0.0400848/0.135056loss in batch 125: 0.0615845/0.134476loss in batch 126: 0.534149/0.137634loss in batch 127: 0.0604706/0.137024loss in batch 128: 0.339645/0.138596loss in batch 129: 0.0487213/0.137894loss in batch 130: 0.341522/0.13945loss in batch 131: 0.0774689/0.138992loss in batch 132: 0.156387/0.139114loss in batch 133: 0.06633/0.138565loss in batch 134: 0.0852966/0.138168loss in batch 135: 0.0183716/0.137283loss in batch 136: 0.0548706/0.136703loss in batch 137: 0.0287018/0.13591loss in batch 138: 0.0271759/0.135132loss in batch 139: 0.128693/0.135086loss in batch 140: 0.607697/0.138428loss in batch 141: 1.02675/0.144699loss in batch 142: 0.042572/0.143982loss in batch 143: 0.0722961/0.143478loss in batch 144: 0.11554/0.14328loss in batch 145: 0.288055/0.144272loss in batch 146: 0.02034/0.143433loss in batch 147: 0.024292/0.142624loss in batch 148: 0.317444/0.143799loss in batch 149: 0.0729675/0.143326loss in batch 150: 0.492889/0.145645loss in batch 151: 0.0349579/0.144913loss in batch 152: 0.206863/0.145325loss in batch 153: 0.0030365/0.144394loss in batch 154: 0.244156/0.145035loss in batch 155: 0.0827332/0.144638loss in batch 156: 0.00341797/0.143753loss in batch 157: 0.0192566/0.14296loss in batch 158: 0.292374/0.14389loss in batch 159: 0.123505/0.143768loss in batch 160: 0.100006/0.143494loss in batch 161: 0.0895996/0.143173loss in batch 162: 0.0153656/0.14238loss in batch 163: 0.00669861/0.141556loss in batch 164: 0.0954285/0.141266loss in batch 165: 0.0222321/0.140564loss in batch 166: 0.246338/0.14119loss in batch 167: 0.036499/0.140564loss in batch 168: 0.0307007/0.139923loss in batch 169: 0.0542145/0.139404loss in batch 170: 0.088562/0.139114loss in batch 171: 0.592026/0.141754loss in batch 172: 0.0311127/0.141113loss in batch 173: 0.0500946/0.140594loss in batch 174: 0.410614/0.14212loss in batch 175: 0.0071106/0.141357loss in batch 176: 0.247482/0.141968loss in batch 177: 0.0460358/0.141434loss in batch 178: 0.335068/0.142502loss in batch 179: 0.00672913/0.141739loss in batch 180: 0.0183258/0.141068loss in batch 181: 0.083786/0.140747loss in batch 182: 0.076828/0.140396loss in batch 183: 0.0187836/0.13974loss in batch 184: 0.0710297/0.139359loss in batch 185: 0.163757/0.139496loss in batch 186: 0.113464/0.139359loss in batch 187: 0.046814/0.13887loss in batch 188: 0.40947/0.140305loss in batch 189: 0.0213623/0.139679loss in batch 190: 0.0126953/0.139008loss in batch 191: 1.16048/0.144333loss in batch 192: 0.483307/0.146088loss in batch 193: 0.0257416/0.145462loss in batch 194: 0.324402/0.146378loss in batch 195: 0.0248413/0.145752loss in batch 196: 0.0380554/0.145218loss in batch 197: 0.0351105/0.144653loss in batch 198: 0.238327/0.145126loss in batch 199: 0.167267/0.145248loss in batch 200: 0.0441895/0.14473loss in batch 201: 0.559555/0.14679loss in batch 202: 0.0452271/0.146301loss in batch 203: 0.025177/0.145706loss in batch 204: 0.129349/0.145615loss in batch 205: 0.00979614/0.144958loss in batch 206: 1.53989/0.151703loss in batch 207: 0.0657349/0.151276loss in batch 208: 0.213898/0.151581loss in batch 209: 0.118851/0.151428loss in batch 210: 0.0699463/0.151047loss in batch 211: 0.0240936/0.150436loss in batch 212: 0.348389/0.151382
done with epoch 19
train_acc: 0.950704 (405/426)
test loss: 0.348388
acc: 0.951047 (136/143)
[0.100128, -0.0894165, -0.0885162, -0.155472, -0.0291138, -0.0454712, -0.126953, -0.0220795, -0.00170898, -0.00314331, -0.361984, -0.15033, -0.00886536, -0.400421, -0.502625, 0.047821, -0.0757141, 0.00732422, 0.0292053, 0.000427246, 0.287216, 0.0496216, -0.227295, -0.0441437, 0.00762939, -0.0483551, -0.013382, 0.057312, -0.0677795, 0.0136566, -0.0136261, 0.0263367, -0.44136, 0.182129, -0.00224304, 0.200531, -0.138565, 0.0829163, -0.132004, 0.186203, 0.729965, -0.00984192, 0.523972, -0.00645447, -0.499847, 0.000549316, -0.0014801, -0.0821228, -0.0178223, 0.0570068, 0.0292969, 0.105881, 0.0431824, -0.056839, -0.0775452, -0.0228729, -0.0240936, -0.0117188, -0.0658112, 0.000488281, 0.315109, 0.129181, -0.0156403, -0.0467072, 0.0133057, -0.059082, 0, 0.00819397, 0.00486755, -0.0106659, -0.48877, 0.0186615, -0.0593719, 0.431747, 0.00268555, -0.0695648, -0.010437, -0.215057, -0.0209351, -0.00131226, 0.15274, 0.000289917, 0.0172424, -0.0147095, 0.0672607, -0.0579681, -0.0235291, -0.00387573, 0.0350952, 0, -0.00750732, 0.454651, -0.531113, 0.00256348, -0.00564575, -0.055542, 0.00914001, -0.620285, -0.0482025, -0.0217743, -0.00128174, -0.0888214, -0.0996857, -0.224838, 0.0322418, -0.010788, 0.00222778, -0.176834, 0.604477, 0.691574, -0.0593109, 0.0045929, 0.0296326, -0.353195, -0.0170593, -0.0491791, -0.401855, -0.0217743, -0.0645294, -0.16217, -0.0415039, -0.0354462, 0.19278, -0.133057, 0.00259399, -0.183609, -0.305252, -0.0348511, -0.0185394, -0.00558472, 0.0831604, -0.00424194, -0.0175476, -0.315201, -0.210876, -0.0138397, -0.307816, 0.476715, 0, -0.0100403, -0.108368, -0.0399017, 0.000427246]
Compiler: ./compile.py -R 64 breast_logistic
	899 triples of Z2^64 left
	53 dabits of Z2^64 left
	360 triples of Z2^64 left
	212 dabits of Z2^64 left
2 threads spent a total of 160.734 seconds (376.243 MB, 3110106 rounds) on the online phase, 206.645 seconds (24042.1 MB, 502175 rounds) on the preprocessing/offline phase, and 368.369 seconds idling.
Join timer: 0 367872
Finish timer: 0.00134428
Join timer: 1 363083
Finish timer: 0.00134428
Communication details (rounds in parallel threads counted double):
Exchanging one-to-one 11384.9 MB in 66717 rounds, taking 27.0524 seconds
Receiving directly 376.243 MB in 1555053 rounds, taking 133.027 seconds
Receiving one-to-one 14239.5 MB in 217729 rounds, taking 22.5581 seconds
Sending directly 376.243 MB in 1555053 rounds, taking 18.3777 seconds
Sending one-to-one 12657.3 MB in 217729 rounds, taking 1.99117 seconds
CPU time = 178.834 (overall core time)
The following benchmarks are including preprocessing (offline phase).
Time = 367.873 seconds 
Data sent = 24418.4 MB in ~3612281 rounds (party 0 only)
Global data sent = 50419 MB (all parties)
Actual cost of program:
  Type int
      22233741        Triples
     131855095           Bits
This program might benefit from some protocol options.
Consider adding the following at the beginning of your code:
	program.use_trunc_pr = True
	program.use_split(2)
This virtual machine was compiled with GCC. Recompile with 'CXX = clang++' in 'CONFIG.mine' for optimal performance.
Command line: ./semi2k-party.x -N 2 -e --ip-file-name /HOST -p 0 -v breast_logistic
