Using statistical security parameter 40
No modulus found in /Player-Data//3-Rp-128/Params-Data, generating 128-bit prime
Current working directory: "/root/MP-SPDZ"
Current working directory: "/root/MP-SPDZ"
Using SGD
done with epoch 0done with epoch 1done with epoch 2done with epoch 3done with epoch 4done with epoch 5done with epoch 6done with epoch 7done with epoch 8done with epoch 9done with epoch 10done with epoch 11done with epoch 12done with epoch 13done with epoch 14done with epoch 15done with epoch 16done with epoch 17done with epoch 18done with epoch 19
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Using SGD
loss in batch 0: 0.694687/0.694687loss in batch 1: 0.71077/0.702728loss in batch 2: 0.76268/0.722717loss in batch 3: 0.622849/0.697754loss in batch 4: 0.778427/0.713882loss in batch 5: 0.646271/0.702621loss in batch 6: 0.581772/0.685349loss in batch 7: 0.630035/0.678436loss in batch 8: 0.606903/0.670486loss in batch 9: 0.683624/0.671799loss in batch 10: 0.9254/0.694855loss in batch 11: 0.662567/0.692169loss in batch 12: 0.812271/0.701401loss in batch 13: 0.660767/0.698502loss in batch 14: 0.612854/0.692795loss in batch 15: 0.787689/0.698715loss in batch 16: 0.698883/0.69873loss in batch 17: 0.657898/0.696457loss in batch 18: 0.669113/0.695023loss in batch 19: 0.648544/0.692703loss in batch 20: 0.625977/0.689529loss in batch 21: 0.617798/0.686264loss in batch 22: 0.807541/0.691528loss in batch 23: 0.625153/0.688766loss in batch 24: 0.631638/0.686493loss in batch 25: 0.63942/0.684677loss in batch 26: 0.543274/0.679443loss in batch 27: 0.627914/0.677597loss in batch 28: 0.613419/0.675385loss in batch 29: 0.644928/0.674377loss in batch 30: 0.711884/0.675568loss in batch 31: 0.728348/0.677231loss in batch 32: 0.606308/0.675079loss in batch 33: 0.664444/0.674759loss in batch 34: 0.642349/0.673843loss in batch 35: 0.640106/0.672913loss in batch 36: 0.605499/0.671082loss in batch 37: 0.650284/0.670532loss in batch 38: 0.668564/0.670486loss in batch 39: 0.569214/0.667953loss in batch 40: 0.58313/0.665878loss in batch 41: 0.617416/0.664734loss in batch 42: 0.730392/0.66626loss in batch 43: 0.497208/0.662415loss in batch 44: 0.629379/0.661682loss in batch 45: 0.570007/0.659683loss in batch 46: 0.596405/0.65834loss in batch 47: 0.621689/0.657578loss in batch 48: 0.719269/0.658829loss in batch 49: 0.614609/0.657959loss in batch 50: 0.593781/0.656693loss in batch 51: 0.680252/0.65715loss in batch 52: 0.586319/0.655807loss in batch 53: 0.607651/0.654907loss in batch 54: 0.537033/0.652771loss in batch 55: 0.509155/0.650208loss in batch 56: 0.514984/0.647842loss in batch 57: 0.642166/0.647736loss in batch 58: 0.844376/0.651077loss in batch 59: 1.01292/0.657104loss in batch 60: 0.644333/0.656891loss in batch 61: 0.573593/0.655548loss in batch 62: 0.437164/0.652084loss in batch 63: 0.372147/0.647705loss in batch 64: 0.40033/0.643906loss in batch 65: 0.695114/0.644669loss in batch 66: 0.405914/0.641113loss in batch 67: 0.395569/0.637497loss in batch 68: 0.416275/0.634308loss in batch 69: 0.339462/0.630081loss in batch 70: 0.706192/0.631165loss in batch 71: 0.338608/0.62709loss in batch 72: 0.926132/0.631195loss in batch 73: 0.719055/0.632385loss in batch 74: 0.308701/0.628052loss in batch 75: 0.317017/0.623978loss in batch 76: 0.351059/0.620438loss in batch 77: 0.722488/0.621735loss in batch 78: 0.665756/0.622299loss in batch 79: 0.326355/0.618591loss in batch 80: 0.702011/0.619614loss in batch 81: 0.328644/0.616074loss in batch 82: 0.330048/0.612625loss in batch 83: 0.297043/0.608871loss in batch 84: 0.906647/0.612381loss in batch 85: 0.611832/0.612366loss in batch 86: 0.551071/0.611664loss in batch 87: 0.34726/0.608658loss in batch 88: 0.323029/0.605438loss in batch 89: 0.263046/0.601654loss in batch 90: 0.702988/0.602768loss in batch 91: 0.282822/0.599274loss in batch 92: 0.706802/0.600433loss in batch 93: 0.789398/0.602448loss in batch 94: 0.659821/0.603058loss in batch 95: 0.32196/0.600113loss in batch 96: 0.581604/0.599945loss in batch 97: 0.323486/0.597107loss in batch 98: 0.700378/0.59816loss in batch 99: 1.03687/0.602554loss in batch 100: 0.661591/0.603119loss in batch 101: 0.832336/0.605362loss in batch 102: 0.470245/0.604065loss in batch 103: 0.486313/0.602936loss in batch 104: 0.48967/0.601852loss in batch 105: 0.538727/0.601257loss in batch 106: 0.526764/0.600555loss in batch 107: 0.458786/0.599258loss in batch 108: 0.442368/0.597809loss in batch 109: 0.500809/0.596924loss in batch 110: 0.521362/0.596237loss in batch 111: 0.457535/0.595016loss in batch 112: 0.470795/0.593918loss in batch 113: 0.511215/0.59317loss in batch 114: 0.573486/0.593018loss in batch 115: 0.452103/0.591797loss in batch 116: 0.542862/0.59137loss in batch 117: 0.597504/0.591431loss in batch 118: 0.656723/0.59198loss in batch 119: 0.68251/0.592728loss in batch 120: 0.368546/0.590881loss in batch 121: 0.489456/0.590042loss in batch 122: 0.363815/0.588211loss in batch 123: 0.580719/0.58815loss in batch 124: 0.658218/0.588699loss in batch 125: 0.482468/0.58786loss in batch 126: 0.462204/0.586868loss in batch 127: 0.544067/0.586548loss in batch 128: 0.685989/0.587311loss in batch 129: 0.556091/0.587067loss in batch 130: 0.400528/0.585648loss in batch 131: 0.373871/0.584045loss in batch 132: 0.356491/0.582336loss in batch 133: 0.396194/0.580948loss in batch 134: 0.492416/0.580276loss in batch 135: 0.420609/0.579102loss in batch 136: 0.39743/0.577789loss in batch 137: 0.41011/0.576569loss in batch 138: 0.462418/0.57576loss in batch 139: 0.603607/0.575943loss in batch 140: 0.256592/0.573685loss in batch 141: 0.551865/0.573532loss in batch 142: 0.464462/0.572769loss in batch 143: 0.603668/0.572983loss in batch 144: 0.58313/0.573059loss in batch 145: 0.449402/0.572205loss in batch 146: 0.362732/0.570786loss in batch 147: 0.639008/0.571228loss in batch 148: 0.499649/0.570755loss in batch 149: 0.405563/0.569656loss in batch 150: 0.368439/0.568329loss in batch 151: 0.529556/0.568085loss in batch 152: 0.591644/0.568237loss in batch 153: 0.438354/0.567383loss in batch 154: 0.378937/0.566162loss in batch 155: 0.3741/0.564941loss in batch 156: 0.406693/0.563934loss in batch 157: 0.287735/0.56218loss in batch 158: 0.496674/0.561768loss in batch 159: 0.498703/0.561371loss in batch 160: 0.390411/0.560318loss in batch 161: 0.447495/0.559616loss in batch 162: 0.294739/0.557999loss in batch 163: 0.618713/0.558365loss in batch 164: 0.360687/0.557159loss in batch 165: 0.352753/0.555923loss in batch 166: 0.267288/0.554199loss in batch 167: 0.576141/0.554337loss in batch 168: 0.332947/0.553024loss in batch 169: 0.291077/0.551483loss in batch 170: 0.219666/0.549545loss in batch 171: 0.982788/0.552063loss in batch 172: 0.229584/0.550201loss in batch 173: 0.520325/0.550018loss in batch 174: 0.930908/0.5522loss in batch 175: 0.208389/0.550247loss in batch 176: 0.356644/0.549149loss in batch 177: 0.413712/0.548401loss in batch 178: 0.59761/0.548676loss in batch 179: 0.666565/0.549332loss in batch 180: 0.236282/0.547607loss in batch 181: 0.839569/0.54921loss in batch 182: 0.718994/0.550125loss in batch 183: 0.663559/0.550751loss in batch 184: 0.646423/0.551254loss in batch 185: 0.37793/0.550323loss in batch 186: 0.414673/0.549606loss in batch 187: 0.371353/0.548645loss in batch 188: 0.346954/0.547592loss in batch 189: 0.455887/0.547104loss in batch 190: 0.384842/0.546265loss in batch 191: 0.634949/0.546722loss in batch 192: 0.554321/0.546753loss in batch 193: 0.509399/0.546555loss in batch 194: 0.469437/0.546173loss in batch 195: 0.432175/0.545578loss in batch 196: 0.237534/0.544022loss in batch 197: 0.506622/0.543839loss in batch 198: 0.965149/0.545944loss in batch 199: 0.36142/0.545029loss in batch 200: 0.369476/0.544144loss in batch 201: 0.508408/0.543976loss in batch 202: 0.451538/0.543518loss in batch 203: 0.435577/0.542984loss in batch 204: 0.31813/0.541885loss in batch 205: 0.297684/0.54071loss in batch 206: 0.541183/0.54071loss in batch 207: 0.479126/0.540421loss in batch 208: 0.348907/0.539505loss in batch 209: 0.349319/0.538605loss in batch 210: 0.386017/0.537872loss in batch 211: 0.278992/0.536652loss in batch 212: 0.194138/0.535034
done with epoch 0
train_acc: 0.79108 (337/426)
test loss: 0.194138
acc: 0.923077 (132/143)
loss in batch 0: 0.513519/0.513519loss in batch 1: 0.293106/0.40332loss in batch 2: 0.240295/0.348969loss in batch 3: 0.202835/0.312439loss in batch 4: 0.714432/0.392838loss in batch 5: 0.562683/0.421143loss in batch 6: 0.345612/0.410355loss in batch 7: 0.373413/0.405731loss in batch 8: 0.648041/0.432663loss in batch 9: 0.32341/0.421738loss in batch 10: 0.356445/0.415802loss in batch 11: 0.224075/0.399826loss in batch 12: 0.17244/0.382324loss in batch 13: 0.437943/0.386307loss in batch 14: 0.553635/0.397446loss in batch 15: 0.287857/0.39061loss in batch 16: 0.769196/0.412872loss in batch 17: 0.267242/0.404785loss in batch 18: 0.431/0.406174loss in batch 19: 0.798035/0.425766loss in batch 20: 0.597549/0.433945loss in batch 21: 0.327148/0.429092loss in batch 22: 0.309128/0.423874loss in batch 23: 0.392258/0.422562loss in batch 24: 0.31189/0.418137loss in batch 25: 0.534241/0.422592loss in batch 26: 0.559845/0.427673loss in batch 27: 0.495789/0.430115loss in batch 28: 0.440536/0.430466loss in batch 29: 0.619812/0.436783loss in batch 30: 0.322189/0.43309loss in batch 31: 0.278687/0.428268loss in batch 32: 0.24173/0.422607loss in batch 33: 0.255402/0.417694loss in batch 34: 0.649445/0.424316loss in batch 35: 0.394379/0.423477loss in batch 36: 0.325043/0.420822loss in batch 37: 0.298126/0.417587loss in batch 38: 0.333542/0.415436loss in batch 39: 0.238525/0.411011loss in batch 40: 0.266693/0.407486loss in batch 41: 0.266037/0.404114loss in batch 42: 0.30452/0.40181loss in batch 43: 0.369171/0.401062loss in batch 44: 0.480896/0.402832loss in batch 45: 0.341431/0.401505loss in batch 46: 0.626633/0.406296loss in batch 47: 0.248566/0.403loss in batch 48: 1.00418/0.415283loss in batch 49: 0.303146/0.41304loss in batch 50: 0.223663/0.409317loss in batch 51: 0.433731/0.409805loss in batch 52: 0.110001/0.404129loss in batch 53: 0.358871/0.403305loss in batch 54: 0.264832/0.400772loss in batch 55: 0.391373/0.400604loss in batch 56: 0.344284/0.399628loss in batch 57: 0.440582/0.40033loss in batch 58: 0.408966/0.400467loss in batch 59: 0.232468/0.397675loss in batch 60: 0.459/0.398682loss in batch 61: 0.43158/0.399216loss in batch 62: 0.56781/0.401886loss in batch 63: 0.214478/0.398972loss in batch 64: 0.165909/0.39537loss in batch 65: 0.68512/0.399765loss in batch 66: 0.119141/0.395569loss in batch 67: 0.186157/0.392487loss in batch 68: 0.580841/0.395233loss in batch 69: 0.54126/0.397308loss in batch 70: 0.198273/0.394501loss in batch 71: 0.378006/0.394272loss in batch 72: 0.249344/0.392303loss in batch 73: 0.30394/0.391098loss in batch 74: 0.392471/0.391113loss in batch 75: 0.503876/0.392593loss in batch 76: 0.225311/0.390442loss in batch 77: 0.500549/0.39183loss in batch 78: 0.409744/0.392075loss in batch 79: 0.36377/0.391708loss in batch 80: 0.239792/0.389847loss in batch 81: 0.259171/0.388245loss in batch 82: 0.388367/0.388245loss in batch 83: 0.273712/0.386887loss in batch 84: 0.305267/0.38591loss in batch 85: 0.305298/0.384995loss in batch 86: 0.353806/0.384628loss in batch 87: 0.302826/0.383698loss in batch 88: 0.281662/0.382553loss in batch 89: 0.167435/0.380157loss in batch 90: 0.330002/0.379608loss in batch 91: 0.404129/0.379868loss in batch 92: 0.139709/0.377289loss in batch 93: 0.317398/0.376663loss in batch 94: 0.328354/0.376144loss in batch 95: 0.222672/0.374542loss in batch 96: 0.195114/0.372696loss in batch 97: 0.345886/0.372421loss in batch 98: 0.278152/0.371475loss in batch 99: 0.482758/0.372574loss in batch 100: 0.301758/0.371887loss in batch 101: 0.245575/0.370636loss in batch 102: 0.164597/0.368637loss in batch 103: 0.271866/0.367722loss in batch 104: 0.176971/0.365906loss in batch 105: 0.498474/0.367142loss in batch 106: 0.504532/0.368423loss in batch 107: 0.595154/0.370544loss in batch 108: 0.747192/0.373993loss in batch 109: 0.373169/0.373978loss in batch 110: 0.387848/0.3741loss in batch 111: 0.219269/0.372726loss in batch 112: 0.324585/0.372299loss in batch 113: 0.175598/0.370575loss in batch 114: 0.374451/0.370605loss in batch 115: 0.235855/0.369446loss in batch 116: 0.35083/0.369293loss in batch 117: 0.314636/0.36882loss in batch 118: 0.432495/0.369354loss in batch 119: 0.398987/0.369598loss in batch 120: 0.226608/0.368423loss in batch 121: 0.186462/0.366928loss in batch 122: 0.322159/0.366577loss in batch 123: 0.464874/0.367355loss in batch 124: 0.299316/0.366806loss in batch 125: 0.264557/0.366013loss in batch 126: 0.149155/0.364288loss in batch 127: 0.72879/0.367142loss in batch 128: 0.405884/0.367432loss in batch 129: 0.170654/0.365936loss in batch 130: 0.357468/0.36586loss in batch 131: 0.313171/0.365463loss in batch 132: 0.1595/0.363907loss in batch 133: 0.542099/0.36525loss in batch 134: 0.327728/0.36496loss in batch 135: 0.259384/0.364197loss in batch 136: 0.20723/0.363052loss in batch 137: 0.318985/0.362732loss in batch 138: 1.00615/0.367355loss in batch 139: 0.526245/0.3685loss in batch 140: 0.286346/0.367905loss in batch 141: 0.179062/0.366577loss in batch 142: 0.166718/0.365173loss in batch 143: 0.327774/0.364929loss in batch 144: 0.252701/0.364136loss in batch 145: 0.414932/0.364502loss in batch 146: 0.349258/0.364395loss in batch 147: 0.384125/0.364532loss in batch 148: 0.308243/0.364136loss in batch 149: 0.0875549/0.362305loss in batch 150: 0.471664/0.363037loss in batch 151: 0.191589/0.361908loss in batch 152: 0.463928/0.362564loss in batch 153: 0.290787/0.362106loss in batch 154: 0.577667/0.363495loss in batch 155: 0.171112/0.362259loss in batch 156: 0.47374/0.362961loss in batch 157: 0.504868/0.363861loss in batch 158: 0.267792/0.363266loss in batch 159: 0.175308/0.362091loss in batch 160: 0.158813/0.360825loss in batch 161: 0.26741/0.36026loss in batch 162: 0.353928/0.360214loss in batch 163: 0.130478/0.35881loss in batch 164: 0.445633/0.359329loss in batch 165: 0.528976/0.360352loss in batch 166: 0.5578/0.361542loss in batch 167: 0.271164/0.361008loss in batch 168: 0.218811/0.360168loss in batch 169: 0.401566/0.360397loss in batch 170: 0.233215/0.35965loss in batch 171: 0.144501/0.358398loss in batch 172: 0.173538/0.357346loss in batch 173: 0.847473/0.360153loss in batch 174: 0.270294/0.35965loss in batch 175: 0.375549/0.359741loss in batch 176: 0.24823/0.359116loss in batch 177: 0.275833/0.358627loss in batch 178: 0.297775/0.358292loss in batch 179: 0.838486/0.360962loss in batch 180: 0.216888/0.360168loss in batch 181: 0.27301/0.359695loss in batch 182: 0.299622/0.35936loss in batch 183: 0.109741/0.358002loss in batch 184: 0.420837/0.358337loss in batch 185: 0.37793/0.358444loss in batch 186: 0.186661/0.357529loss in batch 187: 0.153519/0.356445loss in batch 188: 0.490921/0.357162loss in batch 189: 0.371246/0.357239loss in batch 190: 0.234146/0.356583loss in batch 191: 0.497253/0.357315loss in batch 192: 0.508942/0.358109loss in batch 193: 0.237488/0.357483loss in batch 194: 0.132156/0.356339loss in batch 195: 0.20401/0.35556loss in batch 196: 0.592117/0.35675loss in batch 197: 0.261719/0.356277loss in batch 198: 0.361496/0.356293loss in batch 199: 0.337189/0.356201loss in batch 200: 0.194824/0.355408loss in batch 201: 0.35376/0.355392loss in batch 202: 0.505875/0.356125loss in batch 203: 0.2724/0.355713loss in batch 204: 0.195175/0.354935loss in batch 205: 0.178284/0.35408loss in batch 206: 0.179672/0.353241loss in batch 207: 0.545609/0.354172loss in batch 208: 0.494507/0.354828loss in batch 209: 0.308716/0.354614loss in batch 210: 0.193878/0.353867loss in batch 211: 0.625504/0.355133loss in batch 212: 0.0706177/0.35379
done with epoch 1
train_acc: 0.906103 (386/426)
test loss: 0.0706177
acc: 0.916084 (131/143)
loss in batch 0: 0.282837/0.282837loss in batch 1: 0.590271/0.436554loss in batch 2: 0.324173/0.399094loss in batch 3: 0.546463/0.435944loss in batch 4: 0.309189/0.410583loss in batch 5: 0.368408/0.403564loss in batch 6: 0.126801/0.364029loss in batch 7: 0.256592/0.350601loss in batch 8: 0.254715/0.339935loss in batch 9: 0.326187/0.338562loss in batch 10: 0.105881/0.317413loss in batch 11: 0.306717/0.316513loss in batch 12: 0.222839/0.309311loss in batch 13: 0.183411/0.300323loss in batch 14: 0.481644/0.312408loss in batch 15: 0.298431/0.311539loss in batch 16: 0.870743/0.344421loss in batch 17: 0.314255/0.342743loss in batch 18: 0.264557/0.338638loss in batch 19: 0.35611/0.339508loss in batch 20: 0.451157/0.344833loss in batch 21: 0.472397/0.350632loss in batch 22: 0.220886/0.344986loss in batch 23: 0.150192/0.336868loss in batch 24: 0.228958/0.332565loss in batch 25: 0.225845/0.328445loss in batch 26: 0.25/0.325531loss in batch 27: 0.241974/0.322556loss in batch 28: 0.19342/0.3181loss in batch 29: 0.255646/0.316025loss in batch 30: 0.301285/0.315552loss in batch 31: 0.373383/0.317352loss in batch 32: 0.338379/0.317993loss in batch 33: 0.204788/0.314667loss in batch 34: 0.228012/0.31218loss in batch 35: 0.35347/0.313339loss in batch 36: 0.182343/0.309799loss in batch 37: 0.218948/0.307404loss in batch 38: 0.194443/0.304504loss in batch 39: 0.115479/0.299789loss in batch 40: 0.177856/0.296799loss in batch 41: 0.271011/0.296204loss in batch 42: 0.166306/0.293167loss in batch 43: 0.585098/0.299805loss in batch 44: 0.345383/0.300827loss in batch 45: 0.335541/0.301575loss in batch 46: 0.383301/0.303299loss in batch 47: 0.216263/0.301498loss in batch 48: 0.422134/0.303955loss in batch 49: 0.569061/0.309265loss in batch 50: 0.0831451/0.304825loss in batch 51: 0.148865/0.301834loss in batch 52: 0.10289/0.298065loss in batch 53: 1.50397/0.320404loss in batch 54: 0.0690918/0.315842loss in batch 55: 0.249466/0.314651loss in batch 56: 0.113724/0.311127loss in batch 57: 0.470901/0.313889loss in batch 58: 0.159424/0.311264loss in batch 59: 0.385193/0.3125loss in batch 60: 0.171432/0.310181loss in batch 61: 0.187759/0.308212loss in batch 62: 0.181671/0.306198loss in batch 63: 0.384064/0.307419loss in batch 64: 0.220932/0.306091loss in batch 65: 0.14827/0.303696loss in batch 66: 0.254608/0.302963loss in batch 67: 0.108749/0.30011loss in batch 68: 0.770462/0.306931loss in batch 69: 0.184067/0.305176loss in batch 70: 0.98912/0.314804loss in batch 71: 0.352188/0.315323loss in batch 72: 0.369598/0.316055loss in batch 73: 0.138336/0.31366loss in batch 74: 0.438126/0.315323loss in batch 75: 0.107742/0.312592loss in batch 76: 0.216507/0.31134loss in batch 77: 0.312439/0.311356loss in batch 78: 0.229309/0.310318loss in batch 79: 0.210907/0.309082loss in batch 80: 0.641785/0.313187loss in batch 81: 0.247742/0.312393loss in batch 82: 0.414932/0.313614loss in batch 83: 0.185654/0.312103loss in batch 84: 0.601273/0.315491loss in batch 85: 0.226242/0.314453loss in batch 86: 0.54097/0.317062loss in batch 87: 0.095993/0.314545loss in batch 88: 0.62854/0.318085loss in batch 89: 0.648209/0.321747loss in batch 90: 0.114929/0.319473loss in batch 91: 0.105713/0.317154loss in batch 92: 0.376205/0.31778loss in batch 93: 0.652802/0.32135loss in batch 94: 0.223129/0.320313loss in batch 95: 0.223694/0.319305loss in batch 96: 0.30452/0.319153loss in batch 97: 0.184113/0.31778loss in batch 98: 0.291412/0.31752loss in batch 99: 0.477829/0.319107loss in batch 100: 0.127411/0.317215loss in batch 101: 0.285202/0.316895loss in batch 102: 0.448608/0.318192loss in batch 103: 0.48143/0.319748loss in batch 104: 0.109802/0.317749loss in batch 105: 0.137711/0.316055loss in batch 106: 0.488235/0.317673loss in batch 107: 0.532776/0.319656loss in batch 108: 0.354462/0.319977loss in batch 109: 0.151169/0.318436loss in batch 110: 0.431168/0.319458loss in batch 111: 0.220337/0.318573loss in batch 112: 0.420349/0.319473loss in batch 113: 0.265076/0.319loss in batch 114: 0.238678/0.318298loss in batch 115: 0.0883331/0.316315loss in batch 116: 0.29541/0.316132loss in batch 117: 0.360992/0.316528loss in batch 118: 0.446884/0.317612loss in batch 119: 0.159409/0.316299loss in batch 120: 0.161743/0.315018loss in batch 121: 0.555069/0.316986loss in batch 122: 0.0851593/0.315094loss in batch 123: 0.296539/0.314941loss in batch 124: 0.101654/0.313232loss in batch 125: 0.125839/0.311752loss in batch 126: 0.148468/0.310471loss in batch 127: 0.107788/0.308884loss in batch 128: 0.311142/0.308899loss in batch 129: 0.201019/0.308075loss in batch 130: 0.304688/0.308044loss in batch 131: 0.173721/0.307037loss in batch 132: 0.21579/0.306351loss in batch 133: 0.214493/0.305664loss in batch 134: 0.539719/0.307404loss in batch 135: 0.384766/0.307968loss in batch 136: 0.186691/0.307068loss in batch 137: 0.212143/0.306396loss in batch 138: 0.266647/0.306091loss in batch 139: 0.413147/0.30687loss in batch 140: 0.299789/0.306808loss in batch 141: 0.129898/0.305573loss in batch 142: 0.249924/0.305191loss in batch 143: 0.214447/0.30455loss in batch 144: 0.368881/0.304993loss in batch 145: 0.251602/0.304626loss in batch 146: 0.176224/0.303757loss in batch 147: 0.11998/0.302505loss in batch 148: 0.0842285/0.301056loss in batch 149: 0.0992737/0.299713loss in batch 150: 0.145493/0.298691loss in batch 151: 0.263123/0.298447loss in batch 152: 0.534225/0.300003loss in batch 153: 0.21077/0.299408loss in batch 154: 0.208954/0.298828loss in batch 155: 0.196289/0.298172loss in batch 156: 0.51741/0.299561loss in batch 157: 0.305328/0.299606loss in batch 158: 0.088028/0.298264loss in batch 159: 0.116074/0.297119loss in batch 160: 0.421097/0.297913loss in batch 161: 0.108032/0.296738loss in batch 162: 0.0757446/0.29538loss in batch 163: 0.484695/0.296539loss in batch 164: 0.443283/0.297409loss in batch 165: 0.296829/0.297409loss in batch 166: 0.695663/0.299805loss in batch 167: 0.360596/0.300171loss in batch 168: 0.10495/0.299011loss in batch 169: 0.584427/0.30069loss in batch 170: 0.162857/0.299881loss in batch 171: 0.295502/0.29985loss in batch 172: 0.171249/0.299103loss in batch 173: 0.206894/0.298569loss in batch 174: 0.444611/0.299423loss in batch 175: 0.437027/0.300201loss in batch 176: 0.470566/0.301163loss in batch 177: 0.110489/0.300095loss in batch 178: 0.258896/0.299866loss in batch 179: 0.321106/0.299988loss in batch 180: 0.170319/0.299255loss in batch 181: 0.0984955/0.298157loss in batch 182: 0.302963/0.298187loss in batch 183: 0.230759/0.297821loss in batch 184: 0.264893/0.297638loss in batch 185: 0.195053/0.297089loss in batch 186: 0.296768/0.297089loss in batch 187: 0.359695/0.297409loss in batch 188: 0.112778/0.296448loss in batch 189: 0.212555/0.296005loss in batch 190: 0.213882/0.295578loss in batch 191: 0.183914/0.294998loss in batch 192: 0.232101/0.294662loss in batch 193: 0.430496/0.295364loss in batch 194: 0.263138/0.295197loss in batch 195: 0.0306854/0.293854loss in batch 196: 0.0599976/0.292664loss in batch 197: 0.203445/0.292206loss in batch 198: 0.114212/0.291321loss in batch 199: 0.814926/0.29393loss in batch 200: 0.251755/0.293732loss in batch 201: 0.187286/0.293198loss in batch 202: 0.248642/0.292984loss in batch 203: 0.129379/0.292175loss in batch 204: 0.455566/0.292969loss in batch 205: 0.27182/0.292862loss in batch 206: 0.0661316/0.291779loss in batch 207: 0.491074/0.29274loss in batch 208: 0.126236/0.291946loss in batch 209: 0.179794/0.291412loss in batch 210: 0.180557/0.290878loss in batch 211: 0.668304/0.292648loss in batch 212: 0.252655/0.292465
done with epoch 2
train_acc: 0.920188 (392/426)
test loss: 0.252655
acc: 0.916084 (131/143)
loss in batch 0: 0.17955/0.17955loss in batch 1: 0.104416/0.141983loss in batch 2: 0.97789/0.420609loss in batch 3: 0.189331/0.362793loss in batch 4: 0.295181/0.349274loss in batch 5: 0.619614/0.394333loss in batch 6: 0.836914/0.457565loss in batch 7: 0.129715/0.416565loss in batch 8: 0.0545959/0.376358loss in batch 9: 0.0870819/0.347427loss in batch 10: 0.117676/0.326538loss in batch 11: 0.467789/0.338303loss in batch 12: 0.11261/0.320953loss in batch 13: 0.169296/0.31012loss in batch 14: 0.188858/0.302032loss in batch 15: 0.703979/0.327148loss in batch 16: 0.148392/0.316635loss in batch 17: 0.316788/0.31665loss in batch 18: 0.112183/0.305893loss in batch 19: 0.35582/0.30838loss in batch 20: 0.347702/0.310257loss in batch 21: 0.147308/0.302841loss in batch 22: 0.733994/0.321594loss in batch 23: 0.332138/0.322037loss in batch 24: 0.154587/0.315323loss in batch 25: 0.271591/0.31366loss in batch 26: 0.172043/0.308411loss in batch 27: 0.191544/0.30423loss in batch 28: 0.488571/0.310593loss in batch 29: 0.140289/0.304916loss in batch 30: 0.390778/0.307693loss in batch 31: 0.525665/0.314499loss in batch 32: 0.518433/0.320679loss in batch 33: 0.154129/0.315781loss in batch 34: 0.293213/0.31514loss in batch 35: 0.179321/0.311356loss in batch 36: 0.419968/0.314301loss in batch 37: 0.271408/0.313171loss in batch 38: 0.359207/0.314346loss in batch 39: 0.136612/0.309906loss in batch 40: 0.103088/0.304855loss in batch 41: 0.108795/0.300201loss in batch 42: 0.121475/0.296036loss in batch 43: 0.395218/0.298294loss in batch 44: 0.407486/0.30072loss in batch 45: 0.259186/0.29982loss in batch 46: 0.395325/0.301834loss in batch 47: 0.419846/0.304306loss in batch 48: 0.368713/0.305618loss in batch 49: 0.478378/0.309067loss in batch 50: 0.47963/0.312408loss in batch 51: 0.152039/0.309341loss in batch 52: 0.230392/0.307846loss in batch 53: 0.221695/0.306244loss in batch 54: 0.15715/0.303528loss in batch 55: 0.200119/0.301697loss in batch 56: 0.173828/0.299454loss in batch 57: 0.121078/0.296371loss in batch 58: 0.112045/0.293243loss in batch 59: 0.524368/0.297104loss in batch 60: 0.0798492/0.293533loss in batch 61: 0.0623169/0.28981loss in batch 62: 0.116867/0.287064loss in batch 63: 0.339462/0.287872loss in batch 64: 0.134384/0.285522loss in batch 65: 1.01692/0.2966loss in batch 66: 0.106476/0.293762loss in batch 67: 0.0618744/0.290359loss in batch 68: 0.132584/0.288071loss in batch 69: 0.431564/0.290131loss in batch 70: 0.527664/0.293472loss in batch 71: 0.221527/0.292465loss in batch 72: 0.284973/0.292358loss in batch 73: 0.1082/0.289871loss in batch 74: 0.0850677/0.28714loss in batch 75: 0.174515/0.28566loss in batch 76: 0.245865/0.285156loss in batch 77: 0.274628/0.285019loss in batch 78: 0.333313/0.285629loss in batch 79: 0.326355/0.286133loss in batch 80: 0.265594/0.285873loss in batch 81: 0.499344/0.288483loss in batch 82: 0.195663/0.287354loss in batch 83: 0.291092/0.287415loss in batch 84: 0.0838165/0.285004loss in batch 85: 0.364365/0.285934loss in batch 86: 0.288239/0.285965loss in batch 87: 0.415466/0.28743loss in batch 88: 0.256287/0.287079loss in batch 89: 0.344849/0.287735loss in batch 90: 0.10968/0.285767loss in batch 91: 0.0772552/0.283493loss in batch 92: 0.133896/0.281891loss in batch 93: 0.185089/0.280869loss in batch 94: 0.247925/0.280518loss in batch 95: 0.548096/0.283295loss in batch 96: 0.127411/0.281693loss in batch 97: 0.180069/0.280655loss in batch 98: 0.0137939/0.277969loss in batch 99: 0.176376/0.276947loss in batch 100: 0.122864/0.275421loss in batch 101: 0.107941/0.273773loss in batch 102: 0.687225/0.277802loss in batch 103: 0.298401/0.277985loss in batch 104: 0.455582/0.279678loss in batch 105: 0.314102/0.280014loss in batch 106: 0.113373/0.278442loss in batch 107: 0.169998/0.277435loss in batch 108: 0.164948/0.276413loss in batch 109: 0.152756/0.275299loss in batch 110: 0.326248/0.275757loss in batch 111: 0.049881/0.273727loss in batch 112: 0.0908966/0.272125loss in batch 113: 0.150772/0.271042loss in batch 114: 0.273178/0.271057loss in batch 115: 0.366058/0.271881loss in batch 116: 0.445023/0.273361loss in batch 117: 0.263016/0.273285loss in batch 118: 0.0748749/0.271606loss in batch 119: 0.297211/0.27182loss in batch 120: 0.140411/0.270737loss in batch 121: 0.200714/0.270172loss in batch 122: 0.106796/0.268845loss in batch 123: 0.0833893/0.267334loss in batch 124: 0.370407/0.268173loss in batch 125: 0.0677795/0.266571loss in batch 126: 0.303177/0.266861loss in batch 127: 0.0252991/0.264984loss in batch 128: 0.579865/0.26741loss in batch 129: 0.212402/0.266998loss in batch 130: 0.0714111/0.265503loss in batch 131: 0.219925/0.265152loss in batch 132: 0.141907/0.264221loss in batch 133: 0.0858459/0.262909loss in batch 134: 0.208267/0.262497loss in batch 135: 0.131836/0.26152loss in batch 136: 0.25592/0.26149loss in batch 137: 0.0592194/0.260025loss in batch 138: 0.112549/0.258957loss in batch 139: 0.760391/0.262543loss in batch 140: 0.610596/0.265015loss in batch 141: 0.118546/0.263992loss in batch 142: 0.477386/0.265472loss in batch 143: 0.121994/0.264481loss in batch 144: 0.0779266/0.263199loss in batch 145: 0.725357/0.266357loss in batch 146: 0.591385/0.26857loss in batch 147: 0.0750122/0.267258loss in batch 148: 0.133041/0.266357loss in batch 149: 0.139938/0.265518loss in batch 150: 0.211288/0.265167loss in batch 151: 0.0719604/0.263885loss in batch 152: 0.129852/0.263016loss in batch 153: 0.0860901/0.261856loss in batch 154: 0.101044/0.260834loss in batch 155: 0.171646/0.260254loss in batch 156: 0.106888/0.259277loss in batch 157: 0.196396/0.258881loss in batch 158: 0.153366/0.258209loss in batch 159: 0.233292/0.258057loss in batch 160: 0.114136/0.257156loss in batch 161: 0.215591/0.256912loss in batch 162: 0.344452/0.257446loss in batch 163: 0.122818/0.256638loss in batch 164: 0.375687/0.257355loss in batch 165: 0.193268/0.256958loss in batch 166: 0.108185/0.256058loss in batch 167: 0.288467/0.256256loss in batch 168: 0.343994/0.256775loss in batch 169: 0.0879669/0.255783loss in batch 170: 0.176422/0.255325loss in batch 171: 0.451401/0.25647loss in batch 172: 0.0308533/0.255157loss in batch 173: 0.544739/0.256836loss in batch 174: 0.194565/0.25647loss in batch 175: 0.178253/0.256012loss in batch 176: 0.201157/0.255722loss in batch 177: 0.978668/0.259766loss in batch 178: 0.0536346/0.258636loss in batch 179: 0.403366/0.25943loss in batch 180: 0.202271/0.259125loss in batch 181: 0.645309/0.26123loss in batch 182: 0.227158/0.261047loss in batch 183: 0.124176/0.2603loss in batch 184: 0.157196/0.25975loss in batch 185: 0.369659/0.260345loss in batch 186: 0.333527/0.260727loss in batch 187: 0.280426/0.260834loss in batch 188: 0.15802/0.260284loss in batch 189: 0.455795/0.261322loss in batch 190: 0.272903/0.261383loss in batch 191: 0.255234/0.261353loss in batch 192: 0.484924/0.262512loss in batch 193: 0.270966/0.262543loss in batch 194: 0.0374146/0.261398loss in batch 195: 0.463165/0.262421loss in batch 196: 0.462936/0.263443loss in batch 197: 0.146133/0.262848loss in batch 198: 0.151016/0.262283loss in batch 199: 0.192535/0.261948loss in batch 200: 0.306778/0.262161loss in batch 201: 0.229141/0.261993loss in batch 202: 0.214188/0.261749loss in batch 203: 0.0776215/0.260864loss in batch 204: 0.244446/0.260788loss in batch 205: 0.254211/0.260742loss in batch 206: 0.0639191/0.259796loss in batch 207: 0.178726/0.259399loss in batch 208: 0.241241/0.259323loss in batch 209: 0.0800934/0.258469loss in batch 210: 0.601547/0.260101loss in batch 211: 0.304306/0.2603loss in batch 212: 0.30986/0.260544
done with epoch 3
train_acc: 0.931925 (397/426)
test loss: 0.30986
acc: 0.93007 (133/143)
loss in batch 0: 0.396881/0.396881loss in batch 1: 0.11615/0.256516loss in batch 2: 0.109253/0.207428loss in batch 3: 0.0412598/0.165894loss in batch 4: 0.281418/0.188995loss in batch 5: 0.261841/0.201141loss in batch 6: 0.291336/0.21402loss in batch 7: 0.269852/0.221008loss in batch 8: 0.199463/0.218613loss in batch 9: 0.0932312/0.20607loss in batch 10: 0.165405/0.202377loss in batch 11: 0.275452/0.208466loss in batch 12: 0.158249/0.204605loss in batch 13: 0.48172/0.22438loss in batch 14: 0.185455/0.221802loss in batch 15: 0.0586853/0.211594loss in batch 16: 0.227463/0.21254loss in batch 17: 0.109451/0.206802loss in batch 18: 0.218201/0.207413loss in batch 19: 0.053009/0.199692loss in batch 20: 0.117065/0.195755loss in batch 21: 0.0348969/0.188446loss in batch 22: 0.144302/0.186523loss in batch 23: 0.11673/0.183609loss in batch 24: 0.10408/0.180435loss in batch 25: 0.273636/0.184021loss in batch 26: 0.0871124/0.18042loss in batch 27: 0.570587/0.194366loss in batch 28: 0.350388/0.199738loss in batch 29: 0.0746765/0.195572loss in batch 30: 0.102417/0.192566loss in batch 31: 0.894012/0.214493loss in batch 32: 0.300369/0.217102loss in batch 33: 0.107315/0.213867loss in batch 34: 0.155807/0.212204loss in batch 35: 0.206284/0.212036loss in batch 36: 0.263504/0.21344loss in batch 37: 0.161102/0.212051loss in batch 38: 0.415405/0.217255loss in batch 39: 0.224564/0.217438loss in batch 40: 0.501312/0.22438loss in batch 41: 0.134781/0.222244loss in batch 42: 0.419891/0.226837loss in batch 43: 0.104004/0.224045loss in batch 44: 0.0563507/0.220322loss in batch 45: 0.30397/0.222137loss in batch 46: 0.183319/0.221313loss in batch 47: 0.0744476/0.218246loss in batch 48: 0.26178/0.219147loss in batch 49: 0.125916/0.217285loss in batch 50: 0.197723/0.216888loss in batch 51: 0.242264/0.217377loss in batch 52: 0.0597534/0.214401loss in batch 53: 0.554779/0.220718loss in batch 54: 0.112686/0.21875loss in batch 55: 0.199402/0.218399loss in batch 56: 0.350601/0.220718loss in batch 57: 0.26387/0.221466loss in batch 58: 0.121811/0.219772loss in batch 59: 0.284134/0.220856loss in batch 60: 0.168198/0.219986loss in batch 61: 0.112839/0.218246loss in batch 62: 0.404175/0.221207loss in batch 63: 0.13385/0.219833loss in batch 64: 0.201843/0.219574loss in batch 65: 0.224716/0.219635loss in batch 66: 0.35936/0.221725loss in batch 67: 0.167374/0.220917loss in batch 68: 0.184082/0.220398loss in batch 69: 0.0989532/0.218658loss in batch 70: 0.116028/0.217224loss in batch 71: 0.0482941/0.214859loss in batch 72: 0.136246/0.213791loss in batch 73: 0.148651/0.212921loss in batch 74: 0.565918/0.217621loss in batch 75: 0.0235901/0.215057loss in batch 76: 0.382309/0.217239loss in batch 77: 0.361343/0.219086loss in batch 78: 0.292969/0.220016loss in batch 79: 0.0861816/0.218338loss in batch 80: 0.142105/0.217407loss in batch 81: 0.15358/0.216629loss in batch 82: 0.32782/0.217972loss in batch 83: 0.0980377/0.216537loss in batch 84: 0.685913/0.222061loss in batch 85: 0.196701/0.221771loss in batch 86: 0.222839/0.221786loss in batch 87: 0.29068/0.222565loss in batch 88: 0.0945129/0.221115loss in batch 89: 0.0968018/0.219742loss in batch 90: 0.0461578/0.217819loss in batch 91: 0.191254/0.217545loss in batch 92: 0.166061/0.216995loss in batch 93: 0.101303/0.215759loss in batch 94: 0.124176/0.214798loss in batch 95: 0.466934/0.217422loss in batch 96: 0.118134/0.2164loss in batch 97: 0.0657806/0.214859loss in batch 98: 0.133026/0.214035loss in batch 99: 0.0885925/0.212784loss in batch 100: 0.211823/0.212784loss in batch 101: 0.160095/0.21225loss in batch 102: 0.066391/0.210846loss in batch 103: 0.288467/0.211578loss in batch 104: 0.0887756/0.210403loss in batch 105: 0.76033/0.215607loss in batch 106: 0.0214081/0.213791loss in batch 107: 0.00709534/0.211884loss in batch 108: 1.31476/0.222loss in batch 109: 0.13829/0.221237loss in batch 110: 0.0367126/0.219574loss in batch 111: 0.049881/0.218048loss in batch 112: 0.403412/0.219696loss in batch 113: 0.456711/0.221771loss in batch 114: 0.118256/0.220871loss in batch 115: 0.462875/0.222961loss in batch 116: 0.0702209/0.221664loss in batch 117: 0.045929/0.220169loss in batch 118: 0.211716/0.220093loss in batch 119: 0.135513/0.219391loss in batch 120: 0.0229492/0.217758loss in batch 121: 0.584686/0.220779loss in batch 122: 0.217682/0.220749loss in batch 123: 0.334259/0.221664loss in batch 124: 0.0582886/0.220352loss in batch 125: 0.0847626/0.219284loss in batch 126: 0.100052/0.218338loss in batch 127: 0.07547/0.217224loss in batch 128: 0.257187/0.217545loss in batch 129: 0.29213/0.218109loss in batch 130: 0.197861/0.217957loss in batch 131: 0.396545/0.219315loss in batch 132: 0.358109/0.220352loss in batch 133: 0.0440216/0.21904loss in batch 134: 0.195969/0.218857loss in batch 135: 0.356079/0.219864loss in batch 136: 0.131622/0.219223loss in batch 137: 0.97377/0.224701loss in batch 138: 0.103439/0.223831loss in batch 139: 0.0601349/0.222656loss in batch 140: 0.142365/0.222092loss in batch 141: 0.385559/0.223251loss in batch 142: 0.157974/0.222778loss in batch 143: 0.462387/0.224457loss in batch 144: 0.289032/0.224899loss in batch 145: 0.0783081/0.223892loss in batch 146: 0.104019/0.223068loss in batch 147: 0.0888062/0.222168loss in batch 148: 0.265564/0.222458loss in batch 149: 1.00659/0.227676loss in batch 150: 0.216232/0.2276loss in batch 151: 0.197815/0.227402loss in batch 152: 0.0303802/0.22612loss in batch 153: 0.420349/0.227386loss in batch 154: 0.114624/0.226669loss in batch 155: 0.213608/0.226578loss in batch 156: 0.567581/0.228745loss in batch 157: 0.0306702/0.227493loss in batch 158: 0.641525/0.230087loss in batch 159: 0.127609/0.229462loss in batch 160: 0.13176/0.228851loss in batch 161: 0.180481/0.228546loss in batch 162: 0.14447/0.228027loss in batch 163: 0.259872/0.228226loss in batch 164: 0.0570984/0.227188loss in batch 165: 0.213593/0.227112loss in batch 166: 0.239487/0.227173loss in batch 167: 0.335175/0.227829loss in batch 168: 0.14566/0.227341loss in batch 169: 0.14679/0.226852loss in batch 170: 0.495453/0.228439loss in batch 171: 0.23378/0.22847loss in batch 172: 0.18573/0.22821loss in batch 173: 0.190323/0.227997loss in batch 174: 0.0843658/0.227188loss in batch 175: 0.242889/0.22728loss in batch 176: 0.0435486/0.226227loss in batch 177: 0.467743/0.227585loss in batch 178: 0.0670624/0.2267loss in batch 179: 0.485443/0.228134loss in batch 180: 0.62561/0.230316loss in batch 181: 0.083313/0.229523loss in batch 182: 0.118866/0.228912loss in batch 183: 0.254639/0.22905loss in batch 184: 0.449219/0.230255loss in batch 185: 0.271286/0.230469loss in batch 186: 0.501907/0.231918loss in batch 187: 0.996201/0.235977loss in batch 188: 0.0881805/0.235199loss in batch 189: 0.0929871/0.234451loss in batch 190: 0.24527/0.234497loss in batch 191: 0.205948/0.23436loss in batch 192: 0.306381/0.234741loss in batch 193: 0.115494/0.234116loss in batch 194: 0.0644684/0.233246loss in batch 195: 0.116989/0.232666loss in batch 196: 0.161102/0.2323loss in batch 197: 0.0939026/0.231583loss in batch 198: 0.951248/0.235199loss in batch 199: 0.186783/0.23497loss in batch 200: 0.0825806/0.234207loss in batch 201: 0.10611/0.233582loss in batch 202: 0.577225/0.23526loss in batch 203: 0.168625/0.23494loss in batch 204: 0.337341/0.235443loss in batch 205: 0.361969/0.236053loss in batch 206: 0.232285/0.236038loss in batch 207: 0.745514/0.238495loss in batch 208: 0.0953522/0.237793loss in batch 209: 0.107056/0.237183loss in batch 210: 0.277161/0.237366loss in batch 211: 0.454636/0.238403loss in batch 212: 0.0838013/0.237656
done with epoch 4
train_acc: 0.924883 (394/426)
test loss: 0.0838013
acc: 0.944056 (135/143)
loss in batch 0: 0.526917/0.526917loss in batch 1: 0.186905/0.356918loss in batch 2: 0.134323/0.282715loss in batch 3: 0.371216/0.30484loss in batch 4: 0.28801/0.301468loss in batch 5: 0.107605/0.269165loss in batch 6: 0.907623/0.360382loss in batch 7: 0.345734/0.358551loss in batch 8: 0.0895386/0.328644loss in batch 9: 0.0921326/0.305008loss in batch 10: 1.2299/0.389084loss in batch 11: 0.500244/0.398346loss in batch 12: 0.0978088/0.375229loss in batch 13: 0.0349426/0.350922loss in batch 14: 0.0990906/0.334122loss in batch 15: 0.372772/0.336548loss in batch 16: 0.346161/0.337112loss in batch 17: 0.0363159/0.320404loss in batch 18: 0.0598602/0.306702loss in batch 19: 0.948074/0.338745loss in batch 20: 0.190277/0.33168loss in batch 21: 0.115112/0.321838loss in batch 22: 0.230103/0.317856loss in batch 23: 0.122116/0.309708loss in batch 24: 0.259872/0.307693loss in batch 25: 0.0804596/0.298965loss in batch 26: 0.292068/0.298706loss in batch 27: 0.253738/0.297104loss in batch 28: 0.103348/0.290421loss in batch 29: 0.0981903/0.284027loss in batch 30: 0.22818/0.282227loss in batch 31: 0.147476/0.278loss in batch 32: 0.150513/0.274139loss in batch 33: 0.123444/0.269714loss in batch 34: 0.227707/0.268509loss in batch 35: 0.380539/0.271622loss in batch 36: 0.120346/0.267532loss in batch 37: 0.375259/0.270355loss in batch 38: 0.0692596/0.265213loss in batch 39: 0.444519/0.269684loss in batch 40: 0.0640106/0.264679loss in batch 41: 0.069931/0.26004loss in batch 42: 0.486588/0.265305loss in batch 43: 0.903107/0.279816loss in batch 44: 0.288528/0.279984loss in batch 45: 0.0982971/0.276047loss in batch 46: 0.432739/0.279373loss in batch 47: 0.0376587/0.274338loss in batch 48: 0.120071/0.271194loss in batch 49: 0.0995026/0.267761loss in batch 50: 0.197952/0.266388loss in batch 51: 0.100662/0.263199loss in batch 52: 0.241226/0.262787loss in batch 53: 0.426132/0.265823loss in batch 54: 0.533646/0.270676loss in batch 55: 0.140808/0.268356loss in batch 56: 0.0979919/0.265366loss in batch 57: 0.457245/0.268677loss in batch 58: 0.306229/0.269318loss in batch 59: 0.19989/0.268158loss in batch 60: 0.156403/0.266342loss in batch 61: 0.0514069/0.262878loss in batch 62: 0.176712/0.26149loss in batch 63: 0.0991821/0.258972loss in batch 64: 0.072113/0.256088loss in batch 65: 0.134323/0.254242loss in batch 66: 0.0941315/0.251862loss in batch 67: 0.0774231/0.249283loss in batch 68: 0.541351/0.253525loss in batch 69: 0.295197/0.25412loss in batch 70: 0.0707855/0.251541loss in batch 71: 0.0488586/0.248718loss in batch 72: 0.100571/0.246704loss in batch 73: 0.0970764/0.244659loss in batch 74: 0.0636444/0.242264loss in batch 75: 0.900131/0.250916loss in batch 76: 0.030777/0.248047loss in batch 77: 0.344238/0.249283loss in batch 78: 0.209335/0.248779loss in batch 79: 0.146317/0.247498loss in batch 80: 0.398605/0.249374loss in batch 81: 0.279663/0.249725loss in batch 82: 0.403351/0.251587loss in batch 83: 0.0645294/0.249359loss in batch 84: 0.300522/0.249954loss in batch 85: 0.0457306/0.247589loss in batch 86: 0.10495/0.245941loss in batch 87: 0.248154/0.245972loss in batch 88: 0.600754/0.249954loss in batch 89: 0.255127/0.250015loss in batch 90: 0.172928/0.249161loss in batch 91: 0.156937/0.248169loss in batch 92: 0.152664/0.247147loss in batch 93: 0.115219/0.245728loss in batch 94: 0.0961609/0.244156loss in batch 95: 0.263474/0.24437loss in batch 96: 0.194916/0.243851loss in batch 97: 0.147598/0.242859loss in batch 98: 0.0618439/0.241028loss in batch 99: 0.198441/0.240616loss in batch 100: 0.168289/0.239899loss in batch 101: 0.19635/0.239471loss in batch 102: 0.197479/0.239075loss in batch 103: 0.0723877/0.237457loss in batch 104: 0.164261/0.236771loss in batch 105: 0.176559/0.236206loss in batch 106: 0.0839691/0.234772loss in batch 107: 0.0730133/0.233276loss in batch 108: 0.0255585/0.231369loss in batch 109: 0.0762787/0.229965loss in batch 110: 0.0906372/0.228714loss in batch 111: 0.112732/0.227676loss in batch 112: 0.0233917/0.225861loss in batch 113: 0.097702/0.224731loss in batch 114: 0.359329/0.225906loss in batch 115: 0.655289/0.229614loss in batch 116: 0.131622/0.228775loss in batch 117: 0.0259247/0.227051loss in batch 118: 0.0749969/0.225769loss in batch 119: 0.176758/0.225372loss in batch 120: 0.0868683/0.224228loss in batch 121: 0.012558/0.222488loss in batch 122: 0.399689/0.223923loss in batch 123: 0.0897217/0.222839loss in batch 124: 0.424408/0.224457loss in batch 125: 0.114029/0.223587loss in batch 126: 0.0999146/0.22261loss in batch 127: 0.0930939/0.221588loss in batch 128: 0.0578766/0.220322loss in batch 129: 0.449158/0.222092loss in batch 130: 0.178207/0.221756loss in batch 131: 0.161301/0.221298loss in batch 132: 0.0608368/0.220093loss in batch 133: 0.0693054/0.218964loss in batch 134: 0.144913/0.218414loss in batch 135: 0.512253/0.220566loss in batch 136: 0.0254059/0.219147loss in batch 137: 0.240936/0.219299loss in batch 138: 0.170547/0.218964loss in batch 139: 0.645294/0.222015loss in batch 140: 0.364288/0.223007loss in batch 141: 0.0882111/0.222061loss in batch 142: 0.197266/0.221878loss in batch 143: 0.151749/0.221405loss in batch 144: 0.0826569/0.220444loss in batch 145: 0.127289/0.219803loss in batch 146: 0.300995/0.220367loss in batch 147: 0.0686188/0.21933loss in batch 148: 0.0904694/0.21846loss in batch 149: 0.207825/0.218399loss in batch 150: 0.347977/0.219254loss in batch 151: 0.33812/0.220032loss in batch 152: 0.0834808/0.219147loss in batch 153: 0.0766602/0.218231loss in batch 154: 0.000610352/0.216812loss in batch 155: 0.109085/0.216125loss in batch 156: 0.0813446/0.215256loss in batch 157: 0.229126/0.215363loss in batch 158: 0.0861969/0.214539loss in batch 159: 0.117905/0.213943loss in batch 160: 0.149017/0.213531loss in batch 161: 0.0852814/0.212753loss in batch 162: 0.922836/0.217102loss in batch 163: 0.177917/0.216873loss in batch 164: 0.404572/0.218002loss in batch 165: 0.402863/0.219116loss in batch 166: 0.109482/0.21846loss in batch 167: 0.00259399/0.217163loss in batch 168: 0.0881653/0.216415loss in batch 169: 0.0738373/0.215576loss in batch 170: 0.0907898/0.214828loss in batch 171: 0.104324/0.214203loss in batch 172: 0.139206/0.21376loss in batch 173: 0.243179/0.213928loss in batch 174: 0.805237/0.217316loss in batch 175: 0.391403/0.218307loss in batch 176: 0.171585/0.218033loss in batch 177: 0.0740204/0.217224loss in batch 178: 0.591965/0.21933loss in batch 179: 0.041687/0.218323loss in batch 180: 0.423141/0.219467loss in batch 181: 0.186188/0.219284loss in batch 182: 0.407867/0.220322loss in batch 183: 0.468872/0.221664loss in batch 184: 0.251434/0.221817loss in batch 185: 0.508591/0.223358loss in batch 186: 0.127457/0.222855loss in batch 187: 0.199341/0.222733loss in batch 188: 0.444061/0.223907loss in batch 189: 0.226242/0.223907loss in batch 190: 0.112396/0.223328loss in batch 191: 0.223907/0.223328loss in batch 192: 0.104996/0.222717loss in batch 193: 0.0828857/0.222loss in batch 194: 0.366821/0.222748loss in batch 195: 0.1026/0.222122loss in batch 196: 0.228714/0.222153loss in batch 197: 0.461411/0.223373loss in batch 198: 0.155594/0.223022loss in batch 199: 0.0665894/0.222244loss in batch 200: 0.14151/0.221848loss in batch 201: 0.257813/0.222031loss in batch 202: 0.14595/0.221649loss in batch 203: 0.485886/0.222946loss in batch 204: 0.151566/0.222595loss in batch 205: 0.0385437/0.221695loss in batch 206: 0.130219/0.221252loss in batch 207: 0.164658/0.220993loss in batch 208: 0.125259/0.22052loss in batch 209: 0.200943/0.220444loss in batch 210: 0.328537/0.220947loss in batch 211: 0.0464478/0.220123loss in batch 212: 0.155731/0.219818
done with epoch 5
train_acc: 0.934272 (398/426)
test loss: 0.155731
acc: 0.937063 (134/143)
loss in batch 0: 0.494598/0.494598loss in batch 1: 0.394379/0.444489loss in batch 2: 0.0595703/0.316177loss in batch 3: 0.164749/0.27832loss in batch 4: 0.153839/0.253418loss in batch 5: 0.172379/0.239914loss in batch 6: 0.0375519/0.210999loss in batch 7: 0.390656/0.233475loss in batch 8: 0.0887756/0.217392loss in batch 9: 0.220688/0.217712loss in batch 10: 0.0415649/0.201706loss in batch 11: 0.220505/0.203278loss in batch 12: 0.16655/0.200455loss in batch 13: 0.0666351/0.190887loss in batch 14: 0.100815/0.184875loss in batch 15: 0.062561/0.177246loss in batch 16: 0.291/0.183929loss in batch 17: 0.185684/0.184036loss in batch 18: 0.365936/0.193604loss in batch 19: 0.289139/0.19838loss in batch 20: 0.42366/0.209106loss in batch 21: 0.128067/0.205414loss in batch 22: 0.048172/0.198593loss in batch 23: 0.0595856/0.192795loss in batch 24: 0.129013/0.190247loss in batch 25: 0.0731812/0.185745loss in batch 26: 0.0957336/0.182404loss in batch 27: 0.165894/0.181824loss in batch 28: 0.140625/0.180405loss in batch 29: 0.158035/0.179657loss in batch 30: 0.0909576/0.176788loss in batch 31: 0.0615387/0.173203loss in batch 32: 0.551117/0.184647loss in batch 33: 0.186829/0.184708loss in batch 34: 0.138947/0.183395loss in batch 35: 0.0539246/0.17981loss in batch 36: 0.400757/0.185776loss in batch 37: 0.0678253/0.182663loss in batch 38: 0.16098/0.182114loss in batch 39: 1.08344/0.204636loss in batch 40: 0.213974/0.20488loss in batch 41: 1.00531/0.223938loss in batch 42: 0.155655/0.222351loss in batch 43: 0.065155/0.218765loss in batch 44: 0.130432/0.216797loss in batch 45: 0.411163/0.221039loss in batch 46: 0.367096/0.224136loss in batch 47: 0.238724/0.224457loss in batch 48: 0.311325/0.226212loss in batch 49: 0.170929/0.225113loss in batch 50: 0.132034/0.223282loss in batch 51: 0.320206/0.225159loss in batch 52: 0.351685/0.227539loss in batch 53: 0.135529/0.225845loss in batch 54: 0.145477/0.22438loss in batch 55: 1.24435/0.242584loss in batch 56: 0.0557861/0.239304loss in batch 57: 0.342422/0.241089loss in batch 58: 0.0702972/0.23819loss in batch 59: 0.240631/0.238235loss in batch 60: 0.358261/0.240204loss in batch 61: 0.107117/0.238052loss in batch 62: 0.657883/0.24472loss in batch 63: 0.137802/0.243057loss in batch 64: 0.105057/0.240921loss in batch 65: 0.171875/0.239868loss in batch 66: 0.166763/0.238785loss in batch 67: 0.0296478/0.235703loss in batch 68: 0.397095/0.238052loss in batch 69: 0.250687/0.238235loss in batch 70: 0.366119/0.240036loss in batch 71: 0.111679/0.238251loss in batch 72: 0.149765/0.23703loss in batch 73: 0.137192/0.235687loss in batch 74: 0.339066/0.237061loss in batch 75: 0.277267/0.237595loss in batch 76: 0.380096/0.239441loss in batch 77: 0.209488/0.239059loss in batch 78: 0.0657043/0.236877loss in batch 79: 0.0736389/0.234833loss in batch 80: 0.0716095/0.232819loss in batch 81: 0.211395/0.232544loss in batch 82: 0.19278/0.232071loss in batch 83: 0.141861/0.231003loss in batch 84: 0.288879/0.231674loss in batch 85: 0.108185/0.23024loss in batch 86: 0.0454865/0.228119loss in batch 87: 0.3396/0.229385loss in batch 88: 0.0836639/0.227737loss in batch 89: 0.0164032/0.225403loss in batch 90: 0.255325/0.225739loss in batch 91: 0.0760345/0.224106loss in batch 92: 0.245819/0.224335loss in batch 93: 0.0973969/0.222992loss in batch 94: 0.750687/0.228531loss in batch 95: 0.0615387/0.226807loss in batch 96: 0.0878601/0.225372loss in batch 97: 0.0890503/0.223969loss in batch 98: 0.480484/0.226563loss in batch 99: 0.0682831/0.224991loss in batch 100: 0.204391/0.224792loss in batch 101: 0.140274/0.223953loss in batch 102: 0.206177/0.22377loss in batch 103: 0.02948/0.221924loss in batch 104: 0.814468/0.227554loss in batch 105: 0.1884/0.227188loss in batch 106: 0.0519104/0.225555loss in batch 107: 0.0859833/0.224258loss in batch 108: 0.192062/0.223953loss in batch 109: 0.0529785/0.222412loss in batch 110: 0.152176/0.221771loss in batch 111: 0.154236/0.221176loss in batch 112: 0.552109/0.224091loss in batch 113: 0.248535/0.224304loss in batch 114: 0.0655823/0.222931loss in batch 115: 0.0639496/0.221573loss in batch 116: 0.26445/0.221939loss in batch 117: 0.0305786/0.220306loss in batch 118: 0.0683441/0.21904loss in batch 119: 0.0418091/0.217545loss in batch 120: 0.24855/0.217804loss in batch 121: 0.0163727/0.216156loss in batch 122: 0.0213165/0.214569loss in batch 123: 0.321564/0.215439loss in batch 124: 0.119858/0.214676loss in batch 125: 0.100662/0.21376loss in batch 126: 0.810593/0.218475loss in batch 127: 0.126678/0.217758loss in batch 128: 0.067749/0.216583loss in batch 129: 0.138031/0.215973loss in batch 130: 0.294815/0.216583loss in batch 131: 0.209427/0.216537loss in batch 132: 0.0558167/0.215317loss in batch 133: 0.0872192/0.214371loss in batch 134: 0.0629578/0.213242loss in batch 135: 0.0644684/0.212143loss in batch 136: 0.0768738/0.211166loss in batch 137: 0.169922/0.210861loss in batch 138: 0.086792/0.209976loss in batch 139: 0.0442963/0.208786loss in batch 140: 0.0875092/0.207932loss in batch 141: 0.282364/0.20845loss in batch 142: 0.0693207/0.207489loss in batch 143: 0.0738831/0.206543loss in batch 144: 0.246124/0.206818loss in batch 145: 0.121887/0.206238loss in batch 146: 0.34967/0.207214loss in batch 147: 0.100571/0.206497loss in batch 148: 0.27243/0.20694loss in batch 149: 0.163651/0.20665loss in batch 150: 0.67392/0.209747loss in batch 151: 0.136353/0.209259loss in batch 152: 0.0657043/0.208328loss in batch 153: 0.317825/0.20903loss in batch 154: 0.0832214/0.208221loss in batch 155: 0.022995/0.207031loss in batch 156: 0.127243/0.206528loss in batch 157: 0.069519/0.205658loss in batch 158: 1.16232/0.211685loss in batch 159: 0.244766/0.211884loss in batch 160: 0.058548/0.210938loss in batch 161: 0.150269/0.210571loss in batch 162: 0.544052/0.212601loss in batch 163: 0.031189/0.211502loss in batch 164: 0.184998/0.211349loss in batch 165: 0.0233459/0.210205loss in batch 166: 0.0269165/0.209106loss in batch 167: 0.147095/0.20874loss in batch 168: 0.511978/0.210541loss in batch 169: 0.0425568/0.209549loss in batch 170: 0.110001/0.208969loss in batch 171: 0.242142/0.209152loss in batch 172: 0.140961/0.208771loss in batch 173: 0.0396423/0.207779loss in batch 174: 0.0420227/0.206848loss in batch 175: 0.0509033/0.205948loss in batch 176: 0.324875/0.206619loss in batch 177: 0.0276642/0.205627loss in batch 178: 0.661743/0.208176loss in batch 179: 0.0327301/0.207199loss in batch 180: 0.727234/0.210068loss in batch 181: 0.108337/0.209503loss in batch 182: 0.140839/0.209137loss in batch 183: 0.363373/0.209976loss in batch 184: 0.123627/0.209518loss in batch 185: 0.0768585/0.208786loss in batch 186: 0.527206/0.210495loss in batch 187: 0.119186/0.210007loss in batch 188: 0.103394/0.209457loss in batch 189: 0.356873/0.21022loss in batch 190: 0.0460968/0.209366loss in batch 191: 0.125427/0.208923loss in batch 192: 0.522461/0.210556loss in batch 193: 0.0979919/0.209961loss in batch 194: 0.134079/0.209579loss in batch 195: 0.359955/0.210342loss in batch 196: 0.186707/0.210236loss in batch 197: 0.05159/0.209427loss in batch 198: 0.0636139/0.208694loss in batch 199: 0.239853/0.208847loss in batch 200: 0.418518/0.209885loss in batch 201: 0.0442505/0.209061loss in batch 202: 0.0518951/0.208298loss in batch 203: 0.0168915/0.207367loss in batch 204: 0.0224304/0.206451loss in batch 205: 0.230682/0.206573loss in batch 206: 0.0619812/0.205872loss in batch 207: 0.349136/0.206558loss in batch 208: 0.0745239/0.205933loss in batch 209: 0.264938/0.206207loss in batch 210: 0.147156/0.205933loss in batch 211: 0.226257/0.206039loss in batch 212: 0.21228/0.20607
done with epoch 6
train_acc: 0.941315 (401/426)
test loss: 0.21228
acc: 0.937063 (134/143)
loss in batch 0: 0.141541/0.141541loss in batch 1: 0.403152/0.272354loss in batch 2: 0.212036/0.252243loss in batch 3: 0.0904999/0.211807loss in batch 4: 0.27388/0.224213loss in batch 5: 0.0843811/0.200912loss in batch 6: 0.04599/0.178772loss in batch 7: 0.266434/0.189743loss in batch 8: 0.286163/0.200439loss in batch 9: 0.0672607/0.187134loss in batch 10: 0.237366/0.191711loss in batch 11: 0.117523/0.185516loss in batch 12: 0.644516/0.220825loss in batch 13: 0.0638428/0.20961loss in batch 14: 0.0466614/0.198746loss in batch 15: 0.513245/0.218399loss in batch 16: 0.394409/0.22876loss in batch 17: 0.730194/0.256622loss in batch 18: 0.0222626/0.244278loss in batch 19: 0.164444/0.240295loss in batch 20: 0.0853271/0.23291loss in batch 21: 0.0739288/0.225693loss in batch 22: 0.793472/0.250366loss in batch 23: 0.16832/0.246948loss in batch 24: 0.154617/0.243271loss in batch 25: 0.10762/0.238037loss in batch 26: 0.0197601/0.229965loss in batch 27: 0.485901/0.239105loss in batch 28: 0.649719/0.25325loss in batch 29: 0.0278778/0.245743loss in batch 30: 0.15094/0.242676loss in batch 31: 0.066925/0.237198loss in batch 32: 0.278458/0.238434loss in batch 33: 0.0276489/0.232239loss in batch 34: 0.0303345/0.226486loss in batch 35: 0.132462/0.223877loss in batch 36: 0.0807343/0.220001loss in batch 37: 0.235947/0.220413loss in batch 38: 0.238968/0.220901loss in batch 39: 0.0451965/0.216492loss in batch 40: 0.0709686/0.212952loss in batch 41: 0.0345154/0.208694loss in batch 42: 0.148636/0.207306loss in batch 43: 0.079483/0.204391loss in batch 44: 0.0189819/0.200272loss in batch 45: 0.200577/0.200272loss in batch 46: 0.211639/0.200531loss in batch 47: 0.0804749/0.198029loss in batch 48: 0.172852/0.197525loss in batch 49: 0.212036/0.1978loss in batch 50: 0.117935/0.196228loss in batch 51: 0.0589142/0.193588loss in batch 52: 0.221451/0.194107loss in batch 53: 0.114059/0.192642loss in batch 54: 0.0544891/0.190125loss in batch 55: 0.120834/0.188889loss in batch 56: 0.0609436/0.186646loss in batch 57: 0.0280609/0.183914loss in batch 58: 0.108734/0.182632loss in batch 59: 0.185028/0.182678loss in batch 60: 0.0505371/0.180511loss in batch 61: 0.0909119/0.179062loss in batch 62: 0.187637/0.179199loss in batch 63: 0.0580139/0.177307loss in batch 64: 0.0798645/0.175812loss in batch 65: 0.0640106/0.174118loss in batch 66: 0.0837708/0.172775loss in batch 67: 0.201309/0.173187loss in batch 68: 0.157486/0.172958loss in batch 69: 0.12207/0.172241loss in batch 70: 0.218323/0.172882loss in batch 71: 0.0444641/0.171097loss in batch 72: 0.0289612/0.169144loss in batch 73: 0.771179/0.177277loss in batch 74: 0.0408325/0.175476loss in batch 75: 0.0974121/0.174438loss in batch 76: 0.532318/0.179092loss in batch 77: 0.03862/0.177292loss in batch 78: 0.281723/0.178604loss in batch 79: 0.0749054/0.177307loss in batch 80: 0.0644073/0.175919loss in batch 81: 0.0548096/0.174438loss in batch 82: 0.5952/0.17952loss in batch 83: 0.0675507/0.178177loss in batch 84: 0.0637512/0.176834loss in batch 85: 0.0731659/0.175629loss in batch 86: 0.0577393/0.174271loss in batch 87: 0.0802155/0.173203loss in batch 88: 0.105164/0.17244loss in batch 89: 0.172668/0.17244loss in batch 90: 0.030426/0.170883loss in batch 91: 0.208847/0.17128loss in batch 92: 0.107834/0.170609loss in batch 93: 0.0967865/0.16983loss in batch 94: 0.141769/0.16954loss in batch 95: 0.0665283/0.168457loss in batch 96: 0.619873/0.173111loss in batch 97: 0.0897217/0.172256loss in batch 98: 0.086441/0.171402loss in batch 99: 0.0706482/0.17038loss in batch 100: 0.0518951/0.16922loss in batch 101: 0.515503/0.172607loss in batch 102: 0.0645447/0.171555loss in batch 103: 0.0591736/0.170486loss in batch 104: 0.167007/0.170441loss in batch 105: 0.502838/0.173584loss in batch 106: 0.0829163/0.172729loss in batch 107: 0.0936737/0.171997loss in batch 108: 0.13237/0.171646loss in batch 109: 0.0668335/0.170685loss in batch 110: 0.24707/0.171371loss in batch 111: 0.0569763/0.170349loss in batch 112: 0.262436/0.171173loss in batch 113: 0.145615/0.170944loss in batch 114: 0.454742/0.173401loss in batch 115: 0.0915222/0.172699loss in batch 116: 0.355011/0.174271loss in batch 117: 0.0623932/0.173309loss in batch 118: 0.848907/0.178986loss in batch 119: 0.0746918/0.178131loss in batch 120: 0.188187/0.178192loss in batch 121: 0.0835876/0.177429loss in batch 122: 0.103683/0.176834loss in batch 123: 0.0704193/0.175964loss in batch 124: 0.00679016/0.174606loss in batch 125: 0.0918427/0.173965loss in batch 126: 0.115112/0.173508loss in batch 127: 0.0336151/0.172409loss in batch 128: 0.363251/0.173889loss in batch 129: 0.00971985/0.172623loss in batch 130: 0.0923767/0.172012loss in batch 131: 0.24231/0.172546loss in batch 132: 0.0721741/0.171783loss in batch 133: 0.126053/0.171448loss in batch 134: 0.108063/0.170975loss in batch 135: 0.290543/0.17186loss in batch 136: 0.252106/0.17244loss in batch 137: 0.183014/0.172516loss in batch 138: 0.153/0.172379loss in batch 139: 0.116074/0.171982loss in batch 140: 0.388779/0.173508loss in batch 141: 0.490936/0.175751loss in batch 142: 0.238937/0.176178loss in batch 143: 0.560883/0.178864loss in batch 144: 0.101608/0.178329loss in batch 145: 0.048233/0.177444loss in batch 146: 0.0808716/0.176773loss in batch 147: 0.196625/0.17691loss in batch 148: 0.187546/0.176987loss in batch 149: 0.211624/0.177216loss in batch 150: 0.359451/0.178421loss in batch 151: 0.17688/0.178406loss in batch 152: 0.141129/0.178162loss in batch 153: 0.0896301/0.177597loss in batch 154: 0.426773/0.179214loss in batch 155: 0.184937/0.179245loss in batch 156: 0.0520935/0.178436loss in batch 157: 0.0401459/0.177551loss in batch 158: 0.0174866/0.176544loss in batch 159: 0.297394/0.177307loss in batch 160: 0.0439911/0.176483loss in batch 161: 0.0843201/0.175903loss in batch 162: 0.334732/0.17688loss in batch 163: 0.722351/0.180206loss in batch 164: 0.094986/0.179703loss in batch 165: 0.112427/0.179291loss in batch 166: 1.14932/0.185089loss in batch 167: 1.06136/0.190308loss in batch 168: 0.417175/0.19165loss in batch 169: 0.26265/0.192062loss in batch 170: 0.26561/0.19249loss in batch 171: 0.185745/0.192459loss in batch 172: 0.74968/0.195679loss in batch 173: 0.364182/0.196655loss in batch 174: 0.0233154/0.195663loss in batch 175: 0.422089/0.196945loss in batch 176: 0.177109/0.196838loss in batch 177: 0.19278/0.196808loss in batch 178: 0.0423126/0.195938loss in batch 179: 0.0390778/0.195068loss in batch 180: 0.0800629/0.194443loss in batch 181: 0.139435/0.194138loss in batch 182: 0.0761719/0.193497loss in batch 183: 0.0819092/0.192886loss in batch 184: 0.0473328/0.192108loss in batch 185: 0.0363922/0.191254loss in batch 186: 0.255402/0.191605loss in batch 187: 0.084137/0.191025loss in batch 188: 0.200012/0.191086loss in batch 189: 0.133575/0.190765loss in batch 190: 0.148804/0.190552loss in batch 191: 0.159805/0.190399loss in batch 192: 0.114655/0.190018loss in batch 193: 0.269714/0.190414loss in batch 194: 0.385986/0.191422loss in batch 195: 0.184738/0.191391loss in batch 196: 0.1539/0.191193loss in batch 197: 0.231049/0.191391loss in batch 198: 0.0465088/0.190659loss in batch 199: 0.0745087/0.190094loss in batch 200: 0.635025/0.192307loss in batch 201: 0.127197/0.191986loss in batch 202: 0.260178/0.192322loss in batch 203: 1.26767/0.197586loss in batch 204: 0.0274811/0.196762loss in batch 205: 0.124435/0.196396loss in batch 206: 0.202316/0.196426loss in batch 207: 0.0270844/0.195618loss in batch 208: 0.0794983/0.195068loss in batch 209: 0.25174/0.195328loss in batch 210: 0.681656/0.197647loss in batch 211: 0.520721/0.199173loss in batch 212: 0.107391/0.19873
done with epoch 7
train_acc: 0.934272 (398/426)
test loss: 0.107391
acc: 0.937063 (134/143)
loss in batch 0: 0.0804138/0.0804138loss in batch 1: 0.0399475/0.0601807loss in batch 2: 0.267776/0.129379loss in batch 3: 0.0460205/0.108536loss in batch 4: 0.136978/0.114227loss in batch 5: 0.598953/0.195023loss in batch 6: 0.0532532/0.174759loss in batch 7: 0.176407/0.174957loss in batch 8: 0.241013/0.182297loss in batch 9: 0.0784607/0.171921loss in batch 10: 0.128159/0.167953loss in batch 11: 0.366104/0.184464loss in batch 12: 0.0680389/0.175507loss in batch 13: 0.0696106/0.167938loss in batch 14: 0.144623/0.166382loss in batch 15: 0.37001/0.179108loss in batch 16: 0.0438538/0.171158loss in batch 17: 0.0277252/0.163193loss in batch 18: 0.0288239/0.156113loss in batch 19: 0.342163/0.165421loss in batch 20: 0.31424/0.172501loss in batch 21: 0.101959/0.169296loss in batch 22: 0.0837708/0.165573loss in batch 23: 0.0235596/0.159668loss in batch 24: 0.0479584/0.155197loss in batch 25: 0.0554352/0.151352loss in batch 26: 0.0798492/0.148712loss in batch 27: 0.339172/0.155502loss in batch 28: 0.102524/0.153687loss in batch 29: 0.0410461/0.149918loss in batch 30: 0.283966/0.154251loss in batch 31: 0.343536/0.160172loss in batch 32: 0.194397/0.161209loss in batch 33: 0.23053/0.163239loss in batch 34: 0.185089/0.163879loss in batch 35: 0.0984802/0.162064loss in batch 36: 0.0818176/0.159882loss in batch 37: 0.294022/0.163406loss in batch 38: 0.407181/0.169662loss in batch 39: 0.0732574/0.167252loss in batch 40: 0.110306/0.165863loss in batch 41: 0.522949/0.174362loss in batch 42: 0.011795/0.170593loss in batch 43: 0.0823212/0.168579loss in batch 44: 0.338791/0.172363loss in batch 45: 0.214935/0.173294loss in batch 46: 0.0384216/0.17041loss in batch 47: 0.108337/0.169128loss in batch 48: 0.108093/0.167877loss in batch 49: 0.202347/0.168564loss in batch 50: 0.113647/0.167496loss in batch 51: 0.163315/0.167419loss in batch 52: 0.0364532/0.164948loss in batch 53: 0.0703888/0.163193loss in batch 54: 0.279816/0.165314loss in batch 55: 0.318787/0.16806loss in batch 56: 0.18364/0.16832loss in batch 57: 0.293488/0.170486loss in batch 58: 0.412827/0.174591loss in batch 59: 0.0666809/0.172791loss in batch 60: 0.211899/0.173431loss in batch 61: 0.0460815/0.171371loss in batch 62: 0.0424042/0.169342loss in batch 63: 0.535095/0.175049loss in batch 64: 0.396225/0.178452loss in batch 65: 0.402832/0.181854loss in batch 66: 0.205948/0.182205loss in batch 67: 0.101746/0.181015loss in batch 68: 0.109055/0.179977loss in batch 69: 0.0771027/0.178513loss in batch 70: 0.234192/0.179306loss in batch 71: 0.412109/0.182526loss in batch 72: 0.638016/0.188766loss in batch 73: 0.356628/0.19104loss in batch 74: 0.11438/0.190018loss in batch 75: 0.139786/0.189362loss in batch 76: 1.28056/0.203522loss in batch 77: 0.2724/0.204407loss in batch 78: 0.0871735/0.202927loss in batch 79: 0.0670624/0.201218loss in batch 80: 0.0315399/0.199127loss in batch 81: 0.106354/0.198013loss in batch 82: 0.514328/0.201813loss in batch 83: 0.0849762/0.200424loss in batch 84: 0.025177/0.198349loss in batch 85: 0.279907/0.19931loss in batch 86: 0.0753937/0.197891loss in batch 87: 0.261642/0.198608loss in batch 88: 0.0697632/0.197159loss in batch 89: 0.694885/0.202682loss in batch 90: 0.114914/0.201721loss in batch 91: 0.0948029/0.200562loss in batch 92: 0.0324707/0.198761loss in batch 93: 0.359436/0.20047loss in batch 94: 1.01607/0.209045loss in batch 95: 0.109467/0.208008loss in batch 96: 0.0944824/0.206848loss in batch 97: 0.0217133/0.204956loss in batch 98: 0.0905762/0.203796loss in batch 99: 0.417618/0.205933loss in batch 100: 1.28743/0.216644loss in batch 101: 0.483826/0.219254loss in batch 102: 0.0397644/0.217514loss in batch 103: 0.0836182/0.216232loss in batch 104: 0.0338135/0.214493loss in batch 105: 0.0750122/0.213181loss in batch 106: 0.153427/0.212616loss in batch 107: 0.173523/0.212265loss in batch 108: 0.120407/0.211426loss in batch 109: 0.172882/0.21106loss in batch 110: 0.0440826/0.209564loss in batch 111: 0.0334625/0.207993loss in batch 112: 0.134247/0.207336loss in batch 113: 0.221924/0.207474loss in batch 114: 0.296829/0.208237loss in batch 115: 0.174927/0.207947loss in batch 116: 0.0530243/0.206619loss in batch 117: 0.432526/0.208557loss in batch 118: 0.0241852/0.207001loss in batch 119: 0.10672/0.206161loss in batch 120: 0.0593414/0.204941loss in batch 121: 0.329697/0.205978loss in batch 122: 0.464096/0.208069loss in batch 123: 0.0471191/0.206772loss in batch 124: 0.0346527/0.205383loss in batch 125: 0.104691/0.20459loss in batch 126: 0.0836334/0.203644loss in batch 127: 0.0475616/0.202423loss in batch 128: 0.24498/0.202759loss in batch 129: 0.359299/0.203949loss in batch 130: 0.0218964/0.20256loss in batch 131: 0.174026/0.202347loss in batch 132: 0.149658/0.20195loss in batch 133: 0.0695496/0.200974loss in batch 134: 0.198624/0.200943loss in batch 135: 0.0425568/0.199783loss in batch 136: 0.343124/0.200836loss in batch 137: 0.0731506/0.199905loss in batch 138: 0.030777/0.198685loss in batch 139: 0.0197601/0.197403loss in batch 140: 0.235306/0.197678loss in batch 141: 0.0703278/0.196793loss in batch 142: 0.0228729/0.195557loss in batch 143: 0.104523/0.194931loss in batch 144: 0.057724/0.193985loss in batch 145: 0.0597534/0.193069loss in batch 146: 0.49054/0.195099loss in batch 147: 0.346252/0.196106loss in batch 148: 0.00994873/0.194855loss in batch 149: 0.0784912/0.194077loss in batch 150: 0.0835419/0.193344loss in batch 151: 0.183945/0.193298loss in batch 152: 0.101334/0.192688loss in batch 153: 0.136887/0.192337loss in batch 154: 0.108414/0.191788loss in batch 155: 0.261444/0.19223loss in batch 156: 0.0438385/0.191299loss in batch 157: 0.0483246/0.190399loss in batch 158: 0.0465851/0.189484loss in batch 159: 0.205063/0.189575loss in batch 160: 0.079483/0.188904loss in batch 161: 0.14975/0.18866loss in batch 162: 0.459152/0.190308loss in batch 163: 0.537125/0.192429loss in batch 164: 0.0574951/0.191605loss in batch 165: 0.337952/0.19249loss in batch 166: 0.0574188/0.191681loss in batch 167: 0.00852966/0.190598loss in batch 168: 0.609436/0.193069loss in batch 169: 0.0520782/0.192245loss in batch 170: 0.82077/0.195908loss in batch 171: 0.357635/0.196854loss in batch 172: 0.0953064/0.196274loss in batch 173: 0.216202/0.196381loss in batch 174: 0.0699005/0.195663loss in batch 175: 0.185715/0.195618loss in batch 176: 0.121643/0.19519loss in batch 177: 0.199326/0.195206loss in batch 178: 0.099411/0.194672loss in batch 179: 0.0236969/0.193726loss in batch 180: 0.228271/0.193924loss in batch 181: 0.0702362/0.193237loss in batch 182: 0.042099/0.192413loss in batch 183: 0.0634308/0.191711loss in batch 184: 0.245422/0.192001loss in batch 185: 0.298843/0.192566loss in batch 186: 0.584671/0.194672loss in batch 187: 0.28479/0.19516loss in batch 188: 0.189346/0.195129loss in batch 189: 0.134125/0.194794loss in batch 190: 0.0435944/0.194loss in batch 191: 0.0549469/0.193283loss in batch 192: 0.0141602/0.192352loss in batch 193: 0.129654/0.192032loss in batch 194: 0.0475006/0.191284loss in batch 195: 0.0502319/0.190582loss in batch 196: 0.0693512/0.189957loss in batch 197: 0.0444183/0.189224loss in batch 198: 0.0299835/0.188416loss in batch 199: 0.140625/0.188187loss in batch 200: 0.14151/0.187958loss in batch 201: 0.0971527/0.1875loss in batch 202: 0.204483/0.187592loss in batch 203: 0.0587311/0.186951loss in batch 204: 0.0537262/0.186295loss in batch 205: 0.285339/0.186783loss in batch 206: 0.0539703/0.186142loss in batch 207: 0.691193/0.188568loss in batch 208: 0.372711/0.189453loss in batch 209: 0.035675/0.188721loss in batch 210: 0.0489349/0.188065loss in batch 211: 1.0696/0.192215loss in batch 212: 0.196396/0.19223
done with epoch 8
train_acc: 0.948357 (404/426)
test loss: 0.196396
acc: 0.944056 (135/143)
loss in batch 0: 0.0690918/0.0690918loss in batch 1: 0.0421295/0.055603loss in batch 2: 0.273743/0.128326loss in batch 3: 0.17804/0.140762loss in batch 4: 0.0452423/0.121643loss in batch 5: 0.0456085/0.108963loss in batch 6: 0.0370789/0.0987091loss in batch 7: 0.250961/0.117737loss in batch 8: 0.0752716/0.113022loss in batch 9: 0.010437/0.102768loss in batch 10: 0.147614/0.106842loss in batch 11: 0.0511169/0.102188loss in batch 12: 0.253494/0.113831loss in batch 13: 0.0183411/0.10701loss in batch 14: 0.237549/0.115707loss in batch 15: 0.109314/0.115311loss in batch 16: 0.615524/0.14473loss in batch 17: 0.0116577/0.137344loss in batch 18: 0.048111/0.13266loss in batch 19: 0.0177917/0.126907loss in batch 20: 0.31218/0.135727loss in batch 21: 0.453064/0.150146loss in batch 22: 0.0548401/0.146011loss in batch 23: 0.294418/0.152191loss in batch 24: 0.0558624/0.148346loss in batch 25: 0.311508/0.154602loss in batch 26: 0.122849/0.153442loss in batch 27: 0.134354/0.152756loss in batch 28: 0.136642/0.152206loss in batch 29: 0.281631/0.156525loss in batch 30: 0.341522/0.162476loss in batch 31: 0.0778351/0.159836loss in batch 32: 0.194214/0.160873loss in batch 33: 0.275192/0.164246loss in batch 34: 0.0880127/0.162064loss in batch 35: 0.0466766/0.158859loss in batch 36: 0.66713/0.172607loss in batch 37: 0.0824127/0.170227loss in batch 38: 0.130554/0.169205loss in batch 39: 0.0282593/0.16568loss in batch 40: 0.186951/0.166199loss in batch 41: 0.241898/0.168015loss in batch 42: 0.129364/0.167114loss in batch 43: 0.139282/0.166473loss in batch 44: 0.0969238/0.164932loss in batch 45: 0.182083/0.165298loss in batch 46: 0.129135/0.164536loss in batch 47: 0.00646973/0.16124loss in batch 48: 0.0251617/0.158463loss in batch 49: 0.0566559/0.156433loss in batch 50: 0.241302/0.158081loss in batch 51: 0.337723/0.161545loss in batch 52: 0.167496/0.161667loss in batch 53: 0.0929871/0.16037loss in batch 54: 0.019989/0.157837loss in batch 55: 0.0884857/0.156601loss in batch 56: 0.0899048/0.155426loss in batch 57: 0.0491791/0.15358loss in batch 58: 0.10083/0.152695loss in batch 59: 0.647568/0.16095loss in batch 60: 0.534363/0.167068loss in batch 61: 0.0650787/0.165421loss in batch 62: 0.183212/0.16571loss in batch 63: 0.0453491/0.163818loss in batch 64: 0.113846/0.163055loss in batch 65: 0.457184/0.167511loss in batch 66: 0.699982/0.175461loss in batch 67: 0.0401459/0.173462loss in batch 68: 0.190216/0.173721loss in batch 69: 0.054245/0.172012loss in batch 70: 0.0476685/0.170258loss in batch 71: 0.317169/0.172302loss in batch 72: 0.013504/0.17012loss in batch 73: 0.156174/0.169937loss in batch 74: 0.159714/0.169785loss in batch 75: 0.173813/0.169846loss in batch 76: 0.334503/0.171997loss in batch 77: 0.0869904/0.170898loss in batch 78: 0.194473/0.171204loss in batch 79: 0.121368/0.170578loss in batch 80: 0.0944977/0.169632loss in batch 81: 0.0182037/0.167786loss in batch 82: 0.033844/0.166168loss in batch 83: 0.0706177/0.165039loss in batch 84: 0.304932/0.166672loss in batch 85: 0.059906/0.165436loss in batch 86: 0.456253/0.168777loss in batch 87: 0.103149/0.168045loss in batch 88: 0.568909/0.172546loss in batch 89: 0.107117/0.171814loss in batch 90: 0.218689/0.172333loss in batch 91: 0.983658/0.181152loss in batch 92: 0.187607/0.181229loss in batch 93: 0.260986/0.182053loss in batch 94: 0.285126/0.183151loss in batch 95: 0.0374451/0.181641loss in batch 96: 0.369614/0.183563loss in batch 97: 0.140945/0.183136loss in batch 98: 0.356903/0.184891loss in batch 99: 0.276703/0.185806loss in batch 100: 0.0809631/0.184769loss in batch 101: 0.471893/0.187592loss in batch 102: 0.135406/0.187073loss in batch 103: 0.0702209/0.185959loss in batch 104: 0.395996/0.187958loss in batch 105: 0.355927/0.189529loss in batch 106: 0.205826/0.189697loss in batch 107: 0.0527039/0.188431loss in batch 108: 0.0527954/0.18718loss in batch 109: 0.151443/0.186859loss in batch 110: 0.170837/0.186722loss in batch 111: 0.145386/0.18634loss in batch 112: 0.0591583/0.185211loss in batch 113: 1.33049/0.195251loss in batch 114: 0.057663/0.194077loss in batch 115: 0.0587311/0.192902loss in batch 116: 0.0984344/0.192093loss in batch 117: 0.389755/0.193771loss in batch 118: 0.291138/0.194595loss in batch 119: 0.0526886/0.193405loss in batch 120: 0.0823364/0.192474loss in batch 121: 0.246658/0.192917loss in batch 122: 0.775375/0.197662loss in batch 123: 0.468521/0.199844loss in batch 124: 0.0986786/0.199036loss in batch 125: 0.0273438/0.197678loss in batch 126: 0.0740051/0.196701loss in batch 127: 0.0665131/0.195679loss in batch 128: 0.328964/0.196716loss in batch 129: 0.0860596/0.195862loss in batch 130: 0.0619049/0.194839loss in batch 131: 0.043045/0.193695loss in batch 132: 0.153122/0.19339loss in batch 133: 0.730453/0.197403loss in batch 134: 0.287277/0.198059loss in batch 135: 0.191177/0.198013loss in batch 136: 0.158493/0.197723loss in batch 137: 0.0438843/0.196609loss in batch 138: 0.15004/0.196274loss in batch 139: 0.0623627/0.195313loss in batch 140: 0.55632/0.197876loss in batch 141: 0.0456238/0.196808loss in batch 142: 0.214661/0.19693loss in batch 143: 0.188843/0.196869loss in batch 144: 0.366608/0.198044loss in batch 145: 0.0184784/0.196808loss in batch 146: 0.0286865/0.195679loss in batch 147: 0.148346/0.195343loss in batch 148: 0.171097/0.195175loss in batch 149: 0.0961761/0.194519loss in batch 150: 0.0307465/0.193436loss in batch 151: 0.0355072/0.192413loss in batch 152: 0.0324097/0.19136loss in batch 153: 0.0335693/0.190323loss in batch 154: 0.292618/0.190994loss in batch 155: 0.506424/0.193008loss in batch 156: 0.103439/0.192444loss in batch 157: 0.0299225/0.191422loss in batch 158: 0.0791016/0.190704loss in batch 159: 0.128723/0.190323loss in batch 160: 0.272415/0.190842loss in batch 161: 0.440933/0.192383loss in batch 162: 0.20253/0.192444loss in batch 163: 0.270645/0.192917loss in batch 164: 0.0414734/0.192001loss in batch 165: 0.0449066/0.191116loss in batch 166: 0.0700073/0.190399loss in batch 167: 0.307144/0.191086loss in batch 168: 0.0144806/0.190033loss in batch 169: 0.192108/0.190048loss in batch 170: 0.0798492/0.189407loss in batch 171: 0.0841675/0.188797loss in batch 172: 0.0477753/0.187988loss in batch 173: 0.0981903/0.187469loss in batch 174: 0.0380859/0.186615loss in batch 175: 0.00427246/0.185577loss in batch 176: 0.0514526/0.184814loss in batch 177: 0.118698/0.184448loss in batch 178: 0.434845/0.185837loss in batch 179: 0.0306549/0.184982loss in batch 180: 0.365112/0.185974loss in batch 181: 0.084137/0.18541loss in batch 182: 0.192215/0.185455loss in batch 183: 0.0402832/0.184662loss in batch 184: 0.100433/0.184204loss in batch 185: 0.0886841/0.183701loss in batch 186: 1.15315/0.188873loss in batch 187: 0.231201/0.189102loss in batch 188: 0.171036/0.189011loss in batch 189: 0.0422211/0.188232loss in batch 190: 0.324554/0.18895loss in batch 191: 0.23909/0.189209loss in batch 192: 0.0208282/0.188324loss in batch 193: 0.100296/0.187897loss in batch 194: 0.0349579/0.187103loss in batch 195: 0.107086/0.186691loss in batch 196: 0.12056/0.186356loss in batch 197: 0.245178/0.186646loss in batch 198: 0.0799866/0.186111loss in batch 199: 0.6492/0.188446loss in batch 200: 0.133163/0.188156loss in batch 201: 0.451645/0.189468loss in batch 202: 0.0971527/0.189011loss in batch 203: 0.375412/0.189926loss in batch 204: 0.342987/0.190674loss in batch 205: 0.104935/0.190247loss in batch 206: 0.0256653/0.189453loss in batch 207: 0.103333/0.189041loss in batch 208: 0.065033/0.188446loss in batch 209: 0.0909882/0.187988loss in batch 210: 0.0847168/0.1875loss in batch 211: 0.0511475/0.186844loss in batch 212: 0.047348/0.186203
done with epoch 9
train_acc: 0.941315 (401/426)
test loss: 0.047348
acc: 0.951049 (136/143)
loss in batch 0: 0.00358582/0.00358582loss in batch 1: 0.359131/0.181351loss in batch 2: 0.109879/0.157532loss in batch 3: 0.177689/0.162582loss in batch 4: 0.398666/0.209793loss in batch 5: 0.197937/0.207825loss in batch 6: 0.231369/0.211166loss in batch 7: 1.00677/0.310623loss in batch 8: 0.0819244/0.285217loss in batch 9: 0.0700684/0.263702loss in batch 10: 0.148819/0.25325loss in batch 11: 0.384598/0.264206loss in batch 12: 0.0537262/0.248016loss in batch 13: 1.24283/0.319077loss in batch 14: 0.132065/0.306595loss in batch 15: 0.601517/0.325027loss in batch 16: 0.0681/0.309921loss in batch 17: 0.118423/0.299286loss in batch 18: 0.338394/0.301346loss in batch 19: 0.0507202/0.288818loss in batch 20: 0.24353/0.286667loss in batch 21: 0.186264/0.282089loss in batch 22: 0.384354/0.286545loss in batch 23: 0.40239/0.291367loss in batch 24: 0.369263/0.294479loss in batch 25: 0.207657/0.291153loss in batch 26: 0.0235748/0.281235loss in batch 27: 0.0857391/0.274246loss in batch 28: 0.087677/0.267822loss in batch 29: 0.366272/0.271088loss in batch 30: 0.0610962/0.264328loss in batch 31: 0.105255/0.259354loss in batch 32: 0.134262/0.255569loss in batch 33: 0.295303/0.256729loss in batch 34: 0.09021/0.251968loss in batch 35: 0.231232/0.251404loss in batch 36: 0.141708/0.248428loss in batch 37: 0.330322/0.25058loss in batch 38: 0.926941/0.267929loss in batch 39: 0.0645142/0.262848loss in batch 40: 0.153534/0.260178loss in batch 41: 0.000610352/0.253998loss in batch 42: 0.276688/0.254532loss in batch 43: 0.0949707/0.2509loss in batch 44: 0.231857/0.250473loss in batch 45: 0.203308/0.249451loss in batch 46: 0.0845184/0.245941loss in batch 47: 0.0293579/0.241425loss in batch 48: 0.639801/0.249557loss in batch 49: 0.163864/0.247849loss in batch 50: 0.0290527/0.243546loss in batch 51: 0.363831/0.245865loss in batch 52: 0.0840912/0.242813loss in batch 53: 0.282654/0.243546loss in batch 54: 0.0680542/0.240356loss in batch 55: 0.13533/0.238495loss in batch 56: 0.121918/0.236435loss in batch 57: 0.0727386/0.233627loss in batch 58: 0.0297546/0.230164loss in batch 59: 0.0791626/0.227646loss in batch 60: 0.11644/0.22583loss in batch 61: 0.00675964/0.22229loss in batch 62: 0.0590668/0.219711loss in batch 63: 0.0662689/0.2173loss in batch 64: 0.108948/0.215622loss in batch 65: 0.148178/0.214615loss in batch 66: 0.0735931/0.212509loss in batch 67: 0.0389404/0.209961loss in batch 68: 0.269684/0.210831loss in batch 69: 0.258331/0.211502loss in batch 70: 0.0513306/0.209244loss in batch 71: 0.0594788/0.207153loss in batch 72: 0.098587/0.205673loss in batch 73: 0.43956/0.208847loss in batch 74: 0.0889435/0.207245loss in batch 75: 0.0966187/0.20578loss in batch 76: 0.563782/0.210434loss in batch 77: 0.0387421/0.208221loss in batch 78: 0.173035/0.207779loss in batch 79: 0.00308228/0.205231loss in batch 80: 0.0696869/0.203552loss in batch 81: 0.292053/0.204636loss in batch 82: 0.106873/0.203461loss in batch 83: 0.0685272/0.201843loss in batch 84: 0.047699/0.200027loss in batch 85: 0.100052/0.198883loss in batch 86: 0.0405731/0.197052loss in batch 87: 0.063446/0.195541loss in batch 88: 0.0250854/0.193619loss in batch 89: 0.0632782/0.192169loss in batch 90: 0.529053/0.195877loss in batch 91: 0.0684357/0.194489loss in batch 92: 0.0668793/0.193115loss in batch 93: 0.495605/0.196335loss in batch 94: 0.971344/0.204498loss in batch 95: 0.0905609/0.203308loss in batch 96: 0.0594788/0.201813loss in batch 97: 0.0327301/0.200089loss in batch 98: 0.154068/0.199631loss in batch 99: 0.0319672/0.197952loss in batch 100: 0.0260468/0.196243loss in batch 101: 0.0214691/0.194534loss in batch 102: 0.253067/0.195099loss in batch 103: 0.103729/0.194229loss in batch 104: 0.0834656/0.193176loss in batch 105: 0.14357/0.192703loss in batch 106: 0.579407/0.19632loss in batch 107: 0.395508/0.198166loss in batch 108: 0.0886841/0.197159loss in batch 109: 0.0767517/0.196075loss in batch 110: 0.482483/0.198639loss in batch 111: 0.0131989/0.196991loss in batch 112: 0.0239105/0.19545loss in batch 113: 0.163345/0.195175loss in batch 114: 0.0345001/0.193787loss in batch 115: 0.129166/0.193222loss in batch 116: 0.10701/0.19249loss in batch 117: 0.110229/0.191788loss in batch 118: 0.0514221/0.190598loss in batch 119: 0.168137/0.19043loss in batch 120: 0.147461/0.190063loss in batch 121: 0.358246/0.191437loss in batch 122: 0.0137939/0.190002loss in batch 123: 0.136871/0.189575loss in batch 124: 0.0266418/0.188278loss in batch 125: 0.0874176/0.187469loss in batch 126: 0.0264587/0.186203loss in batch 127: 0.113907/0.185638loss in batch 128: 0.246384/0.186111loss in batch 129: 0.0692139/0.185211loss in batch 130: 0.0919189/0.184509loss in batch 131: 0.463745/0.186615loss in batch 132: 0.026535/0.18541loss in batch 133: 0.0117493/0.184113loss in batch 134: 0.26152/0.184677loss in batch 135: 0.156418/0.184479loss in batch 136: 0.060791/0.183578loss in batch 137: 0.0467529/0.182587loss in batch 138: 0.0541382/0.181656loss in batch 139: 0.0604706/0.180801loss in batch 140: 0.274078/0.181458loss in batch 141: 0.155197/0.181259loss in batch 142: 0.425049/0.182983loss in batch 143: 0.237183/0.18335loss in batch 144: 0.701309/0.18692loss in batch 145: 0.043457/0.185944loss in batch 146: 0.0354614/0.184906loss in batch 147: 0.0995789/0.184341loss in batch 148: 0.0897675/0.183701loss in batch 149: 0.236908/0.184067loss in batch 150: 0.411118/0.185562loss in batch 151: 0.0502014/0.184662loss in batch 152: 0.106796/0.184174loss in batch 153: 0.0413513/0.183243loss in batch 154: 0.0540924/0.182404loss in batch 155: 0.193527/0.18248loss in batch 156: 0.570633/0.184937loss in batch 157: 0.125778/0.18457loss in batch 158: 0.0434875/0.183685loss in batch 159: 0.151489/0.183487loss in batch 160: 0.0794983/0.182846loss in batch 161: 0.140671/0.182571loss in batch 162: 0.319565/0.183426loss in batch 163: 0.0220337/0.182434loss in batch 164: 0.0739594/0.181778loss in batch 165: 0.101044/0.18129loss in batch 166: 0.0665894/0.180603loss in batch 167: 0.224518/0.180862loss in batch 168: 0.172302/0.180817loss in batch 169: 0.392685/0.182068loss in batch 170: 0.0115204/0.181061loss in batch 171: 0.477463/0.182785loss in batch 172: 0.0349274/0.181931loss in batch 173: 0.0424652/0.181122loss in batch 174: 0.0762024/0.180542loss in batch 175: 0.0623627/0.179871loss in batch 176: 0.137741/0.179611loss in batch 177: 0.568481/0.181808loss in batch 178: 0.0101624/0.180847loss in batch 179: 0.311493/0.18158loss in batch 180: 0.12825/0.18129loss in batch 181: 0.0882568/0.180771loss in batch 182: 0.0167084/0.179871loss in batch 183: 0.334335/0.18071loss in batch 184: 0.0713959/0.180115loss in batch 185: 0.0516205/0.179428loss in batch 186: 0.0435791/0.178696loss in batch 187: 0.0549316/0.17804loss in batch 188: 0.0162506/0.177185loss in batch 189: 0.0864105/0.176712loss in batch 190: 0.288361/0.177292loss in batch 191: 0.0113373/0.176422loss in batch 192: 0.340515/0.177277loss in batch 193: 0.38353/0.178345loss in batch 194: 0.171906/0.178314loss in batch 195: 0.0851288/0.177841loss in batch 196: 0.201904/0.177948loss in batch 197: 0.329208/0.178726loss in batch 198: 0.280167/0.17923loss in batch 199: 0.061142/0.178635loss in batch 200: 0.295074/0.17923loss in batch 201: 0.0332336/0.178497loss in batch 202: 0.0274048/0.17775loss in batch 203: 0.056015/0.177155loss in batch 204: 0.188873/0.177216loss in batch 205: 0.0475006/0.17659loss in batch 206: 0.21489/0.176773loss in batch 207: 0.259109/0.17717loss in batch 208: 0.0478058/0.176544loss in batch 209: 0.072052/0.176056loss in batch 210: 0.0667114/0.175522loss in batch 211: 0.728455/0.178146loss in batch 212: 0.33754/0.178894
done with epoch 10
train_acc: 0.941315 (401/426)
test loss: 0.33754
acc: 0.944056 (135/143)
loss in batch 0: 0.269424/0.269424loss in batch 1: 0.451431/0.360428loss in batch 2: 0.0601959/0.260345loss in batch 3: 0.21759/0.249664loss in batch 4: 0.139084/0.227539loss in batch 5: 0.548035/0.28096loss in batch 6: 0.128891/0.259247loss in batch 7: 0.0379181/0.231567loss in batch 8: 0.177841/0.225601loss in batch 9: 0.0912628/0.212158loss in batch 10: 0.0361481/0.196167loss in batch 11: 0.508774/0.222214loss in batch 12: 0.103348/0.213074loss in batch 13: 0.104126/0.205292loss in batch 14: 0.039856/0.19426loss in batch 15: 0.0439758/0.184875loss in batch 16: 0.00669861/0.174393loss in batch 17: 0.0356903/0.166687loss in batch 18: 0.557388/0.187241loss in batch 19: 0.0126495/0.178513loss in batch 20: 0.0579224/0.172775loss in batch 21: 0.0557404/0.16745loss in batch 22: 0.0463409/0.162186loss in batch 23: 0.015976/0.156082loss in batch 24: 0.1353/0.155273loss in batch 25: 0.0553894/0.151413loss in batch 26: 0.129089/0.150589loss in batch 27: 0.144562/0.150375loss in batch 28: 0.0246582/0.146057loss in batch 29: 0.227417/0.148758loss in batch 30: 0.146866/0.148697loss in batch 31: 0.0556946/0.145798loss in batch 32: 0.484222/0.156052loss in batch 33: 0.0926666/0.15419loss in batch 34: 0.02771/0.150558loss in batch 35: 0.0147095/0.14679loss in batch 36: 0.00834656/0.143051loss in batch 37: 0.0631866/0.140945loss in batch 38: 0.039856/0.138351loss in batch 39: 0.0727539/0.136719loss in batch 40: 0.1409/0.13681loss in batch 41: 0.0138702/0.133896loss in batch 42: 0.072113/0.132462loss in batch 43: 0.0678711/0.130997loss in batch 44: 0.439621/0.137848loss in batch 45: 0.162323/0.138367loss in batch 46: 0.00727844/0.13559loss in batch 47: 0.0351563/0.133499loss in batch 48: 0.0978241/0.132767loss in batch 49: 0.562515/0.141357loss in batch 50: 0.0365601/0.139313loss in batch 51: 0.51033/0.146439loss in batch 52: 0.0172882/0.144012loss in batch 53: 0.0676422/0.142593loss in batch 54: 0.23642/0.144287loss in batch 55: 0.127029/0.143982loss in batch 56: 0.0197754/0.141815loss in batch 57: 0.0133057/0.139603loss in batch 58: 0.191635/0.140472loss in batch 59: 0.429871/0.145294loss in batch 60: 0.0435486/0.143631loss in batch 61: 0.0738373/0.142502loss in batch 62: 0.094223/0.141739loss in batch 63: 0.0712738/0.14064loss in batch 64: 0.112366/0.140198loss in batch 65: 0.215057/0.141342loss in batch 66: 0.0124664/0.139404loss in batch 67: 0.227173/0.140701loss in batch 68: 0.0271606/0.139069loss in batch 69: 0.0452881/0.137711loss in batch 70: 0.0511017/0.13649loss in batch 71: 0.316574/0.138992loss in batch 72: 0.22612/0.140198loss in batch 73: 0.0655823/0.139175loss in batch 74: 0.0729218/0.13829loss in batch 75: 0.858109/0.147781loss in batch 76: 0.0716858/0.146774loss in batch 77: 0.0951691/0.146118loss in batch 78: 0.380249/0.149094loss in batch 79: 0.396133/0.152176loss in batch 80: 0.185486/0.152588loss in batch 81: 0.237656/0.153625loss in batch 82: 0.165863/0.153778loss in batch 83: 0.242157/0.154816loss in batch 84: 0.0320282/0.153381loss in batch 85: 0.111267/0.152893loss in batch 86: 0.15625/0.152939loss in batch 87: 0.226074/0.153763loss in batch 88: 0.104904/0.153214loss in batch 89: 0.178604/0.153488loss in batch 90: 0.0703125/0.152573loss in batch 91: 0.131363/0.152344loss in batch 92: 0.0227966/0.150955loss in batch 93: 0.221329/0.151703loss in batch 94: 0.0575562/0.150711loss in batch 95: 0.0564575/0.149734loss in batch 96: 0.53064/0.153656loss in batch 97: 0.0913086/0.153015loss in batch 98: 0.0511932/0.151993loss in batch 99: 0.266449/0.153152loss in batch 100: 0.0681915/0.152298loss in batch 101: 0.110199/0.151886loss in batch 102: 0.0150146/0.150558loss in batch 103: 0.0456238/0.149536loss in batch 104: 0.176071/0.149796loss in batch 105: 0.0136719/0.148514loss in batch 106: 0.519211/0.151978loss in batch 107: 0.0280914/0.150833loss in batch 108: 0.0214539/0.149643loss in batch 109: 0.0319366/0.148575loss in batch 110: 0.312225/0.150055loss in batch 111: 0.0430145/0.149094loss in batch 112: 0.0406494/0.148132loss in batch 113: 0.0196381/0.147018loss in batch 114: 0.056839/0.146225loss in batch 115: 0.137131/0.146133loss in batch 116: 0.0223083/0.145081loss in batch 117: 0.245804/0.14595loss in batch 118: 0.165833/0.146118loss in batch 119: 0.0229492/0.145081loss in batch 120: 0.0376434/0.144196loss in batch 121: 0.0737/0.143616loss in batch 122: 0.132462/0.143524loss in batch 123: 0.121063/0.143341loss in batch 124: 0.25293/0.144211loss in batch 125: 0.0957336/0.143829loss in batch 126: 0.20993/0.144363loss in batch 127: 0.00578308/0.143265loss in batch 128: 0.16951/0.143478loss in batch 129: 0.290436/0.144608loss in batch 130: 0.0489807/0.143875loss in batch 131: 0.517914/0.146698loss in batch 132: 0.0762024/0.146179loss in batch 133: 0.0808411/0.145691loss in batch 134: 0.0430298/0.144928loss in batch 135: 0.0812988/0.144455loss in batch 136: 0.0573425/0.143829loss in batch 137: 0.541122/0.146698loss in batch 138: 0.149994/0.146729loss in batch 139: 0.0588531/0.146103loss in batch 140: 1.04982/0.152512loss in batch 141: 0.233322/0.153076loss in batch 142: 0.260422/0.153824loss in batch 143: 0.65303/0.157303loss in batch 144: 0.157928/0.157303loss in batch 145: 0.327774/0.158463loss in batch 146: 0.506546/0.160828loss in batch 147: 0.0669556/0.160202loss in batch 148: 0.0749969/0.159637loss in batch 149: 0.169388/0.159698loss in batch 150: 0.0670166/0.159073loss in batch 151: 0.17807/0.15921loss in batch 152: 0.0979004/0.158798loss in batch 153: 0.37233/0.160202loss in batch 154: 0.212692/0.160522loss in batch 155: 1.49715/0.169098loss in batch 156: 0.0649414/0.168427loss in batch 157: 0.240738/0.1689loss in batch 158: 0.0298462/0.16803loss in batch 159: 0.0330963/0.167175loss in batch 160: 0.055954/0.166473loss in batch 161: 0.464355/0.168335loss in batch 162: 0.122543/0.168045loss in batch 163: 0.142731/0.167892loss in batch 164: 0.0836639/0.167374loss in batch 165: 0.103806/0.166992loss in batch 166: 0.0851135/0.166504loss in batch 167: 0.291229/0.167252loss in batch 168: 0.0300903/0.166428loss in batch 169: 0.0299683/0.165634loss in batch 170: 0.0418701/0.164902loss in batch 171: 0.350342/0.166loss in batch 172: 0.0916138/0.165558loss in batch 173: 0.0791321/0.165054loss in batch 174: 0.0880585/0.164627loss in batch 175: 0.281677/0.165298loss in batch 176: 0.551132/0.167465loss in batch 177: 0.167557/0.167465loss in batch 178: 0.0206146/0.166656loss in batch 179: 0.0585632/0.166046loss in batch 180: 0.0288086/0.165283loss in batch 181: 0.0504456/0.164658loss in batch 182: 1.24329/0.170563loss in batch 183: 0.0885162/0.170105loss in batch 184: 0.441284/0.17157loss in batch 185: 0.308884/0.172302loss in batch 186: 0.062851/0.171722loss in batch 187: 0.398438/0.172928loss in batch 188: 0.028244/0.172165loss in batch 189: 0.106964/0.171814loss in batch 190: 0.0213013/0.171036loss in batch 191: 0.0416718/0.170364loss in batch 192: 0.099472/0.169983loss in batch 193: 0.281387/0.170563loss in batch 194: 0.0714569/0.170059loss in batch 195: 0.0834656/0.169617loss in batch 196: 0.0057373/0.168777loss in batch 197: 0.10173/0.168442loss in batch 198: 0.470718/0.169968loss in batch 199: 0.354431/0.170883loss in batch 200: 1.01149/0.175079loss in batch 201: 0.305847/0.175705loss in batch 202: 0.105728/0.175369loss in batch 203: 0.138947/0.175186loss in batch 204: 0.569397/0.177124loss in batch 205: 0.239944/0.177429loss in batch 206: 0.0111237/0.17662loss in batch 207: 0.300568/0.177216loss in batch 208: 0.178391/0.177216loss in batch 209: 0.0344086/0.176544loss in batch 210: 0.207687/0.176682loss in batch 211: 0.00624084/0.175888loss in batch 212: 0.243912/0.176193
done with epoch 11
train_acc: 0.93662 (399/426)
test loss: 0.243912
acc: 0.944056 (135/143)
loss in batch 0: 0.0522308/0.0522308loss in batch 1: 0.0349426/0.0435791loss in batch 2: 0.220947/0.102707loss in batch 3: 0.062912/0.0927582loss in batch 4: 0.240433/0.122284loss in batch 5: 0.0764923/0.114655loss in batch 6: 0.174286/0.123169loss in batch 7: 1.32768/0.273743loss in batch 8: 1.07022/0.362244loss in batch 9: 0.0844421/0.334457loss in batch 10: 0.013443/0.305283loss in batch 11: 0.141953/0.291656loss in batch 12: 0.123886/0.278763loss in batch 13: 0.125412/0.267807loss in batch 14: 0.138947/0.259216loss in batch 15: 0.110062/0.249893loss in batch 16: 0.170547/0.245224loss in batch 17: 0.0424652/0.233963loss in batch 18: 0.116425/0.227768loss in batch 19: 0.0994873/0.221375loss in batch 20: 0.321381/0.226135loss in batch 21: 0.429932/0.235397loss in batch 22: 0.0548401/0.227539loss in batch 23: 0.0500946/0.220154loss in batch 24: 0.0387268/0.212875loss in batch 25: 0.303619/0.21637loss in batch 26: 0.0343933/0.209625loss in batch 27: 0.189636/0.208923loss in batch 28: 0.148865/0.206848loss in batch 29: 0.146423/0.204834loss in batch 30: 0.800705/0.224045loss in batch 31: 0.0502319/0.218628loss in batch 32: 0.0328522/0.212997loss in batch 33: 0.178421/0.21199loss in batch 34: 0.0565948/0.207535loss in batch 35: 0.00721741/0.201965loss in batch 36: 0.00787354/0.196732loss in batch 37: 0.18399/0.196396loss in batch 38: 0.114288/0.19429loss in batch 39: 0.284302/0.196533loss in batch 40: 0.288361/0.198792loss in batch 41: 0.00509644/0.194168loss in batch 42: 0.0197601/0.190109loss in batch 43: 0.0378723/0.186661loss in batch 44: 0.46965/0.192932loss in batch 45: 0.39888/0.197418loss in batch 46: 0.0536346/0.194351loss in batch 47: 0.0744934/0.191864loss in batch 48: 0.0840759/0.189651loss in batch 49: 0.447952/0.194824loss in batch 50: 0.0514832/0.192017loss in batch 51: 0.259613/0.193314loss in batch 52: 0.124832/0.192017loss in batch 53: 0.14357/0.191116loss in batch 54: 0.138733/0.19017loss in batch 55: 0.154648/0.189545loss in batch 56: 0.0680542/0.187408loss in batch 57: 0.0810852/0.185577loss in batch 58: 0.141479/0.184814loss in batch 59: 0.122437/0.183792loss in batch 60: 0.0990906/0.182404loss in batch 61: 0.0604706/0.180435loss in batch 62: 0.170929/0.180283loss in batch 63: 0.254761/0.181458loss in batch 64: 0.0168457/0.178909loss in batch 65: 0.25/0.179993loss in batch 66: 0.44632/0.183975loss in batch 67: 0.0696411/0.182281loss in batch 68: 0.122223/0.181412loss in batch 69: 0.114288/0.18045loss in batch 70: 0.0686646/0.178894loss in batch 71: 0.0298004/0.176819loss in batch 72: 0.0335083/0.17485loss in batch 73: 0.0426483/0.173065loss in batch 74: 0.414291/0.176285loss in batch 75: 0.0523682/0.174637loss in batch 76: 0.209763/0.17511loss in batch 77: 0.063858/0.173676loss in batch 78: 0.0405579/0.171997loss in batch 79: 0.0767365/0.170807loss in batch 80: 0.0628662/0.169464loss in batch 81: 0.0283203/0.16774loss in batch 82: 0.0358124/0.166153loss in batch 83: 0.0172119/0.164383loss in batch 84: 0.107361/0.163712loss in batch 85: 0.217194/0.164337loss in batch 86: 0.0210876/0.162689loss in batch 87: 0.339264/0.164703loss in batch 88: 0.446014/0.167847loss in batch 89: 0.167984/0.167862loss in batch 90: 0.0122986/0.166153loss in batch 91: 0.0584412/0.164978loss in batch 92: 0.103638/0.164322loss in batch 93: 0.0865021/0.163498loss in batch 94: 0.110016/0.162918loss in batch 95: 0.0189667/0.161423loss in batch 96: 0.137421/0.161179loss in batch 97: 0.0128937/0.159653loss in batch 98: 0.251007/0.160599loss in batch 99: 0.0212555/0.159195loss in batch 100: 0.0429382/0.158051loss in batch 101: 0.0498352/0.156982loss in batch 102: 0.0262299/0.155716loss in batch 103: 0.229492/0.156433loss in batch 104: 0.0588226/0.155487loss in batch 105: 0.13475/0.155289loss in batch 106: 0.0228577/0.154068loss in batch 107: 0.204224/0.154526loss in batch 108: 0.0703735/0.153748loss in batch 109: 0.00897217/0.152435loss in batch 110: 0.0682678/0.151672loss in batch 111: 0.489792/0.154694loss in batch 112: 0.0196075/0.153503loss in batch 113: 0.698975/0.158295loss in batch 114: 0.0630188/0.157455loss in batch 115: 0.0684204/0.156693loss in batch 116: 0.651993/0.160934loss in batch 117: 0.107513/0.160461loss in batch 118: 0.093338/0.159912loss in batch 119: 0.411377/0.162003loss in batch 120: 0.0870209/0.161377loss in batch 121: 0.00709534/0.160126loss in batch 122: 0.0189514/0.158966loss in batch 123: 0.292755/0.160049loss in batch 124: 0.0298615/0.159012loss in batch 125: 1.0661/0.166214loss in batch 126: 0.431564/0.168304loss in batch 127: 0.0493927/0.167358loss in batch 128: 0.0062561/0.166122loss in batch 129: 0.3638/0.167648loss in batch 130: 0.431351/0.169647loss in batch 131: 0.244888/0.170227loss in batch 132: 0.0639496/0.169418loss in batch 133: 0.0489807/0.168533loss in batch 134: 0.0100708/0.167343loss in batch 135: 0.0154114/0.166229loss in batch 136: 0.388351/0.167847loss in batch 137: 0.0526428/0.167023loss in batch 138: 0.172195/0.167053loss in batch 139: 0.529099/0.169647loss in batch 140: 0.00460815/0.168472loss in batch 141: 0.0903473/0.167923loss in batch 142: 0.0460205/0.167068loss in batch 143: 0.528488/0.169586loss in batch 144: 0.0165863/0.168533loss in batch 145: 0.0694427/0.167847loss in batch 146: 0.352097/0.169098loss in batch 147: 0.0346069/0.168182loss in batch 148: 0.464691/0.170181loss in batch 149: 0.0588989/0.169434loss in batch 150: 0.0706482/0.168777loss in batch 151: 0.286102/0.169556loss in batch 152: 0.0252228/0.168625loss in batch 153: 0.126663/0.168335loss in batch 154: 0.452423/0.170166loss in batch 155: 0.0478516/0.169388loss in batch 156: 0.135086/0.169159loss in batch 157: 0.0748901/0.168579loss in batch 158: 0.179993/0.168655loss in batch 159: 0.0740662/0.16806loss in batch 160: 0.094101/0.167603loss in batch 161: 0.153748/0.167511loss in batch 162: 0.0384827/0.166718loss in batch 163: 0.107941/0.166367loss in batch 164: 0.0337982/0.165558loss in batch 165: 0.0162354/0.164658loss in batch 166: 0.592834/0.167221loss in batch 167: 0.085907/0.166748loss in batch 168: 0.280304/0.167404loss in batch 169: 0.0179138/0.166519loss in batch 170: 0.0353241/0.165756loss in batch 171: 0.209518/0.166016loss in batch 172: 0.049942/0.165344loss in batch 173: 0.0236206/0.16452loss in batch 174: 0.310974/0.165359loss in batch 175: 0.375824/0.166565loss in batch 176: 0.0444946/0.165878loss in batch 177: 0.0710297/0.165344loss in batch 178: 0.118729/0.165085loss in batch 179: 0.424072/0.166519loss in batch 180: 0.417847/0.167908loss in batch 181: 0.0122986/0.167053loss in batch 182: 0.0862579/0.166611loss in batch 183: 0.0499115/0.16597loss in batch 184: 0.0978699/0.165619loss in batch 185: 0.433167/0.167053loss in batch 186: 0.928162/0.171127loss in batch 187: 0.0661621/0.170563loss in batch 188: 0.70076/0.17337loss in batch 189: 0.054306/0.172729loss in batch 190: 0.0157928/0.171921loss in batch 191: 0.0234222/0.171143loss in batch 192: 0.0233459/0.17038loss in batch 193: 0.0476685/0.169754loss in batch 194: 0.0319061/0.169037loss in batch 195: 0.0723267/0.168549loss in batch 196: 0.314392/0.169281loss in batch 197: 0.0611572/0.168732loss in batch 198: 0.0288391/0.168045loss in batch 199: 0.0715027/0.167557loss in batch 200: 0.138428/0.167419loss in batch 201: 0.290359/0.168015loss in batch 202: 0.394562/0.169144loss in batch 203: 0.0539246/0.168579loss in batch 204: 0.0670624/0.168076loss in batch 205: 0.109879/0.167801loss in batch 206: 0.349442/0.168671loss in batch 207: 0.502563/0.170273loss in batch 208: 0.402588/0.171387loss in batch 209: 0.0119324/0.170624loss in batch 210: 0.498199/0.17218loss in batch 211: 0.0459137/0.171585loss in batch 212: 0.102814/0.171265
done with epoch 12
train_acc: 0.941315 (401/426)
test loss: 0.102814
acc: 0.951049 (136/143)
loss in batch 0: 0.0252533/0.0252533loss in batch 1: 0.290375/0.157806loss in batch 2: 0.1035/0.139709loss in batch 3: 0.307861/0.181747loss in batch 4: 0.0315399/0.151703loss in batch 5: 0.0948639/0.142242loss in batch 6: 0.219162/0.153214loss in batch 7: 0.0364685/0.138626loss in batch 8: 0.463074/0.174667loss in batch 9: 0.0650635/0.163712loss in batch 10: 0.118652/0.159607loss in batch 11: 0.116302/0.156021loss in batch 12: 0.0365753/0.14682loss in batch 13: 0.156708/0.147522loss in batch 14: 0.173492/0.149261loss in batch 15: 0.053421/0.143265loss in batch 16: 0.0437469/0.137421loss in batch 17: 0.0566559/0.132919loss in batch 18: 0.0908356/0.130722loss in batch 19: 0.0545654/0.126907loss in batch 20: 0.373474/0.138641loss in batch 21: 0.069931/0.135529loss in batch 22: 0.145432/0.135956loss in batch 23: 0.173096/0.137497loss in batch 24: 0.086319/0.135452loss in batch 25: 0.0548553/0.132355loss in batch 26: 0.240936/0.136383loss in batch 27: 0.13063/0.136169loss in batch 28: 0.253433/0.140228loss in batch 29: 0.0250854/0.136368loss in batch 30: 0.020874/0.132645loss in batch 31: 0.113312/0.13205loss in batch 32: 0.12146/0.131729loss in batch 33: 0.0383911/0.128983loss in batch 34: 0.0374146/0.126358loss in batch 35: 0.215424/0.12883loss in batch 36: 0.11174/0.128372loss in batch 37: 0.168732/0.12944loss in batch 38: 0.0986786/0.128647loss in batch 39: 0.253006/0.13176loss in batch 40: 0.572769/0.142517loss in batch 41: 0.0276794/0.139771loss in batch 42: 0.226425/0.1418loss in batch 43: 0.272537/0.144775loss in batch 44: 0.0373077/0.14238loss in batch 45: 0.0356598/0.140045loss in batch 46: 0.0223694/0.137543loss in batch 47: 0.0355988/0.135422loss in batch 48: 0.0948944/0.134598loss in batch 49: 0.0357971/0.132629loss in batch 50: 0.0443573/0.13089loss in batch 51: 0.0915833/0.130142loss in batch 52: 0.0901489/0.129395loss in batch 53: 0.0516968/0.127945loss in batch 54: 0.0616608/0.12674loss in batch 55: 0.0148163/0.124741loss in batch 56: 0.770035/0.136063loss in batch 57: 0.0152283/0.133972loss in batch 58: 0.27533/0.136368loss in batch 59: 0.543945/0.143158loss in batch 60: 0.0800323/0.142136loss in batch 61: 0.135437/0.142029loss in batch 62: 0.0716095/0.1409loss in batch 63: 0.18335/0.141571loss in batch 64: 0.155975/0.141785loss in batch 65: 0.118774/0.141449loss in batch 66: 0.0461426/0.140015loss in batch 67: 0.11496/0.139648loss in batch 68: 0.0492706/0.138351loss in batch 69: 0.391861/0.141968loss in batch 70: 0.0212097/0.140259loss in batch 71: 0.0684052/0.139267loss in batch 72: 0.1492/0.139404loss in batch 73: 0.0557098/0.13826loss in batch 74: 0.363861/0.141281loss in batch 75: 0.0517578/0.140091loss in batch 76: 0.745544/0.147964loss in batch 77: 0.203598/0.148666loss in batch 78: 0.1082/0.148178loss in batch 79: 0.33403/0.150482loss in batch 80: 0.106888/0.149948loss in batch 81: 0.0364532/0.148575loss in batch 82: 0.0414886/0.147278loss in batch 83: 0.256226/0.148575loss in batch 84: 0.402679/0.151566loss in batch 85: 0.36795/0.154083loss in batch 86: 0.117233/0.153656loss in batch 87: 0.304352/0.155365loss in batch 88: 0.112198/0.154877loss in batch 89: 0.078537/0.154022loss in batch 90: 0.0757751/0.153168loss in batch 91: 0.354324/0.155365loss in batch 92: 0.328308/0.157227loss in batch 93: 0.0478821/0.156067loss in batch 94: 0.0212555/0.154648loss in batch 95: 0.00524902/0.153091loss in batch 96: 0.0146179/0.151657loss in batch 97: 0.0758514/0.150879loss in batch 98: 0.0581055/0.149948loss in batch 99: 0.0669403/0.149109loss in batch 100: 0.108994/0.148712loss in batch 101: 1.51851/0.16214loss in batch 102: 0.022049/0.160797loss in batch 103: 0.0967102/0.160172loss in batch 104: 0.128021/0.159866loss in batch 105: 0.103958/0.159332loss in batch 106: 0.655548/0.163986loss in batch 107: 0.0554047/0.162964loss in batch 108: 0.0178223/0.161636loss in batch 109: 0.86409/0.168015loss in batch 110: 0.0572662/0.167023loss in batch 111: 0.4245/0.169327loss in batch 112: 0.150269/0.169144loss in batch 113: 0.0895081/0.168457loss in batch 114: 0.0323486/0.167282loss in batch 115: 0.405289/0.169327loss in batch 116: 0.0326843/0.168167loss in batch 117: 0.440506/0.170471loss in batch 118: 0.0503387/0.169464loss in batch 119: 0.0493317/0.168457loss in batch 120: 0.158386/0.168381loss in batch 121: 0.0149841/0.167114loss in batch 122: 0.377014/0.168823loss in batch 123: 0.0889587/0.168182loss in batch 124: 0.0598602/0.167313loss in batch 125: 0.19194/0.167511loss in batch 126: 0.0391693/0.166504loss in batch 127: 0.163681/0.166473loss in batch 128: 0.0260315/0.16539loss in batch 129: 0.0442352/0.164459loss in batch 130: 0.0962524/0.16394loss in batch 131: 0.0137939/0.162781loss in batch 132: 1.13728/0.17012loss in batch 133: 0.271271/0.170883loss in batch 134: 0.0394287/0.169891loss in batch 135: 0.0318604/0.168884loss in batch 136: 0.0727539/0.168182loss in batch 137: 0.0196533/0.167114loss in batch 138: 0.0161743/0.166016loss in batch 139: 0.0519257/0.165207loss in batch 140: 0.0224457/0.164185loss in batch 141: 0.195221/0.164413loss in batch 142: 0.228317/0.164856loss in batch 143: 0.0275574/0.16391loss in batch 144: 0.445129/0.165848loss in batch 145: 0.616989/0.168945loss in batch 146: 0.0758057/0.168304loss in batch 147: 0.380066/0.169724loss in batch 148: 0.295807/0.170578loss in batch 149: 0.201935/0.170792loss in batch 150: 0.369385/0.172104loss in batch 151: 0.0825195/0.171509loss in batch 152: 0.106155/0.171097loss in batch 153: 0.165588/0.171051loss in batch 154: 0.256577/0.171616loss in batch 155: 0.266525/0.172211loss in batch 156: 0.275375/0.172882loss in batch 157: 0.000793457/0.171783loss in batch 158: 0.0387268/0.170944loss in batch 159: 0.0891876/0.170425loss in batch 160: 0.123138/0.170151loss in batch 161: 0.0456696/0.169373loss in batch 162: 0.0468903/0.168625loss in batch 163: 0.418518/0.170151loss in batch 164: 0.151031/0.170029loss in batch 165: 0.014267/0.169083loss in batch 166: 0.271576/0.169693loss in batch 167: 0.030777/0.168884loss in batch 168: 0.0188904/0.167984loss in batch 169: 0.0169983/0.167099loss in batch 170: 0.155701/0.167038loss in batch 171: 0.038147/0.16629loss in batch 172: 0.208374/0.166534loss in batch 173: 0.281097/0.167191loss in batch 174: 0.159775/0.167145loss in batch 175: 0.0920105/0.166718loss in batch 176: 0.232437/0.167099loss in batch 177: 0.0816193/0.166611loss in batch 178: 0.0813904/0.166138loss in batch 179: 0.352036/0.167175loss in batch 180: 0.156693/0.167099loss in batch 181: 0.869934/0.170975loss in batch 182: 0.0939178/0.170547loss in batch 183: 0.0323792/0.169785loss in batch 184: 0.190384/0.169907loss in batch 185: 0.0164337/0.169083loss in batch 186: 0.216537/0.169342loss in batch 187: 0.15657/0.169266loss in batch 188: 0.0361328/0.168564loss in batch 189: 0.252228/0.169006loss in batch 190: 0.0188751/0.168228loss in batch 191: 0.0755005/0.16774loss in batch 192: 0.140335/0.167587loss in batch 193: 0.103348/0.167252loss in batch 194: 0.125778/0.167053loss in batch 195: 0.0113068/0.16626loss in batch 196: 0.0435791/0.165634loss in batch 197: 0.0186768/0.164886loss in batch 198: 0.656006/0.167358loss in batch 199: 0.00486755/0.16655loss in batch 200: 0.162735/0.166534loss in batch 201: 0.0354614/0.165878loss in batch 202: 0.0346375/0.165237loss in batch 203: 0.25705/0.16568loss in batch 204: 0.45842/0.167114loss in batch 205: 0.219406/0.167374loss in batch 206: 0.528763/0.169113loss in batch 207: 0.351456/0.169983loss in batch 208: 0.0288239/0.169312loss in batch 209: 0.130539/0.169128loss in batch 210: 0.0402069/0.168518loss in batch 211: 0.0130768/0.167786loss in batch 212: 0.0378418/0.167175
done with epoch 13
train_acc: 0.943662 (402/426)
test loss: 0.0378418
acc: 0.944056 (135/143)
loss in batch 0: 0.108795/0.108795loss in batch 1: 0.0234985/0.0661469loss in batch 2: 0.0678253/0.0667114loss in batch 3: 0.0618744/0.0654907loss in batch 4: 0.112717/0.0749359loss in batch 5: 0.129608/0.0840454loss in batch 6: 0.0302277/0.076355loss in batch 7: 0.00473022/0.0674133loss in batch 8: 0.370438/0.101089loss in batch 9: 1.20308/0.211273loss in batch 10: 1.15936/0.29747loss in batch 11: 0.0442505/0.276367loss in batch 12: 0.0343018/0.257751loss in batch 13: 0.130569/0.248657loss in batch 14: 0.115753/0.239792loss in batch 15: 0.0551758/0.228271loss in batch 16: 0.588669/0.249466loss in batch 17: 0.328583/0.25386loss in batch 18: 0.128143/0.247238loss in batch 19: 0.0862732/0.239197loss in batch 20: 0.0338745/0.229416loss in batch 21: 0.00012207/0.218994loss in batch 22: 0.11499/0.214478loss in batch 23: 0.294693/0.217804loss in batch 24: 0.0302124/0.210312loss in batch 25: 0.0756226/0.205124loss in batch 26: 0.0260162/0.198486loss in batch 27: 1.35158/0.239685loss in batch 28: 0.136307/0.236115loss in batch 29: 0.219009/0.23555loss in batch 30: 0.0723419/0.23027loss in batch 31: 0.0405884/0.22435loss in batch 32: 0.452042/0.231247loss in batch 33: 0.145828/0.228729loss in batch 34: 0.0394287/0.223343loss in batch 35: 0.000259399/0.217133loss in batch 36: 0.109406/0.214218loss in batch 37: 0.428955/0.219864loss in batch 38: 0.688934/0.231903loss in batch 39: 0.590103/0.24086loss in batch 40: 0.119675/0.2379loss in batch 41: 0.501358/0.244171loss in batch 42: 0.0788422/0.240326loss in batch 43: 0.158295/0.238464loss in batch 44: 0.290192/0.239609loss in batch 45: 0.0307312/0.235077loss in batch 46: 0.403595/0.238647loss in batch 47: 0.168854/0.237213loss in batch 48: 0.322556/0.238937loss in batch 49: 0.113327/0.236435loss in batch 50: 0.0306244/0.232391loss in batch 51: 0.265518/0.233032loss in batch 52: 0.0290985/0.229187loss in batch 53: 0.0284271/0.225479loss in batch 54: 0.00775146/0.221512loss in batch 55: 0.145218/0.220154loss in batch 56: 0.0827637/0.217728loss in batch 57: 0.485214/0.222351loss in batch 58: 0.110474/0.220444loss in batch 59: 0.0820007/0.21814loss in batch 60: 0.0380249/0.215195loss in batch 61: 0.270828/0.21608loss in batch 62: 0.0903015/0.214081loss in batch 63: 0.052063/0.211563loss in batch 64: 0.120544/0.210159loss in batch 65: 0.194901/0.20993loss in batch 66: 0.0298004/0.207245loss in batch 67: 0.0756073/0.205307loss in batch 68: 0.227203/0.205612loss in batch 69: 0.0808716/0.203842loss in batch 70: 0.0163879/0.201202loss in batch 71: 0.109131/0.199921loss in batch 72: 0.180801/0.199661loss in batch 73: 0.124649/0.198639loss in batch 74: 0.402237/0.201355loss in batch 75: 0.259659/0.202133loss in batch 76: 0.0919647/0.200699loss in batch 77: 0.057373/0.198853loss in batch 78: 0.0609589/0.197113loss in batch 79: 0.0764771/0.195602loss in batch 80: 0.472763/0.19902loss in batch 81: 0.0332489/0.197006loss in batch 82: 0.0277557/0.194977loss in batch 83: 0.0241852/0.192932loss in batch 84: 0.0630951/0.191406loss in batch 85: 0.0925903/0.190262loss in batch 86: 0.00688171/0.188156loss in batch 87: 0.220749/0.188522loss in batch 88: 0.279449/0.189545loss in batch 89: 0.00616455/0.1875loss in batch 90: 1.16681/0.198257loss in batch 91: 0.145767/0.197693loss in batch 92: 0.14386/0.197128loss in batch 93: 0.295929/0.198181loss in batch 94: 0.0580292/0.196686loss in batch 95: 0.230759/0.197052loss in batch 96: 0.0839996/0.195877loss in batch 97: 0.163788/0.195557loss in batch 98: 0.0376129/0.193954loss in batch 99: 0.0143738/0.192169loss in batch 100: 0.00769043/0.190338loss in batch 101: 0.242554/0.190857loss in batch 102: 0.0306396/0.189285loss in batch 103: 0.023407/0.187698loss in batch 104: 0.155136/0.187393loss in batch 105: 0.0452576/0.18605loss in batch 106: 0.0330353/0.184616loss in batch 107: 0.162796/0.184418loss in batch 108: 0.0155334/0.182861loss in batch 109: 0.0239716/0.181427loss in batch 110: 0.0052948/0.179825loss in batch 111: 0.0331421/0.178528loss in batch 112: 0.129791/0.178085loss in batch 113: 0.0661926/0.177109loss in batch 114: 0.600418/0.180786loss in batch 115: 0.078949/0.179916loss in batch 116: 0.0451202/0.178772loss in batch 117: 0.0821075/0.177948loss in batch 118: 0.466736/0.180359loss in batch 119: 0.551895/0.183472loss in batch 120: 0.273483/0.184219loss in batch 121: 0.072937/0.183304loss in batch 122: 0.0296783/0.182053loss in batch 123: 0.0327759/0.180847loss in batch 124: 0.0499573/0.179794loss in batch 125: 0.251053/0.180359loss in batch 126: 0.2202/0.180679loss in batch 127: 0.0653381/0.179779loss in batch 128: 0.00852966/0.178452loss in batch 129: 0.0280457/0.177292loss in batch 130: 0.029007/0.176147loss in batch 131: 0.0400238/0.175125loss in batch 132: 0.103851/0.174591loss in batch 133: 0.0191803/0.173431loss in batch 134: 0.0435944/0.172485loss in batch 135: 0.0346069/0.171463loss in batch 136: 0.0559387/0.170624loss in batch 137: 0.223343/0.17099loss in batch 138: 0.206589/0.171249loss in batch 139: 0.296906/0.17215loss in batch 140: 0.0671387/0.171402loss in batch 141: 0.158676/0.171326loss in batch 142: 0.0569916/0.170517loss in batch 143: 0.0852814/0.169922loss in batch 144: 0.0225525/0.168915loss in batch 145: 0.0728149/0.168243loss in batch 146: 0.0856476/0.167679loss in batch 147: 0.3311/0.168793loss in batch 148: 0.049057/0.167984loss in batch 149: 0.133011/0.167755loss in batch 150: 0.0516815/0.166977loss in batch 151: 0.0463562/0.166199loss in batch 152: 0.0520477/0.165451loss in batch 153: 0.9832/0.170761loss in batch 154: 0.293213/0.171555loss in batch 155: 0.0741425/0.170929loss in batch 156: 0.116806/0.170578loss in batch 157: 0.0980682/0.17012loss in batch 158: 0.0415649/0.169312loss in batch 159: 0.0190277/0.168365loss in batch 160: 0.0466003/0.167618loss in batch 161: 0.0667572/0.166992loss in batch 162: 0.0462341/0.16626loss in batch 163: 0.257553/0.166809loss in batch 164: 0.189316/0.166946loss in batch 165: 0.0324707/0.166138loss in batch 166: 0.0425415/0.16539loss in batch 167: 0.185608/0.165527loss in batch 168: 0.0860443/0.165039loss in batch 169: 0.0110168/0.164139loss in batch 170: 0.413864/0.165604loss in batch 171: 0.0639801/0.165009loss in batch 172: 0.573074/0.167374loss in batch 173: 0.0858154/0.166901loss in batch 174: 0.328598/0.167816loss in batch 175: 0.015625/0.166962loss in batch 176: 0.0386963/0.166245loss in batch 177: 0.0962677/0.165833loss in batch 178: 0.0149994/0.165009loss in batch 179: 0.200195/0.165192loss in batch 180: 0.14003/0.165054loss in batch 181: 0.734894/0.168182loss in batch 182: 0.0942688/0.167786loss in batch 183: 0.526001/0.169724loss in batch 184: 0.085907/0.169281loss in batch 185: 0.0683289/0.168732loss in batch 186: 0.067688/0.168182loss in batch 187: 0.00700378/0.167343loss in batch 188: 0.0700073/0.166824loss in batch 189: 0.200378/0.166992loss in batch 190: 0.708038/0.16983loss in batch 191: 0.130081/0.169617loss in batch 192: 0.0426636/0.168961loss in batch 193: 0.18927/0.169067loss in batch 194: 0.0296326/0.16835loss in batch 195: 0.121109/0.168106loss in batch 196: 0.344025/0.169006loss in batch 197: 0.04039/0.16835loss in batch 198: 0.23848/0.168701loss in batch 199: 0.0748138/0.168243loss in batch 200: 0.0606689/0.167709loss in batch 201: 0.119858/0.167465loss in batch 202: 0.0590668/0.166931loss in batch 203: 0.139099/0.166794loss in batch 204: 0.0335693/0.166138loss in batch 205: 0.161179/0.166122loss in batch 206: 0.0122528/0.165375loss in batch 207: 0.33194/0.166183loss in batch 208: 0.00794983/0.165421loss in batch 209: 0.115158/0.165192loss in batch 210: 0.00802612/0.164444loss in batch 211: 0.0422516/0.163864loss in batch 212: 0.0182343/0.163177
done with epoch 14
train_acc: 0.950704 (405/426)
test loss: 0.0182343
acc: 0.951049 (136/143)
loss in batch 0: 0.0522156/0.0522156loss in batch 1: 0.0275421/0.0398712loss in batch 2: 0.010498/0.0300903loss in batch 3: 0.0356598/0.0314789loss in batch 4: 0.0212555/0.0294342loss in batch 5: 0.189636/0.0561371loss in batch 6: 0.168213/0.0721436loss in batch 7: 0.161392/0.083313loss in batch 8: 0.0405426/0.0785522loss in batch 9: 0.0209808/0.0727844loss in batch 10: 0.0217438/0.068161loss in batch 11: 0.0642242/0.0678253loss in batch 12: 0.0835266/0.0690308loss in batch 13: 0.0198212/0.0655212loss in batch 14: 0.24437/0.0774384loss in batch 15: 0.409988/0.0982361loss in batch 16: 0.518234/0.122925loss in batch 17: 0.131195/0.123398loss in batch 18: 0.504959/0.143478loss in batch 19: 0.071228/0.139862loss in batch 20: 0.0277863/0.134521loss in batch 21: 0.244614/0.139526loss in batch 22: 0.0187531/0.134277loss in batch 23: 0.58287/0.152969loss in batch 24: 0.0263519/0.147903loss in batch 25: 0.0361176/0.1436loss in batch 26: 0.0790405/0.14122loss in batch 27: 0.0961761/0.139603loss in batch 28: 0.0390778/0.136139loss in batch 29: 0.283569/0.141052loss in batch 30: 0.166306/0.141861loss in batch 31: 0.0901947/0.140244loss in batch 32: 0.000396729/0.136017loss in batch 33: 0.369873/0.142899loss in batch 34: 0.0612946/0.140564loss in batch 35: 0.0293579/0.137466loss in batch 36: 0.114609/0.136856loss in batch 37: 0.543457/0.147568loss in batch 38: 0.327454/0.152161loss in batch 39: 0.386353/0.158035loss in batch 40: 0.386734/0.163605loss in batch 41: 0.0578613/0.161087loss in batch 42: 0.0731354/0.159042loss in batch 43: 0.0310516/0.156128loss in batch 44: 0.0908051/0.154663loss in batch 45: 0.0886993/0.153244loss in batch 46: 0.0548248/0.151154loss in batch 47: 0.0477905/0.148987loss in batch 48: 0.0312042/0.146591loss in batch 49: 0.38945/0.151443loss in batch 50: 0.030014/0.149063loss in batch 51: 0.33168/0.152573loss in batch 52: 0.217377/0.153793loss in batch 53: 0.464798/0.159561loss in batch 54: 0.0246124/0.157104loss in batch 55: 0.518539/0.163574loss in batch 56: 0.176498/0.163788loss in batch 57: 0.0295563/0.161484loss in batch 58: 0.0510254/0.159592loss in batch 59: 0.0336151/0.157516loss in batch 60: 0.808594/0.168182loss in batch 61: 0.028183/0.165909loss in batch 62: 1.47633/0.186722loss in batch 63: 0.0377197/0.184387loss in batch 64: 0.0924988/0.182983loss in batch 65: 0.1763/0.182877loss in batch 66: 0.0116577/0.180328loss in batch 67: 0.449219/0.18428loss in batch 68: 0.295715/0.185883loss in batch 69: 0.0824127/0.184418loss in batch 70: 0.0727386/0.182846loss in batch 71: 0.00996399/0.180435loss in batch 72: 0.0725098/0.178955loss in batch 73: 0.207764/0.179352loss in batch 74: 0.16066/0.179108loss in batch 75: 0.158127/0.178818loss in batch 76: 0.101364/0.177826loss in batch 77: 0.237839/0.178589loss in batch 78: 0.200455/0.178864loss in batch 79: 0.0209503/0.176895loss in batch 80: 0.0343628/0.175125loss in batch 81: 0.0888672/0.174072loss in batch 82: 0.0955658/0.173126loss in batch 83: 0.960541/0.18251loss in batch 84: 0.0205841/0.180603loss in batch 85: 0.0278778/0.178833loss in batch 86: 0.0241089/0.177048loss in batch 87: 0.125977/0.176468loss in batch 88: 0.0339661/0.174866loss in batch 89: 0.0142975/0.17308loss in batch 90: 0.0153961/0.171356loss in batch 91: 0.147278/0.171082loss in batch 92: 0.0318451/0.169586loss in batch 93: 0.325333/0.171249loss in batch 94: 0.242447/0.171997loss in batch 95: 0.034729/0.170563loss in batch 96: 0.127823/0.17012loss in batch 97: 0.0933533/0.169342loss in batch 98: 0.00874329/0.167709loss in batch 99: 0.0900726/0.166946loss in batch 100: 0.0267792/0.165558loss in batch 101: 0.402054/0.167877loss in batch 102: 0.124588/0.167465loss in batch 103: 0.0496063/0.166321loss in batch 104: 0.023941/0.164963loss in batch 105: 0.564316/0.168732loss in batch 106: 0.126007/0.168335loss in batch 107: 0.058075/0.167313loss in batch 108: 0.3797/0.169266loss in batch 109: 0.338776/0.170807loss in batch 110: 0.124863/0.170395loss in batch 111: 0.0659332/0.169464loss in batch 112: 0.0308075/0.168228loss in batch 113: 0.052063/0.167206loss in batch 114: 0.0973816/0.166611loss in batch 115: 0.187958/0.166794loss in batch 116: 0.0297241/0.165619loss in batch 117: 0.189865/0.165833loss in batch 118: 0.0606537/0.164932loss in batch 119: 0.0606384/0.164078loss in batch 120: 0.0357819/0.16301loss in batch 121: 0.149323/0.162903loss in batch 122: 0.094162/0.162338loss in batch 123: 0.0705566/0.161591loss in batch 124: 0.0315399/0.160553loss in batch 125: 0.0415344/0.159622loss in batch 126: 0.193527/0.159882loss in batch 127: 0.218384/0.160324loss in batch 128: 0.168503/0.1604loss in batch 129: 0.346176/0.161835loss in batch 130: 0.0027771/0.160614loss in batch 131: 0.0180359/0.159531loss in batch 132: 0.0864258/0.158981loss in batch 133: 0.069931/0.158325loss in batch 134: 0.0381317/0.15744loss in batch 135: 0.243103/0.158066loss in batch 136: 0.28746/0.158997loss in batch 137: 0.205063/0.159348loss in batch 138: 0.325348/0.160538loss in batch 139: 0.189606/0.160751loss in batch 140: 0.0680542/0.16008loss in batch 141: 0.116394/0.159775loss in batch 142: 0.0558167/0.159058loss in batch 143: 0.998871/0.164886loss in batch 144: 0.0737152/0.164246loss in batch 145: 0.0169373/0.163239loss in batch 146: 0.0643311/0.162567loss in batch 147: 0.0631256/0.161896loss in batch 148: 0.0683746/0.16127loss in batch 149: 0.384354/0.16275loss in batch 150: 0.0306091/0.16188loss in batch 151: 0.0531464/0.161163loss in batch 152: 0.172974/0.16124loss in batch 153: 0.00842285/0.160248loss in batch 154: 0.014679/0.159317loss in batch 155: 0.0498505/0.158615loss in batch 156: 0.187332/0.158798loss in batch 157: 0.608276/0.161636loss in batch 158: 0.00665283/0.16066loss in batch 159: 0.11554/0.16037loss in batch 160: 0.645325/0.163391loss in batch 161: 0.0263519/0.162537loss in batch 162: 0.567215/0.165039loss in batch 163: 0.0448456/0.164291loss in batch 164: 1.30859/0.171234loss in batch 165: 0.428848/0.172791loss in batch 166: 0.133026/0.172546loss in batch 167: 0.471115/0.174332loss in batch 168: 0.0473022/0.173569loss in batch 169: 0.477417/0.175354loss in batch 170: 0.0697021/0.174744loss in batch 171: 0.144684/0.174576loss in batch 172: 0.0225677/0.173691loss in batch 173: 0.0814819/0.173157loss in batch 174: 0.0533447/0.17247loss in batch 175: 0.0368347/0.171707loss in batch 176: 0.0400238/0.170959loss in batch 177: 0.0944977/0.170517loss in batch 178: 0.0910187/0.17009loss in batch 179: 0.068161/0.169525loss in batch 180: 0.00700378/0.16861loss in batch 181: 0.100357/0.168243loss in batch 182: 0.0145874/0.167404loss in batch 183: 0.503021/0.16922loss in batch 184: 0.0373688/0.168518loss in batch 185: 0.102234/0.168167loss in batch 186: 0.157776/0.168091loss in batch 187: 0.0128174/0.167282loss in batch 188: 0.0325775/0.166565loss in batch 189: 0.193146/0.166702loss in batch 190: 0.097641/0.166351loss in batch 191: 0.0178223/0.165573loss in batch 192: 0.0605316/0.165024loss in batch 193: 0.0513306/0.164444loss in batch 194: 0.0526581/0.163879loss in batch 195: 0.0179291/0.163132loss in batch 196: 0.312164/0.163879loss in batch 197: 0.309372/0.164612loss in batch 198: 0.13208/0.164459loss in batch 199: 0.227036/0.164764loss in batch 200: 0.0672913/0.164291loss in batch 201: 0.0458527/0.163681loss in batch 202: 0.0182495/0.162979loss in batch 203: 0.0666809/0.162491loss in batch 204: 0.0884399/0.16214loss in batch 205: 0.0366669/0.161545loss in batch 206: 0.403076/0.162704loss in batch 207: 0.0142975/0.161987loss in batch 208: 0.219299/0.162262loss in batch 209: 0.180374/0.162354loss in batch 210: 0.303101/0.163025loss in batch 211: 0.00750732/0.162292loss in batch 212: 0.0394135/0.161697
done with epoch 15
train_acc: 0.948357 (404/426)
test loss: 0.0394135
acc: 0.944056 (135/143)
loss in batch 0: 0.182953/0.182953loss in batch 1: 0.0473938/0.115173loss in batch 2: 0.0858459/0.105392loss in batch 3: 0.0396423/0.0889587loss in batch 4: 0.0457153/0.080307loss in batch 5: 0.349442/0.125168loss in batch 6: 0.0419617/0.113281loss in batch 7: 0.465866/0.157349loss in batch 8: 0.0322723/0.143448loss in batch 9: 0.0175934/0.130875loss in batch 10: 0.314941/0.147598loss in batch 11: 0.0511627/0.139572loss in batch 12: 0.119522/0.138016loss in batch 13: 0.0261688/0.130035loss in batch 14: 0.0162659/0.122452loss in batch 15: 0.0776367/0.119644loss in batch 16: 0.0360718/0.114731loss in batch 17: 0.111725/0.114563loss in batch 18: 0.100403/0.113831loss in batch 19: 0.497574/0.133011loss in batch 20: 0.00663757/0.126984loss in batch 21: 0.31514/0.135544loss in batch 22: 0.0439911/0.131561loss in batch 23: 0.0145111/0.126678loss in batch 24: 0.0118713/0.122086loss in batch 25: 0.0267944/0.118423loss in batch 26: 0.057724/0.11618loss in batch 27: 0.277115/0.121933loss in batch 28: 0.071228/0.120178loss in batch 29: 0.27034/0.125183loss in batch 30: 0.269867/0.129852loss in batch 31: 0.340454/0.136429loss in batch 32: 0.0607605/0.13414loss in batch 33: 0.215042/0.136505loss in batch 34: 0.028656/0.133438loss in batch 35: 0.0818176/0.132004loss in batch 36: 0.00746155/0.128632loss in batch 37: 0.374893/0.135117loss in batch 38: 0.0624695/0.133255loss in batch 39: 0.048996/0.131149loss in batch 40: 0.00326538/0.128021loss in batch 41: 0.0981445/0.127319loss in batch 42: 0.0299683/0.125061loss in batch 43: 0.526642/0.134186loss in batch 44: 0.0623474/0.132584loss in batch 45: 0.208481/0.134232loss in batch 46: 0.074295/0.13295loss in batch 47: 0.353134/0.137543loss in batch 48: 0.0027771/0.134796loss in batch 49: 0.409058/0.140274loss in batch 50: 0.325211/0.143906loss in batch 51: 0.0357971/0.14183loss in batch 52: 0.229904/0.143494loss in batch 53: 0.0348511/0.141479loss in batch 54: 0.0136108/0.139145loss in batch 55: 0.00505066/0.136749loss in batch 56: 0.0769501/0.135712loss in batch 57: 0.0523071/0.134262loss in batch 58: 0.352676/0.13797loss in batch 59: 0.0396729/0.136337loss in batch 60: 1.15349/0.153loss in batch 61: 0.100418/0.152161loss in batch 62: 0.0487823/0.150513loss in batch 63: 0.0212708/0.148499loss in batch 64: 0.0045166/0.146286loss in batch 65: 0.023468/0.144424loss in batch 66: 0.0688477/0.143295loss in batch 67: 0.292084/0.145493loss in batch 68: 0.24205/0.146881loss in batch 69: 0.633987/0.153839loss in batch 70: 0.079422/0.152786loss in batch 71: 0.0940552/0.151978loss in batch 72: 0.13739/0.151779loss in batch 73: 0.697647/0.159149loss in batch 74: 1.20206/0.173065loss in batch 75: 0.0838165/0.17189loss in batch 76: 0.378464/0.174576loss in batch 77: 0.423508/0.177765loss in batch 78: 0.109741/0.176895loss in batch 79: 0.216751/0.177399loss in batch 80: 0.00192261/0.175232loss in batch 81: 0.0937805/0.17424loss in batch 82: 0.064209/0.172913loss in batch 83: 0.154114/0.172699loss in batch 84: 0.0410004/0.171143loss in batch 85: 0.156097/0.170975loss in batch 86: 0.0136719/0.169159loss in batch 87: 0.0388184/0.167679loss in batch 88: 0.431244/0.170624loss in batch 89: 0.11055/0.169968loss in batch 90: 0.0652618/0.168823loss in batch 91: 0.0498199/0.167526loss in batch 92: 0.0571899/0.166351loss in batch 93: 0.0787354/0.165405loss in batch 94: 0.629944/0.170303loss in batch 95: 0.0303192/0.168839loss in batch 96: 0.0689697/0.167801loss in batch 97: 0.0179749/0.166275loss in batch 98: 1.60039/0.180771loss in batch 99: 0.0687256/0.179642loss in batch 100: 0.0410919/0.178284loss in batch 101: 0.0157776/0.176682loss in batch 102: 0.0183105/0.17514loss in batch 103: 0.0331421/0.173782loss in batch 104: 0.448318/0.176392loss in batch 105: 0.057663/0.175278loss in batch 106: 0.144028/0.174973loss in batch 107: 0.0483856/0.173798loss in batch 108: 0.0417023/0.172607loss in batch 109: 0.0178375/0.171188loss in batch 110: 0.0970001/0.170517loss in batch 111: 0.0538788/0.169479loss in batch 112: 0.0357666/0.168289loss in batch 113: 0.196121/0.168549loss in batch 114: 0.00332642/0.167099loss in batch 115: 0.0409241/0.166016loss in batch 116: 0.0739441/0.165237loss in batch 117: 0.0283051/0.164078loss in batch 118: 0.118103/0.163681loss in batch 119: 0.243057/0.164352loss in batch 120: 0.223022/0.164825loss in batch 121: 0.0689697/0.164047loss in batch 122: 0.160461/0.164001loss in batch 123: 0.82486/0.169342loss in batch 124: 0.088501/0.168701loss in batch 125: 0.475708/0.171143loss in batch 126: 0.0386047/0.17009loss in batch 127: 0.0298462/0.169006loss in batch 128: 0.162888/0.168945loss in batch 129: 0.258942/0.169647loss in batch 130: 0.0180206/0.168488loss in batch 131: 0.0229645/0.167389loss in batch 132: 0.0969238/0.166855loss in batch 133: 0.323776/0.16803loss in batch 134: 0.059845/0.167221loss in batch 135: 0.0195007/0.166138loss in batch 136: 0.295532/0.167068loss in batch 137: 0.624985/0.170395loss in batch 138: 0.0999603/0.169876loss in batch 139: 0.153152/0.169769loss in batch 140: 0.0503845/0.16893loss in batch 141: 0.0394745/0.167999loss in batch 142: 0.0258636/0.167023loss in batch 143: 0.0641479/0.166306loss in batch 144: 0.34697/0.167557loss in batch 145: 0.0325928/0.166626loss in batch 146: 0.155792/0.16655loss in batch 147: 0.024826/0.165588loss in batch 148: 0.412231/0.167252loss in batch 149: 0.0228577/0.16629loss in batch 150: 0.0244446/0.165344loss in batch 151: 0.0563507/0.164627loss in batch 152: 0.16243/0.164627loss in batch 153: 0.376617/0.165985loss in batch 154: 0.0365448/0.165161loss in batch 155: 0.21524/0.165482loss in batch 156: 0.0299377/0.164612loss in batch 157: 0.0871124/0.164124loss in batch 158: 0.0632629/0.163498loss in batch 159: 0.116913/0.163208loss in batch 160: 0.0357819/0.162399loss in batch 161: 0.343201/0.163513loss in batch 162: 0.0857697/0.163055loss in batch 163: 0.0162048/0.16214loss in batch 164: 0.0616913/0.161545loss in batch 165: 0.14444/0.161438loss in batch 166: 0.0290222/0.16066loss in batch 167: 0.0553589/0.160019loss in batch 168: 0.154587/0.159988loss in batch 169: 0.0586853/0.159393loss in batch 170: 0.181549/0.159515loss in batch 171: 0.149826/0.15947loss in batch 172: 0.216141/0.15979loss in batch 173: 0.141861/0.159683loss in batch 174: 0.10762/0.159393loss in batch 175: 0.171799/0.159454loss in batch 176: 0.00924683/0.158615loss in batch 177: 0.247009/0.159103loss in batch 178: 0.065567/0.158585loss in batch 179: 0.026413/0.157867loss in batch 180: 0.0159302/0.157059loss in batch 181: 1.07982/0.16214loss in batch 182: 0.175278/0.162216loss in batch 183: 0.0619354/0.161667loss in batch 184: 0.346207/0.162659loss in batch 185: 0.0405579/0.162003loss in batch 186: 0.0312653/0.161316loss in batch 187: 0.256729/0.161819loss in batch 188: 0.443832/0.1633loss in batch 189: 0.00370789/0.162476loss in batch 190: 0.0340729/0.161804loss in batch 191: 0.409637/0.163086loss in batch 192: 0.00744629/0.162277loss in batch 193: 0.420792/0.16362loss in batch 194: 0.0258789/0.162918loss in batch 195: 0.00808716/0.162125loss in batch 196: 0.0258331/0.161423loss in batch 197: 0.0620728/0.160919loss in batch 198: 0.0518646/0.16037loss in batch 199: 0.035553/0.15976loss in batch 200: 0.0661011/0.159286loss in batch 201: 0.0383759/0.158691loss in batch 202: 0.0359039/0.158081loss in batch 203: 0.0222626/0.15741loss in batch 204: 0.233505/0.157776loss in batch 205: 0.0201111/0.157104loss in batch 206: 0.251328/0.157578loss in batch 207: 0.120087/0.157394loss in batch 208: 0.0785828/0.157013loss in batch 209: 0.379669/0.158081loss in batch 210: 0.00631714/0.157364loss in batch 211: 0.290253/0.157974loss in batch 212: 0.0523682/0.157486
done with epoch 16
train_acc: 0.953052 (406/426)
test loss: 0.0523682
acc: 0.944056 (135/143)
loss in batch 0: 0.131241/0.131241loss in batch 1: 0.253525/0.192383loss in batch 2: 0.0373688/0.140717loss in batch 3: 0.408661/0.207703loss in batch 4: 0.0310974/0.172379loss in batch 5: 0.296768/0.193115loss in batch 6: 0.0447388/0.171906loss in batch 7: 0.0929413/0.162033loss in batch 8: 0.380005/0.186249loss in batch 9: 0.104706/0.178101loss in batch 10: 0.0450592/0.166loss in batch 11: 0.258499/0.173721loss in batch 12: 0.418839/0.192581loss in batch 13: 0.138245/0.18869loss in batch 14: 0.0476685/0.179291loss in batch 15: 0.0513763/0.171295loss in batch 16: 0.571426/0.194824loss in batch 17: 0.0513/0.186859loss in batch 18: 0.368149/0.196396loss in batch 19: 0.160675/0.194611loss in batch 20: 0.0164337/0.186127loss in batch 21: 0.0386963/0.179428loss in batch 22: 0.0203247/0.172516loss in batch 23: 0.0131683/0.165878loss in batch 24: 0.395767/0.175064loss in batch 25: 0.107529/0.17247loss in batch 26: 0.0547028/0.168106loss in batch 27: 0.0297241/0.163177loss in batch 28: 0.583878/0.177673loss in batch 29: 0.00817871/0.172028loss in batch 30: 0.0260315/0.167313loss in batch 31: 0.0114288/0.162445loss in batch 32: 0.0549011/0.15918loss in batch 33: 0.542892/0.170471loss in batch 34: 0.0940094/0.168289loss in batch 35: 0.0251617/0.164322loss in batch 36: 0.0948639/0.16243loss in batch 37: 0.0538788/0.159576loss in batch 38: 0.118195/0.158524loss in batch 39: 0.00804138/0.154739loss in batch 40: 0.00421143/0.151093loss in batch 41: 0.50148/0.159424loss in batch 42: 0.00450134/0.155823loss in batch 43: 0.0357666/0.153091loss in batch 44: 0.0314178/0.150375loss in batch 45: 0.0420227/0.148041loss in batch 46: 0.445221/0.154358loss in batch 47: 0.0131378/0.151413loss in batch 48: 0.0857391/0.15007loss in batch 49: 1.15831/0.170242loss in batch 50: 0.63678/0.179382loss in batch 51: 0.135071/0.178543loss in batch 52: 0.00646973/0.175293loss in batch 53: 0.119568/0.174255loss in batch 54: 0.0589447/0.17215loss in batch 55: 0.02565/0.16954loss in batch 56: 0.262054/0.171158loss in batch 57: 0.190918/0.171509loss in batch 58: 0.00384521/0.168671loss in batch 59: 0.112625/0.16774loss in batch 60: 0.122223/0.166992loss in batch 61: 0.165649/0.166962loss in batch 62: 0.110672/0.166077loss in batch 63: 0.0570374/0.164368loss in batch 64: 0.220154/0.165222loss in batch 65: 0.0445862/0.163391loss in batch 66: 0.33847/0.166016loss in batch 67: 1.08585/0.179535loss in batch 68: 0.192612/0.179733loss in batch 69: 0.011734/0.177322loss in batch 70: 0.062088/0.175705loss in batch 71: 0.0532074/0.174011loss in batch 72: 0.0364532/0.172104loss in batch 73: 0.0743713/0.170792loss in batch 74: 0.102722/0.169876loss in batch 75: 0.0774841/0.168671loss in batch 76: 0.4505/0.172333loss in batch 77: 0.00331116/0.170166loss in batch 78: 0.100571/0.169296loss in batch 79: 0.0496368/0.167801loss in batch 80: 0.0847168/0.166763loss in batch 81: 0.206558/0.167252loss in batch 82: 0.725357/0.173981loss in batch 83: 0.0262451/0.172226loss in batch 84: 0.0319366/0.170563loss in batch 85: 0.0575867/0.16925loss in batch 86: 0.00169373/0.167328loss in batch 87: 0.0219421/0.16568loss in batch 88: 0.676407/0.171402loss in batch 89: 0.0806274/0.17041loss in batch 90: 0.159424/0.170288loss in batch 91: 0.0146637/0.168579loss in batch 92: 0.255386/0.169525loss in batch 93: 0.0281525/0.168015loss in batch 94: 0.446625/0.170959loss in batch 95: 0.335236/0.172668loss in batch 96: 0.00669861/0.170944loss in batch 97: 0.00712585/0.169281loss in batch 98: 0.0193634/0.167755loss in batch 99: 0.0926819/0.167023loss in batch 100: 0.0715179/0.166077loss in batch 101: 0.112411/0.165543loss in batch 102: 0.0457916/0.164383loss in batch 103: 0.0437164/0.163223loss in batch 104: 0.0383301/0.162033loss in batch 105: 0.0428772/0.160919loss in batch 106: 0.166321/0.16095loss in batch 107: 0.0544434/0.159973loss in batch 108: 0.0159912/0.158646loss in batch 109: 0.0388794/0.157562loss in batch 110: 0.104584/0.157089loss in batch 111: 0.0176849/0.155838loss in batch 112: 0.0459137/0.154877loss in batch 113: 0.341385/0.156509loss in batch 114: 0.321472/0.157944loss in batch 115: 0.0131989/0.156693loss in batch 116: 0.00453186/0.15538loss in batch 117: 0.281235/0.156448loss in batch 118: 0.25058/0.157257loss in batch 119: 0.08284/0.156631loss in batch 120: 0.329926/0.158066loss in batch 121: 0.193024/0.15834loss in batch 122: 0.128006/0.158096loss in batch 123: 0.143799/0.15799loss in batch 124: 0.301712/0.159134loss in batch 125: 0.190521/0.159393loss in batch 126: 0.0065155/0.158188loss in batch 127: 0.0666199/0.157455loss in batch 128: 0.45694/0.15979loss in batch 129: 0.384644/0.161514loss in batch 130: 0.0871887/0.16095loss in batch 131: 0.291565/0.161926loss in batch 132: 0.0690308/0.16124loss in batch 133: 0.10173/0.160797loss in batch 134: 0.153183/0.160736loss in batch 135: 0.136826/0.160553loss in batch 136: 0.130356/0.160339loss in batch 137: 0.364105/0.161819loss in batch 138: 0.0227203/0.160828loss in batch 139: 0.0676727/0.160156loss in batch 140: 0.17868/0.160278loss in batch 141: 0.306488/0.161316loss in batch 142: 0.268387/0.162064loss in batch 143: 0.00924683/0.161011loss in batch 144: 0.0527649/0.160263loss in batch 145: 0.0265961/0.159332loss in batch 146: 0.047821/0.158585loss in batch 147: 0.0284576/0.1577loss in batch 148: 0.132507/0.157532loss in batch 149: 0.0500641/0.156815loss in batch 150: 0.65538/0.16011loss in batch 151: 0.224426/0.160538loss in batch 152: 0.00944519/0.159546loss in batch 153: 0.649307/0.162735loss in batch 154: 0.0568542/0.162048loss in batch 155: 0.0141907/0.161102loss in batch 156: 0.0210266/0.160217loss in batch 157: 0.0414886/0.15947loss in batch 158: 0.0302887/0.158646loss in batch 159: 0.0339813/0.157867loss in batch 160: 0.138/0.15773loss in batch 161: 0.13121/0.157578loss in batch 162: 0.0351257/0.15683loss in batch 163: 0.0834198/0.156387loss in batch 164: 0.00900269/0.155487loss in batch 165: 0.128555/0.155319loss in batch 166: 0.158401/0.155334loss in batch 167: 0.137894/0.155228loss in batch 168: 0.0618896/0.154694loss in batch 169: 0.0497284/0.154068loss in batch 170: 0.0714264/0.153595loss in batch 171: 0.0279083/0.152863loss in batch 172: 0.195999/0.153107loss in batch 173: 0.0723724/0.152634loss in batch 174: 0.0350189/0.151962loss in batch 175: 0.0492554/0.151398loss in batch 176: 0.0384827/0.150757loss in batch 177: 0.111191/0.150528loss in batch 178: 0.0311737/0.149857loss in batch 179: 0.0329437/0.1492loss in batch 180: 0.163925/0.149292loss in batch 181: 0.0140076/0.148544loss in batch 182: 0.00491333/0.147766loss in batch 183: 0.0595245/0.147293loss in batch 184: 1.61601/0.155228loss in batch 185: 0.785019/0.1586loss in batch 186: 0.0275574/0.157913loss in batch 187: 0.187744/0.158066loss in batch 188: 0.311829/0.15889loss in batch 189: 0.0160065/0.158127loss in batch 190: 0.28595/0.158798loss in batch 191: 0.233688/0.15918loss in batch 192: 0.0123596/0.158417loss in batch 193: 0.0826111/0.158035loss in batch 194: 0.0962982/0.15773loss in batch 195: 0.0794678/0.157318loss in batch 196: 0.0489502/0.156769loss in batch 197: 0.0818634/0.156403loss in batch 198: 0.0495453/0.155853loss in batch 199: 0.0554657/0.15535loss in batch 200: 0.0658569/0.154907loss in batch 201: 0.451706/0.156387loss in batch 202: 0.0548553/0.155869loss in batch 203: 0.0739136/0.155472loss in batch 204: 0.231262/0.155838loss in batch 205: 0.127029/0.155701loss in batch 206: 0.131836/0.155579loss in batch 207: 0.150085/0.155548loss in batch 208: 0.195847/0.155762loss in batch 209: 0.0384674/0.155197loss in batch 210: 0.368805/0.156219loss in batch 211: 0.0113983/0.155518loss in batch 212: 0.277847/0.156113
done with epoch 17
train_acc: 0.955399 (407/426)
test loss: 0.277847
acc: 0.951049 (136/143)
loss in batch 0: 0.21875/0.21875loss in batch 1: 0.290558/0.254654loss in batch 2: 0.320877/0.276718loss in batch 3: 0.104645/0.233704loss in batch 4: 0.00326538/0.187622loss in batch 5: 1.08842/0.337753loss in batch 6: 0.0415802/0.295456loss in batch 7: 0.0341949/0.262787loss in batch 8: 0.173996/0.25293loss in batch 9: 0.230942/0.250732loss in batch 10: 0.0995636/0.236984loss in batch 11: 0.138641/0.22879loss in batch 12: 0.0599976/0.215805loss in batch 13: 0.146759/0.210861loss in batch 14: 0.00285339/0.197006loss in batch 15: 0.194199/0.196823loss in batch 16: 0.102112/0.191254loss in batch 17: 0.046051/0.183197loss in batch 18: 0.0631714/0.17688loss in batch 19: 0.352432/0.185654loss in batch 20: 0.0340118/0.178436loss in batch 21: 0.0271149/0.171539loss in batch 22: 0.189072/0.172318loss in batch 23: 0.0546265/0.167419loss in batch 24: 0.0264435/0.161774loss in batch 25: 0.0515137/0.157532loss in batch 26: 0.0558777/0.153763loss in batch 27: 0.0722351/0.150848loss in batch 28: 0.062973/0.147827loss in batch 29: 0.0137329/0.143356loss in batch 30: 0.0720978/0.141052loss in batch 31: 0.0126648/0.137039loss in batch 32: 0.077301/0.135223loss in batch 33: 0.141846/0.135422loss in batch 34: 0.00134277/0.131592loss in batch 35: 0.0030365/0.128021loss in batch 36: 0.0355377/0.125519loss in batch 37: 0.00550842/0.122375loss in batch 38: 0.323624/0.127533loss in batch 39: 0.532318/0.13765loss in batch 40: 0.22908/0.139877loss in batch 41: 0.390915/0.145844loss in batch 42: 0.0231628/0.143005loss in batch 43: 0.164566/0.143494loss in batch 44: 0.350067/0.148071loss in batch 45: 0.0876007/0.146774loss in batch 46: 0.11351/0.146072loss in batch 47: 0.0236816/0.143509loss in batch 48: 0.119736/0.143021loss in batch 49: 0.0882263/0.141922loss in batch 50: 0.100632/0.141113loss in batch 51: 0.575439/0.14946loss in batch 52: 0.0627899/0.147827loss in batch 53: 0.072998/0.146454loss in batch 54: 0.245621/0.148254loss in batch 55: 0.106445/0.147507loss in batch 56: 0.0793152/0.146301loss in batch 57: 0.058197/0.144791loss in batch 58: 0.263885/0.14682loss in batch 59: 0.393509/0.150909loss in batch 60: 0.338348/0.153992loss in batch 61: 0.0473633/0.152267loss in batch 62: 0.0703125/0.15097loss in batch 63: 0.275513/0.152924loss in batch 64: 0.0215759/0.150894loss in batch 65: 0.037796/0.149185loss in batch 66: 0.204544/0.150009loss in batch 67: 0.303604/0.152267loss in batch 68: 0.0697784/0.151062loss in batch 69: 0.00436401/0.148972loss in batch 70: 0.0630188/0.147766loss in batch 71: 0.797424/0.156799loss in batch 72: 0.302338/0.158783loss in batch 73: 0.130478/0.158401loss in batch 74: 0.0313873/0.156708loss in batch 75: 0.0604706/0.155441loss in batch 76: 0.0608368/0.154205loss in batch 77: 0.0537109/0.152924loss in batch 78: 0.149124/0.152878loss in batch 79: 0.516815/0.15744loss in batch 80: 0.0345917/0.155914loss in batch 81: 0.00576782/0.154083loss in batch 82: 0.0220184/0.152481loss in batch 83: 0.0577087/0.151352loss in batch 84: 0.00897217/0.149689loss in batch 85: 0.0207062/0.148193loss in batch 86: 0.0157471/0.146667loss in batch 87: 0.0614471/0.145691loss in batch 88: 0.186401/0.146149loss in batch 89: 0.0555267/0.145142loss in batch 90: 0.00289917/0.14357loss in batch 91: 0.0329742/0.14238loss in batch 92: 0.105576/0.141983loss in batch 93: 0.0410767/0.140915loss in batch 94: 0.131592/0.140808loss in batch 95: 0.0790558/0.140167loss in batch 96: 0.022522/0.138947loss in batch 97: 0.0373993/0.137924loss in batch 98: 0.0525513/0.137054loss in batch 99: 0.0445404/0.136139loss in batch 100: 0.178986/0.136551loss in batch 101: 0.0760345/0.135956loss in batch 102: 0.056015/0.135193loss in batch 103: 0.0918274/0.134766loss in batch 104: 0.00193787/0.133499loss in batch 105: 0.486679/0.136841loss in batch 106: 0.100281/0.13649loss in batch 107: 0.569748/0.140503loss in batch 108: 0.0294342/0.139496loss in batch 109: 0.0749054/0.138901loss in batch 110: 0.0364532/0.13797loss in batch 111: 0.569824/0.14183loss in batch 112: 0.0355682/0.140884loss in batch 113: 0.0339661/0.139954loss in batch 114: 0.100449/0.139603loss in batch 115: 0.0639496/0.138962loss in batch 116: 0.0065918/0.137817loss in batch 117: 0.212418/0.138458loss in batch 118: 0.37294/0.140427loss in batch 119: 0.204468/0.140961loss in batch 120: 0.0153198/0.139923loss in batch 121: 0.323608/0.141434loss in batch 122: 0.051651/0.140701loss in batch 123: 0.0562439/0.140015loss in batch 124: 0.197311/0.140472loss in batch 125: 0.279846/0.141586loss in batch 126: 1.93546/0.155716loss in batch 127: 0.0571289/0.154938loss in batch 128: 0.0578461/0.15419loss in batch 129: 0.32901/0.155518loss in batch 130: 0.0997314/0.155106loss in batch 131: 0.163513/0.155167loss in batch 132: 0.186066/0.155411loss in batch 133: 0.0126953/0.154343loss in batch 134: 0.0793457/0.153778loss in batch 135: 0.0129852/0.15274loss in batch 136: 0.593643/0.15596loss in batch 137: 0.250183/0.156647loss in batch 138: 0.00735474/0.155563loss in batch 139: 1.0849/0.162216loss in batch 140: 0.0874939/0.161682loss in batch 141: 0.207825/0.162003loss in batch 142: 0.0610962/0.161301loss in batch 143: 0.0310822/0.1604loss in batch 144: 0.46553/0.162506loss in batch 145: 0.192444/0.162704loss in batch 146: 0.326843/0.163818loss in batch 147: 0.720078/0.167572loss in batch 148: 0.180511/0.167664loss in batch 149: 0.0968933/0.167206loss in batch 150: 0.0675354/0.166534loss in batch 151: 0.259018/0.167145loss in batch 152: 0.0688934/0.166504loss in batch 153: 0.329849/0.167557loss in batch 154: 0.0262451/0.166656loss in batch 155: 0.022049/0.165726loss in batch 156: 0.366226/0.166992loss in batch 157: 0.232437/0.167419loss in batch 158: 0.0350342/0.16658loss in batch 159: 0.0232544/0.165695loss in batch 160: 0.104752/0.165298loss in batch 161: 0.0131683/0.164368loss in batch 162: 0.0448303/0.163635loss in batch 163: 0.0237274/0.162781loss in batch 164: 0.221481/0.163132loss in batch 165: 0.00337219/0.16217loss in batch 166: 0.0426788/0.161469loss in batch 167: 0.102783/0.161118loss in batch 168: 0.174469/0.161179loss in batch 169: 0.232239/0.161606loss in batch 170: 0.0731506/0.161087loss in batch 171: 0.152374/0.161041loss in batch 172: 0.0217743/0.160233loss in batch 173: 0.0119476/0.159378loss in batch 174: 0.340302/0.160416loss in batch 175: 0.015274/0.159592loss in batch 176: 0.0939636/0.15921loss in batch 177: 0.172104/0.159286loss in batch 178: 0.338928/0.160309loss in batch 179: 0.0552521/0.159714loss in batch 180: 0.027832/0.158981loss in batch 181: 0.210861/0.159271loss in batch 182: 0.127609/0.159088loss in batch 183: 0.236832/0.159515loss in batch 184: 0.0724335/0.159042loss in batch 185: 0.00564575/0.158218loss in batch 186: 0.0321503/0.157562loss in batch 187: 0.0308228/0.156876loss in batch 188: 0.538239/0.158905loss in batch 189: 0.308441/0.159683loss in batch 190: 0.172089/0.159744loss in batch 191: 0.0514221/0.15918loss in batch 192: 0.0762482/0.158752loss in batch 193: 0.0611115/0.158249loss in batch 194: 0.00767517/0.157486loss in batch 195: 0.044754/0.156891loss in batch 196: 0.141525/0.15683loss in batch 197: 0.0832367/0.156464loss in batch 198: 0.0374146/0.155853loss in batch 199: 0.0653839/0.155396loss in batch 200: 0.00788879/0.154678loss in batch 201: 0.0476532/0.154144loss in batch 202: 0.0331879/0.153534loss in batch 203: 0.0481262/0.15303loss in batch 204: 0.0733337/0.152634loss in batch 205: 0.00828552/0.151932loss in batch 206: 0.0396576/0.151398loss in batch 207: 0.473083/0.152939loss in batch 208: 0.561996/0.154892loss in batch 209: 0.0830383/0.154541loss in batch 210: 0.115356/0.154358loss in batch 211: 0.0330963/0.153793loss in batch 212: 0.0454865/0.15329
done with epoch 18
train_acc: 0.955399 (407/426)
test loss: 0.0454865
acc: 0.951049 (136/143)
loss in batch 0: 0.28038/0.28038loss in batch 1: 0.0270691/0.153717loss in batch 2: 0.00646973/0.104645loss in batch 3: 0.0345459/0.0871124loss in batch 4: 1.54883/0.379456loss in batch 5: 0.298157/0.365906loss in batch 6: 0.0443268/0.319977loss in batch 7: 0.0400085/0.284973loss in batch 8: 0.0296173/0.256607loss in batch 9: 0.0038147/0.231323loss in batch 10: 0.226837/0.230911loss in batch 11: 0.193893/0.227829loss in batch 12: 0.0973053/0.217789loss in batch 13: 0.106064/0.209808loss in batch 14: 0.0261841/0.197571loss in batch 15: 0.17598/0.196213loss in batch 16: 0.00900269/0.185196loss in batch 17: 0.0773926/0.179214loss in batch 18: 0.189316/0.179749loss in batch 19: 0.0301819/0.172272loss in batch 20: 0.0787811/0.167816loss in batch 21: 0.00891113/0.160583loss in batch 22: 0.0956726/0.157761loss in batch 23: 0.428284/0.169052loss in batch 24: 0.0594177/0.164658loss in batch 25: 0.107834/0.162476loss in batch 26: 0.566818/0.177444loss in batch 27: 0.0713196/0.17366loss in batch 28: 0.0772552/0.170334loss in batch 29: 0.00920105/0.164963loss in batch 30: 0.453522/0.174271loss in batch 31: 0.0226746/0.169525loss in batch 32: 0.0565491/0.166107loss in batch 33: 0.030899/0.16214loss in batch 34: 0.295303/0.165939loss in batch 35: 0.0641022/0.163116loss in batch 36: 0.0912018/0.161163loss in batch 37: 0.0392303/0.157959loss in batch 38: 0.049881/0.155182loss in batch 39: 0.0157013/0.151688loss in batch 40: 0.206772/0.153046loss in batch 41: 0.0349426/0.150223loss in batch 42: 0.110321/0.149307loss in batch 43: 0.0464325/0.146957loss in batch 44: 0.605148/0.15715loss in batch 45: 0.203995/0.158157loss in batch 46: 0.00494385/0.154907loss in batch 47: 0.432556/0.16069loss in batch 48: 0.51503/0.167923loss in batch 49: 0.0839996/0.166245loss in batch 50: 0.0603027/0.164169loss in batch 51: 0.00584412/0.161133loss in batch 52: 0.119934/0.160339loss in batch 53: 0.0676422/0.158615loss in batch 54: 0.0381012/0.156433loss in batch 55: 0.36972/0.160248loss in batch 56: 0.0188293/0.157761loss in batch 57: 0.0138092/0.155273loss in batch 58: 0.022995/0.15303loss in batch 59: 0.0147552/0.150726loss in batch 60: 0.044342/0.148987loss in batch 61: 0.132584/0.148727loss in batch 62: 0.249512/0.150314loss in batch 63: 0.132141/0.15004loss in batch 64: 0.0907135/0.149124loss in batch 65: 0.112091/0.14856loss in batch 66: 0.450226/0.153076loss in batch 67: 0.0688782/0.151825loss in batch 68: 0.0115051/0.149796loss in batch 69: 0.0473633/0.148331loss in batch 70: 0.0325317/0.146698loss in batch 71: 0.0373077/0.145172loss in batch 72: 0.0276489/0.14357loss in batch 73: 0.0643616/0.142517loss in batch 74: 0.0380249/0.141113loss in batch 75: 0.00486755/0.139313loss in batch 76: 0.0171814/0.137726loss in batch 77: 0.0983734/0.137222loss in batch 78: 0.294556/0.139221loss in batch 79: 0.0349274/0.137909loss in batch 80: 0.0133209/0.136383loss in batch 81: 0.0400085/0.135193loss in batch 82: 0.0893097/0.134659loss in batch 83: 0.205536/0.135483loss in batch 84: 0.0656433/0.134674loss in batch 85: 0.0117035/0.133255loss in batch 86: 0.0223541/0.131958loss in batch 87: 0.0315399/0.130814loss in batch 88: 0.0147095/0.129517loss in batch 89: 0.00389099/0.128128loss in batch 90: 0.230606/0.129242loss in batch 91: 0.0622559/0.128525loss in batch 92: 0.0341339/0.127502loss in batch 93: 0.225266/0.12854loss in batch 94: 0.00485229/0.127243loss in batch 95: 0.177612/0.127762loss in batch 96: 0.286957/0.12941loss in batch 97: 0.0366058/0.128464loss in batch 98: 0.144943/0.128616loss in batch 99: 0.358917/0.130936loss in batch 100: 0.319702/0.132813loss in batch 101: 0.749008/0.13884loss in batch 102: 0.471207/0.142075loss in batch 103: 0.0346985/0.141037loss in batch 104: 0.119614/0.140823loss in batch 105: 0.04245/0.139908loss in batch 106: 0.414276/0.142456loss in batch 107: 0.7397/0.147995loss in batch 108: 0.0154419/0.14679loss in batch 109: 0.0271149/0.145706loss in batch 110: 0.12793/0.145538loss in batch 111: 0.0871277/0.145004loss in batch 112: 0.308197/0.146454loss in batch 113: 0.00474548/0.145203loss in batch 114: 0.0759888/0.144623loss in batch 115: 0.093277/0.144165loss in batch 116: 0.0465851/0.143326loss in batch 117: 0.0451202/0.142517loss in batch 118: 0.0731506/0.141922loss in batch 119: 0.00680542/0.140793loss in batch 120: 0.294617/0.142075loss in batch 121: 0.00138855/0.140915loss in batch 122: 0.00958252/0.139847loss in batch 123: 0.0800781/0.139359loss in batch 124: 0.117371/0.139191loss in batch 125: 0.276215/0.140274loss in batch 126: 0.259949/0.14122loss in batch 127: 0.0489502/0.140503loss in batch 128: 0.330429/0.141983loss in batch 129: 0.031662/0.141113loss in batch 130: 0.0131836/0.140137loss in batch 131: 0.0220795/0.139252loss in batch 132: 0.171326/0.139496loss in batch 133: 1.30547/0.148178loss in batch 134: 0.384567/0.149948loss in batch 135: 0.424683/0.151962loss in batch 136: 0.548248/0.154861loss in batch 137: 0.00392151/0.153763loss in batch 138: 0.46492/0.155991loss in batch 139: 0.0348053/0.155121loss in batch 140: 0.149475/0.15509loss in batch 141: 0.0639954/0.154449loss in batch 142: 0.025116/0.153549loss in batch 143: 0.105209/0.153214loss in batch 144: 0.0834045/0.152725loss in batch 145: 0.282135/0.153625loss in batch 146: 0.0184021/0.152695loss in batch 147: 0.423477/0.154541loss in batch 148: 0.110672/0.154236loss in batch 149: 0.00959778/0.153275loss in batch 150: 0.0236664/0.152405loss in batch 151: 0.500687/0.154709loss in batch 152: 0.0973358/0.154327loss in batch 153: 0.0743561/0.153809loss in batch 154: 0.0818176/0.153336loss in batch 155: 0.0247345/0.152512loss in batch 156: 0.0818024/0.152069loss in batch 157: 0.307068/0.153046loss in batch 158: 0.0377045/0.152313loss in batch 159: 0.0590057/0.151749loss in batch 160: 0.0330811/0.151001loss in batch 161: 0.240463/0.15155loss in batch 162: 0.0767365/0.151108loss in batch 163: 0.0705566/0.150604loss in batch 164: 0.0027771/0.149719loss in batch 165: 0.0875092/0.149338loss in batch 166: 0.355972/0.150574loss in batch 167: 0.0122375/0.14975loss in batch 168: 0.00379944/0.148895loss in batch 169: 0.271103/0.149612loss in batch 170: 0.0251312/0.148865loss in batch 171: 0.120956/0.148727loss in batch 172: 0.0108337/0.147919loss in batch 173: 0.243225/0.148468loss in batch 174: 0.137054/0.148407loss in batch 175: 0.52774/0.150558loss in batch 176: 0.00314331/0.149719loss in batch 177: 0.114258/0.149521loss in batch 178: 0.0139618/0.148758loss in batch 179: 0.824387/0.152512loss in batch 180: 0.0953674/0.152206loss in batch 181: 0.0578156/0.151672loss in batch 182: 0.0643921/0.151199loss in batch 183: 0.0805817/0.150833loss in batch 184: 0.070343/0.150391loss in batch 185: 0.115768/0.150208loss in batch 186: 0.126068/0.15007loss in batch 187: 0.00831604/0.149323loss in batch 188: 0.0298004/0.148682loss in batch 189: 0.0691681/0.14827loss in batch 190: 0.0691986/0.147842loss in batch 191: 0.30188/0.148666loss in batch 192: 0.0484161/0.148132loss in batch 193: 0.0309143/0.147522loss in batch 194: 0.0585938/0.147079loss in batch 195: 0.0724182/0.146698loss in batch 196: 0.261414/0.147278loss in batch 197: 0.0183716/0.146637loss in batch 198: 0.0422974/0.146103loss in batch 199: 0.0514526/0.14563loss in batch 200: 1.12869/0.150528loss in batch 201: 0.106384/0.150299loss in batch 202: 0.02388/0.149673loss in batch 203: 0.252335/0.150192loss in batch 204: 0.234131/0.150589loss in batch 205: 0.0344696/0.150024loss in batch 206: 0.194946/0.150253loss in batch 207: 0.0355988/0.149689loss in batch 208: 0.380188/0.150803loss in batch 209: 0.373978/0.151855loss in batch 210: 0.0258942/0.15126loss in batch 211: 0.053833/0.150803loss in batch 212: 0.110458/0.15062
done with epoch 19
train_acc: 0.950704 (405/426)
test loss: 0.110458
acc: 0.958042 (137/143)
[0.108154, -0.0834503, -0.0812225, -0.140427, -0.0270538, -0.0422668, -0.116348, -0.0209961, -0.0018158, -0.00396729, -0.370575, -0.139282, -0.00767517, -0.393936, -0.481094, 0.0559082, -0.0690918, 0.0083313, 0.0289917, 0.000564575, 0.328125, 0.0544586, -0.220444, -0.0403442, 0.00738525, -0.048172, -0.0127716, 0.0598755, -0.0608063, 0.0135345, -0.0133057, 0.0278015, -0.397156, 0.186279, -0.00196838, 0.205673, -0.127228, 0.0930786, -0.122269, 0.202972, 0.759293, -0.00947571, 0.538834, -0.00604248, -0.442429, 0.000610352, -0.00141907, -0.0851746, -0.0178375, 0.0684509, 0.028717, 0.112518, 0.0469055, -0.0514221, -0.0680847, -0.0220947, -0.0201416, -0.013031, -0.0650635, 0.000534058, 0.300751, 0.135696, -0.0127106, -0.0424042, 0.0140533, -0.0607147, 0, 0.00924683, 0.00550842, -0.011795, -0.455276, 0.0180664, -0.0569763, 0.44017, 0.00323486, -0.0669098, -0.0100555, -0.211563, -0.0185699, -0.00170898, 0.170532, 0.000335693, 0.0195007, -0.0139465, 0.0709534, -0.0546417, -0.0207825, -0.00369263, 0.0380707, 0, -0.0071106, 0.49382, -0.51442, 0.0035553, -0.00686646, -0.0548706, 0.0102081, -0.558975, -0.0446777, -0.0230865, -0.00163269, -0.0828552, -0.0903778, -0.222504, 0.0335999, -0.0114899, 0.00273132, -0.182846, 0.6241, 0.660492, -0.0583801, 0.00495911, 0.0327148, -0.340729, -0.0182953, -0.0465698, -0.376709, -0.0197601, -0.0604248, -0.1436, -0.0379639, -0.0327301, 0.191925, -0.132401, 0.00256348, -0.2052, -0.285995, -0.0323639, -0.0186615, -0.00541687, 0.0873566, -0.00466919, -0.0155334, -0.297638, -0.183838, -0.0120392, -0.288345, 0.49501, 6.10352e-05, -0.0106354, -0.102234, -0.0454712, 0.00050354]
Compiler: ./compile.py breast_logistic
	8484 dabits of replicated gfp left
	2022 dabits of replicated gfp left
2 threads spent a total of 663.356 seconds (400.029 MB, 1367614 rounds) on the online phase, 105.813 seconds (3677.61 MB, 91492 rounds) on the preprocessing/offline phase, and 770.154 seconds idling.
Join timer: 0 769660
Finish timer: 0.00069638
Join timer: 1 758741
Finish timer: 0.00069638
Communication details (rounds in parallel threads counted double):
Passing around 399.747 MB in 1367596 rounds, taking 643.293 seconds
Receiving directly 3677.61 MB in 45746 rounds, taking 39.742 seconds
Sending directly 3677.89 MB in 45764 rounds, taking 2.26368 seconds
CPU time = 117.287 (overall core time)
The following benchmarks are including preprocessing (offline phase).
Time = 769.661 seconds 
Data sent = 4077.64 MB in ~1459106 rounds (party 0 only)
Global data sent = 12232.4 MB (all parties)
Actual cost of program:
  Type int
     229817398           Bits
This program might benefit from some protocol options.
Consider adding the following at the beginning of your code:
	program.use_trunc_pr = True
	program.use_edabit(True)
This virtual machine was compiled with GCC. Recompile with 'CXX = clang++' in 'CONFIG.mine' for optimal performance.
Command line: ./replicated-field-party.x --ip-file-name /HOST -p 0 -v breast_logistic
